# VulnHunter V15 Production Training Job - 300TB+ Dataset
$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
type: command

# Job Details
display_name: VulnHunter-V15-Production-300TB-Training
description: Revolutionary enterprise-grade vulnerability detection training on 300TB+ dataset with mathematical techniques
tags:
  model: VulnHunter-V15
  version: "15.0.0"
  dataset_size: "300TB+"
  training_type: "production-massive-scale"
  mathematical_techniques: "8-advanced"
  platforms: "multi-platform"
  enterprise_grade: "true"

# Compute Configuration
compute: vulnhunter-v15-cpu-cluster

# Environment - using scikit-learn for maximum compatibility
environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu:1

# Code
code: ./

# Training Command with full parameters
command: python vulnhunter_v15_production_training.py --model_name "VulnHunter-V15-Production" --model_version "15.0.0" --max_epochs 500 --batch_size_cpu 256 --learning_rate 1e-4 --max_cpu_cores 4 --memory_limit_gb 16 --mathematical_techniques true --enterprise_integration true --enable_monitoring true --save_checkpoints true

# Environment Variables
environment_variables:
  OMP_NUM_THREADS: "4"
  MKL_NUM_THREADS: "4"
  AZURE_ML_TRAINING: "true"
  VULNHUNTER_VERSION: "15.0.0"
  DATASET_SIZE: "300TB+"
  TRAINING_TYPE: "production"