#!/usr/bin/env python3
"""
Train ML Models for Vulnerability Detection
Multi-model ensemble: Random Forest, XGBoost, Neural Network, SVM
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_recall_curve
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
import joblib
import json
import matplotlib
matplotlib.use('Agg')  # Non-interactive backend
import matplotlib.pyplot as plt
import seaborn as sns

class VulnerabilityMLTrainer:
    """
    Comprehensive ML training for vulnerability detection
    """

    def __init__(self, data_file='ml_training_data.csv'):
        print("="*80)
        print("Vulnerability Detection ML Training")
        print("="*80)

        # Load data
        self.df = pd.read_csv(data_file)
        print(f"\n‚úì Loaded {len(self.df)} samples")

        # Prepare features and labels
        self.prepare_data()

        # Initialize models
        self.models = {}
        self.results = {}

    def prepare_data(self):
        """Prepare features and split data"""

        # Select feature columns (exclude metadata)
        exclude_cols = ['code', 'file_path', 'category', 'severity', 'cwe',
                       'language', 'project', 'is_vulnerable', 'context']

        self.feature_cols = [col for col in self.df.columns if col not in exclude_cols]

        print(f"\nüìä Feature Engineering:")
        print(f"  Total features: {len(self.feature_cols)}")
        print(f"  Sample features: {self.feature_cols[:10]}")

        # Prepare X and y
        self.X = self.df[self.feature_cols].fillna(0)
        self.y = self.df['is_vulnerable']

        # Train/test split
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42, stratify=self.y
        )

        print(f"\nüì¶ Dataset Split:")
        print(f"  Training: {len(self.X_train)} samples")
        print(f"  Testing: {len(self.X_test)} samples")
        print(f"  Vulnerable ratio (train): {self.y_train.mean():.2%}")
        print(f"  Vulnerable ratio (test): {self.y_test.mean():.2%}")

        # Scale features for SVM and Neural Network
        self.scaler = StandardScaler()
        self.X_train_scaled = self.scaler.fit_transform(self.X_train)
        self.X_test_scaled = self.scaler.transform(self.X_test)

    def train_random_forest(self):
        """Train Random Forest Classifier"""
        print(f"\n{'='*80}")
        print("Training Random Forest...")
        print(f"{'='*80}")

        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )

        rf.fit(self.X_train, self.y_train)

        # Predictions
        y_pred = rf.predict(self.X_test)
        y_prob = rf.predict_proba(self.X_test)[:, 1]

        # Metrics
        print(f"\nüìä Random Forest Results:")
        print(classification_report(self.y_test, y_pred))

        # Feature importance
        feature_importance = pd.DataFrame({
            'feature': self.feature_cols,
            'importance': rf.feature_importances_
        }).sort_values('importance', ascending=False)

        print(f"\nüéØ Top 10 Important Features:")
        print(feature_importance.head(10).to_string(index=False))

        self.models['random_forest'] = rf
        self.results['random_forest'] = {
            'y_pred': y_pred,
            'y_prob': y_prob,
            'feature_importance': feature_importance.to_dict('records')
        }

        return rf

    def train_xgboost(self):
        """Train XGBoost Classifier"""
        print(f"\n{'='*80}")
        print("Training XGBoost...")
        print(f"{'='*80}")

        # Calculate scale_pos_weight for imbalanced data
        scale_pos_weight = len(self.y_train[self.y_train==0]) / len(self.y_train[self.y_train==1])

        xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=10,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=scale_pos_weight,
            random_state=42,
            n_jobs=-1
        )

        xgb_model.fit(self.X_train, self.y_train)

        # Predictions
        y_pred = xgb_model.predict(self.X_test)
        y_prob = xgb_model.predict_proba(self.X_test)[:, 1]

        # Metrics
        print(f"\nüìä XGBoost Results:")
        print(classification_report(self.y_test, y_pred))

        self.models['xgboost'] = xgb_model
        self.results['xgboost'] = {
            'y_pred': y_pred,
            'y_prob': y_prob
        }

        return xgb_model

    def train_neural_network(self):
        """Train Neural Network (MLP)"""
        print(f"\n{'='*80}")
        print("Training Neural Network...")
        print(f"{'='*80}")

        mlp = MLPClassifier(
            hidden_layer_sizes=(128, 64, 32),
            activation='relu',
            solver='adam',
            learning_rate='adaptive',
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )

        mlp.fit(self.X_train_scaled, self.y_train)

        # Predictions
        y_pred = mlp.predict(self.X_test_scaled)
        y_prob = mlp.predict_proba(self.X_test_scaled)[:, 1]

        # Metrics
        print(f"\nüìä Neural Network Results:")
        print(classification_report(self.y_test, y_pred))

        self.models['neural_network'] = mlp
        self.results['neural_network'] = {
            'y_pred': y_pred,
            'y_prob': y_prob
        }

        return mlp

    def train_svm(self):
        """Train Support Vector Machine"""
        print(f"\n{'='*80}")
        print("Training SVM...")
        print(f"{'='*80}")

        svm = SVC(
            kernel='rbf',
            C=10,
            gamma='scale',
            probability=True,
            random_state=42,
            class_weight='balanced'
        )

        svm.fit(self.X_train_scaled, self.y_train)

        # Predictions
        y_pred = svm.predict(self.X_test_scaled)
        y_prob = svm.predict_proba(self.X_test_scaled)[:, 1]

        # Metrics
        print(f"\nüìä SVM Results:")
        print(classification_report(self.y_test, y_pred))

        self.models['svm'] = svm
        self.results['svm'] = {
            'y_pred': y_pred,
            'y_prob': y_prob
        }

        return svm

    def create_ensemble(self):
        """Create ensemble predictor"""
        print(f"\n{'='*80}")
        print("Creating Ensemble Model...")
        print(f"{'='*80}")

        # Weighted average of probabilities
        ensemble_prob = (
            0.3 * self.results['random_forest']['y_prob'] +
            0.3 * self.results['xgboost']['y_prob'] +
            0.2 * self.results['neural_network']['y_prob'] +
            0.2 * self.results['svm']['y_prob']
        )

        ensemble_pred = (ensemble_prob >= 0.5).astype(int)

        print(f"\nüìä Ensemble Results:")
        print(classification_report(self.y_test, ensemble_pred))

        self.results['ensemble'] = {
            'y_pred': ensemble_pred,
            'y_prob': ensemble_prob
        }

    def evaluate_models(self):
        """Comprehensive evaluation"""
        print(f"\n{'='*80}")
        print("Model Comparison")
        print(f"{'='*80}")

        comparison = []

        for model_name in ['random_forest', 'xgboost', 'neural_network', 'svm', 'ensemble']:
            y_pred = self.results[model_name]['y_pred']
            y_prob = self.results[model_name]['y_prob']

            # Calculate metrics
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

            comparison.append({
                'Model': model_name.replace('_', ' ').title(),
                'Accuracy': accuracy_score(self.y_test, y_pred),
                'Precision': precision_score(self.y_test, y_pred),
                'Recall': recall_score(self.y_test, y_pred),
                'F1': f1_score(self.y_test, y_pred),
                'ROC-AUC': roc_auc_score(self.y_test, y_prob)
            })

        comp_df = pd.DataFrame(comparison)
        print(f"\n{comp_df.to_string(index=False)}")

        # Find best model
        best_model = comp_df.loc[comp_df['F1'].idxmax(), 'Model']
        print(f"\nüèÜ Best Model: {best_model}")

        return comp_df

    def save_models(self):
        """Save trained models"""
        print(f"\n{'='*80}")
        print("Saving Models...")
        print(f"{'='*80}")

        # Save individual models
        for name, model in self.models.items():
            filename = f'model_{name}.joblib'
            joblib.dump(model, filename)
            print(f"‚úì Saved {filename}")

        # Save scaler
        joblib.dump(self.scaler, 'scaler.joblib')
        print(f"‚úì Saved scaler.joblib")

        # Save feature columns
        with open('model_features.json', 'w') as f:
            json.dump(self.feature_cols, f, indent=2)
        print(f"‚úì Saved model_features.json")

        # Save results summary
        summary = {
            'training_samples': len(self.X_train),
            'test_samples': len(self.X_test),
            'num_features': len(self.feature_cols),
            'models_trained': list(self.models.keys()),
        }

        with open('training_summary.json', 'w') as f:
            json.dump(summary, f, indent=2)
        print(f"‚úì Saved training_summary.json")

    def generate_plots(self):
        """Generate visualization plots"""
        print(f"\n{'='*80}")
        print("Generating Visualizations...")
        print(f"{'='*80}")

        # Feature importance plot
        if 'random_forest' in self.results:
            fi = pd.DataFrame(self.results['random_forest']['feature_importance'])

            plt.figure(figsize=(12, 8))
            plt.barh(fi['feature'].head(15), fi['importance'].head(15))
            plt.xlabel('Importance')
            plt.title('Top 15 Feature Importance (Random Forest)')
            plt.tight_layout()
            plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')
            print(f"‚úì Saved feature_importance.png")
            plt.close()

        # Confusion matrices
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        axes = axes.ravel()

        for idx, (model_name, title) in enumerate([
            ('random_forest', 'Random Forest'),
            ('xgboost', 'XGBoost'),
            ('neural_network', 'Neural Network'),
            ('ensemble', 'Ensemble')
        ]):
            cm = confusion_matrix(self.y_test, self.results[model_name]['y_pred'])
            sns.heatmap(cm, annot=True, fmt='d', ax=axes[idx], cmap='Blues')
            axes[idx].set_title(f'{title} Confusion Matrix')
            axes[idx].set_ylabel('Actual')
            axes[idx].set_xlabel('Predicted')

        plt.tight_layout()
        plt.savefig('confusion_matrices.png', dpi=300, bbox_inches='tight')
        print(f"‚úì Saved confusion_matrices.png")
        plt.close()

def main():
    # Initialize trainer
    trainer = VulnerabilityMLTrainer()

    # Train all models
    trainer.train_random_forest()
    trainer.train_xgboost()
    trainer.train_neural_network()
    trainer.train_svm()
    trainer.create_ensemble()

    # Evaluate
    trainer.evaluate_models()

    # Save
    trainer.save_models()
    trainer.generate_plots()

    print(f"\n{'='*80}")
    print("‚úÖ ML Training Complete!")
    print(f"{'='*80}")
    print(f"\nüì¶ Models saved:")
    print(f"  - model_random_forest.joblib")
    print(f"  - model_xgboost.joblib")
    print(f"  - model_neural_network.joblib")
    print(f"  - model_svm.joblib")
    print(f"  - scaler.joblib")
    print(f"  - model_features.json")
    print(f"\nüìä Visualizations:")
    print(f"  - feature_importance.png")
    print(f"  - confusion_matrices.png")

if __name__ == '__main__':
    main()
