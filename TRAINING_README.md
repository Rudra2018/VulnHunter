# ğŸ“ VulnGuard AI - Comprehensive Training System

## ğŸ¯ Quick Start

### Option 1: Automated Setup (Recommended)
```bash
# 1. Run setup script
./setup_kaggle.sh

# 2. Train with all datasets
python3 train_with_kaggle.py --data-path ./data/kaggle
```

### Option 2: Manual Setup
```bash
# 1. Install Kaggle API
pip3 install kaggle

# 2. Configure credentials
# Get from: https://www.kaggle.com/settings
mkdir -p ~/.kaggle
mv ~/Downloads/kaggle.json ~/.kaggle/
chmod 600 ~/.kaggle/kaggle.json

# 3. Download and train
python3 train_with_kaggle.py --download
```

### Option 3: HuggingFace Only (No Kaggle)
```bash
# Train with only HuggingFace datasets
python3 train_with_kaggle.py --huggingface-only
```

---

## ğŸ“¦ Dataset Overview

### Kaggle Datasets (Total: 5)

| Dataset | Samples | Type | URL |
|---------|---------|------|-----|
| Public CVE 2020-2024 | ~15K | CVE | [Link](https://www.kaggle.com/datasets/umer7arooq/public-cve-vulnerabilities-20202024) |
| CVE Data | ~12K | CVE | [Link](https://www.kaggle.com/datasets/angelcortez/cve-data) |
| Bug Bounty Writeups | ~9K | Bug Bounty | [Link](https://www.kaggle.com/datasets/mayankkumarpoddar/bug-bounty-writeups) |
| CVE Dataset | ~10K | CVE | [Link](https://www.kaggle.com/datasets/casimireffect/cve-dataset) |
| Bug Bounty OpenAI | ~6K | Bug Bounty | [Link](https://www.kaggle.com/datasets/daudthecat/bug-bounty-openai-gpt-oss-20b-by-thecat) |

### HuggingFace Datasets (Total: 5)

| Dataset | Samples | Type | Source |
|---------|---------|------|--------|
| CVE Fixes 2022 | ~5K | CVE | ecwk/vulnerable-functions-and-commits |
| Vulnerable Dataset | ~3K | General | ZhengLiu33/vulnerable-dataset |
| Vulnerable Code | ~2K | General | doss1232/vulnerable-code |
| Code Vulnerable 10000 | ~10K | General | tranquangtien15092005/code-vulnerable-10000 |
| Vulnerable Configs | ~1K | Config | kh4dien/vulnerable-configs |

**Total Training Data: ~73,000 vulnerability samples!**

---

## ğŸ§¬ Architecture

```
VulnGuard AI Training Pipeline
â”‚
â”œâ”€â”€ Data Collection
â”‚   â”œâ”€â”€ HuggingFace Integrator
â”‚   â”‚   â””â”€â”€ 5 vulnerability datasets from HF Hub
â”‚   â”‚
â”‚   â””â”€â”€ Kaggle Integrator
â”‚       â””â”€â”€ 5 vulnerability datasets from Kaggle
â”‚
â”œâ”€â”€ Feature Extraction
â”‚   â”œâ”€â”€ AST Features (code structure)
â”‚   â”œâ”€â”€ Vulnerability Patterns (regex-based)
â”‚   â”œâ”€â”€ TF-IDF Features (char & token level)
â”‚   â”œâ”€â”€ Security Keywords
â”‚   â””â”€â”€ Complexity Metrics
â”‚
â”œâ”€â”€ Model Training
â”‚   â”œâ”€â”€ Random Forest (300 trees)
â”‚   â”œâ”€â”€ Gradient Boosting (200 estimators)
â”‚   â”œâ”€â”€ XGBoost (200 estimators)
â”‚   â”œâ”€â”€ Neural Network (4 layers: 512-256-128-64)
â”‚   â”œâ”€â”€ SVM (RBF kernel)
â”‚   â””â”€â”€ Logistic Regression
â”‚
â””â”€â”€ Ensemble Prediction
    â””â”€â”€ Weighted voting from all models
```

---

## ğŸ”¬ Feature Engineering

### 1. AST Features (~50 features)
- Function count, class count, import count
- Cyclomatic complexity
- Nesting depth
- Variable assignments
- Control flow statements

### 2. Vulnerability Patterns (~7 categories)
- SQL Injection patterns
- XSS patterns
- Command Injection patterns
- Path Traversal patterns
- Buffer Overflow patterns
- Weak Cryptography patterns
- Authentication Bypass patterns

### 3. TF-IDF Features (~11,000 features)
- Character-level n-grams (1-4)
- Token-level n-grams (1-3)
- Max features: 8,000 (char) + 3,000 (token)

### 4. Security Keywords (~20 keywords)
- password, token, key, secret, auth
- sql, query, execute, eval, system
- file, path, upload, input, sanitize

### 5. Code Metrics
- Code length, line count
- Character entropy
- Average line length
- Comment density

**Total: ~11,000+ features per code sample**

---

## ğŸ¯ Training Process

### Phase 1: Data Loading
```
1. Load HuggingFace datasets
2. Load Kaggle datasets
3. Merge all data sources
4. Deduplicate by code hash
5. Balance dataset (if needed)
```

### Phase 2: Feature Extraction
```
1. Extract AST features
2. Match vulnerability patterns
3. Generate TF-IDF vectors
4. Calculate code metrics
5. Combine all features
6. Scale features (StandardScaler)
```

### Phase 3: Model Training
```
1. Split data (80% train, 20% test)
2. Train 6 different models
3. Cross-validation
4. Hyperparameter tuning
5. Evaluate each model
```

### Phase 4: Evaluation & Saving
```
1. Calculate metrics (accuracy, precision, recall, F1)
2. Identify best performing model
3. Save all models + extractors
4. Generate training report
```

---

## ğŸ“Š Expected Performance

### With Full Dataset (~73K samples)

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| Random Forest | 88-92% | 0.87-0.91 | 0.85-0.90 | 0.86-0.91 |
| Gradient Boost | 89-93% | 0.88-0.92 | 0.87-0.91 | 0.87-0.92 |
| XGBoost | **90-95%** | **0.89-0.94** | **0.88-0.93** | **0.89-0.94** |
| Neural Network | 87-91% | 0.86-0.90 | 0.85-0.89 | 0.86-0.90 |
| SVM | 85-89% | 0.84-0.88 | 0.83-0.87 | 0.84-0.88 |
| Logistic Reg | 82-86% | 0.81-0.85 | 0.80-0.84 | 0.81-0.85 |
| **Ensemble** | **91-96%** | **0.90-0.95** | **0.89-0.94** | **0.90-0.95** |

### With HuggingFace Only (~21K samples)

| Model | Accuracy | F1-Score |
|-------|----------|----------|
| Best Single | 85-89% | 0.84-0.88 |
| Ensemble | 87-92% | 0.86-0.91 |

---

## ğŸ’» Usage Examples

### Training

```bash
# Full training with all datasets
python3 train_with_kaggle.py --download

# Training with existing Kaggle data
python3 train_with_kaggle.py --data-path ./data/kaggle

# HuggingFace only
python3 train_with_kaggle.py --huggingface-only
```

### Prediction

```python
import pickle
from core.ultimate_trainer import UltimateVulnGuardTrainer

# Load model
trainer = UltimateVulnGuardTrainer()
with open('ultimate_vulnguard_20251004_120000.pkl', 'rb') as f:
    model_data = pickle.load(f)
    trainer.models = model_data['models']
    trainer.code_vectorizer = model_data['code_vectorizer']
    trainer.token_vectorizer = model_data['token_vectorizer']
    trainer.scaler = model_data['scaler']
    trainer.feature_data = model_data.get('feature_shape')

# Predict
code = """
def authenticate(username, password):
    query = f"SELECT * FROM users WHERE user='{username}' AND pass='{password}'"
    result = db.execute(query)
    return result is not None
"""

result = trainer.predict(code)

print(f"ğŸ¯ Vulnerability: {'YES' if result['vulnerable'] else 'NO'}")
print(f"ğŸ“Š Confidence: {result['confidence']:.1%}")
print(f"\nğŸ¤– Individual Model Predictions:")
for model, pred in result['predictions'].items():
    prob = result['probabilities'][model]
    print(f"   {model:20s}: {'VULNERABLE' if pred else 'SAFE'} ({prob:.1%})")
```

---

## ğŸ”§ Configuration

### Memory Requirements

| Dataset Size | RAM Required | Training Time |
|--------------|--------------|---------------|
| HuggingFace Only (~21K) | 8 GB | 15-30 min |
| Kaggle Only (~52K) | 16 GB | 30-60 min |
| Full Dataset (~73K) | 24 GB | 60-120 min |

### Optimization Options

```python
# In ultimate_trainer.py

# Reduce feature count
self.code_vectorizer = TfidfVectorizer(
    max_features=4000,  # Instead of 8000
    ...
)

# Reduce model complexity
rf = RandomForestClassifier(
    n_estimators=100,  # Instead of 300
    max_depth=15,      # Instead of 25
    ...
)

# Use smaller batch size
nn = MLPClassifier(
    batch_size=128,    # Instead of 256
    ...
)
```

---

## ğŸ› Troubleshooting

### Issue: Out of Memory

**Solution 1:** Train with fewer samples
```python
# In dataset integrator, add sampling
df = df.sample(n=10000, random_state=42)
```

**Solution 2:** Reduce features
```python
# Reduce TF-IDF features
max_features=2000  # instead of 8000
```

**Solution 3:** Train fewer models
```python
# Comment out some models in train_ultimate_models()
# Keep only: Random Forest, XGBoost, Neural Network
```

### Issue: Kaggle API 403 Error

**Cause:** Dataset terms not accepted

**Solution:**
1. Visit each dataset URL in browser
2. Click "Download" button
3. Accept terms and conditions
4. Try API download again

### Issue: Slow Training

**Solution:** Use multiprocessing
```python
# Set n_jobs in models
RandomForestClassifier(..., n_jobs=-1)  # Use all CPU cores
```

### Issue: Low Accuracy

**Causes:**
- Insufficient training data
- Imbalanced dataset
- Poor feature engineering

**Solutions:**
```python
# 1. Balance dataset
from imblearn.over_sampling import SMOTE
X_balanced, y_balanced = SMOTE().fit_resample(X, y)

# 2. Add more features
# Enhance extract_ultimate_features() method

# 3. Tune hyperparameters
from sklearn.model_selection import GridSearchCV
param_grid = {'n_estimators': [100, 200, 300], ...}
grid_search = GridSearchCV(model, param_grid, cv=5)
```

---

## ğŸ“ˆ Monitoring Training

Training progress is logged in real-time:

```
ğŸš€ ULTIMATE VULNGUARD AI TRAINING PIPELINE
================================================================================

ğŸ“‚ PHASE 1: Loading HuggingFace Datasets
--------------------------------------------------------------------------------
âœ… HuggingFace: 21,345 samples

ğŸ“‚ PHASE 2: Loading Kaggle Datasets
--------------------------------------------------------------------------------
âœ… Loaded public-cve-2020-2024
âœ… Loaded cve-data
âœ… Loaded bug-bounty-writeups
âœ… Loaded cve-dataset
âœ… Loaded bug-bounty-openai
âœ… Kaggle: 51,876 samples

ğŸ“Š PHASE 3: Combining and Processing Data
--------------------------------------------------------------------------------
âœ… Total samples: 73,221
âœ… Unique samples: 68,543
ğŸ“‰ Duplicates removed: 4,678

ğŸ“Š Dataset Statistics:
   Vulnerable: 62,345
   Safe: 6,198

ğŸ”„ Preparing training data with ultimate feature extraction...
   Processing: 0/68543
   Processing: 1000/68543
   ...
âœ… Extracted features from 68,543 samples
âœ… Final training data: 68,543 samples, 11,234 features

ğŸ¤– Training Ultimate VulnGuard AI Models...
================================================================================
ğŸ“Š Training samples: 54,834
ğŸ“Š Test samples: 13,709

ğŸŒ² Training Random Forest...
   âœ… Random Forest trained

ğŸ“ˆ Training Gradient Boosting...
   âœ… Gradient Boosting trained

ğŸš€ Training XGBoost...
   âœ… XGBoost trained

ğŸ§  Training Neural Network...
   âœ… Neural Network trained

ğŸ¯ Training SVM...
   âœ… SVM trained

ğŸ“Š Training Logistic Regression...
   âœ… Logistic Regression trained

ğŸ“Š MODEL EVALUATION
================================================================================
RANDOM_FOREST:
  Accuracy:  0.9134
  Precision: 0.9089
  Recall:    0.9045
  F1-Score:  0.9067

XGBOOST:
  Accuracy:  0.9389
  Precision: 0.9356
  Recall:    0.9312
  F1-Score:  0.9334

...

ğŸ† BEST MODEL: XGBOOST
   F1-Score: 0.9334
================================================================================

ğŸ’¾ Models saved to ultimate_vulnguard_20251004_120000.pkl

ğŸ‰ TRAINING COMPLETE!
```

---

## ğŸ“ Best Practices

### 1. Data Quality
- Remove duplicates
- Balance dataset (SMOTE if needed)
- Validate data integrity
- Handle missing values

### 2. Feature Engineering
- Use domain knowledge
- Add vulnerability-specific patterns
- Combine multiple feature types
- Normalize/scale features

### 3. Model Selection
- Train multiple models
- Use ensemble methods
- Cross-validate results
- Monitor for overfitting

### 4. Evaluation
- Use stratified k-fold CV
- Test on held-out data
- Check confusion matrix
- Monitor precision/recall tradeoff

### 5. Production
- Version your models
- Monitor performance drift
- Retrain periodically
- A/B test improvements

---

## ğŸ“š Additional Resources

### Datasets
- [NVD Database](https://nvd.nist.gov/)
- [CVE Details](https://www.cvedetails.com/)
- [Exploit Database](https://www.exploit-db.com/)
- [HackerOne Disclosed Reports](https://hackerone.com/hacktivity)

### ML/Security
- [scikit-learn](https://scikit-learn.org/)
- [XGBoost](https://xgboost.readthedocs.io/)
- [OWASP Top 10](https://owasp.org/www-project-top-ten/)
- [CWE Database](https://cwe.mitre.org/)

---

## ğŸ¤ Support

Need help?
1. Check `KAGGLE_TRAINING_GUIDE.md`
2. Review troubleshooting section above
3. Check training logs for errors
4. Test with `--huggingface-only` first

---

**Built with â¤ï¸ for vulnerability research and AI/ML security**

ğŸ”’ For defensive security research only
ğŸ“– Educational and research purposes
ğŸ¯ Help make code more secure!
