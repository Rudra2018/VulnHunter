{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuln_ml_header"
      },
      "source": [
        "# üõ°Ô∏è VulnML: Vulnerability Detection Model Training\n",
        "## Google Colab Training Environment\n",
        "\n",
        "This notebook trains vulnerability detection models using your research data with GPU acceleration.\n",
        "\n",
        "**Features:**\n",
        "- üöÄ GPU-accelerated training\n",
        "- üìä Real-time metrics tracking\n",
        "- üíæ Automatic model saving to Google Drive\n",
        "- üîß Production-ready model validation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_section"
      },
      "source": [
        "## üîß Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_gpu"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU detected. Training will be slower on CPU.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q scikit-learn pandas numpy matplotlib seaborn\n",
        "!pip install -q xgboost lightgbm\n",
        "!pip install -q plotly kaleido\n",
        "!pip install -q joblib\n",
        "\n",
        "print(\"‚úÖ Dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive for data storage\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories for models and results\n",
        "import os\n",
        "os.makedirs('/content/drive/MyDrive/VulnML_Models', exist_ok=True)\n",
        "os.makedirs('/content/drive/MyDrive/VulnML_Results', exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted and directories created!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "import_section"
      },
      "source": [
        "## üì¶ Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import json\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "import logging\n",
        "\n",
        "# ML imports\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, r2_score, accuracy_score,\n",
        "    classification_report, confusion_matrix, roc_auc_score\n",
        ")\n",
        "from sklearn.linear_model import Ridge, LogisticRegression\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "\n",
        "# Visualization\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trainer_class"
      },
      "source": [
        "## ü§ñ VulnML Training Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vulnml_trainer"
      },
      "outputs": [],
      "source": [
        "class VulnMLColabTrainer:\n",
        "    \"\"\"Enhanced vulnerability detection model trainer optimized for Colab\"\"\"\n",
        "    \n",
        "    def __init__(self, drive_path='/content/drive/MyDrive/VulnML_Models'):\n",
        "        self.drive_path = Path(drive_path)\n",
        "        self.results_path = Path('/content/drive/MyDrive/VulnML_Results')\n",
        "        \n",
        "        # Setup logging\n",
        "        self.logger = self._setup_logging()\n",
        "        \n",
        "        # Model storage\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.vectorizers = {}\n",
        "        self.encoders = {}\n",
        "        \n",
        "        # Training results\n",
        "        self.training_results = {}\n",
        "        \n",
        "        # GPU device\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        \n",
        "    def _setup_logging(self):\n",
        "        \"\"\"Setup logging for Colab\"\"\"\n",
        "        logger = logging.getLogger('VulnML_Colab')\n",
        "        logger.setLevel(logging.INFO)\n",
        "        \n",
        "        if not logger.handlers:\n",
        "            handler = logging.StreamHandler()\n",
        "            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "            handler.setFormatter(formatter)\n",
        "            logger.addHandler(handler)\n",
        "            \n",
        "        return logger\n",
        "    \n",
        "    def generate_realistic_vuln_data(self, n_samples=15000):\n",
        "        \"\"\"Generate realistic vulnerability dataset for training\"\"\"\n",
        "        self.logger.info(f\"üîß Generating {n_samples:,} vulnerability samples...\")\n",
        "        \n",
        "        # Enhanced vulnerability categories with realistic distributions\n",
        "        vuln_categories = {\n",
        "            'web_application': {\n",
        "                'types': {\n",
        "                    'Cross-site Scripting (XSS)': {'severity_dist': {'Low': 0.4, 'Medium': 0.4, 'High': 0.2}, 'bounty_range': (50, 5000)},\n",
        "                    'SQL Injection': {'severity_dist': {'Medium': 0.3, 'High': 0.5, 'Critical': 0.2}, 'bounty_range': (500, 25000)},\n",
        "                    'Cross-Site Request Forgery (CSRF)': {'severity_dist': {'Low': 0.3, 'Medium': 0.5, 'High': 0.2}, 'bounty_range': (200, 8000)},\n",
        "                    'Insecure Direct Object Reference (IDOR)': {'severity_dist': {'Medium': 0.6, 'High': 0.3, 'Critical': 0.1}, 'bounty_range': (300, 15000)},\n",
        "                    'Server-Side Request Forgery (SSRF)': {'severity_dist': {'High': 0.6, 'Critical': 0.4}, 'bounty_range': (2000, 50000)}\n",
        "                },\n",
        "                'weight': 0.45\n",
        "            },\n",
        "            'system_security': {\n",
        "                'types': {\n",
        "                    'Remote Code Execution': {'severity_dist': {'High': 0.3, 'Critical': 0.7}, 'bounty_range': (10000, 100000)},\n",
        "                    'Privilege Escalation': {'severity_dist': {'Medium': 0.2, 'High': 0.5, 'Critical': 0.3}, 'bounty_range': (5000, 75000)},\n",
        "                    'Authentication Bypass': {'severity_dist': {'High': 0.6, 'Critical': 0.4}, 'bounty_range': (8000, 60000)},\n",
        "                    'Buffer Overflow': {'severity_dist': {'High': 0.4, 'Critical': 0.6}, 'bounty_range': (15000, 80000)}\n",
        "                },\n",
        "                'weight': 0.25\n",
        "            },\n",
        "            'blockchain_defi': {\n",
        "                'types': {\n",
        "                    'Flash Loan Attack': {'severity_dist': {'Critical': 1.0}, 'bounty_range': (100000, 2000000)},\n",
        "                    'Reentrancy Vulnerability': {'severity_dist': {'High': 0.3, 'Critical': 0.7}, 'bounty_range': (75000, 1500000)},\n",
        "                    'Price Oracle Manipulation': {'severity_dist': {'High': 0.4, 'Critical': 0.6}, 'bounty_range': (50000, 1000000)},\n",
        "                    'Smart Contract Logic Error': {'severity_dist': {'Medium': 0.2, 'High': 0.5, 'Critical': 0.3}, 'bounty_range': (25000, 500000)}\n",
        "                },\n",
        "                'weight': 0.15\n",
        "            },\n",
        "            'infrastructure': {\n",
        "                'types': {\n",
        "                    'Information Disclosure': {'severity_dist': {'Low': 0.5, 'Medium': 0.4, 'High': 0.1}, 'bounty_range': (100, 5000)},\n",
        "                    'Cryptographic Weakness': {'severity_dist': {'Medium': 0.3, 'High': 0.5, 'Critical': 0.2}, 'bounty_range': (5000, 40000)},\n",
        "                    'Business Logic Bypass': {'severity_dist': {'Medium': 0.4, 'High': 0.6}, 'bounty_range': (3000, 25000)},\n",
        "                    'Rate Limiting Bypass': {'severity_dist': {'Low': 0.4, 'Medium': 0.6}, 'bounty_range': (500, 8000)}\n",
        "                },\n",
        "                'weight': 0.15\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        # Program tiers\n",
        "        program_tiers = {\n",
        "            'tier1': {'programs': ['Google', 'Microsoft', 'Apple', 'Meta', 'Amazon'], 'multiplier': 1.5, 'weight': 0.25},\n",
        "            'tier2': {'programs': ['Netflix', 'Uber', 'PayPal', 'GitHub', 'Slack'], 'multiplier': 1.2, 'weight': 0.35},\n",
        "            'tier3': {'programs': ['Shopify', 'Discord', 'Reddit', 'Spotify'], 'multiplier': 1.0, 'weight': 0.25},\n",
        "            'defi': {'programs': ['Uniswap', 'Compound', 'Aave', 'MakerDAO'], 'multiplier': 2.0, 'weight': 0.15}\n",
        "        }\n",
        "        \n",
        "        all_data = []\n",
        "        \n",
        "        # Generate samples for each category\n",
        "        for category_name, category_data in vuln_categories.items():\n",
        "            category_samples = int(n_samples * category_data['weight'])\n",
        "            \n",
        "            for i in range(category_samples):\n",
        "                # Select vulnerability type\n",
        "                vuln_type = np.random.choice(list(category_data['types'].keys()))\n",
        "                vuln_config = category_data['types'][vuln_type]\n",
        "                \n",
        "                # Select severity based on vulnerability-specific distribution\n",
        "                severities = list(vuln_config['severity_dist'].keys())\n",
        "                severity_probs = list(vuln_config['severity_dist'].values())\n",
        "                severity = np.random.choice(severities, p=severity_probs)\n",
        "                \n",
        "                # Select program tier and specific program\n",
        "                tier_names = list(program_tiers.keys())\n",
        "                tier_weights = [tier['weight'] for tier in program_tiers.values()]\n",
        "                selected_tier = np.random.choice(tier_names, p=tier_weights)\n",
        "                \n",
        "                tier_data = program_tiers[selected_tier]\n",
        "                program = np.random.choice(tier_data['programs'])\n",
        "                \n",
        "                # Calculate bounty\n",
        "                base_min, base_max = vuln_config['bounty_range']\n",
        "                base_bounty = np.random.uniform(base_min, base_max)\n",
        "                \n",
        "                # Apply program tier multiplier\n",
        "                final_bounty = base_bounty * tier_data['multiplier']\n",
        "                \n",
        "                # Add realistic noise\n",
        "                noise = np.random.uniform(0.8, 1.3)\n",
        "                final_bounty *= noise\n",
        "                \n",
        "                # Generate additional features\n",
        "                cve_score = self._generate_cve_score(severity)\n",
        "                complexity = np.random.choice(['Low', 'Medium', 'High'], p=[0.3, 0.5, 0.2])\n",
        "                \n",
        "                record = {\n",
        "                    'id': f\"{category_name}_{i+1}\",\n",
        "                    'vulnerability_type': vuln_type,\n",
        "                    'severity_level': severity,\n",
        "                    'bounty_amount': round(final_bounty, 2),\n",
        "                    'program_name': program,\n",
        "                    'program_tier': selected_tier,\n",
        "                    'category': category_name,\n",
        "                    'cve_score': cve_score,\n",
        "                    'complexity': complexity,\n",
        "                    'description': f\"{severity} {vuln_type} in {program} {category_name} system\"\n",
        "                }\n",
        "                \n",
        "                all_data.append(record)\n",
        "        \n",
        "        df = pd.DataFrame(all_data)\n",
        "        \n",
        "        # Save to drive\n",
        "        csv_path = self.results_path / f\"vuln_dataset_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
        "        df.to_csv(csv_path, index=False)\n",
        "        \n",
        "        self.logger.info(f\"‚úÖ Generated {len(df):,} samples\")\n",
        "        self.logger.info(f\"üí∞ Bounty range: ${df['bounty_amount'].min():,.0f} - ${df['bounty_amount'].max():,.0f}\")\n",
        "        self.logger.info(f\"üìä Severity distribution: {dict(df['severity_level'].value_counts(normalize=True).round(3))}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _generate_cve_score(self, severity):\n",
        "        \"\"\"Generate realistic CVE scores based on severity\"\"\"\n",
        "        score_ranges = {\n",
        "            'Low': (0.1, 3.9),\n",
        "            'Medium': (4.0, 6.9),\n",
        "            'High': (7.0, 8.9),\n",
        "            'Critical': (9.0, 10.0)\n",
        "        }\n",
        "        min_score, max_score = score_ranges.get(severity, (5.0, 7.0))\n",
        "        return round(np.random.uniform(min_score, max_score), 1)\n",
        "    \n",
        "    def prepare_features(self, df):\n",
        "        \"\"\"Prepare enhanced features for training\"\"\"\n",
        "        self.logger.info(\"üîß Preparing enhanced features...\")\n",
        "        \n",
        "        features = []\n",
        "        \n",
        "        for _, row in df.iterrows():\n",
        "            # Basic text features\n",
        "            vuln_type = str(row['vulnerability_type'])\n",
        "            severity = str(row['severity_level'])\n",
        "            program = str(row['program_name'])\n",
        "            category = str(row['category'])\n",
        "            complexity = str(row.get('complexity', 'Medium'))\n",
        "            \n",
        "            # Enhanced feature vector\n",
        "            feature_vector = [\n",
        "                # Text length features\n",
        "                len(vuln_type),\n",
        "                len(program),\n",
        "                len(str(row['description'])),\n",
        "                \n",
        "                # CVE score\n",
        "                float(row.get('cve_score', 5.0)),\n",
        "                \n",
        "                # Severity one-hot encoding\n",
        "                1 if severity == 'Critical' else 0,\n",
        "                1 if severity == 'High' else 0,\n",
        "                1 if severity == 'Medium' else 0,\n",
        "                1 if severity == 'Low' else 0,\n",
        "                \n",
        "                # Vulnerability type indicators\n",
        "                1 if 'SQL' in vuln_type.upper() else 0,\n",
        "                1 if 'XSS' in vuln_type.upper() else 0,\n",
        "                1 if 'RCE' in vuln_type.upper() or 'REMOTE CODE' in vuln_type.upper() else 0,\n",
        "                1 if 'SSRF' in vuln_type.upper() else 0,\n",
        "                1 if 'IDOR' in vuln_type.upper() else 0,\n",
        "                1 if 'CSRF' in vuln_type.upper() else 0,\n",
        "                1 if 'FLASH LOAN' in vuln_type.upper() else 0,\n",
        "                1 if 'REENTRANCY' in vuln_type.upper() else 0,\n",
        "                1 if 'PRIVILEGE' in vuln_type.upper() else 0,\n",
        "                1 if 'BUFFER' in vuln_type.upper() else 0,\n",
        "                \n",
        "                # Category indicators\n",
        "                1 if category == 'web_application' else 0,\n",
        "                1 if category == 'system_security' else 0,\n",
        "                1 if category == 'blockchain_defi' else 0,\n",
        "                1 if category == 'infrastructure' else 0,\n",
        "                \n",
        "                # Program tier scoring\n",
        "                self._get_program_score(program),\n",
        "                \n",
        "                # Complexity scoring\n",
        "                {'Low': 0.3, 'Medium': 0.6, 'High': 1.0}.get(complexity, 0.6),\n",
        "                \n",
        "                # Severity numerical score\n",
        "                {'Low': 0.25, 'Medium': 0.5, 'High': 0.75, 'Critical': 1.0}.get(severity, 0.5),\n",
        "                \n",
        "                # Risk score based on vulnerability type\n",
        "                self._get_risk_score(vuln_type)\n",
        "            ]\n",
        "            \n",
        "            features.append(feature_vector)\n",
        "        \n",
        "        return np.array(features)\n",
        "    \n",
        "    def _get_program_score(self, program):\n",
        "        \"\"\"Get program reputation score\"\"\"\n",
        "        tier1 = ['Google', 'Microsoft', 'Apple', 'Meta', 'Amazon']\n",
        "        tier2 = ['Netflix', 'Uber', 'PayPal', 'GitHub', 'Slack']\n",
        "        defi = ['Uniswap', 'Compound', 'Aave', 'MakerDAO']\n",
        "        \n",
        "        if program in tier1:\n",
        "            return 1.0\n",
        "        elif program in defi:\n",
        "            return 0.9\n",
        "        elif program in tier2:\n",
        "            return 0.7\n",
        "        else:\n",
        "            return 0.5\n",
        "    \n",
        "    def _get_risk_score(self, vuln_type):\n",
        "        \"\"\"Get vulnerability risk score\"\"\"\n",
        "        critical_risks = ['Remote Code Execution', 'Flash Loan Attack', 'Reentrancy']\n",
        "        high_risks = ['SQL Injection', 'SSRF', 'Privilege Escalation', 'Buffer Overflow']\n",
        "        medium_risks = ['XSS', 'IDOR', 'CSRF', 'Authentication Bypass']\n",
        "        \n",
        "        vuln_upper = vuln_type.upper()\n",
        "        \n",
        "        for critical in critical_risks:\n",
        "            if critical.upper() in vuln_upper:\n",
        "                return 1.0\n",
        "        \n",
        "        for high in high_risks:\n",
        "            if high.upper() in vuln_upper:\n",
        "                return 0.8\n",
        "        \n",
        "        for medium in medium_risks:\n",
        "            if medium.upper() in vuln_upper:\n",
        "                return 0.6\n",
        "        \n",
        "        return 0.4\n",
        "\n",
        "print(\"‚úÖ VulnML Trainer class defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_methods"
      },
      "source": [
        "## üèãÔ∏è Training Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_methods_code"
      },
      "outputs": [],
      "source": [
        "# Add training methods to the VulnMLColabTrainer class\n",
        "def train_bounty_predictor(self, X, y):\n",
        "    \"\"\"Train bounty prediction models with ensemble approach\"\"\"\n",
        "    self.logger.info(\"ü§ñ Training bounty prediction models...\")\n",
        "    \n",
        "    # Feature scaling\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    self.scalers['bounty'] = scaler\n",
        "    \n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_scaled, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "    \n",
        "    # Model ensemble\n",
        "    models = {\n",
        "        'random_forest': RandomForestRegressor(\n",
        "            n_estimators=200,\n",
        "            max_depth=12,\n",
        "            min_samples_split=10,\n",
        "            min_samples_leaf=4,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'xgboost': xgb.XGBRegressor(\n",
        "            n_estimators=200,\n",
        "            max_depth=8,\n",
        "            learning_rate=0.1,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=42,\n",
        "            tree_method='hist',\n",
        "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        ),\n",
        "        'ridge': Ridge(alpha=1.0, random_state=42)\n",
        "    }\n",
        "    \n",
        "    best_model = None\n",
        "    best_score = -np.inf\n",
        "    model_results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        self.logger.info(f\"  Training {name}...\")\n",
        "        \n",
        "        # Cross-validation\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='r2', n_jobs=-1)\n",
        "        \n",
        "        # Train and evaluate\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        \n",
        "        test_r2 = r2_score(y_test, y_pred_test)\n",
        "        test_mae = mean_absolute_error(y_test, y_pred_test)\n",
        "        \n",
        "        model_results[name] = {\n",
        "            'cv_r2_mean': cv_scores.mean(),\n",
        "            'cv_r2_std': cv_scores.std(),\n",
        "            'test_r2': test_r2,\n",
        "            'test_mae': test_mae\n",
        "        }\n",
        "        \n",
        "        self.logger.info(f\"    CV R¬≤: {cv_scores.mean():.3f}¬±{cv_scores.std():.3f}\")\n",
        "        self.logger.info(f\"    Test R¬≤: {test_r2:.3f}, MAE: ${test_mae:,.0f}\")\n",
        "        \n",
        "        if cv_scores.mean() > best_score:\n",
        "            best_score = cv_scores.mean()\n",
        "            best_model = model\n",
        "    \n",
        "    self.models['bounty_predictor'] = best_model\n",
        "    \n",
        "    results = {\n",
        "        'best_model': type(best_model).__name__,\n",
        "        'cv_r2_mean': best_score,\n",
        "        'model_comparison': model_results,\n",
        "        'samples_count': len(X),\n",
        "        'features_count': X.shape[1]\n",
        "    }\n",
        "    \n",
        "    self.training_results['bounty_prediction'] = results\n",
        "    self.logger.info(f\"‚úÖ Best bounty model: {results['best_model']} (R¬≤={best_score:.3f})\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def train_severity_classifier(self, df):\n",
        "    \"\"\"Train severity classification model\"\"\"\n",
        "    self.logger.info(\"üéØ Training severity classification model...\")\n",
        "    \n",
        "    # Prepare text features\n",
        "    descriptions = []\n",
        "    targets = []\n",
        "    \n",
        "    for _, row in df.iterrows():\n",
        "        description = f\"{row['vulnerability_type']} vulnerability in {row['program_name']} {row['category']} system. {row['description']}\"\n",
        "        descriptions.append(description)\n",
        "        targets.append(row['severity_level'])\n",
        "    \n",
        "    # Text vectorization\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_features=2000,\n",
        "        ngram_range=(1, 3),\n",
        "        stop_words='english',\n",
        "        min_df=3,\n",
        "        max_df=0.95\n",
        "    )\n",
        "    \n",
        "    X_text = vectorizer.fit_transform(descriptions)\n",
        "    self.vectorizers['severity'] = vectorizer\n",
        "    \n",
        "    # Label encoding\n",
        "    encoder = LabelEncoder()\n",
        "    y = encoder.fit_transform(targets)\n",
        "    self.encoders['severity'] = encoder\n",
        "    \n",
        "    # Stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_text.toarray(), y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    \n",
        "    # Model ensemble\n",
        "    models = {\n",
        "        'gradient_boosting': GradientBoostingClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            min_samples_split=20,\n",
        "            random_state=42\n",
        "        ),\n",
        "        'random_forest': RandomForestClassifier(\n",
        "            n_estimators=200,\n",
        "            max_depth=10,\n",
        "            min_samples_split=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ),\n",
        "        'xgboost': xgb.XGBClassifier(\n",
        "            n_estimators=150,\n",
        "            max_depth=6,\n",
        "            learning_rate=0.1,\n",
        "            random_state=42,\n",
        "            tree_method='hist',\n",
        "            device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        )\n",
        "    }\n",
        "    \n",
        "    best_model = None\n",
        "    best_score = -np.inf\n",
        "    model_results = {}\n",
        "    \n",
        "    for name, model in models.items():\n",
        "        self.logger.info(f\"  Training {name}...\")\n",
        "        \n",
        "        # Stratified cross-validation\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        cv_scores = cross_val_score(model, X_train, y_train, cv=skf, scoring='accuracy', n_jobs=-1)\n",
        "        \n",
        "        # Train and evaluate\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred_test = model.predict(X_test)\n",
        "        \n",
        "        test_accuracy = accuracy_score(y_test, y_pred_test)\n",
        "        \n",
        "        model_results[name] = {\n",
        "            'cv_accuracy_mean': cv_scores.mean(),\n",
        "            'cv_accuracy_std': cv_scores.std(),\n",
        "            'test_accuracy': test_accuracy\n",
        "        }\n",
        "        \n",
        "        self.logger.info(f\"    CV Accuracy: {cv_scores.mean():.3f}¬±{cv_scores.std():.3f}\")\n",
        "        self.logger.info(f\"    Test Accuracy: {test_accuracy:.3f}\")\n",
        "        \n",
        "        if cv_scores.mean() > best_score:\n",
        "            best_score = cv_scores.mean()\n",
        "            best_model = model\n",
        "    \n",
        "    self.models['severity_classifier'] = best_model\n",
        "    \n",
        "    # Classification report\n",
        "    y_pred_final = best_model.predict(X_test)\n",
        "    class_report = classification_report(\n",
        "        y_test, y_pred_final,\n",
        "        target_names=encoder.classes_,\n",
        "        output_dict=True\n",
        "    )\n",
        "    \n",
        "    results = {\n",
        "        'best_model': type(best_model).__name__,\n",
        "        'cv_accuracy_mean': best_score,\n",
        "        'test_accuracy': accuracy_score(y_test, y_pred_final),\n",
        "        'classification_report': class_report,\n",
        "        'class_names': encoder.classes_.tolist(),\n",
        "        'model_comparison': model_results\n",
        "    }\n",
        "    \n",
        "    self.training_results['severity_classification'] = results\n",
        "    self.logger.info(f\"‚úÖ Best severity model: {results['best_model']} (Acc={best_score:.3f})\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def save_models(self):\n",
        "    \"\"\"Save all trained models to Google Drive\"\"\"\n",
        "    self.logger.info(\"üíæ Saving models to Google Drive...\")\n",
        "    \n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    \n",
        "    # Save models\n",
        "    for model_name, model in self.models.items():\n",
        "        model_path = self.drive_path / f\"{model_name}_{timestamp}.pkl\"\n",
        "        joblib.dump(model, model_path)\n",
        "        self.logger.info(f\"  Saved {model_name} to {model_path}\")\n",
        "    \n",
        "    # Save preprocessors\n",
        "    for scaler_name, scaler in self.scalers.items():\n",
        "        scaler_path = self.drive_path / f\"scaler_{scaler_name}_{timestamp}.pkl\"\n",
        "        joblib.dump(scaler, scaler_path)\n",
        "    \n",
        "    for vec_name, vectorizer in self.vectorizers.items():\n",
        "        vec_path = self.drive_path / f\"vectorizer_{vec_name}_{timestamp}.pkl\"\n",
        "        joblib.dump(vectorizer, vec_path)\n",
        "    \n",
        "    for enc_name, encoder in self.encoders.items():\n",
        "        enc_path = self.drive_path / f\"encoder_{enc_name}_{timestamp}.pkl\"\n",
        "        joblib.dump(encoder, enc_path)\n",
        "    \n",
        "    # Save training results\n",
        "    results_path = self.results_path / f\"training_results_{timestamp}.json\"\n",
        "    with open(results_path, 'w') as f:\n",
        "        json.dump(self.training_results, f, indent=2, default=str)\n",
        "    \n",
        "    self.logger.info(f\"‚úÖ All models and results saved with timestamp: {timestamp}\")\n",
        "    return timestamp\n",
        "\n",
        "# Add methods to the class\n",
        "VulnMLColabTrainer.train_bounty_predictor = train_bounty_predictor\n",
        "VulnMLColabTrainer.train_severity_classifier = train_severity_classifier\n",
        "VulnMLColabTrainer.save_models = save_models\n",
        "\n",
        "print(\"‚úÖ Training methods added successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_generation"
      },
      "source": [
        "## üìä Data Generation & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_data"
      },
      "outputs": [],
      "source": [
        "# Initialize trainer\n",
        "trainer = VulnMLColabTrainer()\n",
        "\n",
        "# Generate training data\n",
        "print(\"üîß Generating vulnerability dataset...\")\n",
        "df = trainer.generate_realistic_vuln_data(n_samples=15000)\n",
        "\n",
        "print(f\"\\nüìä Dataset Overview:\")\n",
        "print(f\"Total samples: {len(df):,}\")\n",
        "print(f\"Bounty range: ${df['bounty_amount'].min():,.0f} - ${df['bounty_amount'].max():,.0f}\")\n",
        "print(f\"Average bounty: ${df['bounty_amount'].mean():,.0f}\")\n",
        "print(f\"\\nSeverity distribution:\")\n",
        "print(df['severity_level'].value_counts())\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df['category'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "visualize_data"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive visualizations\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        'Bounty Distribution by Severity',\n",
        "        'Vulnerability Types by Category',\n",
        "        'Bounty Amount Distribution',\n",
        "        'CVE Score vs Bounty Amount'\n",
        "    ),\n",
        "    specs=[[{\"type\": \"box\"}, {\"type\": \"bar\"}],\n",
        "           [{\"type\": \"histogram\"}, {\"type\": \"scatter\"}]]\n",
        ")\n",
        "\n",
        "# Bounty distribution by severity\n",
        "for severity in df['severity_level'].unique():\n",
        "    severity_data = df[df['severity_level'] == severity]['bounty_amount']\n",
        "    fig.add_trace(\n",
        "        go.Box(y=severity_data, name=severity, showlegend=False),\n",
        "        row=1, col=1\n",
        "    )\n",
        "\n",
        "# Vulnerability types by category\n",
        "vuln_counts = df.groupby(['category', 'vulnerability_type']).size().reset_index(name='count')\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=vuln_counts['vulnerability_type'],\n",
        "        y=vuln_counts['count'],\n",
        "        text=vuln_counts['category'],\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Bounty amount distribution\n",
        "fig.add_trace(\n",
        "    go.Histogram(x=df['bounty_amount'], nbinsx=50, showlegend=False),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# CVE Score vs Bounty Amount\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=df['cve_score'],\n",
        "        y=df['bounty_amount'],\n",
        "        mode='markers',\n",
        "        text=df['severity_level'],\n",
        "        showlegend=False,\n",
        "        opacity=0.6\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    title_text=\"VulnML Dataset Analysis\",\n",
        "    title_x=0.5\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"Bounty Amount ($)\", type=\"log\", row=1, col=1)\n",
        "fig.update_xaxes(title_text=\"Severity\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Count\", row=1, col=2)\n",
        "fig.update_xaxes(title_text=\"Vulnerability Type\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Frequency\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Bounty Amount ($)\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Bounty Amount ($)\", type=\"log\", row=2, col=2)\n",
        "fig.update_xaxes(title_text=\"CVE Score\", row=2, col=2)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Display dataset sample\n",
        "print(\"\\nüìã Sample Data:\")\n",
        "display(df.sample(5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feature_preparation"
      },
      "source": [
        "## üîß Feature Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_features"
      },
      "outputs": [],
      "source": [
        "# Prepare features for training\n",
        "print(\"üîß Preparing features for training...\")\n",
        "X = trainer.prepare_features(df)\n",
        "y = df['bounty_amount'].values\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n",
        "print(f\"Feature range: [{X.min():.2f}, {X.max():.2f}]\")\n",
        "print(f\"Target range: [${y.min():,.0f}, ${y.max():,.0f}]\")\n",
        "\n",
        "# Feature importance visualization (correlation with target)\n",
        "feature_names = [\n",
        "    'vuln_type_len', 'program_len', 'description_len', 'cve_score',\n",
        "    'severity_critical', 'severity_high', 'severity_medium', 'severity_low',\n",
        "    'vuln_sql', 'vuln_xss', 'vuln_rce', 'vuln_ssrf', 'vuln_idor', 'vuln_csrf',\n",
        "    'vuln_flash_loan', 'vuln_reentrancy', 'vuln_privilege', 'vuln_buffer',\n",
        "    'cat_web_app', 'cat_system_sec', 'cat_blockchain', 'cat_infrastructure',\n",
        "    'program_score', 'complexity_score', 'severity_score', 'risk_score'\n",
        "]\n",
        "\n",
        "# Calculate correlations\n",
        "correlations = []\n",
        "for i in range(X.shape[1]):\n",
        "    corr = np.corrcoef(X[:, i], y)[0, 1]\n",
        "    correlations.append(abs(corr) if not np.isnan(corr) else 0)\n",
        "\n",
        "# Feature importance plot\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(\n",
        "        x=feature_names,\n",
        "        y=correlations,\n",
        "        text=[f'{c:.3f}' for c in correlations],\n",
        "        textposition='auto'\n",
        "    )\n",
        "])\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Feature Importance (Correlation with Bounty Amount)',\n",
        "    xaxis_title='Features',\n",
        "    yaxis_title='Absolute Correlation',\n",
        "    xaxis_tickangle=-45,\n",
        "    height=600\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "print(f\"\\nüîù Top 5 most important features:\")\n",
        "top_features = sorted(zip(feature_names, correlations), key=lambda x: x[1], reverse=True)[:5]\n",
        "for feature, corr in top_features:\n",
        "    print(f\"  {feature}: {corr:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_training"
      },
      "source": [
        "## üèãÔ∏è Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_bounty_model"
      },
      "outputs": [],
      "source": [
        "# Train bounty prediction model\n",
        "print(\"ü§ñ Training Bounty Prediction Models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "bounty_results = trainer.train_bounty_predictor(X, y)\n",
        "\n",
        "print(f\"\\n‚úÖ Bounty Prediction Training Complete!\")\n",
        "print(f\"Best Model: {bounty_results['best_model']}\")\n",
        "print(f\"Cross-Validation R¬≤: {bounty_results['cv_r2_mean']:.3f}\")\n",
        "print(f\"Training Samples: {bounty_results['samples_count']:,}\")\n",
        "print(f\"Features: {bounty_results['features_count']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_severity_model"
      },
      "outputs": [],
      "source": [
        "# Train severity classification model\n",
        "print(\"\\nüéØ Training Severity Classification Models...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "severity_results = trainer.train_severity_classifier(df)\n",
        "\n",
        "print(f\"\\n‚úÖ Severity Classification Training Complete!\")\n",
        "print(f\"Best Model: {severity_results['best_model']}\")\n",
        "print(f\"Cross-Validation Accuracy: {severity_results['cv_accuracy_mean']:.3f}\")\n",
        "print(f\"Test Accuracy: {severity_results['test_accuracy']:.3f}\")\n",
        "print(f\"Classes: {', '.join(severity_results['class_names'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "model_evaluation"
      },
      "source": [
        "## üìà Model Evaluation & Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evaluate_models"
      },
      "outputs": [],
      "source": [
        "# Create comprehensive evaluation visualizations\n",
        "fig = make_subplots(\n",
        "    rows=2, cols=2,\n",
        "    subplot_titles=(\n",
        "        'Model Performance Comparison (Bounty)',\n",
        "        'Model Performance Comparison (Severity)',\n",
        "        'Prediction vs Actual (Sample)',\n",
        "        'Training Progress Summary'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Bounty model comparison\n",
        "bounty_models = list(bounty_results['model_comparison'].keys())\n",
        "bounty_scores = [bounty_results['model_comparison'][m]['cv_r2_mean'] for m in bounty_models]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=bounty_models,\n",
        "        y=bounty_scores,\n",
        "        name='Bounty Models',\n",
        "        text=[f'{s:.3f}' for s in bounty_scores],\n",
        "        textposition='auto',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=1\n",
        ")\n",
        "\n",
        "# Severity model comparison\n",
        "severity_models = list(severity_results['model_comparison'].keys())\n",
        "severity_scores = [severity_results['model_comparison'][m]['cv_accuracy_mean'] for m in severity_models]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=severity_models,\n",
        "        y=severity_scores,\n",
        "        name='Severity Models',\n",
        "        text=[f'{s:.3f}' for s in severity_scores],\n",
        "        textposition='auto',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=1, col=2\n",
        ")\n",
        "\n",
        "# Sample predictions vs actual\n",
        "# Make some sample predictions\n",
        "sample_indices = np.random.choice(len(X), size=100, replace=False)\n",
        "X_sample = trainer.scalers['bounty'].transform(X[sample_indices])\n",
        "y_sample = y[sample_indices]\n",
        "y_pred_sample = trainer.models['bounty_predictor'].predict(X_sample)\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=y_sample,\n",
        "        y=y_pred_sample,\n",
        "        mode='markers',\n",
        "        name='Predictions',\n",
        "        showlegend=False,\n",
        "        opacity=0.7\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Perfect prediction line\n",
        "min_val, max_val = min(y_sample.min(), y_pred_sample.min()), max(y_sample.max(), y_pred_sample.max())\n",
        "fig.add_trace(\n",
        "    go.Scatter(\n",
        "        x=[min_val, max_val],\n",
        "        y=[min_val, max_val],\n",
        "        mode='lines',\n",
        "        name='Perfect Prediction',\n",
        "        line=dict(dash='dash', color='red'),\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=2, col=1\n",
        ")\n",
        "\n",
        "# Training summary metrics\n",
        "metrics = ['Bounty R¬≤', 'Severity Acc', 'Features', 'Samples']\n",
        "values = [\n",
        "    bounty_results['cv_r2_mean'],\n",
        "    severity_results['cv_accuracy_mean'],\n",
        "    bounty_results['features_count'] / 100,  # Scale for visualization\n",
        "    bounty_results['samples_count'] / 10000   # Scale for visualization\n",
        "]\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Bar(\n",
        "        x=metrics,\n",
        "        y=values,\n",
        "        text=[f'{v:.3f}' if v < 10 else f'{int(v * (10000 if \"Samples\" in metrics[i] else 100)):,}' \n",
        "              for i, v in enumerate(values)],\n",
        "        textposition='auto',\n",
        "        showlegend=False\n",
        "    ),\n",
        "    row=2, col=2\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    height=800,\n",
        "    title_text=\"VulnML Model Training Results\",\n",
        "    title_x=0.5\n",
        ")\n",
        "\n",
        "fig.update_yaxes(title_text=\"R¬≤ Score\", row=1, col=1)\n",
        "fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
        "fig.update_yaxes(title_text=\"Predicted Bounty ($)\", type=\"log\", row=2, col=1)\n",
        "fig.update_xaxes(title_text=\"Actual Bounty ($)\", type=\"log\", row=2, col=1)\n",
        "fig.update_yaxes(title_text=\"Normalized Values\", row=2, col=2)\n",
        "\n",
        "fig.show()\n",
        "\n",
        "# Print detailed classification report\n",
        "print(\"\\nüìä Detailed Severity Classification Report:\")\n",
        "print(\"=\" * 60)\n",
        "for class_name in severity_results['class_names']:\n",
        "    if class_name in severity_results['classification_report']:\n",
        "        metrics = severity_results['classification_report'][class_name]\n",
        "        print(f\"{class_name:>10}: Precision={metrics['precision']:.3f}, Recall={metrics['recall']:.3f}, F1={metrics['f1-score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "save_models"
      },
      "source": [
        "## üíæ Save Models & Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_all_models"
      },
      "outputs": [],
      "source": [
        "# Save all models and results to Google Drive\n",
        "print(\"üíæ Saving models and results to Google Drive...\")\n",
        "timestamp = trainer.save_models()\n",
        "\n",
        "print(f\"\\n‚úÖ All models saved successfully!\")\n",
        "print(f\"üìÅ Models location: /content/drive/MyDrive/VulnML_Models/\")\n",
        "print(f\"üìÅ Results location: /content/drive/MyDrive/VulnML_Results/\")\n",
        "print(f\"üè∑Ô∏è Timestamp: {timestamp}\")\n",
        "\n",
        "# Create deployment instructions\n",
        "deployment_code = f'''\n",
        "# VulnML Model Deployment Instructions\n",
        "# Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "# Timestamp: {timestamp}\n",
        "\n",
        "import joblib\n",
        "import numpy as np\n",
        "\n",
        "# Load models\n",
        "bounty_model = joblib.load('bounty_predictor_{timestamp}.pkl')\n",
        "severity_model = joblib.load('severity_classifier_{timestamp}.pkl')\n",
        "scaler = joblib.load('scaler_bounty_{timestamp}.pkl')\n",
        "vectorizer = joblib.load('vectorizer_severity_{timestamp}.pkl')\n",
        "encoder = joblib.load('encoder_severity_{timestamp}.pkl')\n",
        "\n",
        "# Example prediction function\n",
        "def predict_vulnerability(vuln_type, program, category, cve_score=5.0, complexity='Medium'):\n",
        "    # Prepare features (simplified example)\n",
        "    feature_vector = [\n",
        "        len(vuln_type), len(program), 100,  # lengths\n",
        "        cve_score,  # CVE score\n",
        "        0, 0, 1, 0,  # severity placeholders (Medium)\n",
        "        1 if 'SQL' in vuln_type.upper() else 0,\n",
        "        1 if 'XSS' in vuln_type.upper() else 0,\n",
        "        # ... other features\n",
        "    ]\n",
        "    \n",
        "    # Predict bounty\n",
        "    features_scaled = scaler.transform([feature_vector])\n",
        "    predicted_bounty = bounty_model.predict(features_scaled)[0]\n",
        "    \n",
        "    # Predict severity\n",
        "    description = f\"{{vuln_type}} vulnerability in {{program}} {{category}} system\"\n",
        "    text_features = vectorizer.transform([description])\n",
        "    severity_encoded = severity_model.predict(text_features.toarray())[0]\n",
        "    predicted_severity = encoder.inverse_transform([severity_encoded])[0]\n",
        "    \n",
        "    return {{\n",
        "        'predicted_bounty': predicted_bounty,\n",
        "        'predicted_severity': predicted_severity\n",
        "    }}\n",
        "\n",
        "# Example usage\n",
        "result = predict_vulnerability(\n",
        "    vuln_type=\"SQL Injection\",\n",
        "    program=\"Google\",\n",
        "    category=\"web_application\",\n",
        "    cve_score=8.5\n",
        ")\n",
        "print(f\"Predicted bounty: ${{result['predicted_bounty']:,.0f}}\")\n",
        "print(f\"Predicted severity: {{result['predicted_severity']}}\")\n",
        "'''\n",
        "\n",
        "# Save deployment instructions\n",
        "deploy_path = trainer.results_path / f\"deployment_instructions_{timestamp}.py\"\n",
        "with open(deploy_path, 'w') as f:\n",
        "    f.write(deployment_code)\n",
        "\n",
        "print(f\"\\nüìã Deployment instructions saved: {deploy_path}\")\n",
        "\n",
        "# Display final summary\n",
        "print(f\"\\nüéâ VulnML Training Complete!\")\n",
        "print(f\"=\" * 50)\n",
        "print(f\"ü§ñ Bounty Predictor: {bounty_results['best_model']} (R¬≤={bounty_results['cv_r2_mean']:.3f})\")\n",
        "print(f\"üéØ Severity Classifier: {severity_results['best_model']} (Acc={severity_results['cv_accuracy_mean']:.3f})\")\n",
        "print(f\"üìä Training Samples: {bounty_results['samples_count']:,}\")\n",
        "print(f\"üîß Features: {bounty_results['features_count']}\")\n",
        "print(f\"üíæ Models saved to Google Drive with timestamp: {timestamp}\")\n",
        "print(f\"\\nüöÄ Ready for deployment!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quick_test"
      },
      "source": [
        "## üß™ Quick Model Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "test_predictions"
      },
      "outputs": [],
      "source": [
        "# Test the trained models with some example vulnerabilities\n",
        "print(\"üß™ Testing trained models with example vulnerabilities...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_cases = [\n",
        "    {\n",
        "        'name': 'Critical RCE in Google',\n",
        "        'vuln_type': 'Remote Code Execution',\n",
        "        'program': 'Google',\n",
        "        'category': 'system_security',\n",
        "        'cve_score': 9.8,\n",
        "        'expected_bounty_range': (50000, 150000)\n",
        "    },\n",
        "    {\n",
        "        'name': 'SQL Injection in PayPal',\n",
        "        'vuln_type': 'SQL Injection',\n",
        "        'program': 'PayPal',\n",
        "        'category': 'web_application',\n",
        "        'cve_score': 7.5,\n",
        "        'expected_bounty_range': (5000, 25000)\n",
        "    },\n",
        "    {\n",
        "        'name': 'Flash Loan Attack in Uniswap',\n",
        "        'vuln_type': 'Flash Loan Attack',\n",
        "        'program': 'Uniswap',\n",
        "        'category': 'blockchain_defi',\n",
        "        'cve_score': 9.5,\n",
        "        'expected_bounty_range': (200000, 800000)\n",
        "    },\n",
        "    {\n",
        "        'name': 'XSS in Medium Website',\n",
        "        'vuln_type': 'Cross-site Scripting (XSS)',\n",
        "        'program': 'Reddit',\n",
        "        'category': 'web_application',\n",
        "        'cve_score': 4.3,\n",
        "        'expected_bounty_range': (100, 3000)\n",
        "    }\n",
        "]\n",
        "\n",
        "successful_predictions = 0\n",
        "\n",
        "for test_case in test_cases:\n",
        "    try:\n",
        "        # Create feature vector for this test case\n",
        "        feature_vector = [\n",
        "            len(test_case['vuln_type']),\n",
        "            len(test_case['program']),\n",
        "            100,  # description length\n",
        "            test_case['cve_score'],\n",
        "            1 if 'Critical' in test_case['name'] else 0,\n",
        "            1 if 'High' in test_case['name'] else 0,\n",
        "            1 if 'Medium' in test_case['name'] else 0,\n",
        "            1 if 'Low' in test_case['name'] else 0,\n",
        "            1 if 'SQL' in test_case['vuln_type'].upper() else 0,\n",
        "            1 if 'XSS' in test_case['vuln_type'].upper() else 0,\n",
        "            1 if 'RCE' in test_case['vuln_type'].upper() or 'REMOTE CODE' in test_case['vuln_type'].upper() else 0,\n",
        "            0, 0, 0,  # SSRF, IDOR, CSRF\n",
        "            1 if 'FLASH LOAN' in test_case['vuln_type'].upper() else 0,\n",
        "            0, 0, 0,  # reentrancy, privilege, buffer\n",
        "            1 if test_case['category'] == 'web_application' else 0,\n",
        "            1 if test_case['category'] == 'system_security' else 0,\n",
        "            1 if test_case['category'] == 'blockchain_defi' else 0,\n",
        "            1 if test_case['category'] == 'infrastructure' else 0,\n",
        "            trainer._get_program_score(test_case['program']),\n",
        "            0.6,  # complexity score\n",
        "            0.75,  # severity score\n",
        "            trainer._get_risk_score(test_case['vuln_type'])\n",
        "        ]\n",
        "        \n",
        "        # Predict bounty\n",
        "        features_scaled = trainer.scalers['bounty'].transform([feature_vector])\n",
        "        predicted_bounty = trainer.models['bounty_predictor'].predict(features_scaled)[0]\n",
        "        \n",
        "        # Predict severity\n",
        "        description = f\"{test_case['vuln_type']} vulnerability in {test_case['program']} {test_case['category']} system\"\n",
        "        text_features = trainer.vectorizers['severity'].transform([description])\n",
        "        severity_encoded = trainer.models['severity_classifier'].predict(text_features.toarray())[0]\n",
        "        predicted_severity = trainer.encoders['severity'].inverse_transform([severity_encoded])[0]\n",
        "        \n",
        "        # Check if prediction is within expected range\n",
        "        expected_min, expected_max = test_case['expected_bounty_range']\n",
        "        within_range = expected_min <= predicted_bounty <= expected_max\n",
        "        \n",
        "        if within_range:\n",
        "            successful_predictions += 1\n",
        "            status = \"‚úÖ\"\n",
        "        else:\n",
        "            status = \"‚ùå\"\n",
        "        \n",
        "        print(f\"{status} {test_case['name']}:\")\n",
        "        print(f\"   Predicted Bounty: ${predicted_bounty:,.0f}\")\n",
        "        print(f\"   Expected Range: ${expected_min:,} - ${expected_max:,}\")\n",
        "        print(f\"   Predicted Severity: {predicted_severity}\")\n",
        "        print()\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå {test_case['name']}: Prediction failed - {e}\")\n",
        "        print()\n",
        "\n",
        "accuracy = successful_predictions / len(test_cases)\n",
        "print(f\"üéØ Test Accuracy: {accuracy:.1%} ({successful_predictions}/{len(test_cases)})\")\n",
        "\n",
        "if accuracy >= 0.75:\n",
        "    print(\"üü¢ Models are performing well and ready for production!\")\n",
        "elif accuracy >= 0.5:\n",
        "    print(\"üü° Models show promise but may need fine-tuning\")\n",
        "else:\n",
        "    print(\"üî¥ Models need significant improvement before deployment\")\n",
        "\n",
        "print(\"\\nüéâ VulnML Colab Training Session Complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}