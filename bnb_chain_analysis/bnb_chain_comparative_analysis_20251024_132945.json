{
  "analysis_timestamp": "2025-10-24T13:29:42.828744",
  "target_directory": "/Users/ankitthakur/vuln_ml_research/bnb_chain_analysis",
  "models": {
    "classical": {
      "vulnerabilities": [
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/test/ValidatorSet.t.sol",
          "line": 305,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= systemRewardAntiMEVRatio * (block.number % turnLength) / (turnLength - 1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 273,
          "category": "reentrancy",
          "pattern": "address\\(.*\\)\\.call",
          "match": "address(stakeHub).call",
          "severity": "CRITICAL",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 275,
          "category": "reentrancy",
          "pattern": "address\\(.*\\)\\.call",
          "match": "address(stakeHub).call",
          "severity": "CRITICAL",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 353,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= systemRewardAntiMEVRatio * (block.number % turnLength) / (turnLength - 1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 81,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 40,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCoinbase",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 45,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyZeroGasPrice",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 50,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCrossChainContract",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 55,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyValidatorContract",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 60,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlySlash",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 65,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGov",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 70,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGovernor",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 75,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyStakeHub",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 80,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenRecoverPortal",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 76,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 77,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 170,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= request.bnbAmount",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 195,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= _reward",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 196,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= _reward",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 299,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= request.bnbAmount",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 344,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= bnbAmount",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 350,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= bnbAmount",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 31,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 51,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 56,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 61,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 66,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 71,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 76,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 86,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 91,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 96,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 101,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 30,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCoinbase",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0012",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 35,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyZeroGasPrice",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0013",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 40,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyNotInit",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0014",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 45,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyInit",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0015",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 50,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlySlash",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0016",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 55,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenHub",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0017",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 60,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGov",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0018",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 65,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyValidatorContract",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0019",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 70,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCrossChainContract",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0020",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 75,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyRelayerIncentivize",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0021",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 80,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyRelayer",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0022",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 85,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenManager",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0023",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 90,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyStakeHub",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0024",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 95,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGovernorTimelock",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0025",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 100,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenRecoverPortal",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 77,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 76,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenOwner",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 255,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= systemRewardAntiMEVRatio * (block.number % turnLength) / (turnLength - 1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 314,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= weights[i]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 529,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/TokenRecoverPortal.sol",
          "line": 172,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 27",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/SystemReward.sol",
          "line": 24,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyOperator",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 700,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 732,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 1265,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 495,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 588,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= feeCharge",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 56,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= wad",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 25,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wad",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 52,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wad",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 55,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wad",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/extension/Protectable.sol",
          "line": 39,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyProtector",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 93,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 99,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 104,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/RelayerHub.sol",
          "line": 35,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyManager",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/RelayerHub.sol",
          "line": 45,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyProvisionalRelayer",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 95,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyRegisteredContractChannel",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 111,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCabinet",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.8.x/RLPDecode.sol",
          "line": 271,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.8.x/RLPDecode.sol",
          "line": 272,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.8.x/RLPDecode.sol",
          "line": 266,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n\n            src += WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/RLPDecode.sol",
          "line": 263,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/RLPDecode.sol",
          "line": 264,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/RLPDecode.sol",
          "line": 258,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n\n            src += WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/SizeOf.sol",
          "line": 17,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 33,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 39,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 43,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 49,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 53,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 60,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/BytesToTypes.sol",
          "line": 50,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n            }\n        }\n    }\n\n    function bytesToBytes32(uint256 _offst, bytes memory _input, bytes32 _output) internal pure {\n        assembly {\n            mstore(_output, add(_input, _offst))\n            mstore(add(_output, 32), add(add(_input, _offst), 32))\n        }\n    }\n\n    function bytesToInt8(uint256 _offst, bytes memory _input) internal pure returns (int8 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt16(uint256 _offst, bytes memory _input) internal pure returns (int16 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt24(uint256 _offst, bytes memory _input) internal pure returns (int24 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt32(uint256 _offst, bytes memory _input) internal pure returns (int32 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt40(uint256 _offst, bytes memory _input) internal pure returns (int40 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt48(uint256 _offst, bytes memory _input) internal pure returns (int48 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt56(uint256 _offst, bytes memory _input) internal pure returns (int56 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt64(uint256 _offst, bytes memory _input) internal pure returns (int64 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt72(uint256 _offst, bytes memory _input) internal pure returns (int72 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt80(uint256 _offst, bytes memory _input) internal pure returns (int80 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt88(uint256 _offst, bytes memory _input) internal pure returns (int88 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt96(uint256 _offst, bytes memory _input) internal pure returns (int96 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt104(uint256 _offst, bytes memory _input) internal pure returns (int104 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt112(uint256 _offst, bytes memory _input) internal pure returns (int112 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt120(uint256 _offst, bytes memory _input) internal pure returns (int120 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt128(uint256 _offst, bytes memory _input) internal pure returns (int128 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt136(uint256 _offst, bytes memory _input) internal pure returns (int136 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt144(uint256 _offst, bytes memory _input) internal pure returns (int144 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt152(uint256 _offst, bytes memory _input) internal pure returns (int152 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt160(uint256 _offst, bytes memory _input) internal pure returns (int160 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt168(uint256 _offst, bytes memory _input) internal pure returns (int168 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt176(uint256 _offst, bytes memory _input) internal pure returns (int176 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt184(uint256 _offst, bytes memory _input) internal pure returns (int184 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt192(uint256 _offst, bytes memory _input) internal pure returns (int192 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt200(uint256 _offst, bytes memory _input) internal pure returns (int200 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt208(uint256 _offst, bytes memory _input) internal pure returns (int208 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt216(uint256 _offst, bytes memory _input) internal pure returns (int216 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt224(uint256 _offst, bytes memory _input) internal pure returns (int224 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt232(uint256 _offst, bytes memory _input) internal pure returns (int232 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt240(uint256 _offst, bytes memory _input) internal pure returns (int240 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt248(uint256 _offst, bytes memory _input) internal pure returns (int248 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt256(uint256 _offst, bytes memory _input) internal pure returns (int256 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint8(uint256 _offst, bytes memory _input) internal pure returns (uint8 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint16(uint256 _offst, bytes memory _input) internal pure returns (uint16 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint24(uint256 _offst, bytes memory _input) internal pure returns (uint24 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint32(uint256 _offst, bytes memory _input) internal pure returns (uint32 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint40(uint256 _offst, bytes memory _input) internal pure returns (uint40 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint48(uint256 _offst, bytes memory _input) internal pure returns (uint48 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint56(uint256 _offst, bytes memory _input) internal pure returns (uint56 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint64(uint256 _offst, bytes memory _input) internal pure returns (uint64 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint72(uint256 _offst, bytes memory _input) internal pure returns (uint72 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint80(uint256 _offst, bytes memory _input) internal pure returns (uint80 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint88(uint256 _offst, bytes memory _input) internal pure returns (uint88 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint96(uint256 _offst, bytes memory _input) internal pure returns (uint96 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint104(uint256 _offst, bytes memory _input) internal pure returns (uint104 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint112(uint256 _offst, bytes memory _input) internal pure returns (uint112 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint120(uint256 _offst, bytes memory _input) internal pure returns (uint120 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint128(uint256 _offst, bytes memory _input) internal pure returns (uint128 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint136(uint256 _offst, bytes memory _input) internal pure returns (uint136 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint144(uint256 _offst, bytes memory _input) internal pure returns (uint144 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint152(uint256 _offst, bytes memory _input) internal pure returns (uint152 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint160(uint256 _offst, bytes memory _input) internal pure returns (uint160 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint168(uint256 _offst, bytes memory _input) internal pure returns (uint168 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint176(uint256 _offst, bytes memory _input) internal pure returns (uint176 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint184(uint256 _offst, bytes memory _input) internal pure returns (uint184 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint192(uint256 _offst, bytes memory _input) internal pure returns (uint192 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint200(uint256 _offst, bytes memory _input) internal pure returns (uint200 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint208(uint256 _offst, bytes memory _input) internal pure returns (uint208 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint216(uint256 _offst, bytes memory _input) internal pure returns (uint216 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint224(uint256 _offst, bytes memory _input) internal pure returns (uint224 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint232(uint256 _offst, bytes memory _input) internal pure returns (uint232 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint240(uint256 _offst, bytes memory _input) internal pure returns (uint240 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint248(uint256 _offst, bytes memory _input) internal pure returns (uint248 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint256(uint256 _offst, bytes memory _input) internal pure returns (uint256 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/Memory.sol",
          "line": 46,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/Memory.sol",
          "line": 47,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/Memory.sol",
          "line": 42,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n            dest += WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/test/utils/RLPDecode.sol",
          "line": 267,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc-genesis-contract/test/utils/RLPDecode.sol",
          "line": 268,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc-genesis-contract/test/utils/RLPDecode.sol",
          "line": 262,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n\n            src += WORD_SIZE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 201,
          "category": "access_control",
          "pattern": "onlyOwner|onlyAdmin",
          "match": "onlyOwner",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/bind/v2/internal/contracts/db/contract.sol",
          "line": 60,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 18,
          "category": "access_control",
          "pattern": "onlyOwner|onlyAdmin",
          "match": "onlyOwner",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 19,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 18,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyOwner",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/sample_test.go",
          "line": 21,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(i)\n\t}\n\tmean := float64(sum) / float64(len(s))\n\tb.ResetTimer()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/metrics/sample_test.go",
          "line": 35,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(i)\n\t}\n\tmean := float64(sum) / float64(len(s))\n\tb.ResetTimer()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/metrics/sample_test.go",
          "line": 117,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= float64(v[i])\n\t}\n\tavg /= float64(len(v))\n\tif avg > 16 || avg < 14 {\n\t\tt.Errorf(\"out of range [14, 16]: %v\\n\", avg)\n\t}\n}\n\nfunc TestExpDecaySampleRescale(t *testing.T) {\n\ts := NewExpDecaySample(2, 0.001).(*ExpDecaySample)\n\ts.update(time.Now(), 1)\n\ts.update(time.Now().Add(time.Hour+time.Microsecond), 1)\n\tfor _, v := range s.values.Values() {\n\t\tif v.k == 0.0 {\n\t\t\tt.Fatal(\"v.k == 0.0\")\n\t\t}\n\t}\n}\n\nfunc TestExpDecaySampleSnapshot(t *testing.T) {\n\tnow := time.Now()\n\ts := NewExpDecaySample(100, 0.99).(*ExpDecaySample).SetRand(rand.New(rand.NewSource(1)))\n\tfor i := 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/metrics/sample_test.go",
          "line": 190,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(v[i])\n\t}\n\tif exp != sum {\n\t\tt.Errorf(\"sum: %v != %v\\n\", exp, sum)\n\t}\n}\n\nfunc TestUniformSampleSnapshot(t *testing.T) {\n\ts := NewUniformSample(100).(*UniformSample).SetRand(rand.New(rand.NewSource(1)))\n\tfor i := 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/resetting_timer.go",
          "line": 68,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(d)\n}\n\n// UpdateSince records the duration of an event that started at a time and ends now.\nfunc (t *ResettingTimer) UpdateSince(ts time.Time) {\n\tt.Update(time.Since(ts))\n}\n\n// ResettingTimerSnapshot is a point-in-time copy of another ResettingTimer.\ntype ResettingTimerSnapshot struct {\n\tvalues              []int64\n\tmean                float64\n\tmax                 int64\n\tmin                 int64\n\tthresholdBoundaries []float64\n\tcalculated          bool\n}\n\n// Count return the length of the values from snapshot.\nfunc (t *ResettingTimerSnapshot) Count() int {\n\treturn len(t.values)\n}\n\n// Percentiles returns the boundaries for the input percentiles.\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Percentiles(percentiles []float64) []float64 {\n\tt.calc(percentiles)\n\treturn t.thresholdBoundaries\n}\n\n// Mean returns the mean of the snapshotted values\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Mean() float64 {\n\tif !t.calculated {\n\t\tt.calc(nil)\n\t}\n\n\treturn t.mean\n}\n\n// Max returns the max of the snapshotted values\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Max() int64 {\n\tif !t.calculated {\n\t\tt.calc(nil)\n\t}\n\treturn t.max\n}\n\n// Min returns the min of the snapshotted values\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Min() int64 {\n\tif !t.calculated {\n\t\tt.calc(nil)\n\t}\n\treturn t.min\n}\n\nfunc (t *ResettingTimerSnapshot) calc(percentiles []float64) {\n\tscores := CalculatePercentiles(t.values, percentiles)\n\tt.thresholdBoundaries = scores\n\tif len(t.values) == 0 {\n\t\treturn\n\t}\n\tt.min = t.values[0]\n\tt.max = t.values[len(t.values)-1]\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/sample.go",
          "line": 67,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= v\n\t\tif v > max {\n\t\t\tmax = v\n\t\t}\n\t\tif v < min {\n\t\t\tmin = v\n\t\t}\n\t}\n\treturn newSampleSnapshotPrecalculated(count, values, min, max, sum)\n}\n\n// Count returns the count of inputs at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Count() int64 { return s.count }\n\n// Max returns the maximal value at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Max() int64 { return s.max }\n\n// Mean returns the mean value at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Mean() float64 { return s.mean }\n\n// Min returns the minimal value at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Min() int64 { return s.min }\n\n// Percentile returns an arbitrary percentile of values at the time the\n// snapshot was taken.\nfunc (s *sampleSnapshot) Percentile(p float64) float64 {\n\treturn SamplePercentile(s.values, p)\n}\n\n// Percentiles returns a slice of arbitrary percentiles of values at the time\n// the snapshot was taken.\nfunc (s *sampleSnapshot) Percentiles(ps []float64) []float64 {\n\treturn CalculatePercentiles(s.values, ps)\n}\n\n// Size returns the size of the sample at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Size() int { return len(s.values) }\n\n// StdDev returns the standard deviation of values at the time the snapshot was\n// taken.\nfunc (s *sampleSnapshot) StdDev() float64 {\n\tif s.variance == 0.0 {\n\t\ts.variance = SampleVariance(s.mean, s.values)\n\t}\n\treturn math.Sqrt(s.variance)\n}\n\n// Sum returns the sum of values at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Sum() int64 { return s.sum }\n\n// Values returns a copy of the values in the sample.\nfunc (s *sampleSnapshot) Values() []int64 {\n\treturn slices.Clone(s.values)\n}\n\n// Variance returns the variance of values at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Variance() float64 {\n\tif s.variance == 0.0 {\n\t\ts.variance = SampleVariance(s.mean, s.values)\n\t}\n\treturn s.variance\n}\n\n// ExpDecaySample is an exponentially-decaying sample using a forward-decaying\n// priority reservoir.  See Cormode et al's \"Forward Decay: A Practical Time\n// Decay Model for Streaming Systems\".\n//\n// <http://dimacs.rutgers.edu/~graham/pubs/papers/fwddecay.pdf>\ntype ExpDecaySample struct {\n\talpha         float64\n\tcount         int64\n\tmutex         sync.Mutex\n\treservoirSize int\n\tt0, t1        time.Time\n\tvalues        *expDecaySampleHeap\n\trand          *rand.Rand\n}\n\n// NewExpDecaySample constructs a new exponentially-decaying sample with the\n// given reservoir size and alpha.\nfunc NewExpDecaySample(reservoirSize int, alpha float64) Sample {\n\ts := &ExpDecaySample{\n\t\talpha:         alpha,\n\t\treservoirSize: reservoirSize,\n\t\tt0:            time.Now(),\n\t\tvalues:        newExpDecaySampleHeap(reservoirSize),\n\t}\n\ts.t1 = s.t0.Add(rescaleThreshold)\n\treturn s\n}\n\n// SetRand sets the random source (useful in tests)\nfunc (s *ExpDecaySample) SetRand(prng *rand.Rand) Sample {\n\ts.rand = prng\n\treturn s\n}\n\n// Clear clears all samples.\nfunc (s *ExpDecaySample) Clear() {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count = 0\n\ts.t0 = time.Now()\n\ts.t1 = s.t0.Add(rescaleThreshold)\n\ts.values.Clear()\n}\n\n// Snapshot returns a read-only copy of the sample.\nfunc (s *ExpDecaySample) Snapshot() *sampleSnapshot {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\tvar (\n\t\tsamples       = s.values.Values()\n\t\tvalues        = make([]int64, len(samples))\n\t\tmax     int64 = math.MinInt64\n\t\tmin     int64 = math.MaxInt64\n\t\tsum     int64\n\t)\n\tfor i, item := range samples {\n\t\tv := item.v\n\t\tvalues[i] = v\n\t\tsum += v\n\t\tif v > max {\n\t\t\tmax = v\n\t\t}\n\t\tif v < min {\n\t\t\tmin = v\n\t\t}\n\t}\n\treturn newSampleSnapshotPrecalculated(s.count, values, min, max, sum)\n}\n\n// Update samples a new value.\nfunc (s *ExpDecaySample) Update(v int64) {\n\tif !metricsEnabled {\n\t\treturn\n\t}\n\ts.update(time.Now(), v)\n}\n\n// update samples a new value at a particular timestamp.  This is a method all\n// its own to facilitate testing.\nfunc (s *ExpDecaySample) update(t time.Time, v int64) {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count++\n\tif s.values.Size() == s.reservoirSize {\n\t\ts.values.Pop()\n\t}\n\tvar f64 float64\n\tif s.rand != nil {\n\t\tf64 = s.rand.Float64()\n\t} else {\n\t\tf64 = rand.Float64()\n\t}\n\ts.values.Push(expDecaySample{\n\t\tk: math.Exp(t.Sub(s.t0).Seconds()*s.alpha) / f64,\n\t\tv: v,\n\t})\n\tif t.After(s.t1) {\n\t\tvalues := s.values.Values()\n\t\tt0 := s.t0\n\t\ts.values.Clear()\n\t\ts.t0 = t\n\t\ts.t1 = s.t0.Add(rescaleThreshold)\n\t\tfor _, v := range values {\n\t\t\tv.k = v.k * math.Exp(-s.alpha*s.t0.Sub(t0).Seconds())\n\t\t\ts.values.Push(v)\n\t\t}\n\t}\n}\n\n// SamplePercentile returns an arbitrary percentile of the slice of int64.\nfunc SamplePercentile(values []int64, p float64) float64 {\n\treturn CalculatePercentiles(values, []float64{p})[0]\n}\n\n// CalculatePercentiles returns a slice of arbitrary percentiles of the slice of\n// int64. This method returns interpolated results, so e.g. if there are only two\n// values, [0, 10], a 50% percentile will land between them.\n//\n// Note: As a side-effect, this method will also sort the slice of values.\n// Note2: The input format for percentiles is NOT percent! To express 50%, use 0.5, not 50.\nfunc CalculatePercentiles(values []int64, ps []float64) []float64 {\n\tscores := make([]float64, len(ps))\n\tsize := len(values)\n\tif size == 0 {\n\t\treturn scores\n\t}\n\tslices.Sort(values)\n\tfor i, p := range ps {\n\t\tpos := p * float64(size+1)\n\n\t\tif pos < 1.0 {\n\t\t\tscores[i] = float64(values[0])\n\t\t} else if pos >= float64(size) {\n\t\t\tscores[i] = float64(values[size-1])\n\t\t} else {\n\t\t\tlower := float64(values[int(pos)-1])\n\t\t\tupper := float64(values[int(pos)])\n\t\t\tscores[i] = lower + (pos-math.Floor(pos))*(upper-lower)\n\t\t}\n\t}\n\treturn scores\n}\n\n// SampleVariance returns the variance of the slice of int64.\nfunc SampleVariance(mean float64, values []int64) float64 {\n\tif len(values) == 0 {\n\t\treturn 0.0\n\t}\n\tvar sum float64\n\tfor _, v := range values {\n\t\td := float64(v) - mean\n\t\tsum += d * d\n\t}\n\treturn sum / float64(len(values))\n}\n\n// UniformSample implements a uniform sample using Vitter's Algorithm R.\n//\n// <http://www.cs.umd.edu/~samir/498/vitter.pdf>\ntype UniformSample struct {\n\tcount         int64\n\tmutex         sync.Mutex\n\treservoirSize int\n\tvalues        []int64\n\trand          *rand.Rand\n}\n\n// NewUniformSample constructs a new uniform sample with the given reservoir\n// size.\nfunc NewUniformSample(reservoirSize int) Sample {\n\treturn &UniformSample{\n\t\treservoirSize: reservoirSize,\n\t\tvalues:        make([]int64, 0, reservoirSize),\n\t}\n}\n\n// SetRand sets the random source (useful in tests)\nfunc (s *UniformSample) SetRand(prng *rand.Rand) Sample {\n\ts.rand = prng\n\treturn s\n}\n\n// Clear clears all samples.\nfunc (s *UniformSample) Clear() {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count = 0\n\tclear(s.values)\n}\n\n// Snapshot returns a read-only copy of the sample.\nfunc (s *UniformSample) Snapshot() *sampleSnapshot {\n\ts.mutex.Lock()\n\tvalues := slices.Clone(s.values)\n\tcount := s.count\n\ts.mutex.Unlock()\n\treturn newSampleSnapshot(count, values)\n}\n\n// Update samples a new value.\nfunc (s *UniformSample) Update(v int64) {\n\tif !metricsEnabled {\n\t\treturn\n\t}\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count++\n\tif len(s.values) < s.reservoirSize {\n\t\ts.values = append(s.values, v)\n\t\treturn\n\t}\n\tvar r int64\n\tif s.rand != nil {\n\t\tr = s.rand.Int63n(s.count)\n\t} else {\n\t\tr = rand.Int63n(s.count)\n\t}\n\tif r < int64(len(s.values)) {\n\t\ts.values[int(r)] = v\n\t}\n}\n\n// expDecaySample represents an individual sample in a heap.\ntype expDecaySample struct {\n\tk float64\n\tv int64\n}\n\nfunc newExpDecaySampleHeap(reservoirSize int) *expDecaySampleHeap {\n\treturn &expDecaySampleHeap{make([]expDecaySample, 0, reservoirSize)}\n}\n\n// expDecaySampleHeap is a min-heap of expDecaySamples.\n// The internal implementation is copied from the standard library's container/heap\ntype expDecaySampleHeap struct {\n\ts []expDecaySample\n}\n\nfunc (h *expDecaySampleHeap) Clear() {\n\th.s = h.s[:0]\n}\n\nfunc (h *expDecaySampleHeap) Push(s expDecaySample) {\n\tn := len(h.s)\n\th.s = h.s[0 : n+1]\n\th.s[n] = s\n\th.up(n)\n}\n\nfunc (h *expDecaySampleHeap) Pop() expDecaySample {\n\tn := len(h.s) - 1\n\th.s[0], h.s[n] = h.s[n], h.s[0]\n\th.down(0, n)\n\n\tn = len(h.s)\n\ts := h.s[n-1]\n\th.s = h.s[0 : n-1]\n\treturn s\n}\n\nfunc (h *expDecaySampleHeap) Size() int {\n\treturn len(h.s)\n}\n\nfunc (h *expDecaySampleHeap) Values() []expDecaySample {\n\treturn h.s\n}\n\nfunc (h *expDecaySampleHeap) up(j int) {\n\tfor {\n\t\ti := (j - 1) / 2 // parent\n\t\tif i == j || !(h.s[j].k < h.s[i].k) {\n\t\t\tbreak\n\t\t}\n\t\th.s[i], h.s[j] = h.s[j], h.s[i]\n\t\tj = i\n\t}\n}\n\nfunc (h *expDecaySampleHeap) down(i, n int) {\n\tfor {\n\t\tj1 := 2*i + 1\n\t\tif j1 >= n || j1 < 0 { // j1 < 0 after int overflow\n\t\t\tbreak\n\t\t}\n\t\tj := j1 // left child\n\t\tif j2 := j1 + 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/ewma.go",
          "line": 84,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a.alpha * (instantRate - currentRate)\n\ta.rate.Store(math.Float64bits(currentRate))\n}\n\n// Update adds n uncounted events.\nfunc (a *EWMA) Update(n int64) {\n\ta.uncounted.Add(n)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/runtimehistogram.go",
          "line": 101,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(c)\n\t\tsum += h.midpoint(i) * float64(c)\n\t\t// Set max on every iteration\n\t\tedge := h.internal.Buckets[i+1]\n\t\tif math.IsInf(edge, 1) {\n\t\t\tedge = h.internal.Buckets[i]\n\t\t}\n\t\tif edge > max {\n\t\t\tmax = edge\n\t\t}\n\t}\n\th.min = min\n\th.max = int64(max)\n\th.mean = sum / float64(count)\n\th.count = count\n}\n\n// Count returns the sample count.\nfunc (h *runtimeHistogramSnapshot) Count() int64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.count\n}\n\n// Size returns the size of the sample at the time the snapshot was taken.\nfunc (h *runtimeHistogramSnapshot) Size() int {\n\treturn len(h.internal.Counts)\n}\n\n// Mean returns an approximation of the mean.\nfunc (h *runtimeHistogramSnapshot) Mean() float64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.mean\n}\n\nfunc (h *runtimeHistogramSnapshot) midpoint(bucket int) float64 {\n\thigh := h.internal.Buckets[bucket+1]\n\tlow := h.internal.Buckets[bucket]\n\tif math.IsInf(high, 1) {\n\t\t// The edge of the highest bucket can be +Inf, and it's supposed to mean that this\n\t\t// bucket contains all remaining samples > low. We can't get the middle of an\n\t\t// infinite range, so just return the lower bound of this bucket instead.\n\t\treturn low\n\t}\n\tif math.IsInf(low, -1) {\n\t\t// Similarly, we can get -Inf in the left edge of the lowest bucket,\n\t\t// and it means the bucket contains all remaining values < high.\n\t\treturn high\n\t}\n\treturn (low + high) / 2\n}\n\n// StdDev approximates the standard deviation of the histogram.\nfunc (h *runtimeHistogramSnapshot) StdDev() float64 {\n\treturn math.Sqrt(h.Variance())\n}\n\n// Variance approximates the variance of the histogram.\nfunc (h *runtimeHistogramSnapshot) Variance() float64 {\n\tif len(h.internal.Counts) == 0 {\n\t\treturn 0\n\t}\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\tif h.count <= 1 {\n\t\t// There is no variance when there are zero or one items.\n\t\treturn 0\n\t}\n\t// Variance is not calculated in 'calc', because it requires a second iteration.\n\t// Therefore we calculate it lazily in this method, triggered either by\n\t// a direct call to Variance or via StdDev.\n\tif h.variance != 0.0 {\n\t\treturn h.variance\n\t}\n\tvar sum float64\n\n\tfor i, c := range h.internal.Counts {\n\t\tmidpoint := h.midpoint(i)\n\t\td := midpoint - h.mean\n\t\tsum += float64(c) * (d * d)\n\t}\n\th.variance = sum / float64(h.count-1)\n\treturn h.variance\n}\n\n// Percentile computes the p'th percentile value.\nfunc (h *runtimeHistogramSnapshot) Percentile(p float64) float64 {\n\tthreshold := float64(h.Count()) * p\n\tvalues := [1]float64{threshold}\n\th.computePercentiles(values[:])\n\treturn values[0]\n}\n\n// Percentiles computes all requested percentile values.\nfunc (h *runtimeHistogramSnapshot) Percentiles(ps []float64) []float64 {\n\t// Compute threshold values. We need these to be sorted\n\t// for the percentile computation, but restore the original\n\t// order later, so keep the indexes as well.\n\tcount := float64(h.Count())\n\tthresholds := make([]float64, len(ps))\n\tindexes := make([]int, len(ps))\n\tfor i, percentile := range ps {\n\t\tthresholds[i] = count * math.Max(0, math.Min(1.0, percentile))\n\t\tindexes[i] = i\n\t}\n\tsort.Sort(floatsAscendingKeepingIndex{thresholds, indexes})\n\n\t// Now compute. The result is stored back into the thresholds slice.\n\th.computePercentiles(thresholds)\n\n\t// Put the result back into the requested order.\n\tsort.Sort(floatsByIndex{thresholds, indexes})\n\treturn thresholds\n}\n\nfunc (h *runtimeHistogramSnapshot) computePercentiles(thresh []float64) {\n\tvar totalCount float64\n\tfor i, count := range h.internal.Counts {\n\t\ttotalCount += float64(count)\n\n\t\tfor len(thresh) > 0 && thresh[0] < totalCount {\n\t\t\tthresh[0] = h.internal.Buckets[i]\n\t\t\tthresh = thresh[1:]\n\t\t}\n\t\tif len(thresh) == 0 {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Note: runtime/metrics.Float64Histogram is a collection of float64s, but the methods\n// below need to return int64 to satisfy the interface. The histogram provided by runtime\n// also doesn't keep track of individual samples, so results are approximated.\n\n// Max returns the highest sample value.\nfunc (h *runtimeHistogramSnapshot) Max() int64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.max\n}\n\n// Min returns the lowest sample value.\nfunc (h *runtimeHistogramSnapshot) Min() int64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.min\n}\n\n// Sum returns the sum of all sample values.\nfunc (h *runtimeHistogramSnapshot) Sum() int64 {\n\tvar sum float64\n\tfor i := range h.internal.Counts {\n\t\tsum += h.internal.Buckets[i] * float64(h.internal.Counts[i])\n\t}\n\treturn int64(math.Ceil(sum))\n}\n\ntype floatsAscendingKeepingIndex struct {\n\tvalues  []float64\n\tindexes []int\n}\n\nfunc (s floatsAscendingKeepingIndex) Len() int {\n\treturn len(s.values)\n}\n\nfunc (s floatsAscendingKeepingIndex) Less(i, j int) bool {\n\treturn s.values[i] < s.values[j]\n}\n\nfunc (s floatsAscendingKeepingIndex) Swap(i, j int) {\n\ts.values[i], s.values[j] = s.values[j], s.values[i]\n\ts.indexes[i], s.indexes[j] = s.indexes[j], s.indexes[i]\n}\n\ntype floatsByIndex struct {\n\tvalues  []float64\n\tindexes []int\n}\n\nfunc (s floatsByIndex) Len() int {\n\treturn len(s.values)\n}\n\nfunc (s floatsByIndex) Less(i, j int) bool {\n\treturn s.indexes[i] < s.indexes[j]\n}\n\nfunc (s floatsByIndex) Swap(i, j int) {\n\ts.values[i], s.values[j] = s.values[j], s.values[i]\n\ts.indexes[i], s.indexes[j] = s.indexes[j], s.indexes[i]\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/sender_cacher.go",
          "line": 73,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= task.inc {\n\t\t\ttypes.Sender(task.signer, task.txs[i])\n\t\t}\n\t}\n}\n\n// Recover recovers the senders from a batch of transactions and caches them\n// back into the same data structures. There is no validation being done, nor\n// any reaction to invalid signatures. That is up to calling code later.\nfunc (cacher *txSenderCacher) Recover(signer types.Signer, txs []*types.Transaction) {\n\t// If there's nothing to recover, abort\n\tif len(txs) == 0 {\n\t\treturn\n\t}\n\t// Ensure we have meaningful task sizes and schedule the recoveries\n\ttasks := cacher.threads\n\tif len(txs) < tasks*4 {\n\t\ttasks = (len(txs) + 3) / 4\n\t}\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/sender_cacher.go",
          "line": 107,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(block.Transactions())\n\t}\n\ttxs := make([]*types.Transaction, 0, count)\n\tfor _, block := range blocks {\n\t\ttxs = append(txs, block.Transactions()...)\n\t}\n\tcacher.Recover(signer, txs)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state_processor.go",
          "line": 229,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= result.UsedGas\n\n\t// Merge the tx-local access event into the \"block-local\" one, in order to collect\n\t// all values, so that the witness can be built.\n\tif statedb.Database().TrieDB().IsVerkle() {\n\t\tstatedb.AccessEvents().Merge(evm.AccessEvents)\n\t}\n\n\treturn MakeReceipt(evm, result, statedb, blockNumber, blockHash, blockTime, tx, *usedGas, root, receiptProcessors...), nil\n}\n\n// MakeReceipt generates the receipt object for a transaction given its execution result.\nfunc MakeReceipt(evm *vm.EVM, result *ExecutionResult, statedb *state.StateDB, blockNumber *big.Int, blockHash common.Hash, blockTime uint64, tx *types.Transaction, usedGas uint64, root []byte, receiptProcessors ...ReceiptProcessor) *types.Receipt {\n\t// Create a new receipt for the transaction, storing the intermediate root and gas used\n\t// by the tx.\n\treceipt := &types.Receipt{Type: tx.Type(), PostState: root, CumulativeGasUsed: usedGas}\n\tif result.Failed() {\n\t\treceipt.Status = types.ReceiptStatusFailed\n\t} else {\n\t\treceipt.Status = types.ReceiptStatusSuccessful\n\t}\n\treceipt.TxHash = tx.Hash()\n\treceipt.GasUsed = result.UsedGas\n\n\tif tx.Type() == types.BlobTxType {\n\t\treceipt.BlobGasUsed = uint64(len(tx.BlobHashes()) * params.BlobTxBlobGasPerBlob)\n\t\treceipt.BlobGasPrice = evm.Context.BlobBaseFee\n\t}\n\n\t// If the transaction created a contract, store the creation address in the receipt.\n\tif tx.To() == nil {\n\t\treceipt.ContractAddress = crypto.CreateAddress(evm.TxContext.Origin, tx.Nonce())\n\t}\n\n\t// Set the receipt logs and create the bloom filter.\n\treceipt.Logs = statedb.GetLogs(tx.Hash(), blockNumber.Uint64(), blockHash, blockTime)\n\treceipt.BlockHash = blockHash\n\treceipt.BlockNumber = blockNumber\n\treceipt.TransactionIndex = uint(statedb.TxIndex())\n\tfor _, receiptProcessor := range receiptProcessors {\n\t\treceiptProcessor.Apply(receipt)\n\t}\n\treturn receipt\n}\n\n// ApplyTransaction attempts to apply a transaction to the given state database\n// and uses the input parameters for its environment. It returns the receipt\n// for the transaction, gas used and an error if the transaction failed,\n// indicating the block was invalid.\nfunc ApplyTransaction(evm *vm.EVM, gp *GasPool, statedb *state.StateDB, header *types.Header, tx *types.Transaction, usedGas *uint64, receiptProcessors ...ReceiptProcessor) (*types.Receipt, error) {\n\tmsg, err := TransactionToMessage(tx, types.MakeSigner(evm.ChainConfig(), header.Number, header.Time), header.BaseFee)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create a new context to be used in the EVM environment\n\treturn ApplyTransactionWithEVM(msg, gp, statedb, header.Number, header.Hash(), header.Time, tx, usedGas, evm, receiptProcessors...)\n}\n\n// ProcessBeaconBlockRoot applies the EIP-4788 system call to the beacon block root\n// contract. This method is exported to be used in tests.\nfunc ProcessBeaconBlockRoot(beaconRoot common.Hash, evm *vm.EVM) {\n\t// Return immediately if beaconRoot equals the zero hash when using the Parlia engine.\n\tif beaconRoot == (common.Hash{}) {\n\t\tif chainConfig := evm.ChainConfig()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state_processor.go",
          "line": 313,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/state_processor.go",
          "line": 337,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/state_processor.go",
          "line": 376,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state_transition.go",
          "line": 97,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nz * nonZeroGas\n\n\t\tif (math.MaxUint64-gas)/params.TxDataZeroGas < z {\n\t\t\treturn 0, ErrGasUintOverflow\n\t\t}\n\t\tgas += z * params.TxDataZeroGas\n\n\t\tif isContractCreation && isEIP3860 {\n\t\t\tlenWords := toWordSize(dataLen)\n\t\t\tif (math.MaxUint64-gas)/params.InitCodeWordGas < lenWords {\n\t\t\t\treturn 0, ErrGasUintOverflow\n\t\t\t}\n\t\t\tgas += lenWords * params.InitCodeWordGas\n\t\t}\n\t}\n\tif accessList != nil {\n\t\tgas += uint64(len(accessList)) * params.TxAccessListAddressGas\n\t\tgas += uint64(accessList.StorageKeys()) * params.TxAccessListStorageKeyGas\n\t}\n\tif authList != nil {\n\t\tgas += uint64(len(authList)) * params.CallNewAccountGas\n\t}\n\treturn gas, nil\n}\n\n// FloorDataGas computes the minimum gas required for a transaction based on its data tokens (EIP-7623).\nfunc FloorDataGas(data []byte) (uint64, error) {\n\tvar (\n\t\tz      = uint64(bytes.Count(data, []byte{0}))\n\t\tnz     = uint64(len(data)) - z\n\t\ttokens = nz*params.TxTokenPerNonZeroByte + z\n\t)\n\t// Check for overflow\n\tif (math.MaxUint64-params.TxGas)/params.TxCostFloorPerToken < tokens {\n\t\treturn 0, ErrGasUintOverflow\n\t}\n\t// Minimum gas required for a transaction based on its data tokens (EIP-7623).\n\treturn params.TxGas + tokens*params.TxCostFloorPerToken, nil\n}\n\n// toWordSize returns the ceiled word size required for init code payment calculation.\nfunc toWordSize(size uint64) uint64 {\n\tif size > math.MaxUint64-31 {\n\t\treturn math.MaxUint64/32 + 1\n\t}\n\n\treturn (size + 31) / 32\n}\n\n// A Message contains the data derived from a single transaction that is relevant to state\n// processing.\ntype Message struct {\n\tTo                    *common.Address\n\tFrom                  common.Address\n\tNonce                 uint64\n\tValue                 *big.Int\n\tGasLimit              uint64\n\tGasPrice              *big.Int\n\tGasFeeCap             *big.Int\n\tGasTipCap             *big.Int\n\tData                  []byte\n\tAccessList            types.AccessList\n\tBlobGasFeeCap         *big.Int\n\tBlobHashes            []common.Hash\n\tSetCodeAuthorizations []types.SetCodeAuthorization\n\n\t// When SkipNonceChecks is true, the message nonce is not checked against the\n\t// account nonce in state.\n\t//\n\t// This field will be set to true for operations like RPC eth_call\n\t// or the state prefetching.\n\tSkipNonceChecks bool\n\n\t// When SkipFromEOACheck is true, the message sender is not checked to be an EOA.\n\tSkipFromEOACheck bool\n}\n\n// TransactionToMessage converts a transaction into a Message.\nfunc TransactionToMessage(tx *types.Transaction, s types.Signer, baseFee *big.Int) (*Message, error) {\n\tmsg := &Message{\n\t\tNonce:                 tx.Nonce(),\n\t\tGasLimit:              tx.Gas(),\n\t\tGasPrice:              new(big.Int).Set(tx.GasPrice()),\n\t\tGasFeeCap:             new(big.Int).Set(tx.GasFeeCap()),\n\t\tGasTipCap:             new(big.Int).Set(tx.GasTipCap()),\n\t\tTo:                    tx.To(),\n\t\tValue:                 tx.Value(),\n\t\tData:                  tx.Data(),\n\t\tAccessList:            tx.AccessList(),\n\t\tSetCodeAuthorizations: tx.SetCodeAuthorizations(),\n\t\tSkipNonceChecks:       false,\n\t\tSkipFromEOACheck:      false,\n\t\tBlobHashes:            tx.BlobHashes(),\n\t\tBlobGasFeeCap:         tx.BlobGasFeeCap(),\n\t}\n\t// If baseFee provided, set gasPrice to effectiveGasPrice.\n\tif baseFee != nil {\n\t\tmsg.GasPrice = msg.GasPrice.Add(msg.GasTipCap, baseFee)\n\t\tif msg.GasPrice.Cmp(msg.GasFeeCap) > 0 {\n\t\t\tmsg.GasPrice = msg.GasFeeCap\n\t\t}\n\t}\n\tvar err error\n\tmsg.From, err = types.Sender(s, tx)\n\treturn msg, err\n}\n\n// ApplyMessage computes the new state by applying the given message\n// against the old state within the environment.\n//\n// ApplyMessage returns the bytes returned by any EVM execution (if it took place),\n// the gas used (which includes gas refunds) and an error if it failed. An error always\n// indicates a core error meaning that the message would always fail for that particular\n// state and would never be accepted within a block.\nfunc ApplyMessage(evm *vm.EVM, msg *Message, gp *GasPool) (*ExecutionResult, error) {\n\tevm.SetTxContext(NewEVMTxContext(msg))\n\treturn newStateTransition(evm, msg, gp).execute()\n}\n\n// stateTransition represents a state transition.\n//\n// == The State Transitioning Model\n//\n// A state transition is a change made when a transaction is applied to the current world\n// state. The state transitioning model does all the necessary work to work out a valid new\n// state root.\n//\n//  1. Nonce handling\n//  2. Pre pay gas\n//  3. Create a new state object if the recipient is nil\n//  4. Value transfer\n//\n// == If contract creation ==\n//\n//\t4a. Attempt to run transaction data\n//\t4b. If valid, use result as code for the new state object\n//\n// == end ==\n//\n//  5. Run Script section\n//  6. Derive new state root\ntype stateTransition struct {\n\tgp           *GasPool\n\tmsg          *Message\n\tgasRemaining uint64\n\tinitialGas   uint64\n\tstate        vm.StateDB\n\tevm          *vm.EVM\n}\n\n// newStateTransition initialises and returns a new state transition object.\nfunc newStateTransition(evm *vm.EVM, msg *Message, gp *GasPool) *stateTransition {\n\treturn &stateTransition{\n\t\tgp:    gp,\n\t\tevm:   evm,\n\t\tmsg:   msg,\n\t\tstate: evm.StateDB,\n\t}\n}\n\n// to returns the recipient of the message.\nfunc (st *stateTransition) to() common.Address {\n\tif st.msg == nil || st.msg.To == nil /* contract creation */ {\n\t\treturn common.Address{}\n\t}\n\treturn *st.msg.To\n}\n\nfunc (st *stateTransition) buyGas() error {\n\tmgval := new(big.Int).SetUint64(st.msg.GasLimit)\n\tmgval.Mul(mgval, st.msg.GasPrice)\n\tbalanceCheck := new(big.Int).Set(mgval)\n\tif st.msg.GasFeeCap != nil {\n\t\tbalanceCheck.SetUint64(st.msg.GasLimit)\n\t\tbalanceCheck = balanceCheck.Mul(balanceCheck, st.msg.GasFeeCap)\n\t}\n\tbalanceCheck.Add(balanceCheck, st.msg.Value)\n\n\tif st.evm.ChainConfig().IsCancun(st.evm.Context.BlockNumber, st.evm.Context.Time) {\n\t\tif blobGas := st.blobGasUsed()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state_transition.go",
          "line": 531,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= st.calcRefund()\n\tif rules.IsPrague {\n\t\t// After EIP-7623: Data-heavy transactions pay the floor gas.\n\t\tif st.gasUsed() < floorDataGas {\n\t\t\tprev := st.gasRemaining\n\t\t\tst.gasRemaining = st.initialGas - floorDataGas\n\t\t\tif t := st.evm.Config.Tracer",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/state_transition.go",
          "line": 466,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas\n\n\tif rules.IsEIP4762 {\n\t\tst.evm.AccessEvents.AddTxOrigin(msg.From)\n\n\t\tif targetAddr := msg.To",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/state_transition.go",
          "line": 523,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/chain_makers.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= receipt.BlobGasUsed\n\t}\n}\n\n// AddTx adds a transaction to the generated block. If no coinbase has\n// been set, the block's coinbase is set to the zero address.\n//\n// AddTx panics if the transaction cannot be executed. In addition to the protocol-imposed\n// limitations (gas limit, etc.), there are some further limitations on the content of\n// transactions that can be added. Notably, contract code relying on the BLOCKHASH\n// instruction will panic during execution if it attempts to access a block number outside\n// of the range created by GenerateChain.\nfunc (b *BlockGen) AddTx(tx *types.Transaction) {\n\t// Wrap the chain config in an empty BlockChain object to satisfy ChainContext.\n\tbc := &BlockChain{chainConfig: b.cm.config}\n\tb.addTx(bc, vm.Config{}, tx)\n}\n\n// AddTxWithChain adds a transaction to the generated block. If no coinbase has\n// been set, the block's coinbase is set to the zero address.\n//\n// AddTxWithChain panics if the transaction cannot be executed. In addition to the\n// protocol-imposed limitations (gas limit, etc.), there are some further limitations on\n// the content of transactions that can be added. If contract code relies on the BLOCKHASH\n// instruction, the block in chain will be returned.\nfunc (b *BlockGen) AddTxWithChain(bc *BlockChain, tx *types.Transaction) {\n\tb.addTx(bc, vm.Config{}, tx)\n}\n\n// AddTxWithVMConfig adds a transaction to the generated block. If no coinbase has\n// been set, the block's coinbase is set to the zero address.\n// The evm interpreter can be customized with the provided vm config.\nfunc (b *BlockGen) AddTxWithVMConfig(tx *types.Transaction, config vm.Config) {\n\tb.addTx(nil, config, tx)\n}\n\n// GetBalance returns the balance of the given address at the generated block.\nfunc (b *BlockGen) GetBalance(addr common.Address) *uint256.Int {\n\treturn b.statedb.GetBalance(addr)\n}\n\n// AddUncheckedTx forcefully adds a transaction to the block without any validation.\n//\n// AddUncheckedTx will cause consensus failures when used during real\n// chain processing. This is best used in conjunction with raw block insertion.\nfunc (b *BlockGen) AddUncheckedTx(tx *types.Transaction) {\n\tb.txs = append(b.txs, tx)\n}\n\n// AddBlobSidecar add block's blob sidecar for DA checking.\nfunc (b *BlockGen) AddBlobSidecar(sidecar *types.BlobSidecar) {\n\tb.sidecars = append(b.sidecars, sidecar)\n}\n\nfunc (b *BlockGen) HeadBlock() *types.Header {\n\treturn b.header\n}\n\n// Number returns the block number of the block being generated.\nfunc (b *BlockGen) Number() *big.Int {\n\treturn new(big.Int).Set(b.header.Number)\n}\n\n// Timestamp returns the timestamp of the block being generated.\nfunc (b *BlockGen) Timestamp() uint64 {\n\treturn b.header.Time\n}\n\n// BaseFee returns the EIP-1559 base fee of the block being generated.\nfunc (b *BlockGen) BaseFee() *big.Int {\n\treturn new(big.Int).Set(b.header.BaseFee)\n}\n\n// ExcessBlobGas returns the EIP-4844 ExcessBlobGas of the block.\nfunc (b *BlockGen) ExcessBlobGas() uint64 {\n\texcessBlobGas := b.header.ExcessBlobGas\n\tif excessBlobGas == nil {\n\t\treturn 0\n\t}\n\treturn *excessBlobGas\n}\n\n// Gas returns the amount of gas left in the current block.\nfunc (b *BlockGen) Gas() uint64 {\n\treturn b.header.GasLimit - b.header.GasUsed\n}\n\n// Signer returns a valid signer instance for the current block.\nfunc (b *BlockGen) Signer() types.Signer {\n\treturn types.MakeSigner(b.cm.config, b.header.Number, b.header.Time)\n}\n\n// AddUncheckedReceipt forcefully adds a receipts to the block without a\n// backing transaction.\n//\n// AddUncheckedReceipt will cause consensus failures when used during real\n// chain processing. This is best used in conjunction with raw block insertion.\nfunc (b *BlockGen) AddUncheckedReceipt(receipt *types.Receipt) {\n\tb.receipts = append(b.receipts, receipt)\n}\n\n// TxNonce returns the next valid transaction nonce for the\n// account at addr. It panics if the account does not exist.\nfunc (b *BlockGen) TxNonce(addr common.Address) uint64 {\n\tif !b.statedb.Exist(addr) {\n\t\tpanic(\"account does not exist\")\n\t}\n\treturn b.statedb.GetNonce(addr)\n}\n\n// AddUncle adds an uncle header to the generated block.\nfunc (b *BlockGen) AddUncle(h *types.Header) {\n\t// The uncle will have the same timestamp and auto-generated difficulty\n\th.Time = b.header.Time\n\n\tvar parent *types.Header\n\tfor i := b.i - 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/chain_makers.go",
          "line": 318,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(seconds)\n\tif b.header.Time <= b.cm.bottom.Header().Time {\n\t\tpanic(\"block time out of range\")\n\t}\n\tb.header.Difficulty = b.engine.CalcDifficulty(b.cm, b.header.Time, b.parent.Header())\n}\n\n// ConsensusLayerRequests returns the EIP-7685 requests which have accumulated so far.\nfunc (b *BlockGen) ConsensusLayerRequests() [][]byte {\n\treturn b.collectRequests(true)\n}\n\nfunc (b *BlockGen) collectRequests(readonly bool) (requests [][]byte) {\n\tstatedb := b.statedb\n\tif readonly {\n\t\t// The system contracts clear themselves on a system-initiated read.\n\t\t// When reading the requests mid-block, we don't want this behavior, so fork\n\t\t// off the statedb before executing the system calls.\n\t\tstatedb = statedb.Copy()\n\t}\n\n\tif b.cm.config.IsPrague(b.header.Number, b.header.Time) && b.cm.config.Parlia == nil {\n\t\trequests = [][]byte{}\n\t\t// EIP-6110 deposits\n\t\tvar blockLogs []*types.Log\n\t\tfor _, r := range b.receipts {\n\t\t\tblockLogs = append(blockLogs, r.Logs...)\n\t\t}\n\t\tif err := ParseDepositLogs(&requests, blockLogs, b.cm.config)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/block_validator_test.go",
          "line": 138,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(block.Difficulty().Uint64())\n\t\t}\n\t\tpreBlocks = blocks\n\t\tgspec.Config.TerminalTotalDifficulty = big.NewInt(int64(td))\n\t\tpostBlocks, _ = GenerateChain(gspec.Config, preBlocks[len(preBlocks)-1], engine, genDb, 8, nil)\n\t} else {\n\t\tconfig := *params.TestChainConfig\n\t\tgspec = &Genesis{Config: &config}\n\t\tengine = beacon.New(ethash.NewFaker())\n\t\ttd := int(params.GenesisDifficulty.Uint64())\n\t\tgenDb, blocks, _ := GenerateChainWithGenesis(gspec, engine, 8, nil)\n\t\tfor _, block := range blocks {\n\t\t\t// calculate td\n\t\t\ttd += int(block.Difficulty().Uint64())\n\t\t}\n\t\tpreBlocks = blocks\n\t\tgspec.Config.TerminalTotalDifficulty = big.NewInt(int64(td))\n\t\tpostBlocks, _ = GenerateChain(gspec.Config, preBlocks[len(preBlocks)-1], engine, genDb, 8, func(i int, gen *BlockGen) {\n\t\t\tgen.SetPoS()\n\t\t})\n\t}\n\t// Assemble header batch\n\tpreHeaders := make([]*types.Header, len(preBlocks))\n\tfor i, block := range preBlocks {\n\t\tpreHeaders[i] = block.Header()\n\t}\n\tpostHeaders := make([]*types.Header, len(postBlocks))\n\tfor i, block := range postBlocks {\n\t\tpostHeaders[i] = block.Header()\n\t}\n\t// Run the header checker for blocks one-by-one, checking for both valid and invalid nonces\n\tchain, err := NewBlockChain(rawdb.NewMemoryDatabase(), gspec, engine, nil)\n\tdefer chain.Stop()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify the blocks before the merging\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=\\x19f<\\xdbL0\\x9d[O/\\xc2\\u06cf\\x89\\x05k\\xc7^-c\\x10\\x00\\x00\\u07d4W\\xbd\\xdf\\a\\x884\\x00\\x9c\\x89\\u060eb\\x82u\\x9d\\xc4S5\\xb4p\\x89tq|\\xfbh\\x83\\x10\\x00\\x00\\u07d4W\\xbe\\xeaql\\xbd\\x81p\\ns\\xd6\\u007f\\x9f\\xf09R\\x9c-\\x90%\\x89\\n\\u05ce\\xbcZ\\xc6 \\x00\\x00\\u07d4W\\xd02\\xa4=\\x16Nq\\xaa.\\xf3\\xff\\xd8I\\x1b\\nN\\xf1\\xea[\\x89lk\\x93[\\x8b\\xbd@\\x00\\x00\\u07d4W\\xd3\\u07c0O+\\xee\\xe6\\xefS\\xab\\x94\\xcb>\\xe9\\xcfRJ\\x18\\u04c9\\x15Vak\\x96\\x06g\\x00\\x00\\u07d4W\\xd5\\xfd\\x0e=0I3\\x0f\\xfc\\xdc\\xd0 Ei\\x17e{\\xa2\\u0689k\\xf2\\x01\\x95\\xf5T\\xd4\\x00\\x00\\u07d4W\\u0754q\\xcb\\xfa&'\\t\\xf5\\U00106f37t\\xc5\\xf5'\\xb8\\xf8\\x89\\n\\xad\\xec\\x98?\\xcf\\xf4\\x00\\x00\\u07d4W\\xdf#\\xbe\\xbd\\xc6^\\xb7_\\ub732\\xfa\\xd1\\xc0si++\\xaf\\x89\\xd8\\xd7&\\xb7\\x17z\\x80\\x00\\x00\\u07d4X\\x00\\u03410\\x83\\x9e\\x94I]-\\x84\\x15\\xa8\\xea,\\x90\\xe0\\xc5\\u02c9\\n\\u05ce\\xbcZ\\xc6 \\x00\\x00\\xe0\\x94X\\x03\\xe6\\x8b4\\xda\\x12\\x1a\\xef\\b\\xb6\\x02\\xba\\u06ef\\xb4\\xd1$\\x81\\u028a\\x03\\xcf\\xc8.7\\xe9\\xa7@\\x00\\x00\\xe0\\x94X\\x16\\xc2hww\\xb6\\xd7\\u04a2C-Y\\xa4\\x1f\\xa0Y\\xe3\\xa4\\x06\\x8a\\x1cO\\xe4:\\xdb\\n^\\x90\\x00\\x00\\u07d4X\\x1a:\\xf2\\x97\\xef\\xa4Cj)\\xaf\\x00r\\x92\\x9a\\xbf\\x98&\\xf5\\x8b\\x89lk\\x93[\\x8b\\xbd@\\x00\\x00\\xe0\\x94X\\x1b\\x9f\\xd6\\xea\\xe3r\\xf3P\\x1fB\\xeb\\x96\\x19\\xee\\xc8 \\xb7\\x8a\\x84\\x8a\\x04+\\xe2\\xc0\\f\\xa5",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=#{\\x01\\x89K\\xe4\\xe7&{j\\xe0\\x00\\x00\\u07d4\\xa3\\xa9>\\xf9\\xdb\\xea&6&=\\x06\\xd8I/jA\\u0790|\\\"\\x89\\x03@\\xaa\\xd2\\x1b",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=c\\xbf\\xc6\\x02\\x89*Rc\\x91\\xac\\x93v\\x00\\x00\\u07d4\\xb7\\xa7\\xf7|4\\x8f\\x92\\xa9\\xf1\\x10\\fk\\xd8)\\xa8\\xacm\\u007f\\u03d1\\x89b\\xa9\\x92\\xe5:\\n\\xf0\\x00\\x00\\u07d4\\xb7\\xc0w\\x94ft\\xba\\x93A\\xfbLtz]P\\xf5\\xd2\\xdad\\x15\\x8965\\u026d\\xc5\\u07a0\\x00\\x00\\u07d4\\xb7\\xc0\\xd0\\xcc\\vM4-@b\\xba\\xc6$\\xcc\\xc3\\xc7\\f\\xc6\\xda?\\x89\\xd8\\xd7&\\xb7\\x17z\\x80\\x00\\x00\\u07d4\\xb7\\xc9\\xf1+\\x03\\x8esCm\\x17\\xe1\\xc1/\\xfe\\x1a\\xec\\u0373\\xf5\\x8c\\x89\\x1dF\\x01b\\xf5\\x16\\xf0\\x00\\x00\\u07d4\\xb7\\xcck\\x1a\\xcc2\\u0632\\x95\\xdfh\\xed\\x9d^`\\xb8\\xf6L\\xb6{\\x89\\x10CV\\x1a\\x88)0\\x00\\x00\\u07d4\\xb7\\xcehK\\t\\xab\\xdaS8\\x9a\\x87Si\\xf7\\x19X\\xae\\xac",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=\\x8a\\x01EB\\xba\\x12\\xa37\\xc0\\x00\\x00\\xe0\\x94(\\xd7\\xe5\\x86o\\x1d\\x85\\xfd\\x1c\\xeb2\\xbf\\xbe\\x1d\\xfc6\\xdbCEf\\x8a\\x01\\x86B1\\xc6\\x105\\x1c\\x00\\x00\\u0794(\\xd8\\xc3_\\xb7\\xee\\xa6\\\"X!5\\xe3\\xadG\\xa2'\\u0266c\\xbd\\x88\\xfc\\x93c\\x92\\x80\\x1c\\x00\\x00\\u07d4(\\xe4\\xaf0\\u0353\\xf6\\x86\\xa1\\\"\\xad{\\xb1\\x9f\\x8a\\x87\\x85\\xee\\xe3B\\x89q\\xe5",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=\\x88\\xd1A\\xefv\\x9a\\x99\\xbb\\x9e<\\x99Q\\x89\\x05k\\xc7^-c\\x10\\x00\\x00\\xe0\\x94\\xc0\\xa7\\xe8C]\\xff\\x14\\xc2Uws\\x9d\\xb5\\\\$\\u057fW\\xa3\\u064a\\nm\\xd9\\f\\xaeQ\\x14H\\x00\\x00\\u07d4\\xc0\\xae\\x14\\xd7$\\x83./\\xce'x\\xde\\u007f{\\x8d\\xaf{\\x12\\xa9>\\x89\\x01\\x15\\x8eF\\t\\x13\\xd0\\x00\\x00\\u07d4\\xc0\\xaf\\xb7\\u0637\\x93p\\xcf\\xd6c\\u018c\\u01b9p*7\\u035e\\xff\\x8965\\u026d\\xc5\\u07a0\\x00\\x00\\u07d4\\xc0\\xb0\\xb7\\xa8\\xa6\\xe1\\xac\\xdd\\x05\\xe4\\u007f\\x94\\xc0\\x96\\x88\\xaa\\x16\\u01ed\\x8d\\x89\\x03{m\\x02\\xacvq\\x00\\x00\\xe0\\x94\\xc0\\xb3\\xf2D\\xbc\\xa7\\xb7\\xde[H\\xa5>\\u06dc\\xbe\\xab\\vm\\x88\\xc0\\x8a\\x01",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=q5\\u04a8_\\xb5\\xa5q\\u073ei^\\x13\\xfcC\\u034965\\u026d\\xc5\\u07a0\\x00\\x00\\u07d4\\xc7\\x1f\\x1du\\x87?3\\u0732\\xddK9\\x87\\xa1-\\a\\x91\\xa5\\xce'\\x897\\b\\xba\\xed=h\\x90\\x00\\x00\\u07d4\\xc7\\x1f\\x92\\xa3\\xa5J{\\x8c/^\\xa4C\\x05\\xfc\\u02c4\\xee\\xe21H\\x89\\x02\\xb5\\x9c\\xa11\\xd2\\x06\\x00\\x00\\u07d4\\xc7!\\xb2\\xa7\\xaaD\\xc2\\x12\\x98\\xe8P9\\xd0\\x0e.F\\x0eg\\v\\x9c\\x89\\a\\xa1\\xfe\\x16\\x02w\\x00\\x00\\x00\\u07d4\\xc7,\\xb3\\x01%\\x8e\\x91\\xbc\\b\\x99\\x8a\\x80]\\u0452\\xf2\\\\/\\x9a5\\x89 \\t\\xc5\\u023fo\\xdc\\x00\\x00\\xe0\\x94\\xc76\\x8b\\x97\\t\\xa5\\xc1\\xb5\\x1c\\n\\xdf\\x18ze\\xdf\\x14\\xe1+}\\xba\\x8a\\x02\\x02o\\xc7\\u007f\\x03\\u5b80\\x00\\u07d4\\xc79%\\x9e\\u007f\\x85\\xf2e\\x9b\\xef_`\\x9e\\xd8k=Yl \\x1e\\x89\\n\\u05ce\\xbcZ\\xc6 \\x00\\x00\\xe0\\x94\\xc7>!\\x12(\\\"\\x15\\xdc\\ab\\xf3+~\\x80}\\xcd\\x1az\\xae>\\x8a\\x01v\\f\\xbcb",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/gaspool.go",
          "line": 33,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= amount\n\treturn gp\n}\n\n// SubGas deducts the given amount from the pool if enough gas is\n// available and returns an error otherwise.\nfunc (gp *GasPool) SubGas(amount uint64) error {\n\tif uint64(*gp) < amount {\n\t\treturn ErrGasLimitReached\n\t}\n\t*(*uint64)(gp) -= amount\n\treturn nil\n}\n\n// Gas returns the amount of gas remaining in the pool.\nfunc (gp *GasPool) Gas() uint64 {\n\treturn uint64(*gp)\n}\n\n// SetGas sets the amount of gas with the provided number.\nfunc (gp *GasPool) SetGas(gas uint64) {\n\t*(*uint64)(gp) = gas\n}\n\nfunc (gp *GasPool) String() string {\n\treturn fmt.Sprintf(\"%d\", *gp)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/gaspool.go",
          "line": 43,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= amount\n\treturn nil\n}\n\n// Gas returns the amount of gas remaining in the pool.\nfunc (gp *GasPool) Gas() uint64 {\n\treturn uint64(*gp)\n}\n\n// SetGas sets the amount of gas with the provided number.\nfunc (gp *GasPool) SetGas(gas uint64) {\n\t*(*uint64)(gp) = gas\n}\n\nfunc (gp *GasPool) String() string {\n\treturn fmt.Sprintf(\"%d\", *gp)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/data_availability.go",
          "line": 108,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(s.Blobs)\n\t}\n\tmaxBlobPerBlock := eip4844.MaxBlobsPerBlock(chain.Config(), block.Time())\n\tif blobCnt > maxBlobPerBlock {\n\t\treturn fmt.Errorf(\"too many blobs in block: have %d, permitted %d\", blobCnt, maxBlobPerBlock)\n\t}\n\n\t// check blob and versioned hash\n\tfor i, tx := range blobTxs {\n\t\t// check sidecar tx related\n\t\tif sidecars[i].TxHash != tx.Hash() {\n\t\t\treturn fmt.Errorf(\"sidecar's TxHash mismatch with expected transaction, want: %v, have: %v\", sidecars[i].TxHash, tx.Hash())\n\t\t}\n\t\tif sidecars[i].TxIndex != blobTxIndexes[i] {\n\t\t\treturn fmt.Errorf(\"sidecar's TxIndex mismatch with expected transaction, want: %v, have: %v\", sidecars[i].TxIndex, blobTxIndexes[i])\n\t\t}\n\t\tif err := validateBlobSidecar(tx.BlobHashes(), sidecars[i])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/blockchain.go",
          "line": 1658,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeSize\n\t\t\t\tlog.Info(\"Wrote genesis to ancients\")\n\t\t\t}\n\t\t}\n\t\t// Write all chain data to ancients.\n\t\tfirst := blockChain[0]\n\t\tptd := bc.GetTd(first.ParentHash(), first.NumberU64()-1)\n\t\tif ptd == nil {\n\t\t\treturn 0, consensus.ErrUnknownAncestor\n\t\t}\n\t\ttd := new(big.Int).Add(ptd, first.Difficulty())\n\t\twriteSize, err := rawdb.WriteAncientBlocksWithBlobs(bc.db, blockChain, receiptChain, td)\n\t\tif err != nil {\n\t\t\tlog.Error(\"Error importing chain data to ancients\", \"err\", err)\n\t\t\treturn 0, err\n\t\t}\n\t\tsize += writeSize\n\n\t\t// Sync the ancient store explicitly to ensure all data has been flushed to disk.\n\t\tif err := bc.db.SyncAncient()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/blockchain.go",
          "line": 1692,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int32(len(blockChain))\n\t\treturn 0, nil\n\t}\n\n\t// writeLive writes the blockchain and corresponding receipt chain to the active store.\n\t//\n\t// Notably, in different snap sync cycles, the supplied chain may partially reorganize\n\t// existing local chain segments (reorg around the chain tip). The reorganized part\n\t// will be included in the provided chain segment, and stale canonical markers will be\n\t// silently rewritten. Therefore, no explicit reorg logic is needed.\n\twriteLive := func(blockChain types.Blocks, receiptChain []rlp.RawValue) (int, error) {\n\t\tvar (\n\t\t\tskipPresenceCheck = false\n\t\t\tbatch             = bc.db.NewBatch()\n\t\t)\n\t\tfirst := blockChain[0]\n\t\tptd := bc.GetTd(first.ParentHash(), first.NumberU64()-1)\n\t\tif ptd == nil {\n\t\t\treturn 0, consensus.ErrUnknownAncestor\n\t\t}\n\t\ttdSum := new(big.Int).Set(ptd)\n\t\tfor i, block := range blockChain {\n\t\t\ttdSum.Add(tdSum, block.Difficulty())\n\t\t\t// Short circuit insertion if shutting down or processing failed\n\t\t\tif bc.insertStopped() {\n\t\t\t\treturn 0, errInsertionInterrupted\n\t\t\t}\n\t\t\tif !skipPresenceCheck {\n\t\t\t\t// Ignore if the entire data is already known\n\t\t\t\tif bc.HasBlock(block.Hash(), block.NumberU64()) {\n\t\t\t\t\tstats.ignored++\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\t// If block N is not present, neither are the later blocks.\n\t\t\t\t\t// This should be true, but if we are mistaken, the shortcut\n\t\t\t\t\t// here will only cause overwriting of some existing data\n\t\t\t\t\tskipPresenceCheck = true\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Write all the data out into the database\n\t\t\trawdb.WriteTd(batch, block.Hash(), block.NumberU64(), tdSum)\n\t\t\trawdb.WriteCanonicalHash(batch, block.Hash(), block.NumberU64())\n\t\t\trawdb.WriteBlock(batch, block)\n\t\t\trawdb.WriteRawReceipts(batch, block.Hash(), block.NumberU64(), receiptChain[i])\n\t\t\tif bc.chainConfig.IsCancun(block.Number(), block.Time()) {\n\t\t\t\trawdb.WriteBlobSidecars(batch, block.Hash(), block.NumberU64(), block.Sidecars())\n\t\t\t}\n\n\t\t\t// Write everything belongs to the blocks into the database. So that\n\t\t\t// we can ensure all components of body is completed(body, receipts)\n\t\t\t// except transaction indexes(will be created once sync is finished).\n\t\t\tif batch.ValueSize() >= ethdb.IdealBatchSize {\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/blockchain.go",
          "line": 1747,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(batch.ValueSize())\n\t\t\t\tbatch.Reset()\n\t\t\t}\n\t\t\tstats.processed++\n\t\t}\n\t\t// Write everything belongs to the blocks into the database. So that\n\t\t// we can ensure all components of body is completed(body, receipts,\n\t\t// tx indexes)\n\t\tif batch.ValueSize() > 0 {\n\t\t\tsize += int64(batch.ValueSize())\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/blockchain.go",
          "line": 2230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= it.remaining()\n\n\t\t// If there are any still remaining, mark as ignored\n\t\treturn nil, it.index, err\n\n\t// Some other error(except ErrKnownBlock) occurred, abort.\n\t// ErrKnownBlock is allowed here since some known blocks\n\t// still need re-execution to generate snapshots that are missing\n\tcase err != nil && !errors.Is(err, ErrKnownBlock):\n\t\tbc.futureBlocks.Remove(block.Hash())\n\t\tstats.ignored += len(it.chain)\n\t\tbc.reportBlock(block, nil, err)\n\t\treturn nil, it.index, err\n\t}\n\t// Track the singleton witness from this chain insertion (if any)\n\tvar witness *stateless.Witness\n\n\tfor ",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/blockchain.go",
          "line": 2315,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= res.usedGas\n\t\twitness = res.witness\n\n\t\tvar snapDiffItems, snapBufItems common.StorageSize\n\t\tif bc.snaps != nil {\n\t\t\tsnapDiffItems, snapBufItems, _ = bc.snaps.Size()\n\t\t}\n\t\ttrieDiffNodes, trieBufNodes, trieImmutableBufNodes, _ := bc.triedb.Size()\n\t\tstats.report(chain, it.index, snapDiffItems, snapBufItems, trieDiffNodes, trieBufNodes, trieImmutableBufNodes, res.status == CanonStatTy)\n\n\t\t// Print confirmation that a future fork is scheduled, but not yet active.\n\t\tbc.logForkReadiness(block)\n\n\t\tif !setHead {\n\t\t\t// After merge we expect few side chains. Simply count\n\t\t\t// all blocks the CL gives us for GC processing time\n\t\t\tbc.gcproc += res.procTime\n\t\t\treturn witness, it.index, nil // Direct block insertion of a single block\n\t\t}\n\t\tswitch res.status {\n\t\tcase CanonStatTy:\n\t\t\tlog.Debug(\"Inserted new block\", \"number\", block.Number(), \"hash\", block.Hash(),\n\t\t\t\t\"uncles\", len(block.Uncles()), \"txs\", len(block.Transactions()), \"gas\", block.GasUsed(),\n\t\t\t\t\"elapsed\", common.PrettyDuration(time.Since(start)),\n\t\t\t\t\"root\", block.Root())\n\n\t\t\tlastCanon = block\n\n\t\t\t// Only count canonical blocks for GC processing time\n\t\t\tbc.gcproc += res.procTime\n\n\t\tcase SideStatTy:\n\t\t\tlog.Debug(\"Inserted forked block\", \"number\", block.Number(), \"hash\", block.Hash(),\n\t\t\t\t\"diff\", block.Difficulty(), \"elapsed\", common.PrettyDuration(time.Since(start)),\n\t\t\t\t\"txs\", len(block.Transactions()), \"gas\", block.GasUsed(), \"uncles\", len(block.Uncles()),\n\t\t\t\t\"root\", block.Root())\n\n\t\tdefault:\n\t\t\t// This in theory is impossible, but lets be nice to our future selves and leave\n\t\t\t// a log, instead of trying to track down blocks imports that don't emit logs.\n\t\t\tlog.Warn(\"Inserted block with unknown status\", \"number\", block.Number(), \"hash\", block.Hash(),\n\t\t\t\t\"diff\", block.Difficulty(), \"elapsed\", common.PrettyDuration(time.Since(start)),\n\t\t\t\t\"txs\", len(block.Transactions()), \"gas\", block.GasUsed(), \"uncles\", len(block.Uncles()),\n\t\t\t\t\"root\", block.Root())\n\t\t}\n\t\tbc.chainBlockFeed.Send(ChainHeadEvent{block.Header()})\n\t}\n\n\t// Any blocks remaining here? The only ones we care about are the future ones\n\tif block != nil && errors.Is(err, consensus.ErrFutureBlock) {\n\t\tif err := bc.addFutureBlock(block)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/blockchain.go",
          "line": 2377,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= it.remaining()\n\treturn witness, it.index, err\n}\n\nfunc (bc *BlockChain) updateHighestVerifiedHeader(header *types.Header) {\n\tif header == nil || header.Number == nil {\n\t\treturn\n\t}\n\tcurrentBlock := bc.CurrentBlock()\n\treorg, err := bc.forker.ReorgNeededWithFastFinality(currentBlock, header)\n\tif err == nil && reorg {\n\t\tbc.highestVerifiedHeader.Store(types.CopyHeader(header))\n\t\tlog.Trace(\"updateHighestVerifiedHeader\", \"number\", header.Number.Uint64(), \"hash\", header.Hash())\n\t}\n}\n\nfunc (bc *BlockChain) GetHighestVerifiedHeader() *types.Header {\n\treturn bc.highestVerifiedHeader.Load()\n}\n\n// blockProcessingResult is a summary of block processing\n// used for updating the stats.\ntype blockProcessingResult struct {\n\tusedGas  uint64\n\tprocTime time.Duration\n\tstatus   WriteStatus\n\twitness  *stateless.Witness\n}\n\n// processBlock executes and validates the given block. If there was no error\n// it writes the block and associated state to database.\nfunc (bc *BlockChain) processBlock(parentRoot common.Hash, block *types.Block, setHead bool, makeWitness bool) (_ *blockProcessingResult, blockEndErr error) {\n\tvar (\n\t\terr       error\n\t\tstartTime = time.Now()\n\t\tstatedb   *state.StateDB\n\t\tinterrupt atomic.Bool\n\t)\n\tdefer interrupt.Store(true) // terminate the prefetch at the end\n\n\tneedBadSharedStorage := bc.chainConfig.NeedBadSharedStorage(block.Number())\n\tneedPrefetch := needBadSharedStorage || (!bc.cfg.NoPrefetch && len(block.Transactions()) >= prefetchTxNumber)\n\tif !needPrefetch {\n\t\tstatedb, err = state.New(parentRoot, bc.statedb)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\t// If prefetching is enabled, run that against the current state to pre-cache\n\t\t// transactions and probabilistically some of the account/storage trie nodes.\n\t\t//\n\t\t// Note: the main processor and prefetcher share the same reader with a local\n\t\t// cache for mitigating the overhead of state access.\n\t\tprefetch, process, err := bc.statedb.ReadersWithCacheStats(parentRoot)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tthrowaway, err := state.NewWithReader(parentRoot, bc.statedb, prefetch)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstatedb, err = state.NewWithReader(parentRoot, bc.statedb, process)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Upload the statistics of reader at the end\n\t\tdefer func() {\n\t\t\tstats := prefetch.GetStats()\n\t\t\taccountCacheHitPrefetchMeter.Mark(stats.AccountHit)\n\t\t\taccountCacheMissPrefetchMeter.Mark(stats.AccountMiss)\n\t\t\tstorageCacheHitPrefetchMeter.Mark(stats.StorageHit)\n\t\t\tstorageCacheMissPrefetchMeter.Mark(stats.StorageMiss)\n\t\t\tstats = process.GetStats()\n\t\t\taccountCacheHitMeter.Mark(stats.AccountHit)\n\t\t\taccountCacheMissMeter.Mark(stats.AccountMiss)\n\t\t\tstorageCacheHitMeter.Mark(stats.StorageHit)\n\t\t\tstorageCacheMissMeter.Mark(stats.StorageMiss)\n\t\t}()\n\n\t\tgo func(start time.Time, throwaway *state.StateDB, block *types.Block) {\n\t\t\t// Disable tracing for prefetcher executions.\n\t\t\tvmCfg := bc.cfg.VmConfig\n\t\t\tvmCfg.Tracer = nil\n\t\t\tbc.prefetcher.Prefetch(block.Transactions(), block.Header(), block.GasLimit(), throwaway, vmCfg, &interrupt)\n\n\t\t\tblockPrefetchExecuteTimer.Update(time.Since(start))\n\t\t\tif interrupt.Load() {\n\t\t\t\tblockPrefetchInterruptMeter.Mark(1)\n\t\t\t}\n\t\t}(time.Now(), throwaway, block)\n\t}\n\n\t// If we are past Byzantium, enable prefetching to pull in trie node paths\n\t// while processing transactions. Before Byzantium the prefetcher is mostly\n\t// useless due to the intermediate root hashing after each transaction.\n\tvar witness *stateless.Witness\n\tif bc.chainConfig.IsByzantium(block.Number()) {\n\t\t// Generate witnesses either if we're self-testing, or if it's the\n\t\t// only block being inserted. A bit crude, but witnesses are huge,\n\t\t// so we refuse to make an entire chain of them.\n\t\tif bc.cfg.VmConfig.StatelessSelfValidation || makeWitness {\n\t\t\twitness, err = stateless.NewWitness(block.Header(), bc)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tstatedb.StartPrefetcher(\"chain\", witness)\n\t\tdefer statedb.StopPrefetcher()\n\t}\n\n\tif bc.logger != nil && bc.logger.OnBlockStart != nil {\n\t\ttd := bc.GetTd(block.ParentHash(), block.NumberU64()-1)\n\t\tbc.logger.OnBlockStart(tracing.BlockEvent{\n\t\t\tBlock:     block,\n\t\t\tTD:        td,\n\t\t\tFinalized: bc.CurrentFinalBlock(),\n\t\t\tSafe:      bc.CurrentSafeBlock(),\n\t\t})\n\t}\n\tif bc.logger != nil && bc.logger.OnBlockEnd != nil {\n\t\tdefer func() {\n\t\t\tbc.logger.OnBlockEnd(blockEndErr)\n\t\t}()\n\t}\n\n\t// Process block using the parent state as reference point\n\tpstart := time.Now()\n\tstatedb.SetExpectedStateRoot(block.Root())\n\tstatedb.SetNeedBadSharedStorage(needBadSharedStorage)\n\tres, err := bc.processor.Process(block, statedb, bc.cfg.VmConfig)\n\tif err != nil {\n\t\tbc.reportBlock(block, res, err)\n\t\treturn nil, err\n\t}\n\tptime := time.Since(pstart)\n\n\t// Validate the state using the default validator\n\tvstart := time.Now()\n\tif err := bc.validator.ValidateState(block, statedb, res, false)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/core/blockchain.go",
          "line": 2723,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= block.Size()\n\n\t\t// If memory use grew too large, import and continue. Sadly we need to discard\n\t\t// all raised events and logs from notifications since we're too heavy on the\n\t\t// memory here.\n\t\tif len(blocks) >= 2048 || memory > 64*1024*1024 {\n\t\t\tlog.Info(\"Importing heavy sidechain segment\", \"blocks\", len(blocks), \"start\", blocks[0].NumberU64(), \"end\", block.NumberU64())\n\t\t\tif _, _, err := bc.insertChain(blocks, true, false)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/core/blockchain.go",
          "line": 3192,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"\\n  %d: cumulative: %v gas: %v contract: %v status: %v tx: %v logs: %v bloom: %x state: %x\",\n\t\t\ti, receipt.CumulativeGasUsed, receipt.GasUsed, receipt.ContractAddress.Hex(),\n\t\t\treceipt.Status, receipt.TxHash.Hex(), receipt.Logs, receipt.Bloom, receipt.PostState)\n\t}\n\tversion, vcs := version.Info()\n\tplatform := fmt.Sprintf(\"%s %s %s %s\", version, runtime.Version(), runtime.GOARCH, runtime.GOOS)\n\tif vcs != \"\" {\n\t\tvcs = fmt.Sprintf(\"\\nVCS: %s\", vcs)\n\t}\n\n\tif badBlockRecords.Cardinality() < badBlockRecordslimit {\n\t\tbadBlockRecords.Add(block.Hash())\n\t\tbadBlockGauge.Update(int64(badBlockRecords.Cardinality()))\n\t}\n\n\treturn fmt.Sprintf(`\n########## BAD BLOCK #########\nBlock: %v (%#x)\nMiner: %v\nError: %v\nPlatform: %v%v\nChain config: %#v\nReceipts: %v\n##############################\n`, block.Number(), block.Hash(), block.Coinbase(), err, platform, vcs, config, receiptString)\n}\n\n// InsertHeaderChain attempts to insert the given header chain in to the local\n// chain, possibly creating a reorg. If an error is returned, it will return the\n// index number of the failing header as well an error describing what went wrong.\nfunc (bc *BlockChain) InsertHeaderChain(chain []*types.Header) (int, error) {\n\tif len(chain) == 0 {\n\t\treturn 0, nil\n\t}\n\tstart := time.Now()\n\tif i, err := bc.hc.ValidateHeaderChain(chain)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/core/blockchain.go",
          "line": 903,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/core/blockchain.go",
          "line": 928,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/core/blockchain.go",
          "line": 1980,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc/core/blockchain.go",
          "line": 2012,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0012",
          "file": "bsc/core/blockchain.go",
          "line": 2014,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0013",
          "file": "bsc/core/blockchain.go",
          "line": 2028,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0014",
          "file": "bsc/core/blockchain.go",
          "line": 2030,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0015",
          "file": "bsc/core/blockchain.go",
          "line": 2110,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0016",
          "file": "bsc/core/blockchain.go",
          "line": 2114,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0017",
          "file": "bsc/core/blockchain.go",
          "line": 2129,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0018",
          "file": "bsc/core/blockchain.go",
          "line": 2132,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0019",
          "file": "bsc/core/blockchain.go",
          "line": 2360,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0020",
          "file": "bsc/core/blockchain.go",
          "line": 2926,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0021",
          "file": "bsc/core/blockchain.go",
          "line": 2931,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0022",
          "file": "bsc/core/blockchain.go",
          "line": 2967,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0023",
          "file": "bsc/core/blockchain.go",
          "line": 2974,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0024",
          "file": "bsc/core/blockchain.go",
          "line": 3056,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0025",
          "file": "bsc/core/blockchain.go",
          "line": 3058,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0026",
          "file": "bsc/core/blockchain.go",
          "line": 3060,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/blockchain_test.go",
          "line": 1320,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(x)\n\t\tfor _, log := range x {\n\t\t\t// We expect added logs to be in ascending order: 0:0, 0:1, 1:0 ...\n\t\t\thave := 100*int(log.BlockNumber) + int(log.TxIndex)\n\t\t\tif have < prev {\n\t\t\t\tt.Fatalf(\"Expected new logs to arrive in ascending order (%d < %d)\", have, prev)\n\t\t\t}\n\t\t\tprev = have\n\t\t}\n\t}\n\tprev = 0\n\tfor len(rmLogsCh) > 0 {\n\t\tx := <-rmLogsCh\n\t\tcountRm += len(x.Logs)\n\t\tfor _, log := range x.Logs {\n\t\t\t// We expect removed logs to be in ascending order: 0:0, 0:1, 1:0 ...\n\t\t\thave := 100*int(log.BlockNumber) + int(log.TxIndex)\n\t\t\tif have < prev {\n\t\t\t\tt.Fatalf(\"Expected removed logs to arrive in ascending order (%d < %d)\", have, prev)\n\t\t\t}\n\t\t\tprev = have\n\t\t}\n\t}\n\n\tif countNew != wantNew {\n\t\tt.Fatalf(\"wrong number of log events: got %d, want %d\", countNew, wantNew)\n\t}\n\tif countRm != wantRemoved {\n\t\tt.Fatalf(\"wrong number of removed log events: got %d, want %d\", countRm, wantRemoved)\n\t}\n}\n\n// Tests if the canonical block can be fetched from the database during chain insertion.\nfunc TestCanonicalBlockRetrieval(t *testing.T) {\n\ttestCanonicalBlockRetrieval(t, rawdb.HashScheme)\n\ttestCanonicalBlockRetrieval(t, rawdb.PathScheme)\n}\n\nfunc testCanonicalBlockRetrieval(t *testing.T, scheme string) {\n\t_, gspec, blockchain, err := newCanonical(ethash.NewFaker(), 0, true, scheme)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create pristine chain: %v\", err)\n\t}\n\tdefer blockchain.Stop()\n\n\t_, chain, _ := GenerateChainWithGenesis(gspec, ethash.NewFaker(), 10, func(i int, gen *BlockGen) {})\n\n\tvar pend sync.WaitGroup\n\tpend.Add(len(chain))\n\n\tfor i := range chain {\n\t\tgo func(block *types.Block) {\n\t\t\tdefer pend.Done()\n\n\t\t\t// try to retrieve a block by its canonical hash and see if the block data can be retrieved.\n\t\t\tfor {\n\t\t\t\tch := rawdb.ReadCanonicalHash(blockchain.db, block.NumberU64())\n\t\t\t\tif ch == (common.Hash{}) {\n\t\t\t\t\tcontinue // busy wait for canonical hash to be written\n\t\t\t\t}\n\t\t\t\tif ch != block.Hash() {\n\t\t\t\t\tt.Errorf(\"unknown canonical hash, want %s, got %s\", block.Hash().Hex(), ch.Hex())\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfb := rawdb.ReadBlock(blockchain.db, ch, block.NumberU64())\n\t\t\t\tif fb == nil {\n\t\t\t\t\tt.Errorf(\"unable to retrieve block %d for canonical hash: %s\", block.NumberU64(), ch.Hex())\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif fb.Hash() != block.Hash() {\n\t\t\t\t\tt.Errorf(\"invalid block hash for block %d, want %s, got %s\", block.NumberU64(), block.Hash().Hex(), fb.Hash().Hex())\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}(chain[i])\n\n\t\tif _, err := blockchain.InsertChain(types.Blocks{chain[i]})",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/blockchain_test.go",
          "line": 4342,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/headerchain.go",
          "line": 453,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= ancestor\n\t\t\t\treturn ancestorHash, number\n\t\t\t}\n\t\t}\n\t\tif *maxNonCanonical == 0 {\n\t\t\treturn common.Hash{}, 0\n\t\t}\n\t\t*maxNonCanonical--\n\t\tancestor--\n\t\theader := hc.GetHeader(hash, number)\n\t\tif header == nil {\n\t\t\treturn common.Hash{}, 0\n\t\t}\n\t\thash = header.ParentHash\n\t\tnumber--\n\t}\n\treturn hash, number\n}\n\n// GetTd retrieves a block's total difficulty in the canonical chain from the\n// database by hash and number, caching it if found.\nfunc (hc *HeaderChain) GetTd(hash common.Hash, number uint64) *big.Int {\n\t// Short circuit if the td's already in the cache, retrieve otherwise\n\tif cached, ok := hc.tdCache.Get(hash)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/headerchain.go",
          "line": 543,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= number - current\n\t\t\tnumber = current\n\t\t} else {\n\t\t\treturn nil\n\t\t}\n\t}\n\tvar headers []rlp.RawValue\n\t// If we have some of the headers in cache already, use that before going to db.\n\thash := rawdb.ReadCanonicalHash(hc.chainDb, number)\n\tif hash == (common.Hash{}) {\n\t\treturn nil\n\t}\n\tfor count > 0 {\n\t\theader, ok := hc.headerCache.Get(hash)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\trlpData, _ := rlp.EncodeToBytes(header)\n\t\theaders = append(headers, rlpData)\n\t\thash = header.ParentHash\n\t\tcount--\n\t\tnumber--\n\t}\n\t// Read remaining from db\n\tif count > 0 {\n\t\theaders = append(headers, rawdb.ReadHeaderRange(hc.chainDb, number, count)...)\n\t}\n\treturn headers\n}\n\nfunc (hc *HeaderChain) GetCanonicalHash(number uint64) common.Hash {\n\treturn rawdb.ReadCanonicalHash(hc.chainDb, number)\n}\n\n// CurrentHeader retrieves the current head header of the canonical chain. The\n// header is retrieved from the HeaderChain's internal cache.\nfunc (hc *HeaderChain) CurrentHeader() *types.Header {\n\treturn hc.currentHeader.Load()\n}\n\n// SetCurrentHeader sets the in-memory head header marker of the canonical chan\n// as the given header.\nfunc (hc *HeaderChain) SetCurrentHeader(head *types.Header) {\n\thc.currentHeader.Store(head)\n\thc.currentHeaderHash = head.Hash()\n\theadHeaderGauge.Update(head.Number.Int64())\n\tjustifiedBlockGauge.Update(int64(hc.GetJustifiedNumber(head)))\n\tfinalizedBlockGauge.Update(int64(hc.GetFinalizedNumber(head)))\n}\n\ntype (\n\t// UpdateHeadBlocksCallback is a callback function that is called by SetHead\n\t// before head header is updated. The method will return the actual block it\n\t// updated the head to (missing state) and a flag if setHead should continue\n\t// rewinding till that forcefully (exceeded ancient limits)\n\tUpdateHeadBlocksCallback func(ethdb.KeyValueWriter, *types.Header) (*types.Header, bool)\n\n\t// DeleteBlockContentCallback is a callback function that is called by SetHead\n\t// before each header is deleted.\n\tDeleteBlockContentCallback func(ethdb.KeyValueWriter, common.Hash, uint64)\n)\n\n// SetHead rewinds the local chain to a new head. Everything above the new head\n// will be deleted and the new one set.\nfunc (hc *HeaderChain) SetHead(head uint64, updateFn UpdateHeadBlocksCallback, delFn DeleteBlockContentCallback) {\n\thc.setHead(head, 0, updateFn, delFn)\n}\n\n// SetHeadWithTimestamp rewinds the local chain to a new head timestamp. Everything\n// above the new head will be deleted and the new one set.\nfunc (hc *HeaderChain) SetHeadWithTimestamp(time uint64, updateFn UpdateHeadBlocksCallback, delFn DeleteBlockContentCallback) {\n\thc.setHead(0, time, updateFn, delFn)\n}\n\n// setHead rewinds the local chain to a new head block or a head timestamp.\n// Everything above the new head will be deleted and the new one set.\nfunc (hc *HeaderChain) setHead(headBlock uint64, headTime uint64, updateFn UpdateHeadBlocksCallback, delFn DeleteBlockContentCallback) {\n\t// Sanity check that there's no attempt to undo the genesis block. This is\n\t// a fairly synthetic case where someone enables a timestamp based fork\n\t// below the genesis timestamp. It's nice to not allow that instead of the\n\t// entire chain getting deleted.\n\tif headTime > 0 && hc.genesisHeader.Time > headTime {\n\t\t// Note, a critical error is quite brutal, but we should really not reach\n\t\t// this point. Since pre-timestamp based forks it was impossible to have\n\t\t// a fork before block 0, the setHead would always work. With timestamp\n\t\t// forks it becomes possible to specify below the genesis. That said, the\n\t\t// only time we setHead via timestamp is with chain config changes on the\n\t\t// startup, so failing hard there is ok.\n\t\tlog.Crit(\"Rejecting genesis rewind via timestamp\", \"target\", headTime, \"genesis\", hc.genesisHeader.Time)\n\t}\n\tvar (\n\t\tparentHash common.Hash\n\t\tbatch      = hc.chainDb.NewBatch()\n\t\torigin     = true\n\t)\n\tdone := func(header *types.Header) bool {\n\t\tif headTime > 0 {\n\t\t\treturn header.Time <= headTime\n\t\t}\n\t\treturn header.Number.Uint64() <= headBlock\n\t}\n\tfor hdr := hc.CurrentHeader()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/data_availability_test.go",
          "line": 426,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gokzg4844.SerializedScalarSize {\n\t\tfieldElementBytes := randFieldElement()\n\t\tcopy(blob[i:i+gokzg4844.SerializedScalarSize], fieldElementBytes[:])\n\t}\n\treturn blob\n}\n\nfunc randomSidecar() *types.BlobTxSidecar {\n\tblob := randBlob()\n\tcommitment, _ := kzg4844.BlobToCommitment(&blob)\n\tproof, _ := kzg4844.ComputeBlobProof(&blob, commitment)\n\treturn &types.BlobTxSidecar{\n\t\tBlobs:       []kzg4844.Blob{blob},\n\t\tCommitments: []kzg4844.Commitment{commitment},\n\t\tProofs:      []kzg4844.Proof{proof},\n\t}\n}\n\nfunc emptySidecar() *types.BlobTxSidecar {\n\treturn &types.BlobTxSidecar{\n\t\tBlobs:       []kzg4844.Blob{emptyBlob},\n\t\tCommitments: []kzg4844.Commitment{emptyBlobCommit},\n\t\tProofs:      []kzg4844.Proof{emptyBlobProof},\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/blockchain_insert.go",
          "line": 54,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(block.Transactions())\n\t\t\tfor _, sidecar := range block.Sidecars() {\n\t\t\t\tblobs += len(sidecar.Blobs)\n\t\t\t}\n\t\t}\n\t\tend := chain[index]\n\n\t\t// Assemble the log context and send it to the logger\n\t\tcontext := []interface{}{\n\t\t\t\"number\", end.Number(), \"hash\", end.Hash(), \"miner\", end.Coinbase(),\n\t\t\t\"blocks\", st.processed, \"txs\", txs, \"blobs\", blobs, \"mgas\", float64(st.usedGas) / 1000000,\n\t\t\t\"elapsed\", common.PrettyDuration(elapsed), \"mgasps\", mgasps,\n\t\t}\n\t\tblockInsertMgaspsGauge.Update(int64(mgasps))\n\t\tif timestamp := time.Unix(int64(end.Time()), 0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state_processor_test.go",
          "line": 406,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= tx.Gas()\n\t\tnBlobs += len(tx.BlobHashes())\n\t}\n\theader.Root = common.BytesToHash(hasher.Sum(nil))\n\tif config.IsCancun(header.Number, header.Time) {\n\t\texcess := eip4844.CalcExcessBlobGas(config, parent.Header(), header.Time)\n\t\tused := uint64(nBlobs * params.BlobTxBlobGasPerBlob)\n\t\theader.ExcessBlobGas = &excess\n\t\theader.BlobGasUsed = &used\n\n\t\tbeaconRoot := common.HexToHash(\"0xbeac00\")\n\t\tif config.Parlia == nil {\n\t\t\theader.ParentBeaconRoot = &beaconRoot\n\t\t}\n\t}\n\t// Assemble and return the final block for sealing\n\tbody := &types.Body{Transactions: txs}\n\tif config.IsShanghai(header.Number, header.Time) {\n\t\tbody.Withdrawals = []*types.Withdrawal{}\n\t}\n\treturn types.NewBlock(header, body, receipts, trie.NewStackTrie(nil))\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/block_validator.go",
          "line": 92,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(tx.BlobHashes())\n\n\t\t\t\t// If the tx is a blob tx, it must NOT have a sidecar attached to be valid in a block.\n\t\t\t\tif tx.BlobTxSidecar() != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unexpected blob sidecar in transaction at index %d\", i)\n\t\t\t\t}\n\n\t\t\t\t// The individual checks for blob validity (version-check + not empty)\n\t\t\t\t// happens in state transition.\n\t\t\t}\n\n\t\t\t// Check blob gas usage.\n\t\t\tif header.BlobGasUsed != nil {\n\t\t\t\tif want := *header.BlobGasUsed / params.BlobTxBlobGasPerBlob",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/bench_test.go",
          "line": 139,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= params.TxGas\n\t\t\tif gas < params.TxGas {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tto := (from + 1) % naccounts\n\t\t\tburn := new(big.Int).SetUint64(params.TxGas)\n\t\t\tburn.Mul(burn, gen.header.BaseFee)\n\t\t\tavailableFunds.Sub(availableFunds, burn)\n\t\t\tif availableFunds.Cmp(big.NewInt(1)) < 0 {\n\t\t\t\tpanic(\"not enough funds\")\n\t\t\t}\n\t\t\ttx, err := types.SignNewTx(ringKeys[from], signer,\n\t\t\t\t&types.LegacyTx{\n\t\t\t\t\tNonce:    gen.TxNonce(ringAddrs[from]),\n\t\t\t\t\tTo:       &ringAddrs[to],\n\t\t\t\t\tValue:    availableFunds,\n\t\t\t\t\tGas:      params.TxGas,\n\t\t\t\t\tGasPrice: gasPrice,\n\t\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tgen.AddTx(tx)\n\t\t\tfrom = to\n\t\t}\n\t}\n}\n\n// genUncles generates blocks with two uncle headers.\nfunc genUncles(i int, gen *BlockGen) {\n\tif i >= 7 {\n\t\tb2 := gen.PrevBlock(i - 6).Header()\n\t\tb2.Extra = []byte(\"foo\")\n\t\tgen.AddUncle(b2)\n\t\tb3 := gen.PrevBlock(i - 6).Header()\n\t\tb3.Extra = []byte(\"bar\")\n\t\tgen.AddUncle(b3)\n\t}\n}\n\nfunc benchInsertChain(b *testing.B, disk bool, gen func(int, *BlockGen)) {\n\t// Create the database in memory or in a temporary directory.\n\tvar db ethdb.Database\n\tif !disk {\n\t\tdb = rawdb.NewMemoryDatabase()\n\t} else {\n\t\tpdb, err := pebble.New(b.TempDir(), 128, 128, \"\", false)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"cannot create temporary database: %v\", err)\n\t\t}\n\t\tdb = rawdb.NewDatabase(pdb)\n\t\tdefer db.Close()\n\t}\n\t// Generate a chain of b.N blocks using the supplied block\n\t// generator function.\n\tgspec := &Genesis{\n\t\tConfig: params.TestChainConfig,\n\t\tAlloc:  types.GenesisAlloc{benchRootAddr: {Balance: benchRootFunds}},\n\t}\n\t_, chain, _ := GenerateChainWithGenesis(gspec, ethash.NewFaker(), b.N, gen)\n\n\t// Time the insertion of the new chain.\n\t// State and blocks are stored in the same DB.\n\tchainman, _ := NewBlockChain(db, gspec, ethash.NewFaker(), nil)\n\tdefer chainman.Stop()\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\tif i, err := chainman.InsertChain(chain)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txindexer_test.go",
          "line": 91,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t})\n\tvar cases = []struct {\n\t\tlimits []uint64\n\t\ttails  []uint64\n\t}{\n\t\t{\n\t\t\tlimits: []uint64{0, 1, 64, 129, 0},\n\t\t\ttails:  []uint64{0, 128, 65, 0, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{64, 1, 64, 0},\n\t\t\ttails:  []uint64{65, 128, 65, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{127, 1, 64, 0},\n\t\t\ttails:  []uint64{2, 128, 65, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{128, 1, 64, 0},\n\t\t\ttails:  []uint64{1, 128, 65, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{129, 1, 64, 0},\n\t\t\ttails:  []uint64{0, 128, 65, 0},\n\t\t},\n\t}\n\tfor _, c := range cases {\n\t\tdb, _ := rawdb.Open(rawdb.NewMemoryDatabase(), rawdb.OpenOptions{})\n\t\trawdb.WriteAncientBlocks(db, append([]*types.Block{gspec.ToBlock()}, blocks...), types.EncodeBlockReceiptLists(append([]types.Receipts{{}}, receipts...)), big.NewInt(0))\n\n\t\t// Index the initial blocks from ancient store\n\t\tindexer := &txIndexer{\n\t\t\tlimit: 0,\n\t\t\tdb:    db,\n\t\t}\n\t\tfor i, limit := range c.limits {\n\t\t\tindexer.limit = limit\n\t\t\tindexer.run(chainHead, make(chan struct{}), make(chan struct{}))\n\t\t\tverify(t, db, blocks, c.tails[i])\n\t\t}\n\t\tdb.Close()\n\t}\n}\n\nfunc TestTxIndexerRepair(t *testing.T) {\n\tvar (\n\t\ttestBankKey, _  = crypto.GenerateKey()\n\t\ttestBankAddress = crypto.PubkeyToAddress(testBankKey.PublicKey)\n\t\ttestBankFunds   = big.NewInt(1000000000000000000)\n\n\t\tgspec = &Genesis{\n\t\t\tConfig:  params.TestChainConfig,\n\t\t\tAlloc:   types.GenesisAlloc{testBankAddress: {Balance: testBankFunds}},\n\t\t\tBaseFee: big.NewInt(params.InitialBaseFee),\n\t\t}\n\t\tengine    = ethash.NewFaker()\n\t\tnonce     = uint64(0)\n\t\tchainHead = uint64(128)\n\t)\n\t_, blocks, receipts := GenerateChainWithGenesis(gspec, engine, int(chainHead), func(i int, gen *BlockGen) {\n\t\ttx, _ := types.SignTx(types.NewTransaction(nonce, common.HexToAddress(\"0xdeadbeef\"), big.NewInt(1000), params.TxGas, big.NewInt(10*params.InitialBaseFee), nil), types.HomesteadSigner{}, testBankKey)\n\t\tgen.AddTx(tx)\n\t\tnonce += 1\n\t})\n\ttailPointer := func(n uint64) *uint64 {\n\t\treturn &n\n\t}\n\tvar cases = []struct {\n\t\tlimit   uint64\n\t\thead    uint64\n\t\tcutoff  uint64\n\t\texpTail *uint64\n\t}{\n\t\t// if *tail > head => purge indexes\n\t\t{\n\t\t\tlimit:   0,\n\t\t\thead:    chainHead / 2,\n\t\t\tcutoff:  0,\n\t\t\texpTail: tailPointer(0),\n\t\t},\n\t\t{\n\t\t\tlimit:   1,             // tail = 128\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: nil,\n\t\t},\n\t\t{\n\t\t\tlimit:   64,            // tail = 65\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: nil,\n\t\t},\n\t\t{\n\t\t\tlimit:   65,            // tail = 64\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: tailPointer(64),\n\t\t},\n\t\t{\n\t\t\tlimit:   66,            // tail = 63\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: tailPointer(63),\n\t\t},\n\n\t\t// if tail < cutoff => remove indexes below cutoff\n\t\t{\n\t\t\tlimit:   0,         // tail = 0\n\t\t\thead:    chainHead, // head = 128\n\t\t\tcutoff:  chainHead, // cutoff = 128\n\t\t\texpTail: tailPointer(chainHead),\n\t\t},\n\t\t{\n\t\t\tlimit:   1,         // tail = 128\n\t\t\thead:    chainHead, // head = 128\n\t\t\tcutoff:  chainHead, // cutoff = 128\n\t\t\texpTail: tailPointer(128),\n\t\t},\n\t\t{\n\t\t\tlimit:   2,         // tail = 127\n\t\t\thead:    chainHead, // head = 128\n\t\t\tcutoff:  chainHead, // cutoff = 128\n\t\t\texpTail: tailPointer(chainHead),\n\t\t},\n\t\t{\n\t\t\tlimit:   2,             // tail = 127\n\t\t\thead:    chainHead,     // head = 128\n\t\t\tcutoff:  chainHead / 2, // cutoff = 64\n\t\t\texpTail: tailPointer(127),\n\t\t},\n\n\t\t// if head < cutoff => purge indexes\n\t\t{\n\t\t\tlimit:   0,             // tail = 0\n\t\t\thead:    chainHead,     // head = 128\n\t\t\tcutoff:  2 * chainHead, // cutoff = 256\n\t\t\texpTail: nil,\n\t\t},\n\t\t{\n\t\t\tlimit:   64,            // tail = 65\n\t\t\thead:    chainHead,     // head = 128\n\t\t\tcutoff:  chainHead / 2, // cutoff = 64\n\t\t\texpTail: tailPointer(65),\n\t\t},\n\t}\n\tfor _, c := range cases {\n\t\tdb, _ := rawdb.Open(rawdb.NewMemoryDatabase(), rawdb.OpenOptions{})\n\t\tencReceipts := types.EncodeBlockReceiptLists(append([]types.Receipts{{}}, receipts...))\n\t\trawdb.WriteAncientBlocks(db, append([]*types.Block{gspec.ToBlock()}, blocks...), encReceipts, big.NewInt(0))\n\n\t\t// Index the initial blocks from ancient store\n\t\tindexer := &txIndexer{\n\t\t\tlimit: c.limit,\n\t\t\tdb:    db,\n\t\t}\n\t\tindexer.run(chainHead, make(chan struct{}), make(chan struct{}))\n\n\t\tindexer.cutoff = c.cutoff\n\t\tindexer.repair(c.head)\n\n\t\tif c.expTail == nil {\n\t\t\tverifyNoIndex(t, db, blocks)\n\t\t} else {\n\t\t\tverify(t, db, blocks, *c.expTail)\n\t\t}\n\t\tdb.Close()\n\t}\n}\n\nfunc TestTxIndexerReport(t *testing.T) {\n\tvar (\n\t\ttestBankKey, _  = crypto.GenerateKey()\n\t\ttestBankAddress = crypto.PubkeyToAddress(testBankKey.PublicKey)\n\t\ttestBankFunds   = big.NewInt(1000000000000000000)\n\n\t\tgspec = &Genesis{\n\t\t\tConfig:  params.TestChainConfig,\n\t\t\tAlloc:   types.GenesisAlloc{testBankAddress: {Balance: testBankFunds}},\n\t\t\tBaseFee: big.NewInt(params.InitialBaseFee),\n\t\t}\n\t\tengine    = ethash.NewFaker()\n\t\tnonce     = uint64(0)\n\t\tchainHead = uint64(128)\n\t)\n\t_, blocks, receipts := GenerateChainWithGenesis(gspec, engine, int(chainHead), func(i int, gen *BlockGen) {\n\t\ttx, _ := types.SignTx(types.NewTransaction(nonce, common.HexToAddress(\"0xdeadbeef\"), big.NewInt(1000), params.TxGas, big.NewInt(10*params.InitialBaseFee), nil), types.HomesteadSigner{}, testBankKey)\n\t\tgen.AddTx(tx)\n\t\tnonce += 1\n\t})\n\ttailPointer := func(n uint64) *uint64 {\n\t\treturn &n\n\t}\n\tvar cases = []struct {\n\t\thead         uint64\n\t\tlimit        uint64\n\t\tcutoff       uint64\n\t\ttail         *uint64\n\t\texpIndexed   uint64\n\t\texpRemaining uint64\n\t}{\n\t\t// The entire chain is supposed to be indexed\n\t\t{\n\t\t\t// head = 128, limit = 0, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  0,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 0\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 0, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  0,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 1\n\t\t\ttail:         tailPointer(1),\n\t\t\texpIndexed:   128,\n\t\t\texpRemaining: 1,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 0, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  0,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 128\n\t\t\ttail:         tailPointer(chainHead),\n\t\t\texpIndexed:   1,\n\t\t\texpRemaining: 128,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 256, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  256,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 0\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\n\t\t// The chain with specific range is supposed to be indexed\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 0, part of them need to be unindexed\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 64, one of them needs to be unindexed\n\t\t\ttail:         tailPointer(64),\n\t\t\texpIndexed:   65,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 65, all of them have been indexed\n\t\t\ttail:         tailPointer(65),\n\t\t\texpIndexed:   64,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 66, one of them has to be indexed\n\t\t\ttail:         tailPointer(66),\n\t\t\texpIndexed:   63,\n\t\t\texpRemaining: 1,\n\t\t},\n\n\t\t// The chain with configured cutoff, the chain range could be capped\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 66 => index: [66, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 66,\n\n\t\t\t// tail = 0, part of them need to be unindexed\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 66 => index: [66, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 66,\n\n\t\t\t// tail = 66, all of them have been indexed\n\t\t\ttail:         tailPointer(66),\n\t\t\texpIndexed:   63,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 66 => index: [66, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 66,\n\n\t\t\t// tail = 67, one of them has to be indexed\n\t\t\ttail:         tailPointer(67),\n\t\t\texpIndexed:   62,\n\t\t\texpRemaining: 1,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 256 => index: [66, 128]\n\t\t\thead:         chainHead,\n\t\t\tlimit:        0,\n\t\t\tcutoff:       256,\n\t\t\ttail:         nil,\n\t\t\texpIndexed:   0,\n\t\t\texpRemaining: 0,\n\t\t},\n\t}\n\tfor _, c := range cases {\n\t\tdb, _ := rawdb.Open(rawdb.NewMemoryDatabase(), rawdb.OpenOptions{})\n\t\tencReceipts := types.EncodeBlockReceiptLists(append([]types.Receipts{{}}, receipts...))\n\t\trawdb.WriteAncientBlocks(db, append([]*types.Block{gspec.ToBlock()}, blocks...), encReceipts, big.NewInt(0))\n\n\t\t// Index the initial blocks from ancient store\n\t\tindexer := &txIndexer{\n\t\t\tlimit:  c.limit,\n\t\t\tcutoff: c.cutoff,\n\t\t\tdb:     db,\n\t\t}\n\t\tp := indexer.report(c.head, c.tail)\n\t\tif p.Indexed != c.expIndexed {\n\t\t\tt.Fatalf(\"Unexpected indexed: %d, expected: %d\", p.Indexed, c.expIndexed)\n\t\t}\n\t\tif p.Remaining != c.expRemaining {\n\t\t\tt.Fatalf(\"Unexpected remaining: %d, expected: %d\", p.Remaining, c.expRemaining)\n\t\t}\n\t\tdb.Close()\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/chain_makers_test.go",
          "line": 166,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\n\t\t// Verify parent beacon root.\n\t\twant := common.Hash{byte(blocknum)}\n\t\tif got := block.BeaconRoot()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/ethstats/ethstats.go",
          "line": 794,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(basefee.Uint64())\n\t\t}\n\t} else {\n\t\tsync := s.backend.SyncProgress(context.Background())\n\t\tsyncing = !sync.Done()\n\t}\n\t// Assemble the node stats and send it to the server\n\tlog.Trace(\"Sending node details to ethstats\")\n\n\tstats := map[string]interface{}{\n\t\t\"id\": s.node,\n\t\t\"stats\": &nodeStats{\n\t\t\tActive:   true,\n\t\t\tMining:   mining,\n\t\t\tPeers:    s.server.PeerCount(),\n\t\t\tGasPrice: gasprice,\n\t\t\tSyncing:  syncing,\n\t\t\tUptime:   100,\n\t\t},\n\t}\n\treport := map[string][]interface{}{\n\t\t\"emit\": {\"stats\", stats},\n\t}\n\treturn conn.WriteJSON(report)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/tests/init_test.go",
          "line": 284,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/tests/state_test.go",
          "line": 324,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(time.Since(start))\n\t\t\t\trefund += state.StateDB.GetRefund()\n\t\t\t\tgasUsed += msg.GasLimit - leftOverGas\n\n\t\t\t\tstate.StateDB.RevertToSnapshot(snapshot)\n\t\t\t}\n\t\t\tif elapsed < 1 {\n\t\t\t\telapsed = 1\n\t\t\t}\n\t\t\t// Keep it as uint64, multiply 100 to get two digit float later\n\t\t\tmgasps := (100 * 1000 * (gasUsed - refund)) / elapsed\n\t\t\tb.ReportMetric(float64(mgasps)/100, \"mgas/s\")\n\t\t})\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/tests/state_test.go",
          "line": 317,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/preimages.go",
          "line": 55,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(common.HashLength + len(preimage))\n\t}\n}\n\n// preimage retrieves a cached trie node pre-image from memory. If it cannot be\n// found cached, the method queries the persistent database for the content.\nfunc (store *preimageStore) preimage(hash common.Hash) []byte {\n\tstore.lock.RLock()\n\tpreimage := store.preimages[hash]\n\tstore.lock.RUnlock()\n\n\tif preimage != nil {\n\t\treturn preimage\n\t}\n\treturn rawdb.ReadPreimage(store.disk, hash)\n}\n\n// commit flushes the cached preimages into the disk.\nfunc (store *preimageStore) commit(force bool) error {\n\tstore.lock.Lock()\n\tdefer store.lock.Unlock()\n\n\tif store.preimagesSize <= 4*1024*1024 && !force {\n\t\treturn nil\n\t}\n\tbatch := store.disk.NewBatch()\n\trawdb.WritePreimages(batch, store.preimages)\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rlp/decode_test.go",
          "line": 209,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rlp/encbuffer.go",
          "line": 71,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\t\tstrpos += n\n\t\t// write the header\n\t\tenc := head.encode(dst[pos:])\n\t\tpos += len(enc)\n\t}\n\t// copy string data after the last list header\n\tcopy(dst[pos:], buf.str[strpos:])\n}\n\n// writeTo writes the encoder output to w.\nfunc (buf *encBuffer) writeTo(w io.Writer) (err error) {\n\tstrpos := 0\n\tfor _, head := range buf.lheads {\n\t\t// write string data before header\n\t\tif head.offset-strpos > 0 {\n\t\t\tn, err := w.Write(buf.str[strpos:head.offset])\n\t\t\tstrpos += n\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\t// write the header\n\t\tenc := head.encode(buf.sizebuf[:])\n\t\tif _, err = w.Write(enc)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rlp/encbuffer.go",
          "line": 205,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 + intsize(uint64(lh.size))\n\t}\n}\n\nfunc (buf *encBuffer) encode(val interface{}) error {\n\trval := reflect.ValueOf(val)\n\twriter, err := cachedWriter(rval.Type())\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn writer(rval, buf)\n}\n\nfunc (buf *encBuffer) encodeStringHeader(size int) {\n\tif size < 56 {\n\t\tbuf.str = append(buf.str, 0x80+byte(size))\n\t} else {\n\t\tsizesize := putint(buf.sizebuf[1:], uint64(size))\n\t\tbuf.sizebuf[0] = 0xB7 + byte(sizesize)\n\t\tbuf.str = append(buf.str, buf.sizebuf[:sizesize+1]...)\n\t}\n}\n\n// encReader is the io.Reader returned by EncodeToReader.\n// It releases its encbuf at EOF.\ntype encReader struct {\n\tbuf    *encBuffer // the buffer we're reading from. this is nil when we're at EOF.\n\tlhpos  int        // index of list header that we're reading\n\tstrpos int        // current position in string buffer\n\tpiece  []byte     // next piece to be read\n}\n\nfunc (r *encReader) Read(b []byte) (n int, err error) {\n\tfor {\n\t\tif r.piece = r.next()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/rlp/encbuffer.go",
          "line": 250,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nn\n\t\tif nn < len(r.piece) {\n\t\t\t// piece didn't fit, see you next time.\n\t\t\tr.piece = r.piece[nn:]\n\t\t\treturn n, nil\n\t\t}\n\t\tr.piece = nil\n\t}\n}\n\n// next returns the next piece of data to be read.\n// it returns nil at EOF.\nfunc (r *encReader) next() []byte {\n\tswitch {\n\tcase r.buf == nil:\n\t\treturn nil\n\n\tcase r.piece != nil:\n\t\t// There is still data available for reading.\n\t\treturn r.piece\n\n\tcase r.lhpos < len(r.buf.lheads):\n\t\t// We're before the last list header.\n\t\thead := r.buf.lheads[r.lhpos]\n\t\tsizebefore := head.offset - r.strpos\n\t\tif sizebefore > 0 {\n\t\t\t// String data before header.\n\t\t\tp := r.buf.str[r.strpos:head.offset]\n\t\t\tr.strpos += sizebefore\n\t\t\treturn p\n\t\t}\n\t\tr.lhpos++\n\t\treturn head.encode(r.buf.sizebuf[:])\n\n\tcase r.strpos < len(r.buf.str):\n\t\t// String data at the end, after all list headers.\n\t\tp := r.buf.str[r.strpos:]\n\t\tr.strpos = len(r.buf.str)\n\t\treturn p\n\n\tdefault:\n\t\treturn nil\n\t}\n}\n\nfunc encBufferFromWriter(w io.Writer) *encBuffer {\n\tswitch w := w.(type) {\n\tcase EncoderBuffer:\n\t\treturn w.buf\n\tcase *EncoderBuffer:\n\t\treturn w.buf\n\tcase *encBuffer:\n\t\treturn w\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// EncoderBuffer is a buffer for incremental encoding.\n//\n// The zero value is NOT ready for use. To get a usable buffer,\n// create it using NewEncoderBuffer or call Reset.\ntype EncoderBuffer struct {\n\tbuf *encBuffer\n\tdst io.Writer\n\n\townBuffer bool\n}\n\n// NewEncoderBuffer creates an encoder buffer.\nfunc NewEncoderBuffer(dst io.Writer) EncoderBuffer {\n\tvar w EncoderBuffer\n\tw.Reset(dst)\n\treturn w\n}\n\n// Reset truncates the buffer and sets the output destination.\nfunc (w *EncoderBuffer) Reset(dst io.Writer) {\n\tif w.buf != nil && !w.ownBuffer {\n\t\tpanic(\"can't Reset derived EncoderBuffer\")\n\t}\n\n\t// If the destination writer has an *encBuffer, use it.\n\t// Note that w.ownBuffer is left false here.\n\tif dst != nil {\n\t\tif outer := encBufferFromWriter(dst)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rlp/decode.go",
          "line": 119,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= err.ctx[i]\n\t\t}\n\t}\n\treturn fmt.Sprintf(\"rlp: %s for %v%s\", err.msg, err.typ, ctx)\n}\n\nfunc wrapStreamError(err error, typ reflect.Type) error {\n\tswitch err {\n\tcase ErrCanonInt:\n\t\treturn &decodeError{msg: \"non-canonical integer (leading zero bytes)\", typ: typ}\n\tcase ErrCanonSize:\n\t\treturn &decodeError{msg: \"non-canonical size information\", typ: typ}\n\tcase ErrExpectedList:\n\t\treturn &decodeError{msg: \"expected input list\", typ: typ}\n\tcase ErrExpectedString:\n\t\treturn &decodeError{msg: \"expected input string or byte\", typ: typ}\n\tcase errUintOverflow:\n\t\treturn &decodeError{msg: \"input string too long\", typ: typ}\n\tcase errNotAtEOL:\n\t\treturn &decodeError{msg: \"input list has too many elements\", typ: typ}\n\t}\n\treturn err\n}\n\nfunc addErrorContext(err error, ctx string) error {\n\tif decErr, ok := err.(*decodeError)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rlp/decode.go",
          "line": 1127,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nn\n\t}\n\tif err == io.EOF {\n\t\tif n < len(buf) {\n\t\t\terr = io.ErrUnexpectedEOF\n\t\t} else {\n\t\t\t// Readers are allowed to give EOF even though the read succeeded.\n\t\t\t// In such cases, we discard the EOF, like io.ReadFull() does.\n\t\t\terr = nil\n\t\t}\n\t}\n\treturn err\n}\n\n// readByte reads a single byte from the underlying stream.\nfunc (s *Stream) readByte() (byte, error) {\n\tif err := s.willRead(1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/rlp/decode.go",
          "line": 1168,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= n\n\t}\n\treturn nil\n}\n\n// listLimit returns the amount of data remaining in the innermost list.\nfunc (s *Stream) listLimit() (inList bool, limit uint64) {\n\tif len(s.stack) == 0 {\n\t\treturn false, 0\n\t}\n\treturn true, s.stack[len(s.stack)-1]\n}\n\ntype sliceReader []byte\n\nfunc (sr *sliceReader) Read(b []byte) (int, error) {\n\tif len(*sr) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tn := copy(b, *sr)\n\t*sr = (*sr)[n:]\n\treturn n, nil\n}\n\nfunc (sr *sliceReader) ReadByte() (byte, error) {\n\tif len(*sr) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tb := (*sr)[0]\n\t*sr = (*sr)[1:]\n\treturn b, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rlp/encode_test.go",
          "line": 473,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= remaining\n\t\t\t} else {\n\t\t\t\tend = start + 3\n\t\t\t}\n\t\t\tn, err := r.Read(output[start:end])\n\t\t\tend = start + n\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t} else if err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\treturn output, nil\n\t})\n}\n\n// This is a regression test verifying that encReader\n// returns its encbuf to the pool only once.\nfunc TestEncodeToReaderReturnToPool(t *testing.T) {\n\tbuf := make([]byte, 50)\n\twg := new(sync.WaitGroup)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/types.go",
          "line": 276,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n\t\t}\n\t}\n\treturn buf[:]\n}\n\nfunc (a Address) hex() []byte {\n\tvar buf [len(a)*2 + 2]byte\n\tcopy(buf[:2], \"0x\")\n\thex.Encode(buf[2:], a[:])\n\treturn buf[:]\n}\n\n// Format implements fmt.Formatter.\n// Address supports the %v, %s, %q, %x, %X and %d format verbs.\nfunc (a Address) Format(s fmt.State, c rune) {\n\tswitch c {\n\tcase 'v', 's':\n\t\ts.Write(a.checksumHex())\n\tcase 'q':\n\t\tq := []byte{'\"'}\n\t\ts.Write(q)\n\t\ts.Write(a.checksumHex())\n\t\ts.Write(q)\n\tcase 'x', 'X':\n\t\t// %x disables the checksum.\n\t\thex := a.hex()\n\t\tif !s.Flag('#') {\n\t\t\thex = hex[2:]\n\t\t}\n\t\tif c == 'X' {\n\t\t\thex = bytes.ToUpper(hex)\n\t\t}\n\t\ts.Write(hex)\n\tcase 'd':\n\t\tfmt.Fprint(s, ([len(a)]byte)(a))\n\tdefault:\n\t\tfmt.Fprintf(s, \"%%!%c(address=%x)\", c, a)\n\t}\n}\n\n// SetBytes sets the address to the value of b.\n// If b is larger than len(a), b will be cropped from the left.\nfunc (a *Address) SetBytes(b []byte) {\n\tif len(b) > len(a) {\n\t\tb = b[len(b)-AddressLength:]\n\t}\n\tcopy(a[AddressLength-len(b):], b)\n}\n\n// MarshalText returns the hex representation of a.\nfunc (a Address) MarshalText() ([]byte, error) {\n\treturn hexutil.Bytes(a[:]).MarshalText()\n}\n\n// UnmarshalText parses a hash in hex syntax.\nfunc (a *Address) UnmarshalText(input []byte) error {\n\treturn hexutil.UnmarshalFixedText(\"Address\", input, a[:])\n}\n\n// UnmarshalJSON parses a hash in hex syntax.\nfunc (a *Address) UnmarshalJSON(input []byte) error {\n\treturn hexutil.UnmarshalFixedJSON(addressT, input, a[:])\n}\n\n// Scan implements Scanner for database/sql.\nfunc (a *Address) Scan(src interface{}) error {\n\tsrcB, ok := src.([]byte)\n\tif !ok {\n\t\treturn fmt.Errorf(\"can't scan %T into Address\", src)\n\t}\n\tif len(srcB) != AddressLength {\n\t\treturn fmt.Errorf(\"can't scan []byte of len %d into Address, want %d\", len(srcB), AddressLength)\n\t}\n\tcopy(a[:], srcB)\n\treturn nil\n}\n\n// Value implements valuer for database/sql.\nfunc (a Address) Value() (driver.Value, error) {\n\treturn a[:], nil\n}\n\n// ImplementsGraphQLType returns true if Hash implements the specified GraphQL type.\nfunc (a Address) ImplementsGraphQLType(name string) bool { return name == \"Address\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (a *Address) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\terr = a.UnmarshalText([]byte(input))\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for Address\", input)\n\t}\n\treturn err\n}\n\n// UnprefixedAddress allows marshaling an Address without 0x prefix.\ntype UnprefixedAddress Address\n\n// UnmarshalText decodes the address from hex. The 0x prefix is optional.\nfunc (a *UnprefixedAddress) UnmarshalText(input []byte) error {\n\treturn hexutil.UnmarshalFixedUnprefixedText(\"UnprefixedAddress\", input, a[:])\n}\n\n// MarshalText encodes the address as hex.\nfunc (a UnprefixedAddress) MarshalText() ([]byte, error) {\n\treturn []byte(hex.EncodeToString(a[:])), nil\n}\n\n// MixedcaseAddress retains the original string, which may or may not be\n// correctly checksummed\ntype MixedcaseAddress struct {\n\taddr     Address\n\toriginal string\n}\n\n// NewMixedcaseAddress constructor (mainly for testing)\nfunc NewMixedcaseAddress(addr Address) MixedcaseAddress {\n\treturn MixedcaseAddress{addr: addr, original: addr.Hex()}\n}\n\n// NewMixedcaseAddressFromString is mainly meant for unit-testing\nfunc NewMixedcaseAddressFromString(hexaddr string) (*MixedcaseAddress, error) {\n\tif !IsHexAddress(hexaddr) {\n\t\treturn nil, errors.New(\"invalid address\")\n\t}\n\ta := FromHex(hexaddr)\n\treturn &MixedcaseAddress{addr: BytesToAddress(a), original: hexaddr}, nil\n}\n\n// UnmarshalJSON parses MixedcaseAddress\nfunc (ma *MixedcaseAddress) UnmarshalJSON(input []byte) error {\n\tif err := hexutil.UnmarshalFixedJSON(addressT, input, ma.addr[:])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/format.go",
          "line": 76,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/stacktrie_test.go",
          "line": 417,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1024\n\t\t}\n\t\tif hash == (common.Hash{}) {\n\t\t\thash = s.Hash()\n\t\t} else {\n\t\t\tif hash != s.Hash() && false {\n\t\t\t\tb.Fatalf(\"hash wrong, have %x want %x\", s.Hash(), hash)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInsert100K(t *testing.T) {\n\tvar num = 100_000\n\tvar key = make([]byte, 8)\n\tvar val = make([]byte, 20)\n\ts := NewStackTrie(nil)\n\tvar k uint64\n\tfor j := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/trie/stacktrie_test.go",
          "line": 440,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1024\n\t}\n\twant := common.HexToHash(\"0xb0071bd257342925d9d8a9f002b9d2b646a35437aa8b089628ab56e428d29a1a\")\n\tif have := s.Hash()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/verkle_test.go",
          "line": 101,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\t\tcode[i] = 0x60\n\t\t\tcode[i+1] = byte(i % 256)\n\t\t}\n\t\tif err := tr.UpdateAccount(addr, acct, len(code))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/stacktrie_fuzzer_test.go",
          "line": 139,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tif checked != len(nodeset) {\n\t\tpanic(\"node number is not matched\")\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/sync.go",
          "line": 217,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength + uint64(len(code))\n}\n\n// addNode caches a node database write operation.\nfunc (batch *syncMemBatch) addNode(owner common.Hash, path []byte, blob []byte, hash common.Hash) {\n\tif batch.scheme == rawdb.PathScheme {\n\t\tif owner == (common.Hash{}) {\n\t\t\tbatch.size += uint64(len(path) + len(blob))\n\t\t} else {\n\t\t\tbatch.size += common.HashLength + uint64(len(path)+len(blob))\n\t\t}\n\t} else {\n\t\tbatch.size += common.HashLength + uint64(len(blob))\n\t}\n\tbatch.nodes = append(batch.nodes, nodeOp{\n\t\towner: owner,\n\t\tpath:  path,\n\t\tblob:  blob,\n\t\thash:  hash,\n\t})\n}\n\n// delNode caches a node database delete operation.\nfunc (batch *syncMemBatch) delNode(owner common.Hash, path []byte) {\n\tif batch.scheme != rawdb.PathScheme {\n\t\tlog.Error(\"Unexpected node deletion\", \"owner\", owner, \"path\", path, \"scheme\", batch.scheme)\n\t\treturn // deletion is not supported in hash mode.\n\t}\n\tif owner == (common.Hash{}) {\n\t\tbatch.size += uint64(len(path))\n\t} else {\n\t\tbatch.size += common.HashLength + uint64(len(path))\n\t}\n\tbatch.nodes = append(batch.nodes, nodeOp{\n\t\tdel:   true,\n\t\towner: owner,\n\t\tpath:  path,\n\t})\n}\n\n// Sync is the main state trie synchronisation scheduler, which provides yet\n// unknown trie hashes to retrieve, accepts node data associated with said hashes\n// and reconstructs the trie step by step until all is done.\ntype Sync struct {\n\tscheme   string                       // Node scheme descriptor used in database.\n\tdatabase ethdb.Database               // Persistent database to check for existing entries\n\tmembatch *syncMemBatch                // Memory buffer to avoid frequent database writes\n\tnodeReqs map[string]*nodeRequest      // Pending requests pertaining to a trie node path\n\tcodeReqs map[common.Hash]*codeRequest // Pending requests pertaining to a code hash\n\tqueue    *prque.Prque[int64, any]     // Priority queue with the pending requests\n\tfetches  map[int]int                  // Number of active fetches per trie node depth\n}\n\n// NewSync creates a new trie data download scheduler.\nfunc NewSync(root common.Hash, database ethdb.Database, callback LeafCallback, scheme string) *Sync {\n\tts := &Sync{\n\t\tscheme:   scheme,\n\t\tdatabase: database,\n\t\tmembatch: newSyncMemBatch(scheme),\n\t\tnodeReqs: make(map[string]*nodeRequest),\n\t\tcodeReqs: make(map[common.Hash]*codeRequest),\n\t\tqueue:    prque.New[int64, any](nil), // Ugh, can contain both string and hash, whyyy\n\t\tfetches:  make(map[int]int),\n\t}\n\tts.AddSubTrie(root, nil, common.Hash{}, nil, callback)\n\treturn ts\n}\n\n// AddSubTrie registers a new trie to the sync code, rooted at the designated\n// parent for completion tracking. The given path is a unique node path in\n// hex format and contain all the parent path if it's layered trie node.\nfunc (s *Sync) AddSubTrie(root common.Hash, path []byte, parent common.Hash, parentPath []byte, callback LeafCallback) {\n\tif root == types.EmptyRootHash {\n\t\treturn\n\t}\n\towner, inner := ResolvePath(path)\n\texist, inconsistent := s.hasNode(owner, inner, root)\n\tif exist {\n\t\t// The entire subtrie is already present in the database.\n\t\treturn\n\t} else if inconsistent {\n\t\t// There is a pre-existing node with the wrong hash in DB, remove it.\n\t\ts.membatch.delNode(owner, inner)\n\t}\n\t// Assemble the new sub-trie sync request\n\treq := &nodeRequest{\n\t\thash:     root,\n\t\tpath:     path,\n\t\tcallback: callback,\n\t}\n\t// If this sub-trie has a designated parent, link them together\n\tif parent != (common.Hash{}) {\n\t\tancestor := s.nodeReqs[string(parentPath)]\n\t\tif ancestor == nil {\n\t\t\tpanic(fmt.Sprintf(\"sub-trie ancestor not found: %x\", parent))\n\t\t}\n\t\tancestor.deps++\n\t\treq.parent = ancestor\n\t}\n\ts.scheduleNodeRequest(req)\n}\n\n// AddCodeEntry schedules the direct retrieval of a contract code that should not\n// be interpreted as a trie node, but rather accepted and stored into the database\n// as is.\nfunc (s *Sync) AddCodeEntry(hash common.Hash, path []byte, parent common.Hash, parentPath []byte) {\n\t// Short circuit if the entry is empty or already known\n\tif hash == types.EmptyCodeHash {\n\t\treturn\n\t}\n\tif s.membatch.hasCode(hash) {\n\t\treturn\n\t}\n\t// If database says duplicate, the blob is present for sure.\n\t// Note we only check the existence with new code scheme, snap\n\t// sync is expected to run with a fresh new node. Even there\n\t// exists the code with legacy format, fetch and store with\n\t// new scheme anyway.\n\tif rawdb.HasCodeWithPrefix(s.database, hash) {\n\t\treturn\n\t}\n\t// Assemble the new sub-trie sync request\n\treq := &codeRequest{\n\t\tpath: path,\n\t\thash: hash,\n\t}\n\t// If this sub-trie has a designated parent, link them together\n\tif parent != (common.Hash{}) {\n\t\tancestor := s.nodeReqs[string(parentPath)] // the parent of codereq can ONLY be nodereq\n\t\tif ancestor == nil {\n\t\t\tpanic(fmt.Sprintf(\"raw-entry ancestor not found: %x\", parent))\n\t\t}\n\t\tancestor.deps++\n\t\treq.parents = append(req.parents, ancestor)\n\t}\n\ts.scheduleCodeRequest(req)\n}\n\n// Missing retrieves the known missing nodes from the trie for retrieval. To aid\n// both eth/6x style fast sync and snap/1x style state sync, the paths of trie\n// nodes are returned too, as well as separate hash list for codes.\nfunc (s *Sync) Missing(max int) ([]string, []common.Hash, []common.Hash) {\n\tvar (\n\t\tnodePaths  []string\n\t\tnodeHashes []common.Hash\n\t\tcodeHashes []common.Hash\n\t)\n\tfor !s.queue.Empty() && (max == 0 || len(nodeHashes)+len(codeHashes) < max) {\n\t\t// Retrieve the next item in line\n\t\titem, prio := s.queue.Peek()\n\n\t\t// If we have too many already-pending tasks for this depth, throttle\n\t\tdepth := int(prio >> 56)\n\t\tif s.fetches[depth] > maxFetchesPerDepth {\n\t\t\tbreak\n\t\t}\n\t\t// Item is allowed to be scheduled, add it to the task list\n\t\ts.queue.Pop()\n\t\ts.fetches[depth]++\n\n\t\tswitch item := item.(type) {\n\t\tcase common.Hash:\n\t\t\tcodeHashes = append(codeHashes, item)\n\t\tcase string:\n\t\t\treq, ok := s.nodeReqs[item]\n\t\t\tif !ok {\n\t\t\t\tlog.Error(\"Missing node request\", \"path\", item)\n\t\t\t\tcontinue // System very wrong, shouldn't happen\n\t\t\t}\n\t\t\tnodePaths = append(nodePaths, item)\n\t\t\tnodeHashes = append(nodeHashes, req.hash)\n\t\t}\n\t}\n\treturn nodePaths, nodeHashes, codeHashes\n}\n\n// ProcessCode injects the received data for requested item. Note it can\n// happen that the single response commits two pending requests(e.g.\n// there are two requests one for code and one for node but the hash\n// is same). In this case the second response for the same hash will\n// be treated as \"non-requested\" item or \"already-processed\" item but\n// there is no downside.\nfunc (s *Sync) ProcessCode(result CodeSyncResult) error {\n\t// If the code was not requested or it's already processed, bail out\n\treq := s.codeReqs[result.Hash]\n\tif req == nil {\n\t\treturn ErrNotRequested\n\t}\n\tif req.data != nil {\n\t\treturn ErrAlreadyProcessed\n\t}\n\treq.data = result.Data\n\treturn s.commitCodeRequest(req)\n}\n\n// ProcessNode injects the received data for requested item. Note it can\n// happen that the single response commits two pending requests(e.g.\n// there are two requests one for code and one for node but the hash\n// is same). In this case the second response for the same hash will\n// be treated as \"non-requested\" item or \"already-processed\" item but\n// there is no downside.\nfunc (s *Sync) ProcessNode(result NodeSyncResult) error {\n\t// If the trie node was not requested or it's already processed, bail out\n\treq := s.nodeReqs[result.Path]\n\tif req == nil {\n\t\treturn ErrNotRequested\n\t}\n\tif req.data != nil {\n\t\treturn ErrAlreadyProcessed\n\t}\n\t// Decode the node data content and update the request\n\tnode, err := decodeNode(req.hash.Bytes(), result.Data)\n\tif err != nil {\n\t\treturn err\n\t}\n\treq.data = result.Data\n\n\t// Create and schedule a request for all the children nodes\n\trequests, err := s.children(req, node)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(requests) == 0 && req.deps == 0 {\n\t\ts.commitNodeRequest(req)\n\t} else {\n\t\treq.deps += len(requests)\n\t\tfor _, child := range requests {\n\t\t\ts.scheduleNodeRequest(child)\n\t\t}\n\t}\n\treturn nil\n}\n\n// Commit flushes the data stored in the internal membatch out to persistent\n// storage, returning any occurred error. The whole data set will be flushed\n// in an atomic database batch.\nfunc (s *Sync) Commit(dbw ethdb.Batch, stateBatch ethdb.Batch) error {\n\t// Flush the pending node writes into database batch.\n\tvar (\n\t\taccount int\n\t\tstorage int\n\t)\n\tfor _, op := range s.membatch.nodes {\n\t\tif !op.valid() {\n\t\t\treturn fmt.Errorf(\"invalid op, %s\", op.string())\n\t\t}\n\t\tif op.del {\n\t\t\t// node deletion is only supported in path mode.\n\t\t\tif op.owner == (common.Hash{}) {\n\t\t\t\tif stateBatch != nil {\n\t\t\t\t\trawdb.DeleteAccountTrieNode(stateBatch, op.path)\n\t\t\t\t} else {\n\t\t\t\t\trawdb.DeleteAccountTrieNode(dbw, op.path)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif stateBatch != nil {\n\t\t\t\t\trawdb.DeleteStorageTrieNode(stateBatch, op.owner, op.path)\n\t\t\t\t} else {\n\t\t\t\t\trawdb.DeleteStorageTrieNode(dbw, op.owner, op.path)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdeletionGauge.Inc(1)\n\t\t} else {\n\t\t\tif op.owner == (common.Hash{}) {\n\t\t\t\taccount += 1\n\t\t\t} else {\n\t\t\t\tstorage += 1\n\t\t\t}\n\t\t\tif stateBatch != nil {\n\t\t\t\trawdb.WriteTrieNode(stateBatch, op.owner, op.path, op.hash, op.blob, s.scheme)\n\t\t\t} else {\n\t\t\t\trawdb.WriteTrieNode(dbw, op.owner, op.path, op.hash, op.blob, s.scheme)\n\t\t\t}\n\t\t}\n\t}\n\taccountNodeSyncedGauge.Inc(int64(account))\n\tstorageNodeSyncedGauge.Inc(int64(storage))\n\n\t// Flush the pending code writes into database batch.\n\tfor hash, value := range s.membatch.codes {\n\t\trawdb.WriteCode(dbw, hash, value)\n\t}\n\tcodeSyncedGauge.Inc(int64(len(s.membatch.codes)))\n\n\ts.membatch = newSyncMemBatch(s.scheme) // reset the batch\n\treturn nil\n}\n\n// MemSize returns an estimated size (in bytes) of the data held in the membatch.\nfunc (s *Sync) MemSize() uint64 {\n\treturn s.membatch.size\n}\n\n// Pending returns the number of state entries currently pending for download.\nfunc (s *Sync) Pending() int {\n\treturn len(s.nodeReqs) + len(s.codeReqs)\n}\n\n// scheduleNodeRequest inserts a new state retrieval request into the fetch queue. If there\n// is already a pending request for this node, the new request will be discarded\n// and only a parent reference added to the old one.\nfunc (s *Sync) scheduleNodeRequest(req *nodeRequest) {\n\ts.nodeReqs[string(req.path)] = req\n\n\t// Schedule the request for future retrieval. This queue is shared\n\t// by both node requests and code requests.\n\tprio := int64(len(req.path)) << 56 // depth >= 128 will never happen, storage leaves will be included in their parents\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/iterator_test.go",
          "line": 169,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tif elem, ok := elements[crypto.Keccak256Hash(it.Value())]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/trie/iterator_test.go",
          "line": 594,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tif count != len(found) {\n\t\tt.Fatal(\"Find extra trie node via iterator\")\n\t}\n}\n\n// isTrieNode is a helper function which reports if the provided\n// database entry belongs to a trie node or not. Note in tests\n// only single layer trie is used, namely storage trie is not\n// considered at all.\nfunc isTrieNode(scheme string, key, val []byte) (bool, []byte, common.Hash) {\n\tvar (\n\t\tpath []byte\n\t\thash common.Hash\n\t)\n\tif scheme == rawdb.HashScheme {\n\t\tok := rawdb.IsLegacyTrieNode(key, val)\n\t\tif !ok {\n\t\t\treturn false, nil, common.Hash{}\n\t\t}\n\t\thash = common.BytesToHash(key)\n\t} else {\n\t\tok, remain := rawdb.ResolveAccountTrieNodeKey(key)\n\t\tif !ok {\n\t\t\treturn false, nil, common.Hash{}\n\t\t}\n\t\tpath = common.CopyBytes(remain)\n\t\thash = crypto.Keccak256Hash(val)\n\t}\n\treturn true, path, hash\n}\n\nfunc BenchmarkIterator(b *testing.B) {\n\tdiskDb, srcDb, tr, _ := makeTestTrie(rawdb.HashScheme)\n\troot := tr.Hash()\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/trie_test.go",
          "line": 1301,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 100\n\t\t}\n\t\tt.Fatalf(\"have != want\\nhave %q\\nwant %q\", have[i:], want[i:])\n\t}\n}\nfunc printSet(set *trienode.NodeSet) string {\n\tvar out = new(strings.Builder)\n\tfmt.Fprintf(out, \"nodeset owner: %v\\n\", set.Owner)\n\tvar paths []string\n\tfor k := range set.Nodes {\n\t\tpaths = append(paths, k)\n\t}\n\tsort.Strings(paths)\n\n\tfor _, path := range paths {\n\t\tn := set.Nodes[path]\n\t\t// Deletion\n\t\tif n.IsDeleted() {\n\t\t\tfmt.Fprintf(out, \"  [-]: %x\\n\", path)\n\t\t\tcontinue\n\t\t}\n\t\t// Insertion or update\n\t\tfmt.Fprintf(out, \"  [+/*]: %x -> %v \\n\", path, n.Hash)\n\t}\n\tsort.Slice(set.Leaves, func(i, j int) bool {\n\t\ta := set.Leaves[i]\n\t\tb := set.Leaves[j]\n\t\treturn bytes.Compare(a.Parent[:], b.Parent[:]) < 0\n\t})\n\tfor _, n := range set.Leaves {\n\t\tfmt.Fprintf(out, \"[leaf]: %v\\n\", n)\n\t}\n\treturn out.String()\n}\n\nfunc TestTrieCopy(t *testing.T) {\n\ttestTrieCopy(t, []kv{\n\t\t{k: []byte(\"do\"), v: []byte(\"verb\")},\n\t\t{k: []byte(\"ether\"), v: []byte(\"wookiedoo\")},\n\t\t{k: []byte(\"horse\"), v: []byte(\"stallion\")},\n\t\t{k: []byte(\"shaman\"), v: []byte(\"horse\")},\n\t\t{k: []byte(\"doge\"), v: []byte(\"coin\")},\n\t\t{k: []byte(\"dog\"), v: []byte(\"puppy\")},\n\t})\n\n\tvar entries []kv\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/node.go",
          "line": 110,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"%s: <nil> \", indices[i])\n\t\t} else {\n\t\t\tresp += fmt.Sprintf(\"%s: %v\", indices[i], node.fstring(ind+\"  \"))\n\t\t}\n\t}\n\treturn resp + fmt.Sprintf(\"\\n%s] \", ind)\n}\n\nfunc (n *shortNode) fstring(ind string) string {\n\treturn fmt.Sprintf(\"{%x: %v} \", n.Key, n.Val.fstring(ind+\"  \"))\n}\nfunc (n hashNode) fstring(ind string) string {\n\treturn fmt.Sprintf(\"<%x> \", []byte(n))\n}\nfunc (n valueNode) fstring(ind string) string {\n\treturn fmt.Sprintf(\"%x \", []byte(n))\n}\n\nfunc NodeString(hash, buf []byte) string {\n\tnode := mustDecodeNode(hash, buf)\n\treturn node.fstring(\"NodeString: \")\n}\n\n// mustDecodeNode is a wrapper of decodeNode and panic if any error is encountered.\nfunc mustDecodeNode(hash, buf []byte) node {\n\tn, err := decodeNode(hash, buf)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"node %x: %v\", hash, err))\n\t}\n\treturn n\n}\n\n// mustDecodeNodeUnsafe is a wrapper of decodeNodeUnsafe and panic if any error is\n// encountered.\nfunc mustDecodeNodeUnsafe(hash, buf []byte) node {\n\tn, err := decodeNodeUnsafe(hash, buf)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"node %x: %v\", hash, err))\n\t}\n\treturn n\n}\n\n// decodeNode parses the RLP encoding of a trie node. It will deep-copy the passed\n// byte slice for decoding, so it's safe to modify the byte slice afterwards. The-\n// decode performance of this function is not optimal, but it is suitable for most\n// scenarios with low performance requirements and hard to determine whether the\n// byte slice be modified or not.\nfunc decodeNode(hash, buf []byte) (node, error) {\n\treturn decodeNodeUnsafe(hash, common.CopyBytes(buf))\n}\n\n// decodeNodeUnsafe parses the RLP encoding of a trie node. The passed byte slice\n// will be directly referenced by node without bytes deep copy, so the input MUST\n// not be changed after.\nfunc decodeNodeUnsafe(hash, buf []byte) (node, error) {\n\tif len(buf) == 0 {\n\t\treturn nil, io.ErrUnexpectedEOF\n\t}\n\telems, _, err := rlp.SplitList(buf)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"decode error: %v\", err)\n\t}\n\tswitch c, _ := rlp.CountValues(elems)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/verkle.go",
          "line": 374,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(code[codeOffset] - PUSH1 + 1)\n\t\t\t\tif codeOffset+1 >= 31*(i+1) {\n\t\t\t\t\tcodeOffset++\n\t\t\t\t\tchunkOffset = codeOffset - 31*(i+1)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn chunks\n}\n\n// UpdateContractCode implements state.Trie, writing the provided contract code\n// into the trie.\n// Note that the code-size *must* be already saved by a previous UpdateAccount call.\nfunc (t *VerkleTrie) UpdateContractCode(addr common.Address, codeHash common.Hash, code []byte) error {\n\tvar (\n\t\tchunks = ChunkifyCode(code)\n\t\tvalues [][]byte\n\t\tkey    []byte\n\t\terr    error\n\t)\n\tfor i, chunknr := 0, uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/hd.go",
          "line": 112,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint32(bigval.Uint64())\n\n\t\t// Append and repeat\n\t\tresult = append(result, value)\n\t}\n\treturn result, nil\n}\n\n// String implements the stringer interface, converting a binary derivation path\n// to its canonical representation.\nfunc (path DerivationPath) String() string {\n\tresult := \"m\"\n\tfor _, component := range path {\n\t\tvar hardened bool\n\t\tif component >= 0x80000000 {\n\t\t\tcomponent -= 0x80000000\n\t\t\thardened = true\n\t\t}\n\t\tresult = fmt.Sprintf(\"%s/%d\", result, component)\n\t\tif hardened {\n\t\t\tresult += \"'\"\n\t\t}\n\t}\n\treturn result\n}\n\n// MarshalJSON turns a derivation path into its json-serialized string\nfunc (path DerivationPath) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(path.String())\n}\n\n// UnmarshalJSON a json-serialized string back into a derivation path\nfunc (path *DerivationPath) UnmarshalJSON(b []byte) error {\n\tvar dp string\n\tvar err error\n\tif err = json.Unmarshal(b, &dp)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/hd.go",
          "line": 127,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 0x80000000\n\t\t\thardened = true\n\t\t}\n\t\tresult = fmt.Sprintf(\"%s/%d\", result, component)\n\t\tif hardened {\n\t\t\tresult += \"'\"\n\t\t}\n\t}\n\treturn result\n}\n\n// MarshalJSON turns a derivation path into its json-serialized string\nfunc (path DerivationPath) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(path.String())\n}\n\n// UnmarshalJSON a json-serialized string back into a derivation path\nfunc (path *DerivationPath) UnmarshalJSON(b []byte) error {\n\tvar dp string\n\tvar err error\n\tif err = json.Unmarshal(b, &dp)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/manager.go",
          "line": 147,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/params/config.go",
          "line": 733,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"Chain ID:  %v (%s)\\n\", c.ChainID, network)\n\tswitch {\n\tcase c.Parlia != nil:\n\t\tbanner += \"Consensus: Parlia (proof-of-staked--authority)\\n\"\n\tcase c.Ethash != nil:\n\t\tbanner += \"Consensus: Beacon (proof-of-stake), merged from Ethash (proof-of-work)\\n\"\n\tcase c.Clique != nil:\n\t\tbanner += \"Consensus: Beacon (proof-of-stake), merged from Clique (proof-of-authority)\\n\"\n\tdefault:\n\t\tbanner += \"Consensus: unknown\\n\"\n\t}\n\n\treturn banner\n}\n\n// String implements the fmt.Stringer interface.\nfunc (c *ChainConfig) String() string {\n\tvar engine interface{}\n\n\tswitch {\n\tcase c.Ethash != nil:\n\t\tengine = c.Ethash\n\tcase c.Clique != nil:\n\t\tengine = c.Clique\n\tcase c.Parlia != nil:\n\t\tengine = c.Parlia\n\tdefault:\n\t\tengine = \"unknown\"\n\t}\n\n\tvar ShanghaiTime *big.Int\n\tif c.ShanghaiTime != nil {\n\t\tShanghaiTime = big.NewInt(0).SetUint64(*c.ShanghaiTime)\n\t}\n\n\tvar KeplerTime *big.Int\n\tif c.KeplerTime != nil {\n\t\tKeplerTime = big.NewInt(0).SetUint64(*c.KeplerTime)\n\t}\n\n\tvar FeynmanTime *big.Int\n\tif c.FeynmanTime != nil {\n\t\tFeynmanTime = big.NewInt(0).SetUint64(*c.FeynmanTime)\n\t}\n\n\tvar FeynmanFixTime *big.Int\n\tif c.FeynmanFixTime != nil {\n\t\tFeynmanFixTime = big.NewInt(0).SetUint64(*c.FeynmanFixTime)\n\t}\n\n\tvar CancunTime *big.Int\n\tif c.CancunTime != nil {\n\t\tCancunTime = big.NewInt(0).SetUint64(*c.CancunTime)\n\t}\n\n\tvar HaberTime *big.Int\n\tif c.HaberTime != nil {\n\t\tHaberTime = big.NewInt(0).SetUint64(*c.HaberTime)\n\t}\n\n\tvar HaberFixTime *big.Int\n\tif c.HaberFixTime != nil {\n\t\tHaberFixTime = big.NewInt(0).SetUint64(*c.HaberFixTime)\n\t}\n\n\tvar BohrTime *big.Int\n\tif c.BohrTime != nil {\n\t\tBohrTime = big.NewInt(0).SetUint64(*c.BohrTime)\n\t}\n\n\tvar PascalTime *big.Int\n\tif c.PascalTime != nil {\n\t\tPascalTime = big.NewInt(0).SetUint64(*c.PascalTime)\n\t}\n\n\tvar PragueTime *big.Int\n\tif c.PragueTime != nil {\n\t\tPragueTime = big.NewInt(0).SetUint64(*c.PragueTime)\n\t}\n\n\tvar LorentzTime *big.Int\n\tif c.LorentzTime != nil {\n\t\tLorentzTime = big.NewInt(0).SetUint64(*c.LorentzTime)\n\t}\n\n\tvar MaxwellTime *big.Int\n\tif c.MaxwellTime != nil {\n\t\tMaxwellTime = big.NewInt(0).SetUint64(*c.MaxwellTime)\n\t}\n\n\tvar FermiTime *big.Int\n\tif c.FermiTime != nil {\n\t\tFermiTime = big.NewInt(0).SetUint64(*c.FermiTime)\n\t}\n\n\treturn fmt.Sprintf(\"{ChainID: %v, Engine: %v, Homestead: %v DAO: %v DAOSupport: %v EIP150: %v EIP155: %v EIP158: %v Byzantium: %v Constantinople: %v Petersburg: %v Istanbul: %v, Muir Glacier: %v, Ramanujan: %v, Niels: %v, \"+\n\t\t\"MirrorSync: %v, Bruno: %v, Berlin: %v, YOLO v3: %v, CatalystBlock: %v, London: %v, ArrowGlacier: %v, MergeFork:%v, Euler: %v, Gibbs: %v, Nano: %v, Moran: %v, Planck: %v,Luban: %v, Plato: %v, Hertz: %v, Hertzfix: %v, \"+\n\t\t\"ShanghaiTime: %v, KeplerTime: %v, FeynmanTime: %v, FeynmanFixTime: %v, CancunTime: %v, HaberTime: %v, HaberFixTime: %v, BohrTime: %v, PascalTime: %v, PragueTime: %v, LorentzTime: %v, MaxwellTime: %v, FermiTime: %v}\",\n\t\tc.ChainID,\n\t\tengine,\n\t\tc.HomesteadBlock,\n\t\tc.DAOForkBlock,\n\t\tc.DAOForkSupport,\n\t\tc.EIP150Block,\n\t\tc.EIP155Block,\n\t\tc.EIP158Block,\n\t\tc.ByzantiumBlock,\n\t\tc.ConstantinopleBlock,\n\t\tc.PetersburgBlock,\n\t\tc.IstanbulBlock,\n\t\tc.MuirGlacierBlock,\n\t\tc.RamanujanBlock,\n\t\tc.NielsBlock,\n\t\tc.MirrorSyncBlock,\n\t\tc.BrunoBlock,\n\t\tc.BerlinBlock,\n\t\tc.YoloV3Block,\n\t\tc.CatalystBlock,\n\t\tc.LondonBlock,\n\t\tc.ArrowGlacierBlock,\n\t\tc.MergeNetsplitBlock,\n\t\tc.EulerBlock,\n\t\tc.GibbsBlock,\n\t\tc.NanoBlock,\n\t\tc.MoranBlock,\n\t\tc.PlanckBlock,\n\t\tc.LubanBlock,\n\t\tc.PlatoBlock,\n\t\tc.HertzBlock,\n\t\tc.HertzfixBlock,\n\t\tShanghaiTime,\n\t\tKeplerTime,\n\t\tFeynmanTime,\n\t\tFeynmanFixTime,\n\t\tCancunTime,\n\t\tHaberTime,\n\t\tHaberFixTime,\n\t\tBohrTime,\n\t\tPascalTime,\n\t\tPragueTime,\n\t\tLorentzTime,\n\t\tMaxwellTime,\n\t\tFermiTime,\n\t)\n}\n\n// BlobConfig specifies the target and max blobs per block for the associated fork.\ntype BlobConfig struct {\n\tTarget         int    `json:\"target\"`\n\tMax            int    `json:\"max\"`\n\tUpdateFraction uint64 `json:\"baseFeeUpdateFraction\"`\n}\n\n// BlobScheduleConfig determines target and max number of blobs allow per fork.\ntype BlobScheduleConfig struct {\n\tCancun *BlobConfig `json:\"cancun,omitempty\"`\n\tPrague *BlobConfig `json:\"prague,omitempty\"`\n\tOsaka  *BlobConfig `json:\"osaka,omitempty\"`\n\tVerkle *BlobConfig `json:\"verkle,omitempty\"`\n}\n\n// IsHomestead returns whether num is either equal to the homestead block or greater.\nfunc (c *ChainConfig) IsHomestead(num *big.Int) bool {\n\treturn isBlockForked(c.HomesteadBlock, num)\n}\n\n// IsDAOFork returns whether num is either equal to the DAO fork block or greater.\nfunc (c *ChainConfig) IsDAOFork(num *big.Int) bool {\n\treturn isBlockForked(c.DAOForkBlock, num)\n}\n\n// IsEIP150 returns whether num is either equal to the EIP150 fork block or greater.\nfunc (c *ChainConfig) IsEIP150(num *big.Int) bool {\n\treturn isBlockForked(c.EIP150Block, num)\n}\n\n// IsEIP155 returns whether num is either equal to the EIP155 fork block or greater.\nfunc (c *ChainConfig) IsEIP155(num *big.Int) bool {\n\treturn isBlockForked(c.EIP155Block, num)\n}\n\n// IsEIP158 returns whether num is either equal to the EIP158 fork block or greater.\nfunc (c *ChainConfig) IsEIP158(num *big.Int) bool {\n\treturn isBlockForked(c.EIP158Block, num)\n}\n\n// IsByzantium returns whether num is either equal to the Byzantium fork block or greater.\nfunc (c *ChainConfig) IsByzantium(num *big.Int) bool {\n\treturn isBlockForked(c.ByzantiumBlock, num)\n}\n\n// IsConstantinople returns whether num is either equal to the Constantinople fork block or greater.\nfunc (c *ChainConfig) IsConstantinople(num *big.Int) bool {\n\treturn isBlockForked(c.ConstantinopleBlock, num)\n}\n\n// IsRamanujan returns whether num is either equal to the IsRamanujan fork block or greater.\nfunc (c *ChainConfig) IsRamanujan(num *big.Int) bool {\n\treturn isBlockForked(c.RamanujanBlock, num)\n}\n\n// IsOnRamanujan returns whether num is equal to the Ramanujan fork block\nfunc (c *ChainConfig) IsOnRamanujan(num *big.Int) bool {\n\treturn configBlockEqual(c.RamanujanBlock, num)\n}\n\n// IsNiels returns whether num is either equal to the Niels fork block or greater.\nfunc (c *ChainConfig) IsNiels(num *big.Int) bool {\n\treturn isBlockForked(c.NielsBlock, num)\n}\n\n// IsOnNiels returns whether num is equal to the IsNiels fork block\nfunc (c *ChainConfig) IsOnNiels(num *big.Int) bool {\n\treturn configBlockEqual(c.NielsBlock, num)\n}\n\n// IsMirrorSync returns whether num is either equal to the MirrorSync fork block or greater.\nfunc (c *ChainConfig) IsMirrorSync(num *big.Int) bool {\n\treturn isBlockForked(c.MirrorSyncBlock, num)\n}\n\n// IsOnMirrorSync returns whether num is equal to the MirrorSync fork block\nfunc (c *ChainConfig) IsOnMirrorSync(num *big.Int) bool {\n\treturn configBlockEqual(c.MirrorSyncBlock, num)\n}\n\n// IsBruno returns whether num is either equal to the Burn fork block or greater.\nfunc (c *ChainConfig) IsBruno(num *big.Int) bool {\n\treturn isBlockForked(c.BrunoBlock, num)\n}\n\n// IsOnBruno returns whether num is equal to the Burn fork block\nfunc (c *ChainConfig) IsOnBruno(num *big.Int) bool {\n\treturn configBlockEqual(c.BrunoBlock, num)\n}\n\n// IsEuler returns whether num is either equal to the euler fork block or greater.\nfunc (c *ChainConfig) IsEuler(num *big.Int) bool {\n\treturn isBlockForked(c.EulerBlock, num)\n}\n\n// IsOnEuler returns whether num is equal to the euler fork block\nfunc (c *ChainConfig) IsOnEuler(num *big.Int) bool {\n\treturn configBlockEqual(c.EulerBlock, num)\n}\n\n// IsLuban returns whether num is either equal to the first fast finality fork block or greater.\nfunc (c *ChainConfig) IsLuban(num *big.Int) bool {\n\treturn isBlockForked(c.LubanBlock, num)\n}\n\n// IsOnLuban returns whether num is equal to the first fast finality fork block.\nfunc (c *ChainConfig) IsOnLuban(num *big.Int) bool {\n\treturn configBlockEqual(c.LubanBlock, num)\n}\n\n// IsPlato returns whether num is either equal to the second fast finality fork block or greater.\nfunc (c *ChainConfig) IsPlato(num *big.Int) bool {\n\treturn isBlockForked(c.PlatoBlock, num)\n}\n\n// IsOnPlato returns whether num is equal to the second fast finality fork block.\nfunc (c *ChainConfig) IsOnPlato(num *big.Int) bool {\n\treturn configBlockEqual(c.PlatoBlock, num)\n}\n\n// IsHertz returns whether num is either equal to the block of enabling Berlin EIPs or greater.\nfunc (c *ChainConfig) IsHertz(num *big.Int) bool {\n\treturn isBlockForked(c.HertzBlock, num)\n}\n\n// IsOnHertz returns whether num is equal to the fork block of enabling Berlin EIPs.\nfunc (c *ChainConfig) IsOnHertz(num *big.Int) bool {\n\treturn configBlockEqual(c.HertzBlock, num)\n}\n\nfunc (c *ChainConfig) IsHertzfix(num *big.Int) bool {\n\treturn isBlockForked(c.HertzfixBlock, num)\n}\n\nfunc (c *ChainConfig) NeedBadSharedStorage(num *big.Int) bool {\n\tif c.IsHertzfix(num) || c.ChainID == nil {\n\t\treturn false\n\t}\n\n\tif c.ChainID.Cmp(big.NewInt(56)) == 0 && num.Cmp(big.NewInt(33851236)) == 0 {\n\t\treturn true\n\t}\n\n\tif c.ChainID.Cmp(big.NewInt(97)) == 0 && (num.Cmp(big.NewInt(35547779)) == 0 || num.Cmp(big.NewInt(35548081)) == 0) {\n\t\treturn true\n\t}\n\n\treturn false\n}\n\nfunc (c *ChainConfig) IsOnHertzfix(num *big.Int) bool {\n\treturn configBlockEqual(c.HertzfixBlock, num)\n}\n\n// IsMuirGlacier returns whether num is either equal to the Muir Glacier (EIP-2384) fork block or greater.\nfunc (c *ChainConfig) IsMuirGlacier(num *big.Int) bool {\n\treturn isBlockForked(c.MuirGlacierBlock, num)\n}\n\n// IsPetersburg returns whether num is either\n// - equal to or greater than the PetersburgBlock fork block,\n// - OR is nil, and Constantinople is active\nfunc (c *ChainConfig) IsPetersburg(num *big.Int) bool {\n\treturn isBlockForked(c.PetersburgBlock, num) || c.PetersburgBlock == nil && isBlockForked(c.ConstantinopleBlock, num)\n}\n\n// IsIstanbul returns whether num is either equal to the Istanbul fork block or greater.\nfunc (c *ChainConfig) IsIstanbul(num *big.Int) bool {\n\treturn isBlockForked(c.IstanbulBlock, num)\n}\n\n// IsBerlin returns whether num is either equal to the Berlin fork block or greater.\nfunc (c *ChainConfig) IsBerlin(num *big.Int) bool {\n\treturn isBlockForked(c.BerlinBlock, num)\n}\n\n// IsLondon returns whether num is either equal to the London fork block or greater.\nfunc (c *ChainConfig) IsLondon(num *big.Int) bool {\n\treturn isBlockForked(c.LondonBlock, num)\n}\n\n// IsArrowGlacier returns whether num is either equal to the Arrow Glacier (EIP-4345) fork block or greater.\nfunc (c *ChainConfig) IsArrowGlacier(num *big.Int) bool {\n\treturn isBlockForked(c.ArrowGlacierBlock, num)\n}\n\n// IsGrayGlacier returns whether num is either equal to the Gray Glacier (EIP-5133) fork block or greater.\nfunc (c *ChainConfig) IsGrayGlacier(num *big.Int) bool {\n\treturn isBlockForked(c.GrayGlacierBlock, num)\n}\n\n// IsTerminalPoWBlock returns whether the given block is the last block of PoW stage.\nfunc (c *ChainConfig) IsTerminalPoWBlock(parentTotalDiff *big.Int, totalDiff *big.Int) bool {\n\tif c.TerminalTotalDifficulty == nil {\n\t\treturn false\n\t}\n\treturn parentTotalDiff.Cmp(c.TerminalTotalDifficulty) < 0 && totalDiff.Cmp(c.TerminalTotalDifficulty) >= 0\n}\n\n// IsGibbs returns whether num is either equal to the gibbs fork block or greater.\nfunc (c *ChainConfig) IsGibbs(num *big.Int) bool {\n\treturn isBlockForked(c.GibbsBlock, num)\n}\n\n// IsOnGibbs returns whether num is equal to the gibbs fork block\nfunc (c *ChainConfig) IsOnGibbs(num *big.Int) bool {\n\treturn configBlockEqual(c.GibbsBlock, num)\n}\n\nfunc (c *ChainConfig) IsNano(num *big.Int) bool {\n\treturn isBlockForked(c.NanoBlock, num)\n}\n\nfunc (c *ChainConfig) IsOnNano(num *big.Int) bool {\n\treturn configBlockEqual(c.NanoBlock, num)\n}\n\nfunc (c *ChainConfig) IsMoran(num *big.Int) bool {\n\treturn isBlockForked(c.MoranBlock, num)\n}\n\nfunc (c *ChainConfig) IsOnMoran(num *big.Int) bool {\n\treturn configBlockEqual(c.MoranBlock, num)\n}\n\nfunc (c *ChainConfig) IsPlanck(num *big.Int) bool {\n\treturn isBlockForked(c.PlanckBlock, num)\n}\n\nfunc (c *ChainConfig) IsOnPlanck(num *big.Int) bool {\n\treturn configBlockEqual(c.PlanckBlock, num)\n}\n\n// IsShanghai returns whether time is either equal to the Shanghai fork time or greater.\nfunc (c *ChainConfig) IsShanghai(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.ShanghaiTime, time)\n}\n\n// IsOnShanghai returns whether currentBlockTime is either equal to the shanghai fork time or greater firstly.\nfunc (c *ChainConfig) IsOnShanghai(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsShanghai(lastBlockNumber, lastBlockTime) && c.IsShanghai(currentBlockNumber, currentBlockTime)\n}\n\n// IsKepler returns whether time is either equal to the kepler fork time or greater.\nfunc (c *ChainConfig) IsKepler(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.KeplerTime, time)\n}\n\n// IsOnKepler returns whether currentBlockTime is either equal to the kepler fork time or greater firstly.\nfunc (c *ChainConfig) IsOnKepler(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsKepler(lastBlockNumber, lastBlockTime) && c.IsKepler(currentBlockNumber, currentBlockTime)\n}\n\n// IsFeynman returns whether time is either equal to the Feynman fork time or greater.\nfunc (c *ChainConfig) IsFeynman(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.FeynmanTime, time)\n}\n\n// IsOnFeynman returns whether currentBlockTime is either equal to the Feynman fork time or greater firstly.\nfunc (c *ChainConfig) IsOnFeynman(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsFeynman(lastBlockNumber, lastBlockTime) && c.IsFeynman(currentBlockNumber, currentBlockTime)\n}\n\n// IsFeynmanFix returns whether time is either equal to the FeynmanFix fork time or greater.\nfunc (c *ChainConfig) IsFeynmanFix(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.FeynmanFixTime, time)\n}\n\n// IsOnFeynmanFix returns whether currentBlockTime is either equal to the FeynmanFix fork time or greater firstly.\nfunc (c *ChainConfig) IsOnFeynmanFix(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsFeynmanFix(lastBlockNumber, lastBlockTime) && c.IsFeynmanFix(currentBlockNumber, currentBlockTime)\n}\n\n// IsCancun returns whether time is either equal to the Cancun fork time or greater.\nfunc (c *ChainConfig) IsCancun(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.CancunTime, time)\n}\n\n// IsHaber returns whether time is either equal to the Haber fork time or greater.\nfunc (c *ChainConfig) IsHaber(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.HaberTime, time)\n}\n\n// IsHaberFix returns whether time is either equal to the HaberFix fork time or greater.\nfunc (c *ChainConfig) IsHaberFix(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.HaberFixTime, time)\n}\n\n// IsOnHaberFix returns whether currentBlockTime is either equal to the HaberFix fork time or greater firstly.\nfunc (c *ChainConfig) IsOnHaberFix(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsHaberFix(lastBlockNumber, lastBlockTime) && c.IsHaberFix(currentBlockNumber, currentBlockTime)\n}\n\n// IsBohr returns whether time is either equal to the Bohr fork time or greater.\nfunc (c *ChainConfig) IsBohr(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.BohrTime, time)\n}\n\n// IsOnBohr returns whether currentBlockTime is either equal to the Bohr fork time or greater firstly.\nfunc (c *ChainConfig) IsOnBohr(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsBohr(lastBlockNumber, lastBlockTime) && c.IsBohr(currentBlockNumber, currentBlockTime)\n}\n\n// IsPascal returns whether time is either equal to the Pascal fork time or greater.\nfunc (c *ChainConfig) IsPascal(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.PascalTime, time)\n}\n\n// IsOnPascal returns whether currentBlockTime is either equal to the Pascal fork time or greater firstly.\nfunc (c *ChainConfig) IsOnPascal(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsPascal(lastBlockNumber, lastBlockTime) && c.IsPascal(currentBlockNumber, currentBlockTime)\n}\n\n// IsPrague returns whether time is either equal to the Prague fork time or greater.\nfunc (c *ChainConfig) IsPrague(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.PragueTime, time)\n}\n\n// IsOnPrague returns whether currentBlockTime is either equal to the Prague fork time or greater firstly.\nfunc (c *ChainConfig) IsOnPrague(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsPrague(lastBlockNumber, lastBlockTime) && c.IsPrague(currentBlockNumber, currentBlockTime)\n}\n\n// IsLorentz returns whether time is either equal to the Lorentz fork time or greater.\nfunc (c *ChainConfig) IsLorentz(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.LorentzTime, time)\n}\n\n// IsOnLorentz returns whether currentBlockTime is either equal to the Lorentz fork time or greater firstly.\nfunc (c *ChainConfig) IsOnLorentz(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsLorentz(lastBlockNumber, lastBlockTime) && c.IsLorentz(currentBlockNumber, currentBlockTime)\n}\n\n// IsMaxwell returns whether time is either equal to the Maxwell fork time or greater.\nfunc (c *ChainConfig) IsMaxwell(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.MaxwellTime, time)\n}\n\n// IsOnMaxwell returns whether currentBlockTime is either equal to the Maxwell fork time or greater firstly.\nfunc (c *ChainConfig) IsOnMaxwell(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsMaxwell(lastBlockNumber, lastBlockTime) && c.IsMaxwell(currentBlockNumber, currentBlockTime)\n}\n\n// IsFermi returns whether time is either equal to the Fermi fork time or greater.\nfunc (c *ChainConfig) IsFermi(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.FermiTime, time)\n}\n\n// IsOnFermi returns whether currentBlockTime is either equal to the Fermi fork time or greater firstly.\nfunc (c *ChainConfig) IsOnFermi(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsFermi(lastBlockNumber, lastBlockTime) && c.IsFermi(currentBlockNumber, currentBlockTime)\n}\n\n// IsOsaka returns whether time is either equal to the Osaka fork time or greater.\nfunc (c *ChainConfig) IsOsaka(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.OsakaTime, time)\n}\n\n// IsVerkle returns whether time is either equal to the Verkle fork time or greater.\nfunc (c *ChainConfig) IsVerkle(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.VerkleTime, time)\n}\n\n// IsVerkleGenesis checks whether the verkle fork is activated at the genesis block.\n//\n// Verkle mode is considered enabled if the verkle fork time is configured,\n// regardless of whether the local time has surpassed the fork activation time.\n// This is a temporary workaround for verkle devnet testing, where verkle is\n// activated at genesis, and the configured activation date has already passed.\n//\n// In production networks (mainnet and public testnets), verkle activation\n// always occurs after the genesis block, making this function irrelevant in\n// those cases.\nfunc (c *ChainConfig) IsVerkleGenesis() bool {\n\treturn c.EnableVerkleAtGenesis\n}\n\n// IsEIP4762 returns whether eip 4762 has been activated at given block.\nfunc (c *ChainConfig) IsEIP4762(num *big.Int, time uint64) bool {\n\treturn c.IsVerkle(num, time)\n}\n\n// CheckCompatible checks whether scheduled fork transitions have been imported\n// with a mismatching chain configuration.\nfunc (c *ChainConfig) CheckCompatible(newcfg *ChainConfig, height uint64, time uint64) *ConfigCompatError {\n\tvar (\n\t\tbhead = new(big.Int).SetUint64(height)\n\t\tbtime = time\n\t)\n\t// Iterate checkCompatible to find the lowest conflict.\n\tvar lasterr *ConfigCompatError\n\tfor {\n\t\terr := c.checkCompatible(newcfg, bhead, btime)\n\t\tif err == nil || (lasterr != nil && err.RewindToBlock == lasterr.RewindToBlock && err.RewindToTime == lasterr.RewindToTime) {\n\t\t\tbreak\n\t\t}\n\t\tlasterr = err\n\n\t\tif err.RewindToTime > 0 {\n\t\t\tbtime = err.RewindToTime\n\t\t} else {\n\t\t\tbhead.SetUint64(err.RewindToBlock)\n\t\t}\n\t}\n\treturn lasterr\n}\n\n// CheckConfigForkOrder checks that we don't \"skip\" any forks, geth isn't pluggable enough\n// to guarantee that forks can be implemented in a different order than on official networks\nfunc (c *ChainConfig) CheckConfigForkOrder() error {\n\t// skip checking for non-Parlia egine\n\tif c.Parlia == nil {\n\t\treturn nil\n\t}\n\ttype fork struct {\n\t\tname      string\n\t\tblock     *big.Int // forks up to - and including the merge - were defined with block numbers\n\t\ttimestamp *uint64  // forks after the merge are scheduled using timestamps\n\t\toptional  bool     // if true, the fork may be nil and next fork is still allowed\n\t}\n\tvar lastFork fork\n\tfor _, cur := range []fork{\n\t\t{name: \"mirrorSyncBlock\", block: c.MirrorSyncBlock},\n\t\t{name: \"brunoBlock\", block: c.BrunoBlock},\n\t\t{name: \"eulerBlock\", block: c.EulerBlock},\n\t\t{name: \"gibbsBlock\", block: c.GibbsBlock},\n\t\t{name: \"planckBlock\", block: c.PlanckBlock},\n\t\t{name: \"lubanBlock\", block: c.LubanBlock},\n\t\t{name: \"platoBlock\", block: c.PlatoBlock},\n\t\t{name: \"hertzBlock\", block: c.HertzBlock},\n\t\t{name: \"hertzfixBlock\", block: c.HertzfixBlock},\n\t\t{name: \"keplerTime\", timestamp: c.KeplerTime},\n\t\t{name: \"feynmanTime\", timestamp: c.FeynmanTime},\n\t\t{name: \"feynmanFixTime\", timestamp: c.FeynmanFixTime},\n\t\t{name: \"cancunTime\", timestamp: c.CancunTime},\n\t\t{name: \"haberTime\", timestamp: c.HaberTime},\n\t\t{name: \"haberFixTime\", timestamp: c.HaberFixTime},\n\t\t{name: \"bohrTime\", timestamp: c.BohrTime},\n\t\t{name: \"pascalTime\", timestamp: c.PascalTime},\n\t\t{name: \"pragueTime\", timestamp: c.PragueTime},\n\t\t{name: \"osakaTime\", timestamp: c.OsakaTime, optional: true},\n\t\t{name: \"lorentzTime\", timestamp: c.LorentzTime},\n\t\t{name: \"maxwellTime\", timestamp: c.MaxwellTime},\n\t\t{name: \"fermiTime\", timestamp: c.FermiTime},\n\t\t{name: \"verkleTime\", timestamp: c.VerkleTime, optional: true},\n\t} {\n\t\tif lastFork.name != \"\" {\n\t\t\tswitch {\n\t\t\t// Non-optional forks must all be present in the chain config up to the last defined fork\n\t\t\tcase lastFork.block == nil && lastFork.timestamp == nil && (cur.block != nil || cur.timestamp != nil):\n\t\t\t\tif cur.block != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v not enabled, but %v enabled at block %v\",\n\t\t\t\t\t\tlastFork.name, cur.name, cur.block)\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v not enabled, but %v enabled at timestamp %v\",\n\t\t\t\t\t\tlastFork.name, cur.name, *cur.timestamp)\n\t\t\t\t}\n\n\t\t\t// Fork (whether defined by block or timestamp) must follow the fork definition sequence\n\t\t\tcase (lastFork.block != nil && cur.block != nil) || (lastFork.timestamp != nil && cur.timestamp != nil):\n\t\t\t\tif lastFork.block != nil && lastFork.block.Cmp(cur.block) > 0 {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v enabled at block %v, but %v enabled at block %v\",\n\t\t\t\t\t\tlastFork.name, lastFork.block, cur.name, cur.block)\n\t\t\t\t} else if lastFork.timestamp != nil && *lastFork.timestamp > *cur.timestamp {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v enabled at timestamp %v, but %v enabled at timestamp %v\",\n\t\t\t\t\t\tlastFork.name, *lastFork.timestamp, cur.name, *cur.timestamp)\n\t\t\t\t}\n\n\t\t\t\t// Timestamp based forks can follow block based ones, but not the other way around\n\t\t\t\tif lastFork.timestamp != nil && cur.block != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v used timestamp ordering, but %v reverted to block ordering\",\n\t\t\t\t\t\tlastFork.name, cur.name)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If it was optional and not set, then ignore it\n\t\tif !cur.optional || (cur.block != nil || cur.timestamp != nil) {\n\t\t\tlastFork = cur\n\t\t}\n\t}\n\n\t// Check that all forks with blobs explicitly define the blob schedule configuration.\n\tbsc := c.BlobScheduleConfig\n\tif bsc == nil {\n\t\tbsc = new(BlobScheduleConfig)\n\t}\n\tfor _, cur := range []struct {\n\t\tname      string\n\t\ttimestamp *uint64\n\t\tconfig    *BlobConfig\n\t}{\n\t\t{name: \"cancun\", timestamp: c.CancunTime, config: bsc.Cancun},\n\t\t{name: \"prague\", timestamp: c.PragueTime, config: bsc.Prague},\n\t\t{name: \"osaka\", timestamp: c.OsakaTime, config: bsc.Osaka},\n\t} {\n\t\tif cur.config != nil {\n\t\t\tif err := cur.config.validate()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/server.go",
          "line": 175,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" \" + c.node.ID().String()\n\t}\n\ts += \" \" + c.fd.RemoteAddr().String()\n\treturn s\n}\n\nfunc (f connFlag) String() string {\n\ts := \"\"\n\tif f&trustedConn != 0 {\n\t\ts += \"-trusted\"\n\t}\n\tif f&dynDialedConn != 0 {\n\t\ts += \"-dyndial\"\n\t}\n\tif f&staticDialedConn != 0 {\n\t\ts += \"-staticdial\"\n\t}\n\tif f&inboundConn != 0 {\n\t\ts += \"-inbound\"\n\t}\n\tif s != \"\" {\n\t\ts = s[1:]\n\t}\n\treturn s\n}\n\nfunc (c *conn) is(f connFlag) bool {\n\tflags := connFlag(atomic.LoadInt32((*int32)(&c.flags)))\n\treturn flags&f != 0\n}\n\nfunc (c *conn) set(f connFlag, val bool) {\n\tfor {\n\t\toldFlags := connFlag(atomic.LoadInt32((*int32)(&c.flags)))\n\t\tflags := oldFlags\n\t\tif val {\n\t\t\tflags |= f\n\t\t} else {\n\t\t\tflags &= ^f\n\t\t}\n\t\tif atomic.CompareAndSwapInt32((*int32)(&c.flags), int32(oldFlags), int32(flags)) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// LocalNode returns the local node record.\nfunc (srv *Server) LocalNode() *enode.LocalNode {\n\treturn srv.localnode\n}\n\n// Peers returns all connected peers.\nfunc (srv *Server) Peers() []*Peer {\n\tvar ps []*Peer\n\tsrv.doPeerOp(func(peers map[enode.ID]*Peer) {\n\t\tfor _, p := range peers {\n\t\t\tps = append(ps, p)\n\t\t}\n\t})\n\treturn ps\n}\n\n// PeerCount returns the number of connected peers.\nfunc (srv *Server) PeerCount() int {\n\tvar count int\n\tsrv.doPeerOp(func(ps map[enode.ID]*Peer) {\n\t\tcount = len(ps)\n\t})\n\treturn count\n}\n\n// AddPeer adds the given node to the static node set. When there is room in the peer set,\n// the server will connect to the node. If the connection fails for any reason, the server\n// will attempt to reconnect the peer.\nfunc (srv *Server) AddPeer(node *enode.Node) {\n\tsrv.dialsched.addStatic(node)\n}\n\n// RemovePeer removes a node from the static node set. It also disconnects from the given\n// node if it is currently connected as a peer.\n//\n// This method blocks until all protocols have exited and the peer is removed. Do not use\n// RemovePeer in protocol implementations, call Disconnect on the Peer instead.\nfunc (srv *Server) RemovePeer(node *enode.Node) {\n\tvar (\n\t\tch  chan *PeerEvent\n\t\tsub event.Subscription\n\t)\n\n\t// Disconnect the peer on the main loop.\n\tsrv.doPeerOp(func(peers map[enode.ID]*Peer) {\n\t\t// Special case: sending a disconnect request with a hardcoded enode ID will reset the disconnect enode set\n\t\tif node.ID() == magicEnodeID {\n\t\t\tsrv.disconnectEnodeSet = make(map[enode.ID]struct{})\n\t\t\tsrv.log.Debug(\"Reset disconnect enode set\")\n\t\t\treturn\n\t\t}\n\n\t\tsrv.dialsched.removeStatic(node)\n\t\tif peer := peers[node.ID()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/p2p/server.go",
          "line": 1065,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/p2p/server.go",
          "line": 1084,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/message.go",
          "line": 144,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint32(n)\n\tif (err != nil || r.count == 0) && r.eof != nil {\n\t\tr.eof <- struct{}{} // tell Peer that msg has been consumed\n\t\tr.eof = nil\n\t}\n\treturn n, err\n}\n\n// MsgPipe creates a message pipe. Reads on one end are matched\n// with writes on the other. The pipe is full-duplex, both ends\n// implement MsgReadWriter.\nfunc MsgPipe(args ...any) (*MsgPipeRW, *MsgPipeRW) {\n\tnoBlock := false\n\tif len(args) > 0 {\n\t\tnoBlock = args[0].(bool)\n\t}\n\tc1, c2 := make(chan Msg), make(chan Msg)\n\tif noBlock {\n\t\tc1 = make(chan Msg, 1)\n\t\tc2 = make(chan Msg, 1)\n\t}\n\tvar (\n\t\tclosing = make(chan struct{})\n\t\tclosed  = new(atomic.Bool)\n\t\trw1     = &MsgPipeRW{c1, c2, closing, closed, noBlock}\n\t\trw2     = &MsgPipeRW{c2, c1, closing, closed, noBlock}\n\t)\n\treturn rw1, rw2\n}\n\n// ErrPipeClosed is returned from pipe operations after the\n// pipe has been closed.\nvar ErrPipeClosed = errors.New(\"p2p: read or write on closed message pipe\")\n\n// MsgPipeRW is an endpoint of a MsgReadWriter pipe.\ntype MsgPipeRW struct {\n\tw       chan<- Msg\n\tr       <-chan Msg\n\tclosing chan struct{}\n\tclosed  *atomic.Bool\n\tnoBlock bool\n}\n\n// WriteMsg sends a message on the pipe.\n// It blocks until the receiver has consumed the message payload.\nfunc (p *MsgPipeRW) WriteMsg(msg Msg) error {\n\tif !p.closed.Load() {\n\t\tconsumed := make(chan struct{}, 1)\n\t\tmsg.Payload = &eofSignal{msg.Payload, msg.Size, consumed}\n\t\tselect {\n\t\tcase p.w <- msg:\n\t\t\tif p.noBlock {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif msg.Size > 0 {\n\t\t\t\t// wait for payload read or discard\n\t\t\t\tselect {\n\t\t\t\tcase <-consumed:\n\t\t\t\tcase <-p.closing:\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\tcase <-p.closing:\n\t\t}\n\t}\n\treturn ErrPipeClosed\n}\n\n// ReadMsg returns a message sent on the other end of the pipe.\nfunc (p *MsgPipeRW) ReadMsg() (Msg, error) {\n\tif !p.closed.Load() {\n\t\tselect {\n\t\tcase msg := <-p.r:\n\t\t\treturn msg, nil\n\t\tcase <-p.closing:\n\t\t}\n\t}\n\treturn Msg{}, ErrPipeClosed\n}\n\n// Close unblocks any pending ReadMsg and WriteMsg calls on both ends\n// of the pipe. They will return ErrPipeClosed. Close also\n// interrupts any reads from a message payload.\nfunc (p *MsgPipeRW) Close() error {\n\tif p.closed.Swap(true) {\n\t\t// someone else is already closing\n\t\treturn nil\n\t}\n\tclose(p.closing)\n\treturn nil\n}\n\n// ExpectMsg reads a message from r and verifies that its\n// code and encoded RLP content match the provided values.\n// If content is nil, the payload is discarded and not verified.\nfunc ExpectMsg(r MsgReader, code uint64, content interface{}) error {\n\tmsg, err := r.ReadMsg()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif msg.Code != code {\n\t\treturn fmt.Errorf(\"message code mismatch: got %d, expected %d\", msg.Code, code)\n\t}\n\tif content == nil {\n\t\treturn msg.Discard()\n\t}\n\tcontentEnc, err := rlp.EncodeToBytes(content)\n\tif err != nil {\n\t\tpanic(\"content encode error: \" + err.Error())\n\t}\n\tif int(msg.Size) != len(contentEnc) {\n\t\treturn fmt.Errorf(\"message size mismatch: got %d, want %d\", msg.Size, len(contentEnc))\n\t}\n\tactualContent, err := io.ReadAll(msg.Payload)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !bytes.Equal(actualContent, contentEnc) {\n\t\treturn fmt.Errorf(\"message payload mismatch:\\ngot:  %x\\nwant: %x\", actualContent, contentEnc)\n\t}\n\treturn nil\n}\n\n// msgEventer wraps a MsgReadWriter and sends events whenever a message is sent\n// or received\ntype msgEventer struct {\n\tMsgReadWriter\n\n\tfeed          *event.Feed\n\tpeerID        enode.ID\n\tProtocol      string\n\tlocalAddress  string\n\tremoteAddress string\n}\n\n// newMsgEventer returns a msgEventer which sends message events to the given\n// feed\nfunc newMsgEventer(rw MsgReadWriter, feed *event.Feed, peerID enode.ID, proto, remote, local string) *msgEventer {\n\treturn &msgEventer{\n\t\tMsgReadWriter: rw,\n\t\tfeed:          feed,\n\t\tpeerID:        peerID,\n\t\tProtocol:      proto,\n\t\tremoteAddress: remote,\n\t\tlocalAddress:  local,\n\t}\n}\n\n// ReadMsg reads a message from the underlying MsgReadWriter and emits a\n// \"message received\" event\nfunc (ev *msgEventer) ReadMsg() (Msg, error) {\n\tmsg, err := ev.MsgReadWriter.ReadMsg()\n\tif err != nil {\n\t\treturn msg, err\n\t}\n\tev.feed.Send(&PeerEvent{\n\t\tType:          PeerEventTypeMsgRecv,\n\t\tPeer:          ev.peerID,\n\t\tProtocol:      ev.Protocol,\n\t\tMsgCode:       &msg.Code,\n\t\tMsgSize:       &msg.Size,\n\t\tLocalAddress:  ev.localAddress,\n\t\tRemoteAddress: ev.remoteAddress,\n\t})\n\treturn msg, nil\n}\n\n// WriteMsg writes a message to the underlying MsgReadWriter and emits a\n// \"message sent\" event\nfunc (ev *msgEventer) WriteMsg(msg Msg) error {\n\terr := ev.MsgReadWriter.WriteMsg(msg)\n\tif err != nil {\n\t\treturn err\n\t}\n\tev.feed.Send(&PeerEvent{\n\t\tType:          PeerEventTypeMsgSend,\n\t\tPeer:          ev.peerID,\n\t\tProtocol:      ev.Protocol,\n\t\tMsgCode:       &msg.Code,\n\t\tMsgSize:       &msg.Size,\n\t\tLocalAddress:  ev.localAddress,\n\t\tRemoteAddress: ev.remoteAddress,\n\t})\n\treturn nil\n}\n\n// Close closes the underlying MsgReadWriter if it implements the io.Closer\n// interface\nfunc (ev *msgEventer) Close() error {\n\tif v, ok := ev.MsgReadWriter.(io.Closer)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/p2p/message.go",
          "line": 299,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/p2p/message.go",
          "line": 318,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/peer_error.go",
          "line": 46,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \": \" + fmt.Sprintf(format, v...)\n\t}\n\treturn err\n}\n\nfunc (pe *peerError) Error() string {\n\treturn pe.message\n}\n\nvar errProtocolReturned = errors.New(\"protocol returned\")\n\ntype DiscReason uint8\n\nconst (\n\tDiscRequested DiscReason = iota\n\tDiscNetworkError\n\tDiscProtocolError\n\tDiscUselessPeer\n\tDiscTooManyPeers\n\tDiscAlreadyConnected\n\tDiscIncompatibleVersion\n\tDiscInvalidIdentity\n\tDiscQuitting\n\tDiscUnexpectedIdentity\n\tDiscSelf\n\tDiscReadTimeout\n\tDiscSubprotocolError = DiscReason(0x10)\n\n\tDiscInvalid = 0xff\n)\n\nvar discReasonToString = [...]string{\n\tDiscRequested:           \"disconnect requested\",\n\tDiscNetworkError:        \"network error\",\n\tDiscProtocolError:       \"breach of protocol\",\n\tDiscUselessPeer:         \"useless peer\",\n\tDiscTooManyPeers:        \"too many peers\",\n\tDiscAlreadyConnected:    \"already connected\",\n\tDiscIncompatibleVersion: \"incompatible p2p protocol version\",\n\tDiscInvalidIdentity:     \"invalid node identity\",\n\tDiscQuitting:            \"client quitting\",\n\tDiscUnexpectedIdentity:  \"unexpected identity\",\n\tDiscSelf:                \"connected to self\",\n\tDiscReadTimeout:         \"read timeout\",\n\tDiscSubprotocolError:    \"subprotocol error\",\n\tDiscInvalid:             \"invalid disconnect reason\",\n}\n\nfunc (d DiscReason) String() string {\n\tif len(discReasonToString) <= int(d) || discReasonToString[d] == \"\" {\n\t\treturn fmt.Sprintf(\"unknown disconnect reason %d\", d)\n\t}\n\treturn discReasonToString[d]\n}\n\nfunc (d DiscReason) Error() string {\n\treturn d.String()\n}\n\nfunc discReasonForError(err error) DiscReason {\n\tif reason, ok := err.(DiscReason)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/peer.go",
          "line": 515,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= proto.Length\n\n\t\t\t\tcontinue outer\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\nfunc (p *Peer) startProtocols(writeStart <-chan struct{}, writeErr chan<- error) {\n\tp.wg.Add(len(p.running))\n\tfor _, proto := range p.running {\n\t\tproto.closed = p.closed\n\t\tproto.wstart = writeStart\n\t\tproto.werr = writeErr\n\t\tvar rw MsgReadWriter = proto\n\t\tif p.events != nil {\n\t\t\trw = newMsgEventer(rw, p.events, p.ID(), proto.Name, p.Info().Network.RemoteAddress, p.Info().Network.LocalAddress)\n\t\t}\n\t\tp.log.Trace(fmt.Sprintf(\"Starting protocol %s/%d\", proto.Name, proto.Version))\n\t\tgo func() {\n\t\t\tdefer p.wg.Done()\n\t\t\terr := proto.Run(p, rw)\n\t\t\tif err == nil {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d returned\", proto.Name, proto.Version))\n\t\t\t\terr = errProtocolReturned\n\t\t\t} else if !errors.Is(err, io.EOF) {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d failed\", proto.Name, proto.Version), \"err\", err)\n\t\t\t}\n\t\t\tp.protoErr <- err\n\t\t}()\n\t}\n}\n\n// getProto finds the protocol responsible for handling\n// the given message code.\nfunc (p *Peer) getProto(code uint64) (*protoRW, error) {\n\tfor _, proto := range p.running {\n\t\tif code >= proto.offset && code < proto.offset+proto.Length {\n\t\t\treturn proto, nil\n\t\t}\n\t}\n\treturn nil, newPeerError(errInvalidMsgCode, \"%d\", code)\n}\n\ntype protoRW struct {\n\tProtocol\n\tin     chan Msg        // receives read messages\n\tclosed <-chan struct{} // receives when peer is shutting down\n\twstart <-chan struct{} // receives when write may start\n\twerr   chan<- error    // for write results\n\toffset uint64\n\tw      MsgWriter\n}\n\nfunc (rw *protoRW) WriteMsg(msg Msg) (err error) {\n\tif msg.Code >= rw.Length {\n\t\treturn newPeerError(errInvalidMsgCode, \"not handled\")\n\t}\n\tmsg.meterCap = rw.cap()\n\tmsg.meterCode = msg.Code\n\n\tmsg.Code += rw.offset\n\n\tselect {\n\tcase <-rw.wstart:\n\t\terr = rw.w.WriteMsg(msg)\n\t\t// Report write status back to Peer.run. It will initiate\n\t\t// shutdown if the error is non-nil and unblock the next write\n\t\t// otherwise. The calling protocol code should exit for errors\n\t\t// as well but we don't want to rely on that.\n\t\trw.werr <- err\n\tcase <-rw.closed:\n\t\terr = ErrShuttingDown\n\t}\n\treturn err\n}\n\nfunc (rw *protoRW) ReadMsg() (Msg, error) {\n\tselect {\n\tcase msg := <-rw.in:\n\t\tmsg.Code -= rw.offset\n\t\treturn msg, nil\n\tcase <-rw.closed:\n\t\treturn Msg{}, io.EOF\n\t}\n}\n\n// PeerInfo represents a short summary of the information known about a connected\n// peer. Sub-protocol independent fields are contained and initialized here, with\n// protocol specifics delegated to all connected sub-protocols.\ntype PeerInfo struct {\n\tENR     string   `json:\"enr,omitempty\"` // Ethereum Node Record\n\tEnode   string   `json:\"enode\"`         // Node URL\n\tID      string   `json:\"id\"`            // Unique node identifier\n\tName    string   `json:\"name\"`          // Name of the node, including client type, version, OS, custom data\n\tCaps    []string `json:\"caps\"`          // Protocols advertised by this peer\n\tNetwork struct {\n\t\tLocalAddress  string `json:\"localAddress\"`  // Local endpoint of the TCP data connection\n\t\tRemoteAddress string `json:\"remoteAddress\"` // Remote endpoint of the TCP data connection\n\t\tInbound       bool   `json:\"inbound\"`\n\t\tTrusted       bool   `json:\"trusted\"`\n\t\tStatic        bool   `json:\"static\"`\n\t} `json:\"network\"`\n\tProtocols   map[string]interface{} `json:\"protocols\"` // Sub-protocol specific metadata fields\n\tLatency     int64                  `json:\"latency\"`   // the estimate latency from ping msg\n\tEVNPeerFlag bool                   `json:\"evnPeerFlag\"`\n}\n\n// Info gathers and returns a collection of metadata known about a peer.\nfunc (p *Peer) Info() *PeerInfo {\n\t// Gather the protocol capabilities\n\tvar caps []string\n\tfor _, cap := range p.Caps() {\n\t\tcaps = append(caps, cap.String())\n\t}\n\t// Assemble the generic peer metadata\n\tinfo := &PeerInfo{\n\t\tEnode:       p.Node().URLv4(),\n\t\tID:          p.ID().String(),\n\t\tName:        p.Fullname(),\n\t\tCaps:        caps,\n\t\tProtocols:   make(map[string]interface{}, len(p.running)),\n\t\tLatency:     p.latency.Load(),\n\t\tEVNPeerFlag: p.EVNPeerFlag.Load(),\n\t}\n\tif p.Node().Seq() > 0 {\n\t\tinfo.ENR = p.Node().String()\n\t}\n\tinfo.Network.LocalAddress = p.LocalAddr().String()\n\tinfo.Network.RemoteAddress = p.RemoteAddr().String()\n\tinfo.Network.Inbound = p.rw.is(inboundConn)\n\t// After Maxwell, we treat all EVN peers as trusted\n\tinfo.Network.Trusted = p.rw.is(trustedConn) || p.EVNPeerFlag.Load()\n\tinfo.Network.Static = p.rw.is(staticDialedConn)\n\n\t// Gather all the running protocol infos\n\tfor _, proto := range p.running {\n\t\tprotoInfo := interface{}(\"unknown\")\n\t\tif query := proto.Protocol.PeerInfo",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/p2p/peer.go",
          "line": 511,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= old.Length\n\t\t\t\t}\n\t\t\t\t// Assign the new match\n\t\t\t\tresult[cap.Name] = &protoRW{Protocol: proto, offset: offset, in: make(chan Msg), w: rw}\n\t\t\t\toffset += proto.Length\n\n\t\t\t\tcontinue outer\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\nfunc (p *Peer) startProtocols(writeStart <-chan struct{}, writeErr chan<- error) {\n\tp.wg.Add(len(p.running))\n\tfor _, proto := range p.running {\n\t\tproto.closed = p.closed\n\t\tproto.wstart = writeStart\n\t\tproto.werr = writeErr\n\t\tvar rw MsgReadWriter = proto\n\t\tif p.events != nil {\n\t\t\trw = newMsgEventer(rw, p.events, p.ID(), proto.Name, p.Info().Network.RemoteAddress, p.Info().Network.LocalAddress)\n\t\t}\n\t\tp.log.Trace(fmt.Sprintf(\"Starting protocol %s/%d\", proto.Name, proto.Version))\n\t\tgo func() {\n\t\t\tdefer p.wg.Done()\n\t\t\terr := proto.Run(p, rw)\n\t\t\tif err == nil {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d returned\", proto.Name, proto.Version))\n\t\t\t\terr = errProtocolReturned\n\t\t\t} else if !errors.Is(err, io.EOF) {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d failed\", proto.Name, proto.Version), \"err\", err)\n\t\t\t}\n\t\t\tp.protoErr <- err\n\t\t}()\n\t}\n}\n\n// getProto finds the protocol responsible for handling\n// the given message code.\nfunc (p *Peer) getProto(code uint64) (*protoRW, error) {\n\tfor _, proto := range p.running {\n\t\tif code >= proto.offset && code < proto.offset+proto.Length {\n\t\t\treturn proto, nil\n\t\t}\n\t}\n\treturn nil, newPeerError(errInvalidMsgCode, \"%d\", code)\n}\n\ntype protoRW struct {\n\tProtocol\n\tin     chan Msg        // receives read messages\n\tclosed <-chan struct{} // receives when peer is shutting down\n\twstart <-chan struct{} // receives when write may start\n\twerr   chan<- error    // for write results\n\toffset uint64\n\tw      MsgWriter\n}\n\nfunc (rw *protoRW) WriteMsg(msg Msg) (err error) {\n\tif msg.Code >= rw.Length {\n\t\treturn newPeerError(errInvalidMsgCode, \"not handled\")\n\t}\n\tmsg.meterCap = rw.cap()\n\tmsg.meterCode = msg.Code\n\n\tmsg.Code += rw.offset\n\n\tselect {\n\tcase <-rw.wstart:\n\t\terr = rw.w.WriteMsg(msg)\n\t\t// Report write status back to Peer.run. It will initiate\n\t\t// shutdown if the error is non-nil and unblock the next write\n\t\t// otherwise. The calling protocol code should exit for errors\n\t\t// as well but we don't want to rely on that.\n\t\trw.werr <- err\n\tcase <-rw.closed:\n\t\terr = ErrShuttingDown\n\t}\n\treturn err\n}\n\nfunc (rw *protoRW) ReadMsg() (Msg, error) {\n\tselect {\n\tcase msg := <-rw.in:\n\t\tmsg.Code -= rw.offset\n\t\treturn msg, nil\n\tcase <-rw.closed:\n\t\treturn Msg{}, io.EOF\n\t}\n}\n\n// PeerInfo represents a short summary of the information known about a connected\n// peer. Sub-protocol independent fields are contained and initialized here, with\n// protocol specifics delegated to all connected sub-protocols.\ntype PeerInfo struct {\n\tENR     string   `json:\"enr,omitempty\"` // Ethereum Node Record\n\tEnode   string   `json:\"enode\"`         // Node URL\n\tID      string   `json:\"id\"`            // Unique node identifier\n\tName    string   `json:\"name\"`          // Name of the node, including client type, version, OS, custom data\n\tCaps    []string `json:\"caps\"`          // Protocols advertised by this peer\n\tNetwork struct {\n\t\tLocalAddress  string `json:\"localAddress\"`  // Local endpoint of the TCP data connection\n\t\tRemoteAddress string `json:\"remoteAddress\"` // Remote endpoint of the TCP data connection\n\t\tInbound       bool   `json:\"inbound\"`\n\t\tTrusted       bool   `json:\"trusted\"`\n\t\tStatic        bool   `json:\"static\"`\n\t} `json:\"network\"`\n\tProtocols   map[string]interface{} `json:\"protocols\"` // Sub-protocol specific metadata fields\n\tLatency     int64                  `json:\"latency\"`   // the estimate latency from ping msg\n\tEVNPeerFlag bool                   `json:\"evnPeerFlag\"`\n}\n\n// Info gathers and returns a collection of metadata known about a peer.\nfunc (p *Peer) Info() *PeerInfo {\n\t// Gather the protocol capabilities\n\tvar caps []string\n\tfor _, cap := range p.Caps() {\n\t\tcaps = append(caps, cap.String())\n\t}\n\t// Assemble the generic peer metadata\n\tinfo := &PeerInfo{\n\t\tEnode:       p.Node().URLv4(),\n\t\tID:          p.ID().String(),\n\t\tName:        p.Fullname(),\n\t\tCaps:        caps,\n\t\tProtocols:   make(map[string]interface{}, len(p.running)),\n\t\tLatency:     p.latency.Load(),\n\t\tEVNPeerFlag: p.EVNPeerFlag.Load(),\n\t}\n\tif p.Node().Seq() > 0 {\n\t\tinfo.ENR = p.Node().String()\n\t}\n\tinfo.Network.LocalAddress = p.LocalAddr().String()\n\tinfo.Network.RemoteAddress = p.RemoteAddr().String()\n\tinfo.Network.Inbound = p.rw.is(inboundConn)\n\t// After Maxwell, we treat all EVN peers as trusted\n\tinfo.Network.Trusted = p.rw.is(trustedConn) || p.EVNPeerFlag.Load()\n\tinfo.Network.Static = p.rw.is(staticDialedConn)\n\n\t// Gather all the running protocol infos\n\tfor _, proto := range p.running {\n\t\tprotoInfo := interface{}(\"unknown\")\n\t\tif query := proto.Protocol.PeerInfo",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/dial.go",
          "line": 242,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= d.startStaticDials(slots)\n\t\tif slots > 0 {\n\t\t\tnodesCh = d.nodesIn\n\t\t} else {\n\t\t\tnodesCh = nil\n\t\t}\n\t\td.rearmHistoryTimer()\n\t\td.logStats()\n\n\t\tselect {\n\t\tcase node := <-nodesCh:\n\t\t\tif err := d.checkDial(node)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/p2p/dial.go",
          "line": 600,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 2\n\t\tif t.resolveDelay > maxResolveDelay {\n\t\t\tt.resolveDelay = maxResolveDelay\n\t\t}\n\t\td.log.Debug(\"Resolving node failed\", \"id\", node.ID(), \"newdelay\", t.resolveDelay)\n\t\treturn false\n\t}\n\t// The node was found.\n\tt.resolveDelay = initialResolveDelay\n\tt.destPtr.Store(resolved)\n\tresAddr, _ := resolved.TCPEndpoint()\n\td.log.Debug(\"Resolved node\", \"id\", resolved.ID(), \"addr\", resAddr)\n\treturn true\n}\n\n// dial performs the actual connection attempt.\nfunc (t *dialTask) dial(d *dialScheduler, dest *enode.Node) error {\n\tdialMeter.Mark(1)\n\tfd, err := d.dialer.Dial(d.ctx, dest)\n\tif err != nil {\n\t\taddr, _ := dest.TCPEndpoint()\n\t\td.log.Trace(\"Dial error\", \"id\", dest.ID(), \"addr\", addr, \"conn\", t.flags, \"err\", cleanupDialErr(err))\n\t\tdialConnectionError.Mark(1)\n\t\treturn &dialError{err}\n\t}\n\treturn d.setupFunc(newMeteredConn(fd), t.flags, dest)\n}\n\nfunc (t *dialTask) String() string {\n\tnode := t.dest()\n\tid := node.ID()\n\treturn fmt.Sprintf(\"%v %x %v:%d\", t.flags, id[:8], node.IPAddr(), node.TCP())\n}\n\nfunc cleanupDialErr(err error) error {\n\tif netErr, ok := err.(*net.OpError)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/server_test.go",
          "line": 500,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"doEncHandshake,\"\n\treturn c.pubkey, c.encHandshakeErr\n}\n\nfunc (c *setupTransport) doProtoHandshake(our *protoHandshake) (*protoHandshake, error) {\n\tc.calls += \"doProtoHandshake,\"\n\tif c.protoHandshakeErr != nil {\n\t\treturn nil, c.protoHandshakeErr\n\t}\n\treturn &c.phs, nil\n}\nfunc (c *setupTransport) close(err error) {\n\tc.calls += \"close,\"\n\tc.closeErr = err\n}\n\n// setupConn shouldn't write to/read from the connection.\nfunc (c *setupTransport) WriteMsg(Msg) error {\n\tpanic(\"WriteMsg called on setupTransport\")\n}\nfunc (c *setupTransport) ReadMsg() (Msg, error) {\n\tpanic(\"ReadMsg called on setupTransport\")\n}\n\nfunc newkey() *ecdsa.PrivateKey {\n\tkey, err := crypto.GenerateKey()\n\tif err != nil {\n\t\tpanic(\"couldn't generate key: \" + err.Error())\n\t}\n\treturn key\n}\n\nfunc randomID() (id enode.ID) {\n\tfor i := range id {\n\t\tid[i] = byte(rand.Intn(255))\n\t}\n\treturn id\n}\n\n// This test checks that inbound connections are throttled by IP.\nfunc TestServerInboundThrottle(t *testing.T) {\n\tconst timeout = 5 * time.Second\n\tnewTransportCalled := make(chan struct{})\n\tsrv := &Server{\n\t\tConfig: Config{\n\t\t\tPrivateKey:  newkey(),\n\t\t\tListenAddr:  \"127.0.0.1:0\",\n\t\t\tMaxPeers:    10,\n\t\t\tNoDial:      true,\n\t\t\tNoDiscovery: true,\n\t\t\tProtocols:   []Protocol{discard},\n\t\t\tLogger:      testlog.Logger(t, log.LvlTrace),\n\t\t},\n\t\tnewTransport: func(fd net.Conn, dialDest *ecdsa.PublicKey) transport {\n\t\t\tnewTransportCalled <- struct{}{}\n\t\t\treturn newRLPX(fd, dialDest)\n\t\t},\n\t\tlistenFunc: func(network, laddr string) (net.Listener, error) {\n\t\t\tfakeAddr := &net.TCPAddr{IP: net.IP{95, 33, 21, 2}, Port: 4444}\n\t\t\treturn listenFakeAddr(network, laddr, fakeAddr)\n\t\t},\n\t}\n\tif err := srv.Start()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/log/handler_glog.go",
          "line": 118,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"(/.*)?\"\n\t\t\t} else if comp != \"\" {\n\t\t\t\tmatcher += \"/\" + regexp.QuoteMeta(comp)\n\t\t\t}\n\t\t}\n\t\tif !strings.HasSuffix(parts[0], \".go\") {\n\t\t\tmatcher += \"/[^/]+\\\\.go\"\n\t\t}\n\t\tmatcher = matcher + \"$\"\n\n\t\tre, _ := regexp.Compile(matcher)\n\t\tfilter = append(filter, pattern{re, level})\n\t}\n\t// Swap out the vmodule pattern for the new filter system\n\th.lock.Lock()\n\tdefer h.lock.Unlock()\n\n\th.patterns = filter\n\th.siteCache = make(map[uintptr]slog.Level)\n\th.override.Store(len(filter) != 0)\n\n\treturn nil\n}\n\n// Enabled implements slog.Handler, reporting whether the handler handles records\n// at the given level.\nfunc (h *GlogHandler) Enabled(ctx context.Context, lvl slog.Level) bool {\n\t// fast-track skipping logging if override not enabled and the provided verbosity is above configured\n\treturn h.override.Load() || slog.Level(h.level.Load()) <= lvl\n}\n\n// WithAttrs implements slog.Handler, returning a new Handler whose attributes\n// consist of both the receiver's attributes and the arguments.\nfunc (h *GlogHandler) WithAttrs(attrs []slog.Attr) slog.Handler {\n\th.lock.RLock()\n\tsiteCache := maps.Clone(h.siteCache)\n\th.lock.RUnlock()\n\n\tpatterns := []pattern{}\n\tpatterns = append(patterns, h.patterns...)\n\n\tres := GlogHandler{\n\t\torigin:    h.origin.WithAttrs(attrs),\n\t\tpatterns:  patterns,\n\t\tsiteCache: siteCache,\n\t\tlocation:  h.location,\n\t}\n\n\tres.level.Store(h.level.Load())\n\tres.override.Store(h.override.Load())\n\treturn &res\n}\n\n// WithGroup implements slog.Handler, returning a new Handler with the given\n// group appended to the receiver's existing groups.\n//\n// Note, this function is not implemented.\nfunc (h *GlogHandler) WithGroup(name string) slog.Handler {\n\tpanic(\"not implemented\")\n}\n\n// Handle implements slog.Handler, filtering a log record through the global,\n// local and backtrace filters, finally emitting it if either allow it through.\nfunc (h *GlogHandler) Handle(_ context.Context, r slog.Record) error {\n\t// If the global log level allows, fast track logging\n\tif slog.Level(h.level.Load()) <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\n\t// Check callsite cache for previously calculated log levels\n\th.lock.RLock()\n\tlvl, ok := h.siteCache[r.PC]\n\th.lock.RUnlock()\n\n\t// If we didn't cache the callsite yet, calculate it\n\tif !ok {\n\t\th.lock.Lock()\n\n\t\tfs := runtime.CallersFrames([]uintptr{r.PC})\n\t\tframe, _ := fs.Next()\n\n\t\tfor _, rule := range h.patterns {\n\t\t\tif rule.pattern.MatchString(fmt.Sprintf(\"+%s\", frame.File)) {\n\t\t\t\th.siteCache[r.PC], lvl, ok = rule.level, rule.level, true\n\t\t\t}\n\t\t}\n\t\t// If no rule matched, remember to drop log the next time\n\t\tif !ok {\n\t\t\th.siteCache[r.PC] = 0\n\t\t}\n\t\th.lock.Unlock()\n\t}\n\tif lvl <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/log/handler_glog.go",
          "line": 85,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=3\"\n//\t sets V to 3 in all files of any packages whose import path contains \"foo\"\nfunc (h *GlogHandler) Vmodule(ruleset string) error {\n\tvar filter []pattern\n\tfor _, rule := range strings.Split(ruleset, \",\") {\n\t\t// Empty strings such as from a trailing comma can be ignored\n\t\tif len(rule) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\t// Ensure we have a pattern = level filter rule\n\t\tparts := strings.Split(rule, \"=\")\n\t\tif len(parts) != 2 {\n\t\t\treturn errVmoduleSyntax\n\t\t}\n\t\tparts[0] = strings.TrimSpace(parts[0])\n\t\tparts[1] = strings.TrimSpace(parts[1])\n\t\tif len(parts[0]) == 0 || len(parts[1]) == 0 {\n\t\t\treturn errVmoduleSyntax\n\t\t}\n\t\t// Parse the level and if correct, assemble the filter rule\n\t\tl, err := strconv.Atoi(parts[1])\n\t\tif err != nil {\n\t\t\treturn errVmoduleSyntax\n\t\t}\n\t\tlevel := FromLegacyLevel(l)\n\n\t\tif level == LevelCrit {\n\t\t\tcontinue // Ignore. It's harmless but no point in paying the overhead.\n\t\t}\n\t\t// Compile the rule pattern into a regular expression\n\t\tmatcher := \".*\"\n\t\tfor _, comp := range strings.Split(parts[0], \"/\") {\n\t\t\tif comp == \"*\" {\n\t\t\t\tmatcher += \"(/.*)?\"\n\t\t\t} else if comp != \"\" {\n\t\t\t\tmatcher += \"/\" + regexp.QuoteMeta(comp)\n\t\t\t}\n\t\t}\n\t\tif !strings.HasSuffix(parts[0], \".go\") {\n\t\t\tmatcher += \"/[^/]+\\\\.go\"\n\t\t}\n\t\tmatcher = matcher + \"$\"\n\n\t\tre, _ := regexp.Compile(matcher)\n\t\tfilter = append(filter, pattern{re, level})\n\t}\n\t// Swap out the vmodule pattern for the new filter system\n\th.lock.Lock()\n\tdefer h.lock.Unlock()\n\n\th.patterns = filter\n\th.siteCache = make(map[uintptr]slog.Level)\n\th.override.Store(len(filter) != 0)\n\n\treturn nil\n}\n\n// Enabled implements slog.Handler, reporting whether the handler handles records\n// at the given level.\nfunc (h *GlogHandler) Enabled(ctx context.Context, lvl slog.Level) bool {\n\t// fast-track skipping logging if override not enabled and the provided verbosity is above configured\n\treturn h.override.Load() || slog.Level(h.level.Load()) <= lvl\n}\n\n// WithAttrs implements slog.Handler, returning a new Handler whose attributes\n// consist of both the receiver's attributes and the arguments.\nfunc (h *GlogHandler) WithAttrs(attrs []slog.Attr) slog.Handler {\n\th.lock.RLock()\n\tsiteCache := maps.Clone(h.siteCache)\n\th.lock.RUnlock()\n\n\tpatterns := []pattern{}\n\tpatterns = append(patterns, h.patterns...)\n\n\tres := GlogHandler{\n\t\torigin:    h.origin.WithAttrs(attrs),\n\t\tpatterns:  patterns,\n\t\tsiteCache: siteCache,\n\t\tlocation:  h.location,\n\t}\n\n\tres.level.Store(h.level.Load())\n\tres.override.Store(h.override.Load())\n\treturn &res\n}\n\n// WithGroup implements slog.Handler, returning a new Handler with the given\n// group appended to the receiver's existing groups.\n//\n// Note, this function is not implemented.\nfunc (h *GlogHandler) WithGroup(name string) slog.Handler {\n\tpanic(\"not implemented\")\n}\n\n// Handle implements slog.Handler, filtering a log record through the global,\n// local and backtrace filters, finally emitting it if either allow it through.\nfunc (h *GlogHandler) Handle(_ context.Context, r slog.Record) error {\n\t// If the global log level allows, fast track logging\n\tif slog.Level(h.level.Load()) <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\n\t// Check callsite cache for previously calculated log levels\n\th.lock.RLock()\n\tlvl, ok := h.siteCache[r.PC]\n\th.lock.RUnlock()\n\n\t// If we didn't cache the callsite yet, calculate it\n\tif !ok {\n\t\th.lock.Lock()\n\n\t\tfs := runtime.CallersFrames([]uintptr{r.PC})\n\t\tframe, _ := fs.Next()\n\n\t\tfor _, rule := range h.patterns {\n\t\t\tif rule.pattern.MatchString(fmt.Sprintf(\"+%s\", frame.File)) {\n\t\t\t\th.siteCache[r.PC], lvl, ok = rule.level, rule.level, true\n\t\t\t}\n\t\t}\n\t\t// If no rule matched, remember to drop log the next time\n\t\tif !ok {\n\t\t\th.siteCache[r.PC] = 0\n\t\t}\n\t\th.lock.Unlock()\n\t}\n\tif lvl <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/handler.go",
          "line": 963,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(hashes)\n\t\tpeer.AsyncSendTransactions(hashes)\n\t}\n\tfor peer, hashes := range annos {\n\t\tannCount += len(hashes)\n\t\tpeer.AsyncSendPooledTransactionHashes(hashes)\n\t}\n\tlog.Debug(\"Distributed transactions\", \"plaintxs\", len(txs)-blobTxs-largeTxs, \"blobtxs\", blobTxs, \"largetxs\", largeTxs,\n\t\t\"bcastpeers\", len(txset), \"bcastcount\", directCount, \"annpeers\", len(annos), \"anncount\", annCount)\n}\n\n// ReannounceTransactions will announce a batch of local pending transactions\n// to a square root of all peers.\nfunc (h *handler) ReannounceTransactions(txs types.Transactions) {\n\thashes := make([]common.Hash, 0, txs.Len())\n\tfor _, tx := range txs {\n\t\thashes = append(hashes, tx.Hash())\n\t}\n\n\t// Announce transactions hash to a batch of peers\n\tpeersCount := uint(math.Sqrt(float64(h.peers.len())))\n\tpeers := h.peers.headPeers(peersCount)\n\tfor _, peer := range peers {\n\t\tpeer.AsyncSendPooledTransactionHashes(hashes)\n\t}\n\tlog.Debug(\"Transaction reannounce\", \"txs\", len(txs),\n\t\t\"announce packs\", peersCount, \"announced hashes\", peersCount*uint(len(hashes)))\n}\n\n// BroadcastVote will propagate a batch of votes to all peers\n// which are not known to already have the given vote.\nfunc (h *handler) BroadcastVote(vote *types.VoteEnvelope) {\n\tvar (\n\t\tdirectCount int // Count of announcements made\n\t\tdirectPeers int\n\n\t\tvoteMap = make(map[*ethPeer]*types.VoteEnvelope) // Set peer->hash to transfer directly\n\t)\n\n\t// Broadcast vote to a batch of peers not knowing about it\n\tpeers := h.peers.peersWithoutVote(vote.Hash())\n\theadBlock := h.chain.CurrentBlock()\n\tcurrentTD := h.chain.GetTd(headBlock.Hash(), headBlock.Number.Uint64())\n\tfor _, peer := range peers {\n\t\t_, peerTD := peer.Head()\n\t\tdeltaTD := new(big.Int).Abs(new(big.Int).Sub(currentTD, peerTD))\n\t\tif deltaTD.Cmp(big.NewInt(deltaTdThreshold)) < 1 && peer.bscExt != nil {\n\t\t\tvoteMap[peer] = vote\n\t\t}\n\t}\n\n\tfor peer, _vote := range voteMap {\n\t\tdirectPeers++\n\t\tdirectCount += 1\n\t\tvotes := []*types.VoteEnvelope{_vote}\n\t\tpeer.bscExt.AsyncSendVotes(votes)\n\t}\n\tlog.Debug(\"Vote broadcast\", \"vote packs\", directPeers, \"broadcast vote\", directCount)\n}\n\n// minedBroadcastLoop sends mined blocks to connected peers.\nfunc (h *handler) minedBroadcastLoop() {\n\tdefer h.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase obj := <-h.minedBlockSub.Chan():\n\t\t\tif obj == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ev, ok := obj.Data.(core.NewSealedBlockEvent)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/backend.go",
          "line": 195,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= config.TrieDirtyCache * 3 / 5\n\t\t\tconfig.SnapshotCache += config.TrieDirtyCache * 2 / 5\n\t\t} else {\n\t\t\tconfig.TrieCleanCache += config.TrieDirtyCache\n\t\t}\n\t\tconfig.TrieDirtyCache = 0\n\t}\n\tif config.StateScheme == rawdb.PathScheme && config.TrieDirtyCache > pathdb.MaxDirtyBufferSize()/1024/1024 {\n\t\tlog.Info(\"Capped dirty cache size\", \"provided\", common.StorageSize(config.TrieDirtyCache)*1024*1024,\n\t\t\t\"adjusted\", common.StorageSize(pathdb.MaxDirtyBufferSize()))\n\t\tlog.Info(\"Clean cache size\", \"provided\", common.StorageSize(config.TrieCleanCache)*1024*1024,\n\t\t\t\"adjusted\", common.StorageSize(config.TrieCleanCache+config.TrieDirtyCache-pathdb.MaxDirtyBufferSize()/1024/1024)*1024*1024)\n\t\tconfig.TrieCleanCache += config.TrieDirtyCache - pathdb.MaxDirtyBufferSize()/1024/1024\n\t\tconfig.TrieDirtyCache = pathdb.MaxDirtyBufferSize() / 1024 / 1024\n\t}\n\tlog.Info(\"Allocated memory caches\",\n\t\t\"state_scheme\", config.StateScheme,\n\t\t\"trie_clean_cache\", common.StorageSize(config.TrieCleanCache)*1024*1024,\n\t\t\"trie_dirty_cache\", common.StorageSize(config.TrieDirtyCache)*1024*1024,\n\t\t\"snapshot_cache\", common.StorageSize(config.SnapshotCache)*1024*1024)\n\t// Try to recover offline state pruning only in hash-based.\n\tif config.StateScheme == rawdb.HashScheme {\n\t\tif err := pruner.RecoverPruning(stack.ResolvePath(\"\"), chainDb, config.TriesInMemory)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/handler_bsc_test.go",
          "line": 30,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 521,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(event.Txs)\n\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\tt.Errorf(\"sink %d: transaction propagation timed out: have %d, want %d\", i, arrived, len(txs))\n\t\t\t\ttimeout = true\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Tests that local pending transactions get propagated to peers.\nfunc TestTransactionPendingReannounce(t *testing.T) {\n\tt.Parallel()\n\n\t// Create a source handler to announce transactions from and a sink handler\n\t// to receive them.\n\tsource := newTestHandler()\n\tdefer source.close()\n\n\tsink := newTestHandler()\n\tdefer sink.close()\n\tsink.handler.acceptTxs.Store(true) // mark synced to accept transactions\n\n\tsourcePipe, sinkPipe := p2p.MsgPipe()\n\tdefer sourcePipe.Close()\n\tdefer sinkPipe.Close()\n\n\tsourcePeer := eth.NewPeer(eth.ETH68, p2p.NewPeer(enode.ID{0}, \"\", nil), sourcePipe, source.txpool)\n\tsinkPeer := eth.NewPeer(eth.ETH68, p2p.NewPeer(enode.ID{0}, \"\", nil), sinkPipe, sink.txpool)\n\tdefer sourcePeer.Close()\n\tdefer sinkPeer.Close()\n\n\tgo source.handler.runEthPeer(sourcePeer, func(peer *eth.Peer) error {\n\t\treturn eth.Handle((*ethHandler)(source.handler), peer)\n\t})\n\tgo sink.handler.runEthPeer(sinkPeer, func(peer *eth.Peer) error {\n\t\treturn eth.Handle((*ethHandler)(sink.handler), peer)\n\t})\n\n\t// Subscribe transaction pools\n\ttxCh := make(chan core.NewTxsEvent, 1024)\n\tsub := sink.txpool.SubscribeTransactions(txCh, false)\n\tdefer sub.Unsubscribe()\n\n\ttxs := make([]*types.Transaction, 64)\n\tfor nonce := range txs {\n\t\ttx := types.NewTransaction(uint64(nonce), common.Address{}, big.NewInt(0), 100000, big.NewInt(0), nil)\n\t\ttx, _ = types.SignTx(tx, types.HomesteadSigner{}, testKey)\n\n\t\ttxs[nonce] = tx\n\t}\n\tsource.txpool.ReannouceTransactions(txs)\n\n\tfor arrived := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 576,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(event.Txs)\n\t\tcase <-time.NewTimer(time.Second).C:\n\t\t\tt.Errorf(\"sink: transaction propagation timed out: have %d, want %d\", arrived, len(txs))\n\t\t}\n\t}\n}\n\n// Tests that blocks are broadcast to a sqrt number of peers only.\nfunc TestBroadcastBlock1Peer(t *testing.T)    { testBroadcastBlock(t, 1, 1) }\nfunc TestBroadcastBlock2Peers(t *testing.T)   { testBroadcastBlock(t, 2, 1) }\nfunc TestBroadcastBlock3Peers(t *testing.T)   { testBroadcastBlock(t, 3, 1) }\nfunc TestBroadcastBlock4Peers(t *testing.T)   { testBroadcastBlock(t, 4, 2) }\nfunc TestBroadcastBlock5Peers(t *testing.T)   { testBroadcastBlock(t, 5, 2) }\nfunc TestBroadcastBlock8Peers(t *testing.T)   { testBroadcastBlock(t, 9, 3) }\nfunc TestBroadcastBlock12Peers(t *testing.T)  { testBroadcastBlock(t, 12, 3) }\nfunc TestBroadcastBlock16Peers(t *testing.T)  { testBroadcastBlock(t, 16, 4) }\nfunc TestBroadcastBloc26Peers(t *testing.T)   { testBroadcastBlock(t, 26, 5) }\nfunc TestBroadcastBlock100Peers(t *testing.T) { testBroadcastBlock(t, 100, 10) }\n\nfunc testBroadcastBlock(t *testing.T, peers, bcasts int) {\n\tt.Parallel()\n\n\t// Create a source handler to broadcast blocks from and a number of sinks\n\t// to receive them.\n\tsource := newTestHandlerWithBlocks(1)\n\tdefer source.close()\n\n\tsinks := make([]*testEthHandler, peers)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 58,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 62,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 66,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 70,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/handler_test.go",
          "line": 130,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/handler_test.go",
          "line": 142,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/handler_test.go",
          "line": 376,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/api_debug.go",
          "line": 407,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= delta {\n\t\tif time.Since(lastLog) > 8*time.Second {\n\t\t\tlog.Info(\"Finding roots\", \"from\", start, \"to\", end, \"at\", i)\n\t\t\tlastLog = time.Now()\n\t\t}\n\t\tif i < int64(pivot) {\n\t\t\tcontinue\n\t\t}\n\t\th := api.eth.BlockChain().GetHeaderByNumber(uint64(i))\n\t\tif h == nil {\n\t\t\treturn 0, fmt.Errorf(\"missing header %d\", i)\n\t\t}\n\t\tif ok, _ := api.eth.ChainDb().Has(h.Root[:])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/build/ci.go",
          "line": 142,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \".exe\"\n\t}\n\treturn filepath.Join(GOBIN, name)\n}\n\nfunc main() {\n\tlog.SetFlags(log.Lshortfile)\n\n\tif !build.FileExist(filepath.Join(\"build\", \"ci.go\")) {\n\t\tlog.Fatal(\"this script must be run from the root of the repository\")\n\t}\n\tif len(os.Args) < 2 {\n\t\tlog.Fatal(\"need subcommand as first argument\")\n\t}\n\tswitch os.Args[1] {\n\tcase \"install\":\n\t\tdoInstall(os.Args[2:])\n\tcase \"test\":\n\t\tdoTest(os.Args[2:])\n\tcase \"lint\":\n\t\tdoLint(os.Args[2:])\n\tcase \"check_generate\":\n\t\tdoCheckGenerate()\n\tcase \"check_baddeps\":\n\t\tdoCheckBadDeps()\n\tcase \"archive\":\n\t\tdoArchive(os.Args[2:])\n\tcase \"dockerx\":\n\t\tdoDockerBuildx(os.Args[2:])\n\tcase \"debsrc\":\n\t\tdoDebianSource(os.Args[2:])\n\tcase \"nsis\":\n\t\tdoWindowsInstaller(os.Args[2:])\n\tcase \"purge\":\n\t\tdoPurge(os.Args[2:])\n\tcase \"sanitycheck\":\n\t\tdoSanityCheck()\n\tdefault:\n\t\tlog.Fatal(\"unknown command \", os.Args[1])\n\t}\n}\n\n// Compiling\n\nfunc doInstall(cmdline []string) {\n\tvar (\n\t\tdlgo       = flag.Bool(\"dlgo\", false, \"Download Go and build with it\")\n\t\tarch       = flag.String(\"arch\", \"\", \"Architecture to cross build for\")\n\t\tcc         = flag.String(\"cc\", \"\", \"C compiler to cross build with\")\n\t\tstaticlink = flag.Bool(\"static\", false, \"Create statically-linked executable\")\n\t\toutput     = flag.String(\"o\", \"\", \"Output directory for build artifacts\")\n\t)\n\tflag.CommandLine.Parse(cmdline)\n\tenv := build.Env()\n\n\t// Configure the toolchain.\n\ttc := build.GoToolchain{GOARCH: *arch, CC: *cc}\n\tif *dlgo {\n\t\tcsdb := download.MustLoadChecksums(\"build/checksums.txt\")\n\t\ttc.Root = build.DownloadGo(csdb)\n\t}\n\t// Disable CLI markdown doc generation in release builds.\n\tbuildTags := []string{\"urfave_cli_no_docs\"}\n\n\t// Enable linking the CKZG library since we can make it work with additional flags.\n\tif env.UbuntuVersion != \"trusty\" {\n\t\tbuildTags = append(buildTags, \"ckzg\")\n\t}\n\n\t// Configure the build.\n\tgobuild := tc.Go(\"build\", buildFlags(env, *staticlink, buildTags)...)\n\n\t// We use -trimpath to avoid leaking local paths into the built executables.\n\tgobuild.Args = append(gobuild.Args, \"-trimpath\")\n\n\t// Show packages during build.\n\tgobuild.Args = append(gobuild.Args, \"-v\")\n\n\t// Now we choose what we're even building.\n\t// Default: collect all 'main' packages in cmd/ and build those.\n\tpackages := flag.Args()\n\tif len(packages) == 0 {\n\t\tpackages = build.FindMainPackages(\"./cmd\")\n\t}\n\n\t// Do the build!\n\tfor _, pkg := range packages {\n\t\targs := slices.Clone(gobuild.Args)\n\t\toutputPath := executablePath(path.Base(pkg))\n\t\tif output != nil && *output != \"\" {\n\t\t\toutputPath = *output\n\t\t}\n\t\targs = append(args, \"-o\", outputPath)\n\t\targs = append(args, pkg)\n\t\tbuild.MustRun(&exec.Cmd{Path: gobuild.Path, Args: args, Env: gobuild.Env})\n\t}\n}\n\n// buildFlags returns the go tool flags for building.\nfunc buildFlags(env build.Environment, staticLinking bool, buildTags []string) (flags []string) {\n\tvar ld []string\n\t// See https://github.com/golang/go/issues/33772#issuecomment-528176001\n\t// We need to set --buildid to the linker here, and also pass --build-id to the\n\t// cgo-linker further down.\n\tld = append(ld, \"--buildid=none\")\n\tif env.Commit != \"\" {\n\t\tld = append(ld, \"-X\", \"github.com/ethereum/go-ethereum/internal/version.gitCommit=\"+env.Commit)\n\t\tld = append(ld, \"-X\", \"github.com/ethereum/go-ethereum/internal/version.gitDate=\"+env.Date)\n\t}\n\t// Strip DWARF on darwin. This used to be required for certain things,\n\t// and there is no downside to this, so we just keep doing it.\n\tif runtime.GOOS == \"darwin\" {\n\t\tld = append(ld, \"-s\")\n\t}\n\tif runtime.GOOS == \"linux\" {\n\t\t// Enforce the stacksize to 8M, which is the case on most platforms apart from\n\t\t// alpine Linux.\n\t\t// See https://sourceware.org/binutils/docs-2.23.1/ld/Options.html#Options\n\t\t// regarding the options --build-id=none and --strip-all. It is needed for\n\t\t// reproducible builds",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/build/ci.go",
          "line": 468,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"v\" + os.Getenv(\"GOARM\")\n\t}\n\tbase := fmt.Sprintf(\"golangci-lint-%s-%s-%s\", version, runtime.GOOS, arch)\n\tarchivePath := filepath.Join(cachedir, base+ext)\n\tif err := csdb.DownloadFileFromKnownURL(archivePath)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/build/ci.go",
          "line": 516,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \".zip\"\n\t} else {\n\t\tarchiveName += \".tar.gz\"\n\t}\n\n\tarchivePath := path.Join(cachedir, archiveName)\n\tif err := csdb.DownloadFileFromKnownURL(archivePath)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/build/ci.go",
          "line": 610,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= os.Getenv(\"GOARM\")\n\t}\n\tif arch == \"android\" {\n\t\tplatform = \"android-all\"\n\t}\n\tif arch == \"ios\" {\n\t\tplatform = \"ios-all\"\n\t}\n\treturn platform + \"-\" + archiveVersion\n}\n\nfunc archiveUpload(archive string, blobstore string, signer string, signifyVar string) error {\n\t// If signing was requested, generate the signature files\n\tif signer != \"\" {\n\t\tkey := getenvBase64(signer)\n\t\tif err := build.PGPSignFile(archive, archive+\".asc\", string(key))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/build/ci.go",
          "line": 988,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"+build\" + meta.Env.Buildnum\n\t}\n\tif meta.Distro != \"\" {\n\t\tvsn += \"+\" + meta.Distro\n\t}\n\treturn vsn\n}\n\n// ExeList returns the list of all executable packages.\nfunc (meta debMetadata) ExeList() string {\n\tnames := make([]string, len(meta.Executables))\n\tfor i, e := range meta.Executables {\n\t\tnames[i] = meta.ExeName(e)\n\t}\n\treturn strings.Join(names, \", \")\n}\n\n// ExeName returns the package name of an executable package.\nfunc (meta debMetadata) ExeName(exe debExecutable) string {\n\tif isUnstableBuild(meta.Env) {\n\t\treturn exe.Package() + \"-unstable\"\n\t}\n\treturn exe.Package()\n}\n\n// ExeConflicts returns the content of the Conflicts field\n// for executable packages.\nfunc (meta debMetadata) ExeConflicts(exe debExecutable) string {\n\tif isUnstableBuild(meta.Env) {\n\t\t// Set up the conflicts list so that the *-unstable packages\n\t\t// cannot be installed alongside the regular version.\n\t\t//\n\t\t// https://www.debian.org/doc/debian-policy/ch-relationships.html\n\t\t// is very explicit about Conflicts: and says that Breaks: should\n\t\t// be preferred and the conflicting files should be handled via\n\t\t// alternates. We might do this eventually but using a conflict is\n\t\t// easier now.\n\t\treturn \"ethereum, \" + exe.Package()\n\t}\n\treturn \"\"\n}\n\nfunc stageDebianSource(tmpdir string, meta debMetadata) (pkgdir string) {\n\tpkg := meta.Name() + \"-\" + meta.VersionString()\n\tpkgdir = filepath.Join(tmpdir, pkg)\n\tif err := os.Mkdir(pkgdir, 0755)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/build/ci.go",
          "line": 1112,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"-\" + env.Commit[:8]\n\t}\n\tinstaller, err := filepath.Abs(\"geth-\" + archiveBasename(*arch, version.Archive(env.Commit)) + \".exe\")\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to convert installer file path: %v\", err)\n\t}\n\tbuild.MustRunCommand(\"makensis.exe\",\n\t\t\"/DOUTPUTFILE=\"+installer,\n\t\t\"/DMAJORVERSION=\"+ver[0],\n\t\t\"/DMINORVERSION=\"+ver[1],\n\t\t\"/DBUILDVERSION=\"+ver[2],\n\t\t\"/DARCH=\"+*arch,\n\t\tfilepath.Join(*workdir, \"geth.nsi\"),\n\t)\n\t// Sign and publish installer.\n\tif err := archiveUpload(installer, *upload, *signer, *signify)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/node/config.go",
          "line": 347,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"/\" + c.UserIdent\n\t}\n\tif c.Version != \"\" {\n\t\tname += \"/v\" + c.Version\n\t}\n\tname += \"/\" + runtime.GOOS + \"-\" + runtime.GOARCH\n\tname += \"/\" + runtime.Version()\n\treturn name\n}\n\nfunc (c *Config) name() string {\n\tif c.Name == \"\" {\n\t\tprogname := strings.TrimSuffix(filepath.Base(os.Args[0]), \".exe\")\n\t\tif progname == \"\" {\n\t\t\tpanic(\"empty executable name, set Config.Name\")\n\t\t}\n\t\treturn progname\n\t}\n\treturn c.Name\n}\n\n// These resources are resolved differently for \"geth\" instances.\nvar isOldGethResource = map[string]bool{\n\t\"chaindata\":          true,\n\t\"nodes\":              true,\n\t\"nodekey\":            true,\n\t\"static-nodes.json\":  false, // no warning for these because they have their\n\t\"trusted-nodes.json\": false, // own separate warning.\n}\n\n// ResolvePath resolves path in the instance directory.\nfunc (c *Config) ResolvePath(path string) string {\n\tif filepath.IsAbs(path) {\n\t\treturn path\n\t}\n\tif c.DataDir == \"\" {\n\t\treturn \"\"\n\t}\n\t// Backwards-compatibility: ensure that data directory files created\n\t// by geth 1.4 are used if they exist.\n\tif warn, isOld := isOldGethResource[path]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/node/rpcstack_test.go",
          "line": 264,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey, value := extraHeaders[i], extraHeaders[i+1]\n\t\theaders.Set(key, value)\n\t}\n\tconn, _, err := websocket.DefaultDialer.Dial(url, headers)\n\tif conn != nil {\n\t\tconn.Close()\n\t}\n\treturn err\n}\n\n// rpcRequest performs a JSON-RPC request to the given URL.\nfunc rpcRequest(t *testing.T, url, method string, extraHeaders ...string) *http.Response {\n\tt.Helper()\n\n\tbody := fmt.Sprintf(`{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"%s\",\"params\":[]}`, method)\n\treturn baseRpcRequest(t, url, body, extraHeaders...)\n}\n\nfunc batchRpcRequest(t *testing.T, url string, methods []string, extraHeaders ...string) *http.Response {\n\treqs := make([]string, len(methods))\n\tfor i, m := range methods {\n\t\treqs[i] = fmt.Sprintf(`{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"%s\",\"params\":[]}`, m)\n\t}\n\tbody := fmt.Sprintf(`[%s]`, strings.Join(reqs, \",\"))\n\treturn baseRpcRequest(t, url, body, extraHeaders...)\n}\n\nfunc baseRpcRequest(t *testing.T, url, bodyStr string, extraHeaders ...string) *http.Response {\n\tt.Helper()\n\n\t// Create the request.\n\tbody := bytes.NewReader([]byte(bodyStr))\n\treq, err := http.NewRequest(http.MethodPost, url, body)\n\tif err != nil {\n\t\tt.Fatal(\"could not create http request:\", err)\n\t}\n\treq.Header.Set(\"content-type\", \"application/json\")\n\treq.Header.Set(\"accept-encoding\", \"identity\")\n\n\t// Apply extra headers.\n\tif len(extraHeaders)%2 != 0 {\n\t\tpanic(\"odd extraHeaders length\")\n\t}\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/node/rpcstack_test.go",
          "line": 308,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey, value := extraHeaders[i], extraHeaders[i+1]\n\t\tif strings.EqualFold(key, \"host\") {\n\t\t\treq.Host = value\n\t\t} else {\n\t\t\treq.Header.Set(key, value)\n\t\t}\n\t}\n\n\t// Perform the request.\n\tt.Logf(\"checking RPC/HTTP on %s %v\", url, extraHeaders)\n\tresp, err := http.DefaultClient.Do(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tt.Cleanup(func() { resp.Body.Close() })\n\treturn resp\n}\n\ntype testClaim map[string]interface{}\n\nfunc (testClaim) Valid() error {\n\treturn nil\n}\n\nfunc TestJWT(t *testing.T) {\n\tvar secret = []byte(\"secret\")\n\tissueToken := func(secret []byte, method jwt.SigningMethod, input map[string]interface{}) string {\n\t\tif method == nil {\n\t\t\tmethod = jwt.SigningMethodHS256\n\t\t}\n\t\tss, _ := jwt.NewWithClaims(method, testClaim(input)).SignedString(secret)\n\t\treturn ss\n\t}\n\tcfg := rpcEndpointConfig{jwtSecret: []byte(\"secret\")}\n\thttpcfg := &httpConfig{rpcEndpointConfig: cfg}\n\twscfg := &wsConfig{Origins: []string{\"*\"}, rpcEndpointConfig: cfg}\n\tsrv := createAndStartServer(t, httpcfg, true, wscfg, nil)\n\twsUrl := fmt.Sprintf(\"ws://%v\", srv.listenAddr())\n\thtUrl := fmt.Sprintf(\"http://%v\", srv.listenAddr())\n\n\texpOk := []func() string{\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\"iat\": time.Now().Unix()}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\"iat\": time.Now().Unix() + 4}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\"iat\": time.Now().Unix() - 4}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\n\t\t\t\t\"iat\": time.Now().Unix(),\n\t\t\t\t\"exp\": time.Now().Unix() + 2,\n\t\t\t}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\n\t\t\t\t\"iat\": time.Now().Unix(),\n\t\t\t\t\"bar\": \"baz\",\n\t\t\t}))\n\t\t},\n\t}\n\tfor i, tokenFn := range expOk {\n\t\ttoken := tokenFn()\n\t\tif err := wsRequest(t, wsUrl, \"Authorization\", token)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/node/node_auth_test.go",
          "line": 217,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tif i > len(provs) {\n\t\t\ti = len(provs)\n\t\t}\n\t\treturn provs[i-1](header)\n\t}\n}\n\nfunc offsetTimeAuth(secret [32]byte, offset time.Duration) rpc.HTTPAuth {\n\treturn func(header http.Header) error {\n\t\ttoken := jwt.NewWithClaims(jwt.SigningMethodHS256, jwt.MapClaims{\n\t\t\t\"iat\": &jwt.NumericDate{Time: time.Now().Add(offset)},\n\t\t})\n\t\ts, err := token.SignedString(secret[:])\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create JWT token: %w\", err)\n\t\t}\n\t\theader.Set(\"Authorization\", \"Bearer \"+s)\n\t\treturn nil\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/node/rpcstack.go",
          "line": 165,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= h.wsConfig.prefix\n\t\t}\n\t\th.log.Info(\"WebSocket enabled\", \"url\", url)\n\t}\n\t// if server is websocket only, return after logging\n\tif !h.rpcAllowed() {\n\t\treturn nil\n\t}\n\t// Log http endpoint.\n\th.log.Info(\"HTTP server started\",\n\t\t\"endpoint\", listener.Addr(), \"auth\", (h.httpConfig.jwtSecret != nil),\n\t\t\"prefix\", h.httpConfig.prefix,\n\t\t\"cors\", strings.Join(h.httpConfig.CorsAllowedOrigins, \",\"),\n\t\t\"vhosts\", strings.Join(h.httpConfig.Vhosts, \",\"),\n\t)\n\n\t// Log all handlers mounted on server.\n\tvar paths []string\n\tfor path := range h.handlerNames {\n\t\tpaths = append(paths, path)\n\t}\n\tsort.Strings(paths)\n\tlogged := make(map[string]bool, len(paths))\n\tfor _, path := range paths {\n\t\tname := h.handlerNames[path]\n\t\tif !logged[name] {\n\t\t\tlog.Info(name+\" enabled\", \"url\", \"http://\"+listener.Addr().String()+path)\n\t\t\tlogged[name] = true\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (h *httpServer) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\t// check if ws request and serve if ws enabled\n\tws := h.wsHandler.Load()\n\tif ws != nil && isWebsocket(r) {\n\t\tif checkPath(r, ws.prefix) {\n\t\t\tws.ServeHTTP(w, r)\n\t\t}\n\t\treturn\n\t}\n\n\t// if http-rpc is enabled, try to serve request\n\trpc := h.httpHandler.Load()\n\tif rpc != nil {\n\t\t// First try to route in the mux.\n\t\t// Requests to a path below root are handled by the mux,\n\t\t// which has all the handlers registered via Node.RegisterHandler.\n\t\t// These are made available when RPC is enabled.\n\t\tmuxHandler, pattern := h.mux.Handler(r)\n\t\tif pattern != \"\" {\n\t\t\tmuxHandler.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\n\t\tif checkPath(r, rpc.prefix) {\n\t\t\trpc.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\t}\n\tw.WriteHeader(http.StatusNotFound)\n}\n\n// checkPath checks whether a given request URL matches a given path prefix.\nfunc checkPath(r *http.Request, path string) bool {\n\t// if no prefix has been specified, request URL must be on root\n\tif path == \"\" {\n\t\treturn r.URL.Path == \"/\"\n\t}\n\t// otherwise, check to make sure prefix matches\n\treturn len(r.URL.Path) >= len(path) && r.URL.Path[:len(path)] == path\n}\n\n// validatePrefix checks if 'path' is a valid configuration value for the RPC prefix option.\nfunc validatePrefix(what, path string) error {\n\tif path == \"\" {\n\t\treturn nil\n\t}\n\tif path[0] != '/' {\n\t\treturn fmt.Errorf(`%s RPC path prefix %q does not contain leading \"/\"`, what, path)\n\t}\n\tif strings.ContainsAny(path, \"?#\") {\n\t\t// This is just to avoid confusion. While these would match correctly (i.e. they'd\n\t\t// match if URL-escaped into path), it's not easy to understand for users when\n\t\t// setting that on the command line.\n\t\treturn fmt.Errorf(\"%s RPC path prefix %q contains URL meta-characters\", what, path)\n\t}\n\treturn nil\n}\n\n// stop shuts down the HTTP server.\nfunc (h *httpServer) stop() {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\th.doStop()\n}\n\nfunc (h *httpServer) doStop() {\n\tif h.listener == nil {\n\t\treturn // not running\n\t}\n\n\t// Shut down the server.\n\thttpHandler := h.httpHandler.Load()\n\twsHandler := h.wsHandler.Load()\n\tif httpHandler != nil {\n\t\th.httpHandler.Store(nil)\n\t\thttpHandler.server.Stop()\n\t}\n\tif wsHandler != nil {\n\t\th.wsHandler.Store(nil)\n\t\twsHandler.server.Stop()\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)\n\tdefer cancel()\n\terr := h.server.Shutdown(ctx)\n\tif err != nil && err == ctx.Err() {\n\t\th.log.Warn(\"HTTP server graceful shutdown timed out\")\n\t\th.server.Close()\n\t}\n\n\th.listener.Close()\n\th.log.Info(\"HTTP server stopped\", \"endpoint\", h.listener.Addr())\n\n\t// Clear out everything to allow re-configuring it later.\n\th.host, h.port, h.endpoint = \"\", 0, \"\"\n\th.server, h.listener = nil, nil\n}\n\n// enableRPC turns on JSON-RPC over HTTP on the server.\nfunc (h *httpServer) enableRPC(apis []rpc.API, config httpConfig) error {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tif h.rpcAllowed() {\n\t\treturn errors.New(\"JSON-RPC over HTTP is already enabled\")\n\t}\n\n\t// Create RPC server and handler.\n\tsrv := rpc.NewServer()\n\tsrv.SetBatchLimits(config.batchItemLimit, config.batchResponseSizeLimit)\n\tif config.httpBodyLimit > 0 {\n\t\tsrv.SetHTTPBodyLimit(config.httpBodyLimit)\n\t}\n\tif err := RegisterApis(apis, config.Modules, srv)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/node/rpcstack.go",
          "line": 546,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(n)\n\tif w.hasLength && w.written >= w.contentLength {\n\t\t// The HTTP handler has finished writing the entire uncompressed response. Close\n\t\t// the gzip stream to ensure the footer will be seen by the client in case the\n\t\t// response is flushed after this call to write.\n\t\terr = w.gz.Close()\n\t}\n\treturn n, err\n}\n\nfunc (w *gzipResponseWriter) Flush() {\n\tif w.gz != nil {\n\t\tw.gz.Flush()\n\t}\n\tif f, ok := w.resp.(http.Flusher)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/subscription.go",
          "line": 142,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/subscription.go",
          "line": 165,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/handler.go",
          "line": 240,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(resp.Result)\n\t\t\t\tif responseBytes > h.batchResponseMaxSize {\n\t\t\t\t\terr := &internalServerError{errcodeResponseTooLarge, errMsgResponseTooLarge}\n\t\t\t\t\tcallBuffer.respondWithError(cp.ctx, h.conn, err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif timer != nil {\n\t\t\ttimer.Stop()\n\t\t}\n\n\t\th.addSubscriptions(cp.notifiers)\n\t\tcallBuffer.write(cp.ctx, h.conn)\n\t\tfor _, n := range cp.notifiers {\n\t\t\tn.activate()\n\t\t}\n\t})\n}\n\nfunc (h *handler) respondWithBatchTooLarge(cp *callProc, batch []*jsonrpcMessage) {\n\tresp := errorMessage(&invalidRequestError{errMsgBatchTooLarge})\n\t// Find the first call and add its \"id\" field to the error.\n\t// This is the best we can do, given that the protocol doesn't have a way\n\t// of reporting an error for the entire batch.\n\tfor _, msg := range batch {\n\t\tif msg.isCall() {\n\t\t\tresp.ID = msg.ID\n\t\t\tbreak\n\t\t}\n\t}\n\th.conn.writeJSON(cp.ctx, []*jsonrpcMessage{resp}, true)\n}\n\n// handleMsg handles a single non-batch message.\nfunc (h *handler) handleMsg(ctx context.Context, msg *jsonrpcMessage) {\n\tmsgs := []*jsonrpcMessage{msg}\n\th.handleResponses(msgs, func(msg *jsonrpcMessage) {\n\t\th.startCallProc(func(cp *callProc) {\n\t\t\th.handleNonBatchCall(cp, ctx, msg)\n\t\t})\n\t})\n}\n\nfunc (h *handler) handleNonBatchCall(cp *callProc, reqCtx context.Context, msg *jsonrpcMessage) {\n\tvar (\n\t\tresponded sync.Once\n\t\ttimer     *time.Timer\n\t\tcancel    context.CancelFunc\n\t)\n\tcp.ctx, cancel = context.WithCancel(cp.ctx)\n\tdefer cancel()\n\n\t// Cancel the request context after timeout and send an error response. Since the\n\t// running method might not return immediately on timeout, we must wait for the\n\t// timeout concurrently with processing the request.\n\tif timeout, ok := ContextRequestTimeout(cp.ctx)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/handler.go",
          "line": 593,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/testservice_test.go",
          "line": 145,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/testservice_test.go",
          "line": 157,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/service.go",
          "line": 205,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/http_test.go",
          "line": 138,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/http_test.go",
          "line": 162,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/rpc/http_test.go",
          "line": 204,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/websocket_test.go",
          "line": 101,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/websocket_test.go",
          "line": 110,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/rpc/websocket_test.go",
          "line": 153,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/rpc/websocket_test.go",
          "line": 161,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/rpc/websocket_test.go",
          "line": 196,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/rpc/websocket_test.go",
          "line": 284,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/rpc/websocket_test.go",
          "line": 453,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/client.go",
          "line": 354,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/client.go",
          "line": 423,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/rpc/client.go",
          "line": 482,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/rpc/client.go",
          "line": 527,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/http.go",
          "line": 401,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 100 * time.Millisecond\n\t\tsetTimeout(wt)\n\t}\n\n\treturn timeout, hasTimeout\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/websocket.go",
          "line": 125,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" (HTTP status \" + e.status + \")\"\n\t}\n\treturn s\n}\n\nfunc (e wsHandshakeError) Unwrap() error {\n\treturn e.err\n}\n\nfunc originIsAllowed(allowedOrigins mapset.Set[string], browserOrigin string) bool {\n\tit := allowedOrigins.Iterator()\n\tfor origin := range it.C {\n\t\tif ruleAllowsOrigin(origin, browserOrigin) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc ruleAllowsOrigin(allowedOrigin string, browserOrigin string) bool {\n\tvar (\n\t\tallowedScheme, allowedHostname, allowedPort string\n\t\tbrowserScheme, browserHostname, browserPort string\n\t\terr                                         error\n\t)\n\tallowedScheme, allowedHostname, allowedPort, err = parseOriginURL(allowedOrigin)\n\tif err != nil {\n\t\tlog.Warn(\"Error parsing allowed origin specification\", \"spec\", allowedOrigin, \"error\", err)\n\t\treturn false\n\t}\n\tbrowserScheme, browserHostname, browserPort, err = parseOriginURL(browserOrigin)\n\tif err != nil {\n\t\tlog.Warn(\"Error parsing browser 'Origin' field\", \"Origin\", browserOrigin, \"error\", err)\n\t\treturn false\n\t}\n\tif allowedScheme != \"\" && allowedScheme != browserScheme {\n\t\treturn false\n\t}\n\tif allowedHostname != \"\" && allowedHostname != browserHostname {\n\t\treturn false\n\t}\n\tif allowedPort != \"\" && allowedPort != browserPort {\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc parseOriginURL(origin string) (string, string, string, error) {\n\tparsedURL, err := url.Parse(strings.ToLower(origin))\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", err\n\t}\n\tvar scheme, hostname, port string\n\tif strings.Contains(origin, \"://\") {\n\t\tscheme = parsedURL.Scheme\n\t\thostname = parsedURL.Hostname()\n\t\tport = parsedURL.Port()\n\t} else {\n\t\tscheme = \"\"\n\t\thostname = parsedURL.Scheme\n\t\tport = parsedURL.Opaque\n\t\tif hostname == \"\" {\n\t\t\thostname = origin\n\t\t}\n\t}\n\treturn scheme, hostname, port, nil\n}\n\n// DialWebsocketWithDialer creates a new RPC client using WebSocket.\n//\n// The context is used for the initial connection establishment. It does not\n// affect subsequent interactions with the client.\n//\n// Deprecated: use DialOptions and the WithWebsocketDialer option.\nfunc DialWebsocketWithDialer(ctx context.Context, endpoint, origin string, dialer websocket.Dialer) (*Client, error) {\n\tcfg := new(clientConfig)\n\tcfg.wsDialer = &dialer\n\tif origin != \"\" {\n\t\tcfg.setHeader(\"origin\", origin)\n\t}\n\tconnect, err := newClientTransportWS(endpoint, cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newClient(ctx, cfg, connect)\n}\n\n// DialWebsocket creates a new RPC client that communicates with a JSON-RPC server\n// that is listening on the given endpoint.\n//\n// The context is used for the initial connection establishment. It does not\n// affect subsequent interactions with the client.\nfunc DialWebsocket(ctx context.Context, endpoint, origin string) (*Client, error) {\n\tcfg := new(clientConfig)\n\tif origin != \"\" {\n\t\tcfg.setHeader(\"origin\", origin)\n\t}\n\tconnect, err := newClientTransportWS(endpoint, cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newClient(ctx, cfg, connect)\n}\n\nfunc newClientTransportWS(endpoint string, cfg *clientConfig) (reconnectFunc, error) {\n\tdialer := cfg.wsDialer\n\tif dialer == nil {\n\t\tdialer = &websocket.Dialer{\n\t\t\tReadBufferSize:  wsReadBuffer,\n\t\t\tWriteBufferSize: wsWriteBuffer,\n\t\t\tWriteBufferPool: wsBufferPool,\n\t\t\tProxy:           http.ProxyFromEnvironment,\n\t\t}\n\t}\n\n\tdialURL, header, err := wsClientHeaders(endpoint, \"\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmaps.Copy(header, cfg.httpHeaders)\n\n\tconnect := func(ctx context.Context) (ServerCodec, error) {\n\t\theader := header.Clone()\n\t\tif cfg.httpAuth != nil {\n\t\t\tif err := cfg.httpAuth(header)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rpc/client_test.go",
          "line": 49,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/rpc/client_test.go",
          "line": 65,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/rpc/client_test.go",
          "line": 70,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/rpc/client_test.go",
          "line": 87,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/rpc/client_test.go",
          "line": 108,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/rpc/client_test.go",
          "line": 816,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/subscription.go",
          "line": 216,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 2\n\t\tif s.waitTime > s.backoffMax {\n\t\t\ts.waitTime = s.backoffMax\n\t\t}\n\t}\n\n\tt := time.NewTimer(s.waitTime)\n\tdefer t.Stop()\n\tselect {\n\tcase <-t.C:\n\t\treturn false\n\tcase <-s.unsub:\n\t\treturn true\n\t}\n}\n\n// SubscriptionScope provides a facility to unsubscribe multiple subscriptions at once.\n//\n// For code that handle more than one subscription, a scope can be used to conveniently\n// unsubscribe all of them with a single call. The example demonstrates a typical use in a\n// larger program.\n//\n// The zero value is ready to use.\ntype SubscriptionScope struct {\n\tmu     sync.Mutex\n\tsubs   map[*scopeSub]struct{}\n\tclosed bool\n}\n\ntype scopeSub struct {\n\tsc *SubscriptionScope\n\ts  Subscription\n}\n\n// Track starts tracking a subscription. If the scope is closed, Track returns nil. The\n// returned subscription is a wrapper. Unsubscribing the wrapper removes it from the\n// scope.\nfunc (sc *SubscriptionScope) Track(s Subscription) Subscription {\n\tsc.mu.Lock()\n\tdefer sc.mu.Unlock()\n\tif sc.closed {\n\t\treturn nil\n\t}\n\tif sc.subs == nil {\n\t\tsc.subs = make(map[*scopeSub]struct{})\n\t}\n\tss := &scopeSub{sc, s}\n\tsc.subs[ss] = struct{}{}\n\treturn ss\n}\n\n// Close calls Unsubscribe on all tracked subscriptions and prevents further additions to\n// the tracked set. Calls to Track after Close return nil.\nfunc (sc *SubscriptionScope) Close() {\n\tsc.mu.Lock()\n\tdefer sc.mu.Unlock()\n\tif sc.closed {\n\t\treturn\n\t}\n\tsc.closed = true\n\tfor s := range sc.subs {\n\t\ts.s.Unsubscribe()\n\t}\n\tsc.subs = nil\n}\n\n// Count returns the number of tracked subscriptions.\n// It is meant to be used for debugging.\nfunc (sc *SubscriptionScope) Count() int {\n\tsc.mu.Lock()\n\tdefer sc.mu.Unlock()\n\treturn len(sc.subs)\n}\n\nfunc (s *scopeSub) Unsubscribe() {\n\ts.s.Unsubscribe()\n\ts.sc.mu.Lock()\n\tdefer s.sc.mu.Unlock()\n\tdelete(s.sc.subs, s)\n}\n\nfunc (s *scopeSub) Err() <-chan error {\n\treturn s.s.Err()\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/feed.go",
          "line": 228,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \", \"\n//             }\n//             switch cas.Dir {\n//             case reflect.SelectSend:\n//                     s += fmt.Sprintf(\"%v<-\", cas.Chan.Interface())\n//             case reflect.SelectRecv:\n//                     s += fmt.Sprintf(\"<-%v\", cas.Chan.Interface())\n//             }\n//     }\n//     return s + \"]\"\n// }\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/feedof_test.go",
          "line": 64,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/event/feedof_test.go",
          "line": 67,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/event/feedof_test.go",
          "line": 83,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/event/feedof_test.go",
          "line": 130,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/event/feedof_test.go",
          "line": 167,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/event/feedof_test.go",
          "line": 199,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/event/feedof_test.go",
          "line": 216,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/event/feedof_test.go",
          "line": 272,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/multisub_test.go",
          "line": 38,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/event/multisub_test.go",
          "line": 48,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/event/multisub_test.go",
          "line": 65,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/event/multisub_test.go",
          "line": 72,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/event/multisub_test.go",
          "line": 102,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/event/multisub_test.go",
          "line": 109,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/event/multisub_test.go",
          "line": 126,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/event/multisub_test.go",
          "line": 133,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/event/multisub_test.go",
          "line": 162,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/event/multisub_test.go",
          "line": 169,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/example_feed_test.go",
          "line": 58,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/example_scope_test.go",
          "line": 36,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/event/example_scope_test.go",
          "line": 42,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/event/feed_test.go",
          "line": 31,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/event/feed_test.go",
          "line": 33,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/event/feed_test.go",
          "line": 42,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/event/feed_test.go",
          "line": 48,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/event/feed_test.go",
          "line": 120,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/event/feed_test.go",
          "line": 123,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/event/feed_test.go",
          "line": 139,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/event/feed_test.go",
          "line": 186,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/event/feed_test.go",
          "line": 223,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/event/feed_test.go",
          "line": 255,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/event/feed_test.go",
          "line": 272,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc/event/feed_test.go",
          "line": 328,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/console/console.go",
          "line": 304,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"coinbase: \" + eth.coinbase + \"\\n\"",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/console/console.go",
          "line": 306,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"at block: \" + eth.blockNumber + \" (\" + new Date(1000 * eth.getBlock(eth.blockNumber).timestamp) + \")\\n\"",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/console/console.go",
          "line": 308,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" datadir: \" + admin.datadir + \"\\n\"",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/console/console.go",
          "line": 312,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= res.String()\n\t}\n\t// List all the supported modules for the user to call\n\tif apis, err := c.client.SupportedModules()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/console/console.go",
          "line": 321,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" modules: \" + strings.Join(modules, \" \") + \"\\n\"\n\t}\n\tmessage += \"\\nTo exit, press ctrl-d or type exit\"\n\tfmt.Fprintln(c.printer, message)\n}\n\n// Evaluate executes code and pretty prints the result to the specified output\n// stream.\nfunc (c *Console) Evaluate(statement string) {\n\tdefer func() {\n\t\tif r := recover()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/console/console.go",
          "line": 446,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= line + \"\\n\"\n\t\t\tindents = countIndents(input)\n\t\t\tif indents <= 0 {\n\t\t\t\tprompt = c.prompt\n\t\t\t} else {\n\t\t\t\tprompt = strings.Repeat(\".\", indents*3) + \" \"\n\t\t\t}\n\t\t\t// If all the needed lines are present, save the command and run it.\n\t\t\tif indents <= 0 {\n\t\t\t\tif len(input) > 0 && input[0] != ' ' && !passwordRegexp.MatchString(input) {\n\t\t\t\t\tif command := strings.TrimSpace(input)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/console/bridge.go",
          "line": 93,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/console/bridge.go",
          "line": 98,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/console/bridge.go",
          "line": 151,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/miner/bid_simulator.go",
          "line": 833,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= receipt.GasUsed\n\t\t\t\teffectiveTip, er := tx.EffectiveGasTip(bidRuntime.env.header.BaseFee)\n\t\t\t\tif er != nil {\n\t\t\t\t\terr = errors.New(\"failed to calculate effective tip\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif bidRuntime.env.header.BaseFee != nil {\n\t\t\t\t\teffectiveTip.Add(effectiveTip, bidRuntime.env.header.BaseFee)\n\t\t\t\t}\n\n\t\t\t\tgasFee := new(big.Int).Mul(effectiveTip, new(big.Int).SetUint64(receipt.GasUsed))\n\t\t\t\tbidGasFee.Add(bidGasFee, gasFee)\n\n\t\t\t\tif tx.Type() == types.BlobTxType {\n\t\t\t\t\tblobFee := new(big.Int).Mul(receipt.BlobGasPrice, new(big.Int).SetUint64(receipt.BlobGasUsed))\n\t\t\t\t\tbidGasFee.Add(bidGasFee, blobFee)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// if bid txs are all from mempool, do not check gas price\n\t\tif bidGasUsed != 0 {\n\t\t\tbidGasPrice := new(big.Int).Div(bidGasFee, new(big.Int).SetUint64(bidGasUsed))\n\t\t\tif bidGasPrice.Cmp(b.minGasPrice) < 0 {\n\t\t\t\terr = fmt.Errorf(\"bid gas price is lower than min gas price, bid:%v, min:%v\", bidGasPrice, b.minGasPrice)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// if enable greedy merge, fill bid env with transactions from mempool\n\tgreedyMergeElapsed := time.Duration(0)\n\tif *b.config.GreedyMergeTx {\n\t\tendingBidsExtra := 20 * time.Millisecond // Add a buffer to ensure ending bids before `delayLeftOver`\n\t\tminTimeLeftForEndingBids := b.delayLeftOver + endingBidsExtra\n\t\tdelay := b.engine.Delay(b.chain, bidRuntime.env.header, &minTimeLeftForEndingBids)\n\t\tif delay != nil && *delay > 0 {\n\t\t\tgreedyMergeStartTs := time.Now()\n\t\t\tbidTxsSet := mapset.NewThreadUnsafeSetWithSize[common.Hash](len(bidRuntime.bid.Txs))\n\t\t\tfor _, tx := range bidRuntime.bid.Txs {\n\t\t\t\tbidTxsSet.Add(tx.Hash())\n\t\t\t}\n\t\t\tstopTimer := time.NewTimer(*delay)\n\t\t\tdefer stopTimer.Stop()\n\t\t\tfillErr := b.bidWorker.fillTransactions(interruptCh, bidRuntime.env, stopTimer, bidTxsSet)\n\n\t\t\t// recalculate the packed reward\n\t\t\tbidRuntime.packReward(*b.config.ValidatorCommission)\n\t\t\tgreedyMergeElapsed = time.Since(greedyMergeStartTs)\n\n\t\t\tlog.Debug(\"BidSimulator: greedy merge stopped\", \"block\", bidRuntime.env.header.Number,\n\t\t\t\t\"builder\", bidRuntime.bid.Builder, \"tx count\", bidRuntime.env.tcount-bidTxLen+1, \"err\", fillErr, \"greedyMergeElapsed\", greedyMergeElapsed)\n\t\t}\n\t}\n\n\t// commit payBidTx at the end of the block\n\tbidRuntime.env.gasPool.AddGas(params.PayBidTxGasLimit)\n\terr = bidRuntime.commitTransaction(b.chain, b.chainConfig, payBidTx, true)\n\tif err != nil {\n\t\tlog.Error(\"BidSimulator: failed to commit tx\", \"builder\", bidRuntime.bid.Builder,\n\t\t\t\"bidHash\", bidRuntime.bid.Hash(), \"tx\", payBidTx.Hash(), \"err\", err)\n\t\terr = fmt.Errorf(\"invalid tx in bid, %v\", err)\n\t\treturn\n\t}\n\n\tbestBid := b.GetBestBid(parentHash)\n\tsimElapsed := time.Since(startTS)\n\tif bestBid == nil {\n\t\twinResult := \"true[first]\"\n\t\tlog.Info(\"[BID RESULT]\", \"win\", winResult, \"builder\", bidRuntime.bid.Builder, \"hash\", bidRuntime.bid.Hash().TerminalString(), \"simElapsed\", simElapsed)\n\t} else if bidRuntime.bid.Hash() != bestBid.bid.Hash() { // skip log flushing when only one bid is present\n\t\tlog.Info(\"[BID RESULT]\",\n\t\t\t\"win\", bidRuntime.packedBlockReward.Cmp(bestBid.packedBlockReward) > 0,\n\n\t\t\t\"bidHash\", bidRuntime.bid.Hash().TerminalString(),\n\t\t\t\"bestHash\", bestBid.bid.Hash().TerminalString(),\n\n\t\t\t\"bidGasFee\", weiToEtherStringF6(bidRuntime.packedBlockReward),\n\t\t\t\"bestGasFee\", weiToEtherStringF6(bestBid.packedBlockReward),\n\n\t\t\t\"bidBlockTx\", bidRuntime.env.tcount,\n\t\t\t\"bestBlockTx\", bestBid.env.tcount,\n\n\t\t\t\"simElapsed\", simElapsed,\n\t\t)\n\t}\n\tconst minGasForSpeedMetric = 30_000_000\n\tif bidRuntime.bid.GasUsed > minGasForSpeedMetric {\n\t\ttimeCostMs := (simElapsed - greedyMergeElapsed).Milliseconds()\n\t\tif timeCostMs > 0 {\n\t\t\tsimulateSpeedGauge.Update(int64(float64(bidRuntime.bid.GasUsed) / float64(timeCostMs) / 1000))\n\t\t}\n\t}\n\n\t// this is the simplest strategy: best for all the delegators.\n\tif bestBid == nil || bidRuntime.packedBlockReward.Cmp(bestBid.packedBlockReward) > 0 {\n\t\tb.SetBestBid(bidRuntime.bid.ParentHash, bidRuntime)\n\t\tbidRuntime.duration = time.Since(startTS)\n\t\tbidSimTimer.UpdateSince(startTS)\n\t\tsuccess = true\n\t}\n}\n\n// reportIssue reports the issue to the mev-sentry\nfunc (b *bidSimulator) reportIssue(bidRuntime *BidRuntime, err error) {\n\tmetrics.GetOrRegisterCounter(fmt.Sprintf(\"bid/err/%v\", bidRuntime.bid.Builder), nil).Inc(1)\n\n\tcli := b.builders[bidRuntime.bid.Builder]\n\tif cli != nil {\n\t\terr = cli.ReportIssue(context.Background(), &types.BidIssue{\n\t\t\tValidator: bidRuntime.env.header.Coinbase,\n\t\t\tBuilder:   bidRuntime.bid.Builder,\n\t\t\tBidHash:   bidRuntime.bid.Hash(),\n\t\t\tMessage:   err.Error(),\n\t\t})\n\n\t\tif err != nil {\n\t\t\tlog.Warn(\"BidSimulator: failed to report issue\", \"builder\", bidRuntime.bid.Builder, \"err\", err)\n\t\t}\n\t}\n}\n\ntype BidRuntime struct {\n\tbid *types.Bid\n\n\tenv *environment\n\n\texpectedBlockReward     *big.Int\n\texpectedValidatorReward *big.Int\n\n\tpackedBlockReward     *big.Int\n\tpackedValidatorReward *big.Int\n\n\tfinished chan struct{}\n\tduration time.Duration\n}\n\nfunc newBidRuntime(newBid *types.Bid, validatorCommission uint64) (*BidRuntime, error) {\n\t// check the block reward and validator reward of the newBid\n\texpectedBlockReward := newBid.GasFee\n\texpectedValidatorReward := new(big.Int).Mul(expectedBlockReward, big.NewInt(int64(validatorCommission)))\n\texpectedValidatorReward.Div(expectedValidatorReward, big.NewInt(10000))\n\texpectedValidatorReward.Sub(expectedValidatorReward, newBid.BuilderFee)\n\n\tif expectedValidatorReward.Cmp(big.NewInt(0)) < 0 {\n\t\t// damage self profit, ignore\n\t\tlog.Debug(\"BidSimulator: invalid bid, validator reward is less than 0, ignore\",\n\t\t\t\"builder\", newBid.Builder, \"bidHash\", newBid.Hash().Hex())\n\t\treturn nil, fmt.Errorf(\"validator reward is less than 0, value: %s, commissionConfig: %d\", expectedValidatorReward, validatorCommission)\n\t}\n\n\tbidRuntime := &BidRuntime{\n\t\tbid:                     newBid,\n\t\texpectedBlockReward:     expectedBlockReward,\n\t\texpectedValidatorReward: expectedValidatorReward,\n\t\tpackedBlockReward:       big.NewInt(0),\n\t\tpackedValidatorReward:   big.NewInt(0),\n\t\tfinished:                make(chan struct{}),\n\t}\n\n\treturn bidRuntime, nil\n}\n\nfunc (r *BidRuntime) validReward() bool {\n\treturn r.packedBlockReward.Cmp(r.expectedBlockReward) >= 0 &&\n\t\tr.packedValidatorReward.Cmp(r.expectedValidatorReward) >= 0\n}\n\nfunc (r *BidRuntime) isExpectedBetterThan(other *BidRuntime) bool {\n\treturn r.expectedBlockReward.Cmp(other.expectedBlockReward) >= 0 &&\n\t\tr.expectedValidatorReward.Cmp(other.expectedValidatorReward) >= 0\n}\n\n// packReward calculates packedBlockReward and packedValidatorReward\nfunc (r *BidRuntime) packReward(validatorCommission uint64) {\n\tr.packedBlockReward = r.env.state.GetBalance(consensus.SystemAddress).ToBig()\n\tr.packedValidatorReward = new(big.Int).Mul(r.packedBlockReward, big.NewInt(int64(validatorCommission)))\n\tr.packedValidatorReward.Div(r.packedValidatorReward, big.NewInt(10000))\n\tr.packedValidatorReward.Sub(r.packedValidatorReward, r.bid.BuilderFee)\n}\n\nfunc (r *BidRuntime) commitTransaction(chain *core.BlockChain, chainConfig *params.ChainConfig, tx *types.Transaction, unRevertible bool) error {\n\tvar (\n\t\tenv = r.env\n\t\tsc  *types.BlobSidecar\n\t)\n\n\t// Start executing the transaction\n\tr.env.state.SetTxContext(tx.Hash(), r.env.tcount)\n\n\tif tx.Type() == types.BlobTxType {\n\t\tsc = types.NewBlobSidecarFromTx(tx)\n\t\tif sc == nil {\n\t\t\treturn errors.New(\"blob transaction without blobs in miner\")\n\t\t}\n\t\t// Checking against blob gas limit: It's kind of ugly to perform this check here, but there\n\t\t// isn't really a better place right now. The blob gas limit is checked at block validation time\n\t\t// and not during execution. This means core.ApplyTransaction will not return an error if the\n\t\t// tx has too many blobs. So we have to explicitly check it here.\n\t\tif (env.blobs + len(sc.Blobs)) > eip4844.MaxBlobsPerBlock(chainConfig, r.env.header.Time) {\n\t\t\treturn errors.New(\"max data blobs reached\")\n\t\t}\n\t}\n\n\treceipt, err := core.ApplyTransaction(env.evm, env.gasPool, env.state, env.header, tx,\n\t\t&env.header.GasUsed, core.NewReceiptBloomGenerator())\n\tif err != nil {\n\t\treturn err\n\t} else if unRevertible && receipt.Status == types.ReceiptStatusFailed {\n\t\treturn errors.New(\"no revertible transaction failed\")\n\t}\n\n\tif tx.Type() == types.BlobTxType {\n\t\tsc.TxIndex = uint64(len(env.txs))\n\t\tenv.txs = append(env.txs, tx.WithoutBlobTxSidecar())\n\t\tenv.receipts = append(env.receipts, receipt)\n\t\tenv.sidecars = append(env.sidecars, sc)\n\t\tenv.blobs += len(sc.Blobs)\n\t\t*env.header.BlobGasUsed += receipt.BlobGasUsed\n\t} else {\n\t\tenv.txs = append(env.txs, tx)\n\t\tenv.receipts = append(env.receipts, receipt)\n\t}\n\n\tr.env.tcount++\n\n\treturn nil\n}\n\nfunc weiToEtherStringF6(wei *big.Int) string {\n\tf, _ := new(big.Float).Quo(new(big.Float).SetInt(wei), big.NewFloat(params.Ether)).Float64()\n\treturn strconv.FormatFloat(f, 'f', 6, 64)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/miner/worker.go",
          "line": 784,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(sc.Blobs)\n\t*env.header.BlobGasUsed += receipt.BlobGasUsed\n\treturn receipt.Logs, nil\n}\n\n// applyTransaction runs the transaction. If execution fails, state and gas pool are reverted.\nfunc (w *worker) applyTransaction(env *environment, tx *types.Transaction, receiptProcessors ...core.ReceiptProcessor) (*types.Receipt, error) {\n\tvar (\n\t\tsnap = env.state.Snapshot()\n\t\tgp   = env.gasPool.Gas()\n\t)\n\n\treceipt, err := core.ApplyTransaction(env.evm, env.gasPool, env.state, env.header, tx, &env.header.GasUsed, receiptProcessors...)\n\tif err != nil {\n\t\tenv.state.RevertToSnapshot(snap)\n\t\tenv.gasPool.SetGas(gp)\n\t}\n\treturn receipt, err\n}\n\nfunc (w *worker) commitTransactions(env *environment, plainTxs, blobTxs *transactionsByPriceAndNonce,\n\tinterruptCh chan int32, stopTimer *time.Timer) error {\n\tgasLimit := env.header.GasLimit\n\tif env.gasPool == nil {\n\t\tenv.gasPool = new(core.GasPool).AddGas(gasLimit)\n\t\tif p, ok := w.engine.(*parlia.Parlia)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/miner/worker.go",
          "line": 994,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/miner/ordering_test.go",
          "line": 102,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= count\n\t}\n\t// Sort the transactions and cross check the nonce ordering\n\ttxset := newTransactionsByPriceAndNonce(signer, groups, baseFee)\n\n\ttxs := types.Transactions{}\n\tfor tx, _ := txset.Peek()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/beacon/types/committee.go",
          "line": 188,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= bits.OnesCount8(v)\n\t}\n\treturn count\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/beacon/fakebeacon/utils.go",
          "line": 45,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= (ts-headerTime)/3 + 1\n\t\t} else {\n\t\t\t// search one by one\n\t\t\tfor headerTime >= ts {\n\t\t\t\theader, err = backend.HeaderByNumber(ctx, rpc.BlockNumber(estimateEndNumber-1))\n\t\t\t\tif err != nil {\n\t\t\t\t\ttime.Sleep(time.Duration(rand.Int()%180) * time.Millisecond)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\theaderTime = int64(header.Time)\n\t\t\t\tif headerTime == ts {\n\t\t\t\t\treturn header, nil\n\t\t\t\t}\n\t\t\t\testimateEndNumber -= 1\n\t\t\t\tif headerTime < ts { //found the real endNumber\n\t\t\t\t\treturn nil, fmt.Errorf(\"block not found by time %d\", ts)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/beacon/fakebeacon/utils.go",
          "line": 32,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\t\ttime.Sleep(time.Duration(rand.Int()%180) * time.Millisecond)\n\t\t\tcontinue\n\t\t}\n\t\theaderTime := int64(header.Time)\n\t\tif headerTime == ts {\n\t\t\treturn header, nil\n\t\t}\n\n\t\t// let the estimateEndNumber a little bigger than real value\n\t\tif headerTime > ts+8 {\n\t\t\testimateEndNumber -= (headerTime - ts) / 3\n\t\t} else if headerTime < ts {\n\t\t\testimateEndNumber += (ts-headerTime)/3 + 1\n\t\t} else {\n\t\t\t// search one by one\n\t\t\tfor headerTime >= ts {\n\t\t\t\theader, err = backend.HeaderByNumber(ctx, rpc.BlockNumber(estimateEndNumber-1))\n\t\t\t\tif err != nil {\n\t\t\t\t\ttime.Sleep(time.Duration(rand.Int()%180) * time.Millisecond)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\theaderTime = int64(header.Time)\n\t\t\t\tif headerTime == ts {\n\t\t\t\t\treturn header, nil\n\t\t\t\t}\n\t\t\t\testimateEndNumber -= 1\n\t\t\t\tif headerTime < ts { //found the real endNumber\n\t\t\t\t\treturn nil, fmt.Errorf(\"block not found by time %d\", ts)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/ethdb/remotedb/remotedb.go",
          "line": 44,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/ethdb/remotedb/remotedb.go",
          "line": 53,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/ethdb/remotedb/remotedb.go",
          "line": 66,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/ethdb/leveldb/leveldb.go",
          "line": 277,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" Level |   Tables   |    Size(MB)   |    Time(sec)  |    Read(MB)   |   Write(MB)\\n\" +\n\t\t\t\"-------+------------+---------------+---------------+---------------+---------------\\n\"\n\t\tfor level, size := range stats.LevelSizes {\n\t\t\tread := stats.LevelRead[level]\n\t\t\twrite := stats.LevelWrite[level]\n\t\t\tduration := stats.LevelDurations[level]\n\t\t\ttables := stats.LevelTablesCounts[level]\n\n\t\t\tif tables == 0 && duration == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttotalTables += tables\n\t\t\ttotalSize += size\n\t\t\ttotalRead += read\n\t\t\ttotalWrite += write\n\t\t\ttotalDuration += duration\n\t\t\tmessage += fmt.Sprintf(\" %3d   | %10d | %13.5f | %13.5f | %13.5f | %13.5f\\n\",\n\t\t\t\tlevel, tables, float64(size)/1048576.0, duration.Seconds(),\n\t\t\t\tfloat64(read)/1048576.0, float64(write)/1048576.0)\n\t\t}\n\t\tmessage += \"-------+------------+---------------+---------------+---------------+---------------\\n\"\n\t\tmessage += fmt.Sprintf(\" Total | %10d | %13.5f | %13.5f | %13.5f | %13.5f\\n\",\n\t\t\ttotalTables, float64(totalSize)/1048576.0, totalDuration.Seconds(),\n\t\t\tfloat64(totalRead)/1048576.0, float64(totalWrite)/1048576.0)\n\t\tmessage += \"-------+------------+---------------+---------------+---------------+---------------\\n\\n\"\n\t}\n\tmessage += fmt.Sprintf(\"Read(MB):%.5f Write(MB):%.5f\\n\", float64(stats.IORead)/1048576.0, float64(stats.IOWrite)/1048576.0)\n\tmessage += fmt.Sprintf(\"BlockCache(MB):%.5f FileCache:%d\\n\", float64(stats.BlockCacheSize)/1048576.0, stats.OpenedTablesCount)\n\tmessage += fmt.Sprintf(\"MemoryCompaction:%d Level0Compaction:%d NonLevel0Compaction:%d SeekCompaction:%d\\n\", stats.MemComp, stats.Level0Comp, stats.NonLevel0Comp, stats.SeekComp)\n\tmessage += fmt.Sprintf(\"WriteDelayCount:%d WriteDelayDuration:%s Paused:%t\\n\", stats.WriteDelayCount, common.PrettyDuration(stats.WriteDelayDuration), stats.WritePaused)\n\tmessage += fmt.Sprintf(\"Snapshots:%d Iterators:%d\\n\", stats.AliveSnapshots, stats.AliveIterators)\n\treturn message, nil\n}\n\n// Compact flattens the underlying data store for the given key range. In essence,\n// deleted and overwritten versions are discarded, and the data is rearranged to\n// reduce the cost of operations needed to access them.\n//\n// A nil start is treated as a key before all keys in the data store",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/ethdb/leveldb/leveldb.go",
          "line": 380,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= t.Nanoseconds()\n\t\t}\n\t\tcompactions[i%2][2] = stats.LevelRead.Sum()\n\t\tcompactions[i%2][3] = stats.LevelWrite.Sum()\n\t\t// Update all the requested meters\n\t\tdb.diskSizeGauge.Update(compactions[i%2][0])\n\t\tdb.compTimeMeter.Mark(compactions[i%2][1] - compactions[(i-1)%2][1])\n\t\tdb.compReadMeter.Mark(compactions[i%2][2] - compactions[(i-1)%2][2])\n\t\tdb.compWriteMeter.Mark(compactions[i%2][3] - compactions[(i-1)%2][3])\n\t\tvar (\n\t\t\tdelayN   = int64(stats.WriteDelayCount)\n\t\t\tduration = stats.WriteDelayDuration\n\t\t\tpaused   = stats.WritePaused\n\t\t)\n\t\tdb.writeDelayNMeter.Mark(delayN - delaystats[0])\n\t\tdb.writeDelayMeter.Mark(duration.Nanoseconds() - delaystats[1])\n\t\t// If a warning that db is performing compaction has been displayed, any subsequent\n\t\t// warnings will be withheld for one minute not to overwhelm the user.\n\t\tif paused && delayN-delaystats[0] == 0 && duration.Nanoseconds()-delaystats[1] == 0 &&\n\t\t\ttime.Now().After(lastWritePaused.Add(degradationWarnInterval)) {\n\t\t\tdb.log.Warn(\"Database compacting, degraded performance\")\n\t\t\tlastWritePaused = time.Now()\n\t\t}\n\t\tdelaystats[0], delaystats[1] = delayN, duration.Nanoseconds()\n\n\t\tvar (\n\t\t\tnRead  = int64(stats.IORead)\n\t\t\tnWrite = int64(stats.IOWrite)\n\t\t)\n\t\tdb.diskReadMeter.Mark(nRead - iostats[0])\n\t\tdb.diskWriteMeter.Mark(nWrite - iostats[1])\n\t\tiostats[0], iostats[1] = nRead, nWrite\n\n\t\tdb.memCompGauge.Update(int64(stats.MemComp))\n\t\tdb.level0CompGauge.Update(int64(stats.Level0Comp))\n\t\tdb.nonlevel0CompGauge.Update(int64(stats.NonLevel0Comp))\n\t\tdb.seekCompGauge.Update(int64(stats.SeekComp))\n\n\t\tfor i, tables := range stats.LevelTablesCounts {\n\t\t\t// Append metrics for additional layers\n\t\t\tif i >= len(db.levelsGauge) {\n\t\t\t\tdb.levelsGauge = append(db.levelsGauge, metrics.NewRegisteredGauge(namespace+fmt.Sprintf(\"tables/level%v\", i), nil))\n\t\t\t}\n\t\t\tdb.levelsGauge[i].Update(int64(tables))\n\t\t}\n\n\t\t// Sleep a bit, then repeat the stats collection\n\t\tselect {\n\t\tcase errc = <-db.quitChan:\n\t\t\t// Quit requesting, stop hammering the database\n\t\tcase <-timer.C:\n\t\t\ttimer.Reset(refresh)\n\t\t\t// Timeout, gather a new set of stats\n\t\t}\n\t}\n\n\tif errc == nil {\n\t\terrc = <-db.quitChan\n\t}\n\terrc <- merr\n}\n\n// batch is a write-only leveldb batch that commits changes to its host database\n// when Write is called. A batch cannot be used concurrently.\ntype batch struct {\n\tdb   *leveldb.DB\n\tb    *leveldb.Batch\n\tsize int\n}\n\n// Put inserts the given value into the batch for later committing.\nfunc (b *batch) Put(key, value []byte) error {\n\tb.b.Put(key, value)\n\tb.size += len(key) + len(value)\n\treturn nil\n}\n\n// Delete inserts the key removal into the batch for later committing.\nfunc (b *batch) Delete(key []byte) error {\n\tb.b.Delete(key)\n\tb.size += len(key)\n\treturn nil\n}\n\n// DeleteRange removes all keys in the range [start, end) from the batch for\n// later committing, inclusive on start, exclusive on end.\n//\n// Note that this is a fallback implementation as leveldb does not natively\n// support range deletion in batches. It iterates through the database to find\n// keys in the range and adds them to the batch for deletion.\nfunc (b *batch) DeleteRange(start, end []byte) error {\n\t// Create an iterator to scan through the keys in the range\n\tslice := &util.Range{\n\t\tStart: start, // If nil, it represents the key before all keys\n\t\tLimit: end,   // If nil, it represents the key after all keys\n\t}\n\tit := b.db.NewIterator(slice, nil)\n\tdefer it.Release()\n\n\tvar count int\n\tfor it.Next() {\n\t\tcount++\n\t\tkey := it.Key()\n\t\tif count > 10000 { // should not block for more than a second\n\t\t\treturn ethdb.ErrTooManyKeys\n\t\t}\n\t\t// Add this key to the batch for deletion\n\t\tb.b.Delete(key)\n\t\tb.size += len(key)\n\t}\n\tif err := it.Error()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 561,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(levelMetrics.BytesCompacted)\n\t\t\tnWrite += int64(levelMetrics.BytesFlushed)\n\t\t\tcompWrite += int64(levelMetrics.BytesCompacted)\n\t\t\tcompRead += int64(levelMetrics.BytesRead)\n\t\t}\n\n\t\tnWrite += int64(stats.WAL.BytesWritten)\n\n\t\tcompWrites[i%2] = compWrite\n\t\tcompReads[i%2] = compRead\n\t\tnWrites[i%2] = nWrite\n\n\t\td.writeDelayNMeter.Mark(writeDelayCounts[i%2] - writeDelayCounts[(i-1)%2])\n\t\td.writeDelayMeter.Mark(writeDelayTimes[i%2] - writeDelayTimes[(i-1)%2])\n\t\t// Print a warning log if writing has been stalled for a while. The log will\n\t\t// be printed per minute to avoid overwhelming users.\n\t\tif d.writeStalled.Load() && writeDelayCounts[i%2] == writeDelayCounts[(i-1)%2] &&\n\t\t\ttime.Now().After(lastWriteStallReport.Add(degradationWarnInterval)) {\n\t\t\td.log.Warn(\"Database compacting, degraded performance\")\n\t\t\tlastWriteStallReport = time.Now()\n\t\t}\n\t\td.compTimeMeter.Mark(compTimes[i%2] - compTimes[(i-1)%2])\n\t\td.compReadMeter.Mark(compReads[i%2] - compReads[(i-1)%2])\n\t\td.compWriteMeter.Mark(compWrites[i%2] - compWrites[(i-1)%2])\n\t\td.diskSizeGauge.Update(int64(stats.DiskSpaceUsage()))\n\t\td.diskReadMeter.Mark(0) // pebble doesn't track non-compaction reads\n\t\td.diskWriteMeter.Mark(nWrites[i%2] - nWrites[(i-1)%2])\n\n\t\t// See https://github.com/cockroachdb/pebble/pull/1628#pullrequestreview-1026664054\n\t\tmanuallyAllocated := stats.BlockCache.Size + int64(stats.MemTable.Size) + int64(stats.MemTable.ZombieSize)\n\t\td.manualMemAllocGauge.Update(manuallyAllocated)\n\t\td.memCompGauge.Update(stats.Flush.Count)\n\t\td.nonlevel0CompGauge.Update(nonLevel0CompCount)\n\t\td.level0CompGauge.Update(level0CompCount)\n\t\td.seekCompGauge.Update(stats.Compact.ReadCount)\n\t\td.liveCompGauge.Update(stats.Compact.NumInProgress)\n\t\td.liveCompSizeGauge.Update(stats.Compact.InProgressBytes)\n\t\td.liveIterGauge.Update(stats.TableIters)\n\n\t\td.liveMemTablesGauge.Update(stats.MemTable.Count)\n\t\td.zombieMemTablesGauge.Update(stats.MemTable.ZombieCount)\n\t\td.estimatedCompDebtGauge.Update(int64(stats.Compact.EstimatedDebt))\n\t\td.tableCacheHitGauge.Update(stats.TableCache.Hits)\n\t\td.tableCacheMissGauge.Update(stats.TableCache.Misses)\n\t\td.blockCacheHitGauge.Update(stats.BlockCache.Hits)\n\t\td.blockCacheMissGauge.Update(stats.BlockCache.Misses)\n\t\td.filterHitGauge.Update(stats.Filter.Hits)\n\t\td.filterMissGauge.Update(stats.Filter.Misses)\n\n\t\tfor i, level := range stats.Levels {\n\t\t\t// Append metrics for additional layers\n\t\t\tif i >= len(d.levelsGauge) {\n\t\t\t\td.levelsGauge = append(d.levelsGauge, metrics.GetOrRegisterGauge(namespace+fmt.Sprintf(\"tables/level%v\", i), nil))\n\t\t\t}\n\t\t\td.levelsGauge[i].Update(level.NumFiles)\n\t\t}\n\n\t\t// Sleep a bit, then repeat the stats collection\n\t\tselect {\n\t\tcase errc = <-d.quitChan:\n\t\t\t// Quit requesting, stop hammering the database\n\t\tcase <-timer.C:\n\t\t\ttimer.Reset(refresh)\n\t\t\t// Timeout, gather a new set of stats\n\t\t}\n\t}\n\terrc <- nil\n}\n\n// batch is a write-only batch that commits changes to its host database\n// when Write is called. A batch cannot be used concurrently.\ntype batch struct {\n\tb    *pebble.Batch\n\tdb   *Database\n\tsize int\n}\n\n// Put inserts the given value into the batch for later committing.\nfunc (b *batch) Put(key, value []byte) error {\n\tif err := b.b.Set(key, value, nil)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 643,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(key) + len(value)\n\treturn nil\n}\n\n// Delete inserts the key removal into the batch for later committing.\nfunc (b *batch) Delete(key []byte) error {\n\tif err := b.b.Delete(key, nil)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 652,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(key)\n\treturn nil\n}\n\n// DeleteRange removes all keys in the range [start, end) from the batch for\n// later committing, inclusive on start, exclusive on end.\nfunc (b *batch) DeleteRange(start, end []byte) error {\n\t// There is no special flag to represent the end of key range\n\t// in pebble(nil in leveldb). Use an ugly hack to construct a\n\t// large key to represent it.\n\tif end == nil {\n\t\tend = ethdb.MaximumKey\n\t}\n\tif err := b.b.DeleteRange(start, end, nil)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 669,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(start) + len(end)\n\treturn nil\n}\n\n// ValueSize retrieves the amount of data queued up for writing.\nfunc (b *batch) ValueSize() int {\n\treturn b.size\n}\n\n// Write flushes any accumulated data to disk.\nfunc (b *batch) Write() error {\n\tb.db.quitLock.RLock()\n\tdefer b.db.quitLock.RUnlock()\n\tif b.db.closed {\n\t\treturn pebble.ErrClosed\n\t}\n\treturn b.b.Commit(b.db.writeOptions)\n}\n\n// Reset resets the batch for reuse.\nfunc (b *batch) Reset() {\n\tb.b.Reset()\n\tb.size = 0\n}\n\n// Replay replays the batch contents.\nfunc (b *batch) Replay(w ethdb.KeyValueWriter) error {\n\treader := b.b.Reader()\n\tfor {\n\t\tkind, k, v, ok, err := reader.Next()\n\t\tif !ok || err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// The (k,v) slices might be overwritten if the batch is reset/reused,\n\t\t// and the receiver should copy them if they are to be retained long-term.\n\t\tif kind == pebble.InternalKeyKindSet {\n\t\t\tif err = w.Put(k, v)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/ethdb/memorydb/memorydb.go",
          "line": 318,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(key) + len(value)\n\treturn nil\n}\n\n// Delete inserts the key removal into the batch for later committing.\nfunc (b *batch) Delete(key []byte) error {\n\tb.writes = append(b.writes, keyvalue{key: string(key), delete: true})\n\tb.size += len(key)\n\treturn nil\n}\n\n// DeleteRange removes all keys in the range [start, end) from the batch for later committing.\nfunc (b *batch) DeleteRange(start, end []byte) error {\n\tb.writes = append(b.writes, keyvalue{\n\t\trangeFrom: bytes.Clone(start),\n\t\trangeTo:   bytes.Clone(end),\n\t\tdelete:    true,\n\t})\n\tb.size += len(start) + len(end)\n\treturn nil\n}\n\n// ValueSize retrieves the amount of data queued up for writing.\nfunc (b *batch) ValueSize() int {\n\treturn b.size\n}\n\n// Write flushes any accumulated data to the memory database.\nfunc (b *batch) Write() error {\n\tb.db.lock.Lock()\n\tdefer b.db.lock.Unlock()\n\n\tif b.db.db == nil {\n\t\treturn errMemorydbClosed\n\t}\n\tfor _, entry := range b.writes {\n\t\tif entry.delete {\n\t\t\tif entry.key != \"\" {\n\t\t\t\t// Single key deletion\n\t\t\t\tdelete(b.db.db, entry.key)\n\t\t\t} else {\n\t\t\t\t// Range deletion (inclusive of start, exclusive of end)\n\t\t\t\tfor key := range b.db.db {\n\t\t\t\t\tif entry.rangeFrom != nil && key < string(entry.rangeFrom) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif entry.rangeTo != nil && key >= string(entry.rangeTo) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tdelete(b.db.db, key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tb.db.db[entry.key] = entry.value\n\t}\n\treturn nil\n}\n\n// Reset resets the batch for reuse.\nfunc (b *batch) Reset() {\n\tb.writes = b.writes[:0]\n\tb.size = 0\n}\n\n// Replay replays the batch contents.\nfunc (b *batch) Replay(w ethdb.KeyValueWriter) error {\n\tfor _, entry := range b.writes {\n\t\tif entry.delete {\n\t\t\tif entry.key != \"\" {\n\t\t\t\t// Single key deletion\n\t\t\t\tif err := w.Delete([]byte(entry.key))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/ethdb/memorydb/memorydb.go",
          "line": 427,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\treturn it.index < len(it.keys)\n}\n\n// Error returns any accumulated error. Exhausting all the key/value pairs\n// is not considered to be an error. A memory iterator cannot encounter errors.\nfunc (it *iterator) Error() error {\n\treturn nil\n}\n\n// Key returns the key of the current key/value pair, or nil if done. The caller\n// should not modify the contents of the returned slice, and its contents may\n// change on the next call to Next.\nfunc (it *iterator) Key() []byte {\n\t// Short circuit if iterator is not in a valid position\n\tif it.index < 0 || it.index >= len(it.keys) {\n\t\treturn nil\n\t}\n\treturn []byte(it.keys[it.index])\n}\n\n// Value returns the value of the current key/value pair, or nil if done. The\n// caller should not modify the contents of the returned slice, and its contents\n// may change on the next call to Next.\nfunc (it *iterator) Value() []byte {\n\t// Short circuit if iterator is not in a valid position\n\tif it.index < 0 || it.index >= len(it.keys) {\n\t\treturn nil\n\t}\n\treturn it.values[it.index]\n}\n\n// Release releases associated resources. Release should always succeed and can\n// be called multiple times without causing error.\nfunc (it *iterator) Release() {\n\tit.index, it.keys, it.values = -1, nil, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/ethdb/dbtest/testsuite.go",
          "line": 924,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 {\n\t\tkeys = append(keys, randBytes(ksize))\n\t\tvals = append(vals, randBytes(vsize))\n\t}\n\tif order {\n\t\tslices.SortFunc(keys, bytes.Compare)\n\t}\n\treturn keys, vals\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 255,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 285,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 342,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 592,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 658,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 768,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/gasprice/gasprice_test.go",
          "line": 223,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= b.Difficulty().Uint64()\n\t})\n\n\t// Construct testing chain\n\tgspec.Config.TerminalTotalDifficulty = new(big.Int).SetUint64(td)\n\tchain, err := core.NewBlockChain(db, gspec, engine, &core.BlockChainConfig{NoPrefetch: true})\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create local chain, %v\", err)\n\t}\n\tif i, err := chain.InsertChain(blocks)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/gasprice/feehistory.go",
          "line": 150,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= sorter[txIndex].gasUsed\n\t\t}\n\t\tbf.results.reward[i] = sorter[txIndex].reward\n\t}\n}\n\n// resolveBlockRange resolves the specified block range to absolute block numbers while also\n// enforcing backend specific limitations. The pending block and corresponding receipts are\n// also returned if requested and available.\n// Note: an error is only returned if retrieving the head header has failed. If there are no\n// retrievable blocks in the specified range then zero block count is returned with no error.\nfunc (oracle *Oracle) resolveBlockRange(ctx context.Context, reqEnd rpc.BlockNumber, blocks uint64) (*types.Block, []*types.Receipt, uint64, uint64, error) {\n\tvar (\n\t\theadBlock       *types.Header\n\t\tpendingBlock    *types.Block\n\t\tpendingReceipts types.Receipts\n\t\terr             error\n\t)\n\n\t// Get the chain's current head.\n\tif headBlock, err = oracle.backend.HeaderByNumber(ctx, rpc.LatestBlockNumber)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 61,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/catalyst/simulated_beacon_test.go",
          "line": 200,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(block.Transactions())\n\t\t\tincludedWxs += len(block.Withdrawals())\n\t\t\t// ensure all withdrawals/txs included. this will take two blocks b/c number of withdrawals > 10\n\t\t\tif includedTxs == txCount && includedWxs == wxCount {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tabort.Reset(10 * time.Second)\n\t\tcase <-abort.C:\n\t\t\tt.Fatalf(\"timed out without including all withdrawals/txs: have txs %d, want %d, have wxs %d, want %d\",\n\t\t\t\tincludedTxs, txCount, includedWxs, wxCount)\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/fetcher/tx_fetcher.go",
          "line": 336,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= addTxsBatchSize {\n\t\tend := i + addTxsBatchSize\n\t\tif end > len(txs) {\n\t\t\tend = len(txs)\n\t\t}\n\t\tvar (\n\t\t\tduplicate   int64\n\t\t\tunderpriced int64\n\t\t\totherreject int64\n\t\t)\n\t\tbatch := txs[i:end]\n\n\t\tfor j, err := range f.addTxs(peer, batch) {\n\t\t\t// Track the transaction hash if the price is too low for us.\n\t\t\t// Avoid re-request this transaction when we receive another\n\t\t\t// announcement.\n\t\t\tif errors.Is(err, txpool.ErrUnderpriced) || errors.Is(err, txpool.ErrReplaceUnderpriced) || errors.Is(err, txpool.ErrTxGasPriceTooLow) {\n\t\t\t\tf.underpriced.Add(batch[j].Hash(), batch[j].Time())\n\t\t\t}\n\t\t\t// Track a few interesting failure types\n\t\t\tswitch {\n\t\t\tcase err == nil: // Noop, but need to handle to not count these\n\n\t\t\tcase errors.Is(err, txpool.ErrAlreadyKnown):\n\t\t\t\tduplicate++\n\n\t\t\tcase errors.Is(err, txpool.ErrUnderpriced) || errors.Is(err, txpool.ErrReplaceUnderpriced) || errors.Is(err, txpool.ErrTxGasPriceTooLow):\n\t\t\t\tunderpriced++\n\n\t\t\tdefault:\n\t\t\t\totherreject++\n\t\t\t}\n\t\t\tadded = append(added, batch[j].Hash())\n\t\t\tmetas = append(metas, txMetadata{\n\t\t\t\tkind: batch[j].Type(),\n\t\t\t\tsize: uint32(batch[j].Size()),\n\t\t\t})\n\t\t}\n\t\tknownMeter.Mark(duplicate)\n\t\tunderpricedMeter.Mark(underpriced)\n\t\totherRejectMeter.Mark(otherreject)\n\n\t\t// If 'other reject' is >25% of the deliveries in any batch, sleep a bit.\n\t\tif otherreject > addTxsBatchSize/4 {\n\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\tlog.Debug(\"Peer delivering stale transactions\", \"peer\", peer, \"rejected\", otherreject)\n\t\t}\n\t}\n\tselect {\n\tcase f.cleanup <- &txDelivery{origin: peer, hashes: added, metas: metas, direct: direct}:\n\t\treturn nil\n\tcase <-f.quit:\n\t\treturn errTerminated\n\t}\n}\n\n// Drop should be called when a peer disconnects. It cleans up all the internal\n// data structures of the given node.\nfunc (f *TxFetcher) Drop(peer string) error {\n\tselect {\n\tcase f.drop <- &txDrop{peer: peer}:\n\t\treturn nil\n\tcase <-f.quit:\n\t\treturn errTerminated\n\t}\n}\n\n// Start boots up the announcement based synchroniser, accepting and processing\n// hash notifications and block fetches until termination requested.\nfunc (f *TxFetcher) Start() {\n\tgo f.loop()\n}\n\n// Stop terminates the announcement based synchroniser, canceling all pending\n// operations.\nfunc (f *TxFetcher) Stop() {\n\tclose(f.quit)\n}\n\nfunc (f *TxFetcher) loop() {\n\tvar (\n\t\twaitTimer    = new(mclock.Timer)\n\t\ttimeoutTimer = new(mclock.Timer)\n\n\t\twaitTrigger    = make(chan struct{}, 1)\n\t\ttimeoutTrigger = make(chan struct{}, 1)\n\t)\n\tfor {\n\t\tselect {\n\t\tcase ann := <-f.notify:\n\t\t\t// Drop part of the new announcements if there are too many accumulated.\n\t\t\t// Note, we could but do not filter already known transactions here as\n\t\t\t// the probability of something arriving between this call and the pre-\n\t\t\t// filter outside is essentially zero.\n\t\t\tused := len(f.waitslots[ann.origin]) + len(f.announces[ann.origin])\n\t\t\tif used >= maxTxAnnounces {\n\t\t\t\t// This can happen if a set of transactions are requested but not\n\t\t\t\t// all fulfilled, so the remainder are rescheduled without the cap\n\t\t\t\t// check. Should be fine as the limit is in the thousands and the\n\t\t\t\t// request size in the hundreds.\n\t\t\t\ttxAnnounceDOSMeter.Mark(int64(len(ann.hashes)))\n\t\t\t\tbreak\n\t\t\t}\n\t\t\twant := used + len(ann.hashes)\n\t\t\tif want > maxTxAnnounces {\n\t\t\t\ttxAnnounceDOSMeter.Mark(int64(want - maxTxAnnounces))\n\n\t\t\t\tann.hashes = ann.hashes[:maxTxAnnounces-used]\n\t\t\t\tann.metas = ann.metas[:maxTxAnnounces-used]\n\t\t\t}\n\t\t\t// All is well, schedule the remainder of the transactions\n\t\t\tvar (\n\t\t\t\tidleWait   = len(f.waittime) == 0\n\t\t\t\t_, oldPeer = f.announces[ann.origin]\n\t\t\t\thasBlob    bool\n\n\t\t\t\t// nextSeq returns the next available sequence number for tagging\n\t\t\t\t// transaction announcement and also bump it internally.\n\t\t\t\tnextSeq = func() uint64 {\n\t\t\t\t\tseq := f.txSeq\n\t\t\t\t\tf.txSeq++\n\t\t\t\t\treturn seq\n\t\t\t\t}\n\t\t\t)\n\t\t\tfor i, hash := range ann.hashes {\n\t\t\t\t// If the transaction is already downloading, add it to the list\n\t\t\t\t// of possible alternates (in case the current retrieval fails) and\n\t\t\t\t// also account it for the peer.\n\t\t\t\tif f.alternates[hash] != nil {\n\t\t\t\t\tf.alternates[hash][ann.origin] = struct{}{}\n\n\t\t\t\t\t// Stage 2 and 3 share the set of origins per tx\n\t\t\t\t\tif announces := f.announces[ann.origin]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/fetcher/tx_fetcher.go",
          "line": 957,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.size)\n\t\t\treturn bytes < maxTxRetrievalSize\n\t\t})\n\t\t// If any hashes were allocated, request them from the peer\n\t\tif len(hashes) > 0 {\n\t\t\tf.requests[peer] = &txRequest{hashes: hashes, time: f.clock.Now()}\n\t\t\ttxRequestOutMeter.Mark(int64(len(hashes)))\n\t\t\tp := peer\n\t\t\tgopool.Submit(func() {\n\t\t\t\t// Try to fetch the transactions, but in case of a request\n\t\t\t\t// failure (e.g. peer disconnected), reschedule the hashes.\n\t\t\t\tif err := f.fetchTxs(p, hashes)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/downloader/downloader.go",
          "line": 1147,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(proced)\n\t\t} else {\n\t\t\t// A malicious node might withhold advertised headers indefinitely\n\t\t\tif n := len(headers)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/downloader/downloader.go",
          "line": 1204,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(headers))\n\t\t}\n\t\t// If we're still skeleton filling snap sync, check pivot staleness\n\t\t// before continuing to the next skeleton filling\n\t\tif skeleton && pivot > 0 {\n\t\t\tpivoting = true\n\t\t}\n\t}\n}\n\n// fillHeaderSkeleton concurrently retrieves headers from all our available peers\n// and maps them to the provided skeleton header chain.\n//\n// Any partial results from the beginning of the skeleton is (if possible) forwarded\n// immediately to the header processor to keep the rest of the pipeline full even\n// in the case of header stalls.\n//\n// The method returns the entire filled skeleton and also the number of headers\n// already forwarded for processing.\nfunc (d *Downloader) fillHeaderSkeleton(from uint64, skeleton []*types.Header) ([]*types.Header, []common.Hash, int, error) {\n\tlog.Debug(\"Filling up skeleton\", \"from\", from)\n\td.queue.ScheduleSkeleton(from, skeleton)\n\n\terr := d.concurrentFetch((*headerQueue)(d), false)\n\tif err != nil {\n\t\tlog.Debug(\"Skeleton fill failed\", \"err\", err)\n\t}\n\tfilled, hashes, proced := d.queue.RetrieveHeaders()\n\tif err == nil {\n\t\tlog.Debug(\"Skeleton fill succeeded\", \"filled\", len(filled), \"processed\", proced)\n\t}\n\treturn filled, hashes, proced, err\n}\n\n// fetchBodies iteratively downloads the scheduled block bodies, taking any\n// available peers, reserving a chunk of blocks for each, waiting for delivery\n// and also periodically checking for timeouts.\nfunc (d *Downloader) fetchBodies(from uint64, beaconMode bool) error {\n\tlog.Debug(\"Downloading block bodies\", \"origin\", from)\n\terr := d.concurrentFetch((*bodyQueue)(d), beaconMode)\n\n\tlog.Debug(\"Block body download terminated\", \"err\", err)\n\treturn err\n}\n\n// fetchReceipts iteratively downloads the scheduled block receipts, taking any\n// available peers, reserving a chunk of receipts for each, waiting for delivery\n// and also periodically checking for timeouts.\nfunc (d *Downloader) fetchReceipts(from uint64, beaconMode bool) error {\n\tlog.Debug(\"Downloading receipts\", \"origin\", from)\n\terr := d.concurrentFetch((*receiptQueue)(d), beaconMode)\n\n\tlog.Debug(\"Receipt download terminated\", \"err\", err)\n\treturn err\n}\n\n// processHeaders takes batches of retrieved headers from an input channel and\n// keeps processing and scheduling them into the header chain and downloader's\n// queue until the stream ends or a failure occurs.\nfunc (d *Downloader) processHeaders(origin uint64, td, ttd *big.Int, beaconMode bool) error {\n\tvar (\n\t\tmode  = d.getMode()\n\t\ttimer = time.NewTimer(time.Second)\n\t)\n\tdefer timer.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-d.cancelCh:\n\t\t\treturn errCanceled\n\n\t\tcase task := <-d.headerProcCh:\n\t\t\t// Terminate header processing if we synced up\n\t\t\tif task == nil || len(task.headers) == 0 {\n\t\t\t\t// Notify everyone that headers are fully processed\n\t\t\t\tfor _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase ch <- false:\n\t\t\t\t\tcase <-d.cancelCh:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\t// Otherwise split the chunk of headers into batches and process them\n\t\t\theaders, hashes, scheduled := task.headers, task.hashes, false\n\n\t\t\tfor len(headers) > 0 {\n\t\t\t\t// Terminate if something failed in between processing chunks\n\t\t\t\tselect {\n\t\t\t\tcase <-d.cancelCh:\n\t\t\t\t\treturn errCanceled\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\t// Select the next chunk of headers to import\n\t\t\t\tlimit := maxHeadersProcess\n\t\t\t\tif limit > len(headers) {\n\t\t\t\t\tlimit = len(headers)\n\t\t\t\t}\n\t\t\t\tchunkHeaders := headers[:limit]\n\t\t\t\tchunkHashes := hashes[:limit]\n\n\t\t\t\t// Split the headers around the chain cutoff\n\t\t\t\tvar cutoff int\n\t\t\t\tif mode == ethconfig.SnapSync && d.chainCutoffNumber != 0 {\n\t\t\t\t\tcutoff = sort.Search(len(chunkHeaders), func(i int) bool {\n\t\t\t\t\t\treturn chunkHeaders[i].Number.Uint64() >= d.chainCutoffNumber\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\t// Insert the header chain into the ancient store (with block bodies and\n\t\t\t\t// receipts set to nil) if they fall before the cutoff.\n\t\t\t\tif mode == ethconfig.SnapSync && cutoff != 0 {\n\t\t\t\t\tif n, err := d.blockchain.InsertHeadersBeforeCutoff(chunkHeaders[:cutoff])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/downloader/downloader.go",
          "line": 1347,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(limit)\n\t\t\t}\n\t\t\t// Update the highest block number we know if a higher one is found.\n\t\t\td.syncStatsLock.Lock()\n\t\t\tif d.syncStatsChainHeight < origin {\n\t\t\t\td.syncStatsChainHeight = origin - 1\n\t\t\t}\n\t\t\td.syncStatsLock.Unlock()\n\n\t\t\t// Signal the downloader of the availability of new tasks\n\t\t\tif scheduled {\n\t\t\t\tfor _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase ch <- true:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (d *Downloader) checkStalling(td *big.Int, beaconMode bool) error {\n\t// If we're in legacy sync mode, we need to check total difficulty\n\t// violations from malicious peers. That is not needed in beacon\n\t// mode and we can skip to terminating sync.\n\tif !beaconMode {\n\t\t// If no headers were retrieved at all, the peer violated its TD promise that it had a\n\t\t// better chain compared to ours. The only exception is if its promised blocks were\n\t\t// already imported by other means (e.g. fetcher):\n\t\t//\n\t\t// R <remote peer>, L <local node>: Both at block 10\n\t\t// R: Mine block 11, and propagate it to L\n\t\t// L: Queue block 11 for import\n\t\t// L: Notice that R's head and TD increased compared to ours, start sync\n\t\t// L: Import of block 11 finishes\n\t\t// L: Sync begins, and finds common ancestor at 11\n\t\t// L: Request new headers up from 11 (R's TD was higher, it must have something)\n\t\t// R: Nothing to give\n\t\thead := d.blockchain.CurrentBlock()\n\t\tif td.Cmp(d.blockchain.GetTd(head.Hash(), head.Number.Uint64())) > 0 {\n\t\t\treturn errStallingPeer\n\t\t}\n\t\t// If snap or light syncing, ensure promised headers are indeed delivered. This is\n\t\t// needed to detect scenarios where an attacker feeds a bad pivot and then bails out\n\t\t// of delivering the post-pivot blocks that would flag the invalid content.\n\t\t//\n\t\t// This check cannot be executed \"as is\" for full imports, since blocks may still be\n\t\t// queued for processing when the header download completes. However, as long as the\n\t\t// peer gave us something useful, we're already happy/progressed (above check).\n\t\tif d.getMode() == SnapSync {\n\t\t\thead := d.blockchain.CurrentHeader()\n\t\t\tif td.Cmp(d.blockchain.GetTd(head.Hash(), head.Number.Uint64())) > 0 {\n\t\t\t\treturn errStallingPeer\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// processFullSyncContent takes fetch results from the queue and imports them into the chain.\nfunc (d *Downloader) processFullSyncContent(ttd *big.Int, beaconMode bool) error {\n\tfor {\n\t\tresults := d.queue.Results(true)\n\t\tif len(results) == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tstop := make(chan struct{})\n\t\tif d.chainInsertHook != nil {\n\t\t\td.chainInsertHook(results, stop)\n\t\t}\n\t\tif err := d.importBlockResults(results)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/downloader/queue.go",
          "line": 390,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uncle.Size()\n\t\t}\n\t\tsize += common.StorageSize(len(result.Receipts))\n\t\tfor _, tx := range result.Transactions {\n\t\t\tsize += common.StorageSize(tx.Size())\n\t\t}\n\t\tsize += common.StorageSize(result.Withdrawals.Size())\n\t\tq.resultSize = common.StorageSize(blockCacheSizeWeight)*size +\n\t\t\t(1-common.StorageSize(blockCacheSizeWeight))*q.resultSize\n\t}\n\t// Using the newly calibrated result size, figure out the new throttle limit\n\t// on the result cache\n\tthrottleThreshold := uint64((common.StorageSize(blockCacheMemory) + q.resultSize - 1) / q.resultSize)\n\tthrottleThreshold = q.resultCache.SetThrottleThreshold(throttleThreshold)\n\n\t// With results removed from the cache, wake throttled fetchers\n\tfor _, ch := range []chan bool{q.blockWakeCh, q.receiptWakeCh} {\n\t\tselect {\n\t\tcase ch <- true:\n\t\tdefault:\n\t\t}\n\t}\n\t// Log some info at certain times\n\tif time.Since(q.logTime) >= 60*time.Second {\n\t\tq.logTime = time.Now()\n\n\t\tinfo := q.Stats()\n\t\tinfo = append(info, \"throttle\", throttleThreshold)\n\t\tlog.Debug(\"Downloader queue stats\", info...)\n\t}\n\treturn results\n}\n\nfunc (q *queue) Stats() []interface{} {\n\tq.lock.RLock()\n\tdefer q.lock.RUnlock()\n\n\treturn q.stats()\n}\n\nfunc (q *queue) stats() []interface{} {\n\treturn []interface{}{\n\t\t\"receiptTasks\", q.receiptTaskQueue.Size(),\n\t\t\"blockTasks\", q.blockTaskQueue.Size(),\n\t\t\"itemSize\", q.resultSize,\n\t}\n}\n\n// ReserveHeaders reserves a set of headers for the given peer, skipping any\n// previously failed batches.\nfunc (q *queue) ReserveHeaders(p *peerConnection, count int) *fetchRequest {\n\tq.lock.Lock()\n\tdefer q.lock.Unlock()\n\n\t// Short circuit if the peer's already downloading something (sanity check to\n\t// not corrupt state)\n\tif _, ok := q.headerPendPool[p.id]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/downloader/queue.go",
          "line": 763,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= MaxHeaderFetch\n\t}\n\tif ready > 0 {\n\t\t// Headers are ready for delivery, gather them and push forward (non blocking)\n\t\tprocessHeaders := make([]*types.Header, ready)\n\t\tcopy(processHeaders, q.headerResults[q.headerProced:q.headerProced+ready])\n\n\t\tprocessHashes := make([]common.Hash, ready)\n\t\tcopy(processHashes, q.headerHashes[q.headerProced:q.headerProced+ready])\n\n\t\tselect {\n\t\tcase headerProcCh <- &headerTask{\n\t\t\theaders: processHeaders,\n\t\t\thashes:  processHashes,\n\t\t}:\n\t\t\tlogger.Trace(\"Pre-scheduled new headers\", \"count\", len(processHeaders), \"from\", processHeaders[0].Number)\n\t\t\tq.headerProced += len(processHeaders)\n\t\tdefault:\n\t\t}\n\t}\n\t// Check for termination and return\n\tif len(q.headerTaskPool) == 0 {\n\t\tq.headerContCh <- false\n\t}\n\treturn len(headers), nil\n}\n\n// DeliverBodies injects a block body retrieval response into the results queue.\n// The method returns the number of blocks bodies accepted from the delivery and\n// also wakes any threads waiting for data delivery.\nfunc (q *queue) DeliverBodies(id string, txLists [][]*types.Transaction, txListHashes []common.Hash,\n\tuncleLists [][]*types.Header, uncleListHashes []common.Hash,\n\twithdrawalLists [][]*types.Withdrawal, withdrawalListHashes []common.Hash, sidecars []types.BlobSidecars,\n) (int, error) {\n\tq.lock.Lock()\n\tdefer q.lock.Unlock()\n\n\tvalidate := func(index int, header *types.Header) error {\n\t\tif txListHashes[index] != header.TxHash {\n\t\t\treturn errInvalidBody\n\t\t}\n\t\tif uncleListHashes[index] != header.UncleHash {\n\t\t\treturn errInvalidBody\n\t\t}\n\t\tif header.WithdrawalsHash == nil {\n\t\t\t// nil hash means that withdrawals should not be present in body\n\t\t\tif withdrawalLists[index] != nil {\n\t\t\t\treturn errInvalidBody\n\t\t\t}\n\t\t} else { // non-nil hash: body must have withdrawals\n\t\t\tif withdrawalLists[index] == nil {\n\t\t\t\treturn errInvalidBody\n\t\t\t}\n\t\t\tif withdrawalListHashes[index] != *header.WithdrawalsHash {\n\t\t\t\treturn errInvalidBody\n\t\t\t}\n\t\t}\n\t\t// Blocks must have a number of blobs corresponding to the header gas usage,\n\t\t// and zero before the Cancun hardfork.\n\t\tvar blobs int\n\t\tfor _, tx := range txLists[index] {\n\t\t\t// Count the number of blobs to validate against the header's blobGasUsed\n\t\t\tblobs += len(tx.BlobHashes())\n\n\t\t\t// Validate the data blobs individually too\n\t\t\tif tx.Type() == types.BlobTxType {\n\t\t\t\tif len(tx.BlobHashes()) == 0 {\n\t\t\t\t\treturn errInvalidBody\n\t\t\t\t}\n\t\t\t\tfor _, hash := range tx.BlobHashes() {\n\t\t\t\t\tif !kzg4844.IsValidVersionedHash(hash[:]) {\n\t\t\t\t\t\treturn errInvalidBody\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif tx.BlobTxSidecar() != nil {\n\t\t\t\t\treturn errInvalidBody\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif header.BlobGasUsed != nil {\n\t\t\tif want := *header.BlobGasUsed / params.BlobTxBlobGasPerBlob",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/downloader/resultstore.go",
          "line": 181,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(limit)\n\tr.indexIncomplete.Add(int32(-limit))\n\n\treturn results\n}\n\n// Prepare initialises the offset with the given block number\nfunc (r *resultStore) Prepare(offset uint64) {\n\tr.lock.Lock()\n\tdefer r.lock.Unlock()\n\n\tif r.resultOffset < offset {\n\t\tr.resultOffset = offset\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/downloader/downloader_test.go",
          "line": 1225,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= span + 1\n\t\t}\n\t\treturn r\n\t}\n\tfor i, tt := range testCases {\n\t\tfrom, count, span, max := calculateRequestSpan(tt.remoteHeight, tt.localHeight)\n\t\tdata := reqs(int(from), count, span)\n\n\t\tif max != uint64(data[len(data)-1]) {\n\t\t\tt.Errorf(\"test %d: wrong last value %d != %d\", i, data[len(data)-1], max)\n\t\t}\n\t\tfailed := false\n\t\tif len(data) != len(tt.expected) {\n\t\t\tfailed = true\n\t\t\tt.Errorf(\"test %d: length wrong, expected %d got %d\", i, len(tt.expected), len(data))\n\t\t} else {\n\t\t\tfor j, n := range data {\n\t\t\t\tif n != tt.expected[j] {\n\t\t\t\t\tfailed = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif failed {\n\t\t\tres := strings.ReplaceAll(fmt.Sprint(data), \" \", \",\")\n\t\t\texp := strings.ReplaceAll(fmt.Sprint(tt.expected), \" \", \",\")\n\t\t\tt.Logf(\"got: %v\\n\", res)\n\t\t\tt.Logf(\"exp: %v\\n\", exp)\n\t\t\tt.Errorf(\"test %d: wrong values\", i)\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/downloader/queue_test.go",
          "line": 297,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= l\n\t\t}\n\t}()\n\twg.Add(1)\n\tgo func() {\n\t\t// collect results\n\t\tdefer wg.Done()\n\t\ttot := 0\n\t\tfor {\n\t\t\tres := q.Results(true)\n\t\t\ttot += len(res)\n\t\t\tfmt.Printf(\"got %d results, %d tot\\n\", len(res), tot)\n\t\t\t// Now we can forget about these\n\t\t\tworld.forget(res[len(res)-1].Header.Number.Uint64())\n\t\t}\n\t}()\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\t// reserve body fetch\n\t\ti := 4\n\t\tfor {\n\t\t\tpeer := dummyPeer(fmt.Sprintf(\"peer-%d\", i))\n\t\t\tf, _, _ := q.ReserveBodies(peer, rand.Intn(30))\n\t\t\tif f != nil {\n\t\t\t\tvar (\n\t\t\t\t\temptyList []*types.Header\n\t\t\t\t\ttxset     [][]*types.Transaction\n\t\t\t\t\tuncleset  [][]*types.Header\n\t\t\t\t)\n\t\t\t\tnumToSkip := rand.Intn(len(f.Headers))\n\t\t\t\tfor _, hdr := range f.Headers[0 : len(f.Headers)-numToSkip] {\n\t\t\t\t\ttxset = append(txset, world.getTransactions(hdr.Number.Uint64()))\n\t\t\t\t\tuncleset = append(uncleset, emptyList)\n\t\t\t\t}\n\t\t\t\tvar (\n\t\t\t\t\ttxsHashes   = make([]common.Hash, len(txset))\n\t\t\t\t\tuncleHashes = make([]common.Hash, len(uncleset))\n\t\t\t\t)\n\t\t\t\thasher := trie.NewStackTrie(nil)\n\t\t\t\tfor i, txs := range txset {\n\t\t\t\t\ttxsHashes[i] = types.DeriveSha(types.Transactions(txs), hasher)\n\t\t\t\t}\n\t\t\t\tfor i, uncles := range uncleset {\n\t\t\t\t\tuncleHashes[i] = types.CalcUncleHash(uncles)\n\t\t\t\t}\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t\t_, err := q.DeliverBodies(peer.id, txset, txsHashes, uncleset, uncleHashes, nil, nil, nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tfmt.Printf(\"delivered %d bodies %v\\n\", len(txset), err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ti++\n\t\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\t}\n\t\t}\n\t}()\n\tgo func() {\n\t\tdefer wg.Done()\n\t\t// reserve receiptfetch\n\t\tpeer := dummyPeer(\"peer-3\")\n\t\tfor {\n\t\t\tf, _, _ := q.ReserveReceipts(peer, rand.Intn(50))\n\t\t\tif f != nil {\n\t\t\t\tvar rcs []types.Receipts\n\t\t\t\tfor _, hdr := range f.Headers {\n\t\t\t\t\trcs = append(rcs, world.getReceipts(hdr.Number.Uint64()))\n\t\t\t\t}\n\t\t\t\thasher := trie.NewStackTrie(nil)\n\t\t\t\thashes := make([]common.Hash, len(rcs))\n\t\t\t\tfor i, receipt := range rcs {\n\t\t\t\t\thashes[i] = types.DeriveSha(receipt, hasher)\n\t\t\t\t}\n\t\t\t\t_, err := q.DeliverReceipts(peer.id, types.EncodeBlockReceiptLists(rcs), hashes)\n\t\t\t\tif err != nil {\n\t\t\t\t\tfmt.Printf(\"delivered %d receipts %v\\n\", len(rcs), err)\n\t\t\t\t}\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t} else {\n\t\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\t}\n\t\t}\n\t}()\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/downloader/peer.go",
          "line": 224,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/downloader/peer.go",
          "line": 241,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/tracker.go",
          "line": 68,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t\tt.oldest += uint64(count)\n\t\tcopy(t.used, t.used[count:])\n\n\t\t// Clean up the array tail since they are useless now.\n\t\tfor i := t.limit - count",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/api.go",
          "line": 434,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(txs))\n\t\t}\n\t})\n\n\t// Keep reading the trace results and stream them to result channel.\n\tretCh := make(chan *blockTraceResult)\n\tgopool.Submit(func() {\n\t\tdefer close(retCh)\n\t\tvar (\n\t\t\tnext = start.NumberU64() + 1\n\t\t\tdone = make(map[uint64]*blockTraceResult)\n\t\t)\n\t\tfor res := range resCh {\n\t\t\t// Queue up next received result\n\t\t\tresult := &blockTraceResult{\n\t\t\t\tBlock:  hexutil.Uint64(res.block.NumberU64()),\n\t\t\t\tHash:   res.block.Hash(),\n\t\t\t\tTraces: res.results,\n\t\t\t}\n\t\t\tdone[uint64(result.Block)] = result\n\n\t\t\t// Stream completed traces to the result channel\n\t\t\tfor result, ok := done[next]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 1106,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t})\n\tbackend.refHook = func() { ref.Add(1) }\n\tbackend.relHook = func() { rel.Add(1) }\n\tapi := NewAPI(backend)\n\n\tsingle := `{\"txHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\",\"result\":{\"gas\":21000,\"failed\":false,\"returnValue\":\"0x\",\"structLogs\":[]}}`\n\tvar cases = []struct {\n\t\tstart  uint64\n\t\tend    uint64\n\t\tconfig *TraceConfig\n\t}{\n\t\t{0, 50, nil},  // the entire chain range, blocks [1, 50]\n\t\t{10, 20, nil}, // the middle chain range, blocks [11, 20]\n\t}\n\tfor _, c := range cases {\n\t\tref.Store(0)\n\t\trel.Store(0)\n\n\t\tfrom, _ := api.blockByNumber(context.Background(), rpc.BlockNumber(c.start))\n\t\tto, _ := api.blockByNumber(context.Background(), rpc.BlockNumber(c.end))\n\t\tresCh := api.traceChain(from, to, c.config, nil)\n\n\t\tnext := c.start + 1\n\t\tfor result := range resCh {\n\t\t\tif have, want := uint64(result.Block), next",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 1145,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t\tif next != c.end+1 {\n\t\t\tt.Error(\"Missing tracing block\")\n\t\t}\n\n\t\tif nref, nrel := ref.Load(), rel.Load()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/bsc/handler.go",
          "line": 178,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/bsc/handshake.go",
          "line": 25,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/bsc/dispatcher.go",
          "line": 60,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/bsc/peer.go",
          "line": 138,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\treturn p.periodCounter > uint(secondsPerPeriod*receiveRateLimitPerSecond)\n}\n\n// broadcastVotes is a write loop that schedules votes broadcasts\n// to the remote peer. The goal is to have an async writer that does not lock up\n// node internals and at the same time rate limits queued data.\nfunc (p *Peer) broadcastVotes() {\n\tfor {\n\t\tselect {\n\t\tcase votes := <-p.voteBroadcast:\n\t\t\tif err := p.sendVotes(votes)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/bsc/peer.go",
          "line": 112,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 311,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(common.HashLength + len(account))\n\t\taccounts = append(accounts, &AccountData{\n\t\t\tHash: hash,\n\t\t\tBody: account,\n\t\t})\n\t\t// If we've exceeded the request threshold, abort\n\t\tif bytes.Compare(hash[:], req.Limit[:]) >= 0 {\n\t\t\tbreak\n\t\t}\n\t\tif size > req.Bytes {\n\t\t\tbreak\n\t\t}\n\t}\n\tit.Release()\n\n\t// Generate the Merkle proofs for the first and last account\n\tproof := trienode.NewProofSet()\n\tif err := tr.Prove(req.Origin[:], proof)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 407,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(common.HashLength + len(slot))\n\t\t\tstorage = append(storage, &StorageData{\n\t\t\t\tHash: hash,\n\t\t\t\tBody: slot,\n\t\t\t})\n\t\t\t// If we've exceeded the request threshold, abort\n\t\t\tif bytes.Compare(hash[:], limit[:]) >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(storage) > 0 {\n\t\t\tslots = append(slots, storage)\n\t\t}\n\t\tit.Release()\n\n\t\t// Generate the Merkle proofs for the first and last storage slot, but\n\t\t// only if the response was capped. If the entire storage trie included\n\t\t// in the response, no need for any proofs.\n\t\tif origin != (common.Hash{}) || (abort && len(storage) > 0) {\n\t\t\t// Request started at a non-zero hash or was capped prematurely, add\n\t\t\t// the endpoint Merkle proofs\n\t\t\taccTrie, err := trie.NewStateTrie(trie.StateTrieID(req.Root), chain.TrieDB())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tacc, err := accTrie.GetAccountByHash(account)\n\t\t\tif err != nil || acc == nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tid := trie.StorageTrieID(req.Root, account, acc.Root)\n\t\t\tstTrie, err := trie.NewStateTrie(id, chain.TrieDB())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tproof := trienode.NewProofSet()\n\t\t\tif err := stTrie.Prove(origin[:], proof)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 483,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(blob))\n\t\t}\n\t\tif bytes > req.Bytes {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn codes\n}\n\n// ServiceGetTrieNodesQuery assembles the response to a trie nodes query.\n// It is exposed to allow external packages to test protocol behavior.\nfunc ServiceGetTrieNodesQuery(chain *core.BlockChain, req *GetTrieNodesPacket, start time.Time) ([][]byte, error) {\n\tif req.Bytes > softResponseLimit {\n\t\treq.Bytes = softResponseLimit\n\t}\n\t// Make sure we have the state associated with the request\n\ttriedb := chain.TrieDB()\n\n\taccTrie, err := trie.NewStateTrie(trie.StateTrieID(req.Root), triedb)\n\tif err != nil {\n\t\t// We don't have the requested state available, bail out\n\t\treturn nil, nil\n\t}\n\t// The 'reader' might be nil, in which case we cannot serve storage slots\n\t// via snapshot.\n\tvar reader database.StateReader\n\tif chain.Snapshots() != nil {\n\t\treader = chain.Snapshots().Snapshot(req.Root)\n\t}\n\tif reader == nil {\n\t\treader, _ = triedb.StateReader(req.Root)\n\t}\n\t// Retrieve trie nodes until the packet size limit is reached\n\tvar (\n\t\tnodes [][]byte\n\t\tbytes uint64\n\t\tloads int // Trie hash expansions to count database reads\n\t)\n\tfor _, pathset := range req.Paths {\n\t\tswitch len(pathset) {\n\t\tcase 0:\n\t\t\t// Ensure we penalize invalid requests\n\t\t\treturn nil, fmt.Errorf(\"%w: zero-item pathset requested\", errBadRequest)\n\n\t\tcase 1:\n\t\t\t// If we're only retrieving an account trie node, fetch it directly\n\t\t\tblob, resolved, err := accTrie.GetNode(pathset[0])\n\t\t\tloads += resolved // always account database reads, even for failures\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tnodes = append(nodes, blob)\n\t\t\tbytes += uint64(len(blob))\n\n\t\tdefault:\n\t\t\tvar stRoot common.Hash\n\n\t\t\t// Storage slots requested, open the storage trie and retrieve from there\n\t\t\tif reader == nil {\n\t\t\t\t// We don't have the requested state snapshotted yet (or it is stale),\n\t\t\t\t// but can look up the account via the trie instead.\n\t\t\t\taccount, err := accTrie.GetAccountByHash(common.BytesToHash(pathset[0]))\n\t\t\t\tloads += 8 // We don't know the exact cost of lookup, this is an estimate\n\t\t\t\tif err != nil || account == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tstRoot = account.Root\n\t\t\t} else {\n\t\t\t\taccount, err := reader.Account(common.BytesToHash(pathset[0]))\n\t\t\t\tloads++ // always account database reads, even for failures\n\t\t\t\tif err != nil || account == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tstRoot = common.BytesToHash(account.Root)\n\t\t\t}\n\t\t\tid := trie.StorageTrieID(req.Root, common.BytesToHash(pathset[0]), stRoot)\n\t\t\tstTrie, err := trie.NewStateTrie(id, triedb)\n\t\t\tloads++ // always account database reads, even for failures\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor _, path := range pathset[1:] {\n\t\t\t\tblob, resolved, err := stTrie.GetNode(path)\n\t\t\t\tloads += resolved // always account database reads, even for failures\n\t\t\t\tif err != nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tnodes = append(nodes, blob)\n\t\t\t\tbytes += uint64(len(blob))\n\n\t\t\t\t// Sanity check limits to avoid DoS on the store trie loads\n\t\t\t\tif bytes > req.Bytes || loads > maxTrieNodeLookups || time.Since(start) > maxTrieNodeTimeSpent {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Abort request processing if we've exceeded our limits\n\t\tif bytes > req.Bytes || loads > maxTrieNodeLookups || time.Since(start) > maxTrieNodeTimeSpent {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nodes, nil\n}\n\n// NodeInfo represents a short summary of the `snap` sub-protocol metadata\n// known about the host peer.\ntype NodeInfo struct{}\n\n// nodeInfo retrieves some `snap` protocol metadata about the running host node.\nfunc nodeInfo(chain *core.BlockChain) *NodeInfo {\n\treturn &NodeInfo{}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 161,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 193,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 227,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 254,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 780,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(key) + len(value))\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\ttask.genTrie = newHashTrie(task.genBatch)\n\t\t\t\t}\n\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\ttask.genTrie = newPathTrie(common.Hash{}, task.Next != common.Hash{}, s.db, task.genBatch)\n\t\t\t\t}\n\t\t\t\t// Restore leftover storage tasks\n\t\t\t\tfor accountHash, subtasks := range task.SubTasks {\n\t\t\t\t\tfor _, subtask := range subtasks {\n\t\t\t\t\t\tsubtask.genBatch = ethdb.HookedBatch{\n\t\t\t\t\t\t\tBatch: stateDiskDB.NewBatch(),\n\t\t\t\t\t\t\tOnPut: func(key []byte, value []byte) {\n\t\t\t\t\t\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\t\t\tsubtask.genTrie = newHashTrie(subtask.genBatch)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\t\t\tsubtask.genTrie = newPathTrie(accountHash, subtask.Next != common.Hash{}, s.db, subtask.genBatch)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\ts.lock.Lock()\n\t\t\tdefer s.lock.Unlock()\n\n\t\t\ts.snapped = len(s.tasks) == 0\n\n\t\t\ts.accountSynced = progress.AccountSynced\n\t\t\ts.accountBytes = progress.AccountBytes\n\t\t\ts.bytecodeSynced = progress.BytecodeSynced\n\t\t\ts.bytecodeBytes = progress.BytecodeBytes\n\t\t\ts.storageSynced = progress.StorageSynced\n\t\t\ts.storageBytes = progress.StorageBytes\n\n\t\t\ts.trienodeHealSynced = progress.TrienodeHealSynced\n\t\t\ts.trienodeHealBytes = progress.TrienodeHealBytes\n\t\t\ts.bytecodeHealSynced = progress.BytecodeHealSynced\n\t\t\ts.bytecodeHealBytes = progress.BytecodeHealBytes\n\t\t\treturn\n\t\t}\n\t}\n\t// Either we've failed to decode the previous state, or there was none.\n\t// Start a fresh sync by chunking up the account range and scheduling\n\t// them for retrieval.\n\ts.tasks = nil\n\ts.accountSynced, s.accountBytes = 0, 0\n\ts.bytecodeSynced, s.bytecodeBytes = 0, 0\n\ts.storageSynced, s.storageBytes = 0, 0\n\ts.trienodeHealSynced, s.trienodeHealBytes = 0, 0\n\ts.bytecodeHealSynced, s.bytecodeHealBytes = 0, 0\n\n\tvar next common.Hash\n\tstep := new(big.Int).Sub(\n\t\tnew(big.Int).Div(\n\t\t\tnew(big.Int).Exp(common.Big2, common.Big256, nil),\n\t\t\tbig.NewInt(int64(accountConcurrency)),\n\t\t), common.Big1,\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 852,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(key) + len(value))\n\t\t\t},\n\t\t}\n\t\tvar tr genTrie\n\t\tif s.scheme == rawdb.HashScheme {\n\t\t\ttr = newHashTrie(batch)\n\t\t}\n\t\tif s.scheme == rawdb.PathScheme {\n\t\t\ttr = newPathTrie(common.Hash{}, next != common.Hash{}, s.db, batch)\n\t\t}\n\t\ts.tasks = append(s.tasks, &accountTask{\n\t\t\tNext:           next,\n\t\t\tLast:           last,\n\t\t\tSubTasks:       make(map[common.Hash][]*storageTask),\n\t\t\tgenBatch:       batch,\n\t\t\tstateCompleted: make(map[common.Hash]struct{}),\n\t\t\tgenTrie:        tr,\n\t\t})\n\t\tlog.Debug(\"Created account sync task\", \"from\", next, \"last\", last)\n\t\tnext = common.BigToHash(new(big.Int).Add(last.Big(), common.Big1))\n\t}\n}\n\n// saveSyncStatus marshals the remaining sync tasks into leveldb.\nfunc (s *Syncer) saveSyncStatus() {\n\t// Serialize any partial progress to disk before spinning down\n\tfor _, task := range s.tasks {\n\t\t// Claim the right boundary as incomplete before flushing the\n\t\t// accumulated nodes in batch, the nodes on right boundary\n\t\t// will be discarded and cleaned up by this call.\n\t\ttask.genTrie.commit(false)\n\t\tif err := task.genBatch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2021,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= codes\n\ts.bytecodeBytes += bytes\n\n\tlog.Debug(\"Persisted set of bytecodes\", \"count\", codes, \"bytes\", bytes)\n\n\t// If this delivery completed the last pending task, forward the account task\n\t// to the next chunk\n\tif res.task.pend == 0 {\n\t\ts.forwardAccountTask(res.task)\n\t\treturn\n\t}\n\t// Some accounts are still incomplete, leave as is for the storage and contract\n\t// task assigners to pick up and fill.\n}\n\n// processStorageResponse integrates an already validated storage response\n// into the account tasks.\nfunc (s *Syncer) processStorageResponse(res *storageResponse) {\n\t// Switch the subtask from pending to idle\n\tif res.subTask != nil {\n\t\tres.subTask.req = nil\n\t}\n\n\tvar usingMultDatabase bool\n\tbatch := ethdb.HookedBatch{\n\t\tBatch: s.db.GetStateStore().NewBatch(),\n\t\tOnPut: func(key []byte, value []byte) {\n\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t},\n\t}\n\tvar snapBatch ethdb.HookedBatch\n\tif s.db.HasSeparateStateStore() {\n\t\tusingMultDatabase = true\n\t\tsnapBatch = ethdb.HookedBatch{\n\t\t\tBatch: s.db.NewBatch(),\n\t\t\tOnPut: func(key []byte, value []byte) {\n\t\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t\t},\n\t\t}\n\t}\n\n\tvar (\n\t\tslots           int\n\t\toldStorageBytes = s.storageBytes\n\t)\n\t// Iterate over all the accounts and reconstruct their storage tries from the\n\t// delivered slots\n\tfor i, account := range res.accounts {\n\t\t// If the account was not delivered, reschedule it\n\t\tif i >= len(res.hashes) {\n\t\t\tres.mainTask.stateTasks[account] = res.roots[i]\n\t\t\tcontinue\n\t\t}\n\t\t// State was delivered, if complete mark as not needed any more, otherwise\n\t\t// mark the account as needing healing\n\t\tfor j, hash := range res.mainTask.res.hashes {\n\t\t\tif account != hash {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tacc := res.mainTask.res.accounts[j]\n\n\t\t\t// If the packet contains multiple contract storage slots, all\n\t\t\t// but the last are surely complete. The last contract may be\n\t\t\t// chunked, so check it's continuation flag.\n\t\t\tif res.subTask == nil && res.mainTask.needState[j] && (i < len(res.hashes)-1 || !res.cont) {\n\t\t\t\tres.mainTask.needState[j] = false\n\t\t\t\tres.mainTask.pend--\n\t\t\t\tres.mainTask.stateCompleted[account] = struct{}{} // mark it as completed\n\t\t\t\tsmallStorageGauge.Inc(1)\n\t\t\t}\n\t\t\t// If the last contract was chunked, mark it as needing healing\n\t\t\t// to avoid writing it out to disk prematurely.\n\t\t\tif res.subTask == nil && !res.mainTask.needHeal[j] && i == len(res.hashes)-1 && res.cont {\n\t\t\t\tres.mainTask.needHeal[j] = true\n\t\t\t}\n\t\t\t// If the last contract was chunked, we need to switch to large\n\t\t\t// contract handling mode\n\t\t\tif res.subTask == nil && i == len(res.hashes)-1 && res.cont {\n\t\t\t\t// If we haven't yet started a large-contract retrieval, create\n\t\t\t\t// the subtasks for it within the main account task\n\t\t\t\tif tasks, ok := res.mainTask.SubTasks[account]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2134,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(key) + len(value))\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\tvar tr genTrie\n\t\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\t\ttr = newHashTrie(batch)\n\t\t\t\t\t}\n\t\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\t\t// Keep the left boundary as it's the first range.\n\t\t\t\t\t\ttr = newPathTrie(account, false, s.db, batch)\n\t\t\t\t\t}\n\t\t\t\t\ttasks = append(tasks, &storageTask{\n\t\t\t\t\t\tNext:     common.Hash{},\n\t\t\t\t\t\tLast:     r.End(),\n\t\t\t\t\t\troot:     acc.Root,\n\t\t\t\t\t\tgenBatch: batch,\n\t\t\t\t\t\tgenTrie:  tr,\n\t\t\t\t\t})\n\t\t\t\t\tfor r.Next() {\n\t\t\t\t\t\tbatch := ethdb.HookedBatch{\n\t\t\t\t\t\t\tBatch: s.db.GetStateStore().NewBatch(),\n\t\t\t\t\t\t\tOnPut: func(key []byte, value []byte) {\n\t\t\t\t\t\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar tr genTrie\n\t\t\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\t\t\ttr = newHashTrie(batch)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\t\t\ttr = newPathTrie(account, true, s.db, batch)\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttasks = append(tasks, &storageTask{\n\t\t\t\t\t\t\tNext:     r.Start(),\n\t\t\t\t\t\t\tLast:     r.End(),\n\t\t\t\t\t\t\troot:     acc.Root,\n\t\t\t\t\t\t\tgenBatch: batch,\n\t\t\t\t\t\t\tgenTrie:  tr,\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\t\t\t\t\tfor _, task := range tasks {\n\t\t\t\t\t\tlog.Debug(\"Created storage sync task\", \"account\", account, \"root\", acc.Root, \"from\", task.Next, \"last\", task.Last)\n\t\t\t\t\t}\n\t\t\t\t\tres.mainTask.SubTasks[account] = tasks\n\n\t\t\t\t\t// Since we've just created the sub-tasks, this response\n\t\t\t\t\t// is surely for the first one (zero origin)\n\t\t\t\t\tres.subTask = tasks[0]\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we're in large contract delivery mode, forward the subtask\n\t\t\tif res.subTask != nil {\n\t\t\t\t// Ensure the response doesn't overflow into the subsequent task\n\t\t\t\tlast := res.subTask.Last.Big()\n\t\t\t\t// Find the first overflowing key. While at it, mark res as complete\n\t\t\t\t// if we find the range to include or pass the 'last'\n\t\t\t\tindex := sort.Search(len(res.hashes[i]), func(k int) bool {\n\t\t\t\t\tcmp := res.hashes[i][k].Big().Cmp(last)\n\t\t\t\t\tif cmp >= 0 {\n\t\t\t\t\t\tres.cont = false\n\t\t\t\t\t}\n\t\t\t\t\treturn cmp > 0\n\t\t\t\t})\n\t\t\t\tif index >= 0 {\n\t\t\t\t\t// cut off excess\n\t\t\t\t\tres.hashes[i] = res.hashes[i][:index]\n\t\t\t\t\tres.slots[i] = res.slots[i][:index]\n\t\t\t\t}\n\t\t\t\t// Forward the relevant storage chunk (even if created just now)\n\t\t\t\tif res.cont {\n\t\t\t\t\tres.subTask.Next = incHash(res.hashes[i][len(res.hashes[i])-1])\n\t\t\t\t} else {\n\t\t\t\t\tres.subTask.done = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Iterate over all the complete contracts, reconstruct the trie nodes and\n\t\t// push them to disk. If the contract is chunked, the trie nodes will be\n\t\t// reconstructed later.\n\t\tslots += len(res.hashes[i])\n\n\t\tif i < len(res.hashes)-1 || res.subTask == nil {\n\t\t\t// no need to make local reassignment of account: this closure does not outlive the loop\n\t\t\tvar tr genTrie\n\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\ttr = newHashTrie(batch)\n\t\t\t}\n\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t// Keep the left boundary as it's complete\n\t\t\t\ttr = newPathTrie(account, false, s.db, batch)\n\t\t\t}\n\t\t\tfor j := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2283,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(slots)\n\n\tlog.Debug(\"Persisted set of storage slots\", \"accounts\", len(res.hashes), \"slots\", slots, \"bytes\", s.storageBytes-oldStorageBytes)\n\n\t// If this delivery completed the last pending task, forward the account task\n\t// to the next chunk\n\tif res.mainTask.pend == 0 {\n\t\ts.forwardAccountTask(res.mainTask)\n\t\treturn\n\t}\n\t// Some accounts are still incomplete, leave as is for the storage and contract\n\t// task assigners to pick up and fill.\n}\n\n// processTrienodeHealResponse integrates an already validated trienode response\n// into the healer tasks.\nfunc (s *Syncer) processTrienodeHealResponse(res *trienodeHealResponse) {\n\tvar (\n\t\tstart = time.Now()\n\t\tfills int\n\t)\n\tfor i, hash := range res.hashes {\n\t\tnode := res.nodes[i]\n\n\t\t// If the trie node was not delivered, reschedule it\n\t\tif node == nil {\n\t\t\tres.task.trieTasks[res.paths[i]] = res.hashes[i]\n\t\t\tcontinue\n\t\t}\n\t\tfills++\n\n\t\t// Push the trie node into the state syncer\n\t\ts.trienodeHealSynced++\n\t\ts.trienodeHealBytes += common.StorageSize(len(node))\n\n\t\terr := s.healer.scheduler.ProcessNode(trie.NodeSyncResult{Path: res.paths[i], Data: node})\n\t\tswitch err {\n\t\tcase nil:\n\t\tcase trie.ErrAlreadyProcessed:\n\t\t\ts.trienodeHealDups++\n\t\tcase trie.ErrNotRequested:\n\t\t\ts.trienodeHealNops++\n\t\tdefault:\n\t\t\tlog.Error(\"Invalid trienode processed\", \"hash\", hash, \"err\", err)\n\t\t}\n\t}\n\ts.commitHealer(false)\n\n\t// Calculate the processing rate of one filled trie node\n\trate := float64(fills) / (float64(time.Since(start)) / float64(time.Second))\n\n\t// Update the currently measured trienode queueing and processing throughput.\n\t//\n\t// The processing rate needs to be updated uniformly independent if we've\n\t// processed 1x100 trie nodes or 100x1 to keep the rate consistent even in\n\t// the face of varying network packets. As such, we cannot just measure the\n\t// time it took to process N trie nodes and update once, we need one update\n\t// per trie node.\n\t//\n\t// Naively, that would be:\n\t//\n\t//   for i:=0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2416,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(node))\n\n\t\terr := s.healer.scheduler.ProcessCode(trie.CodeSyncResult{Hash: hash, Data: node})\n\t\tswitch err {\n\t\tcase nil:\n\t\tcase trie.ErrAlreadyProcessed:\n\t\t\ts.bytecodeHealDups++\n\t\tcase trie.ErrNotRequested:\n\t\t\ts.bytecodeHealNops++\n\t\tdefault:\n\t\t\tlog.Error(\"Invalid bytecode processed\", \"hash\", hash, \"err\", err)\n\t\t}\n\t}\n\ts.commitHealer(false)\n}\n\n// forwardAccountTask takes a filled account task and persists anything available\n// into the database, after which it forwards the next account marker so that the\n// task's next chunk may be filled.\nfunc (s *Syncer) forwardAccountTask(task *accountTask) {\n\t// Remove any pending delivery\n\tres := task.res\n\tif res == nil {\n\t\treturn // nothing to forward\n\t}\n\ttask.res = nil\n\n\t// Persist the received account segments. These flat state maybe\n\t// outdated during the sync, but it can be fixed later during the\n\t// snapshot generation.\n\toldAccountBytes := s.accountBytes\n\n\tbatch := ethdb.HookedBatch{\n\t\tBatch: s.db.NewBatch(),\n\t\tOnPut: func(key []byte, value []byte) {\n\t\t\ts.accountBytes += common.StorageSize(len(key) + len(value))\n\t\t},\n\t}\n\tfor i, hash := range res.hashes {\n\t\tif task.needCode[i] || task.needState[i] {\n\t\t\tbreak\n\t\t}\n\t\tslim := types.SlimAccountRLP(*res.accounts[i])\n\t\trawdb.WriteAccountSnapshot(batch, hash, slim)\n\n\t\tif !task.needHeal[i] {\n\t\t\t// If the storage task is complete, drop it into the stack trie\n\t\t\t// to generate account trie nodes for it\n\t\t\tfull, err := types.FullAccountRLP(slim) // TODO(karalabe): Slim parsing can be omitted\n\t\t\tif err != nil {\n\t\t\t\tpanic(err) // Really shouldn't ever happen\n\t\t\t}\n\t\t\ttask.genTrie.update(hash[:], full)\n\t\t} else {\n\t\t\t// If the storage task is incomplete, explicitly delete the corresponding\n\t\t\t// account item from the account trie to ensure that all nodes along the\n\t\t\t// path to the incomplete storage trie are cleaned up.\n\t\t\tif err := task.genTrie.delete(hash[:])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2482,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(res.accounts))\n\n\t// Task filling persisted, push it the chunk marker forward to the first\n\t// account still missing data.\n\tfor i, hash := range res.hashes {\n\t\tif task.needCode[i] || task.needState[i] {\n\t\t\treturn\n\t\t}\n\t\ttask.Next = incHash(hash)\n\n\t\t// Remove the completion flag once the account range is pushed\n\t\t// forward. The leftover accounts will be skipped in the next\n\t\t// cycle.\n\t\tdelete(task.stateCompleted, hash)\n\t}\n\t// All accounts marked as complete, track if the entire task is done\n\ttask.done = !res.cont\n\n\t// Error out if there is any leftover completion flag.\n\tif task.done && len(task.stateCompleted) != 0 {\n\t\tpanic(fmt.Errorf(\"storage completion flags should be emptied, %d left\", len(task.stateCompleted)))\n\t}\n\t// Stack trie could have generated trie nodes, push them to disk (we need to\n\t// flush after finalizing task.done. It's fine even if we crash and lose this\n\t// write as it will only cause more data to be downloaded during heal.\n\tif task.done {\n\t\ttask.genTrie.commit(task.Last == common.MaxHash)\n\t\tif err := task.genBatch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2528,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(account))\n\t}\n\tfor _, node := range proof {\n\t\tsize += common.StorageSize(len(node))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering range of accounts\", \"hashes\", len(hashes), \"accounts\", len(accounts), \"proofs\", len(proof), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2641,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(code))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering set of bytecodes\", \"bytecodes\", len(bytecodes), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2744,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(common.HashLength * len(hashset))\n\t\thashCount += len(hashset)\n\t}\n\tfor _, slotset := range slots {\n\t\tfor _, slot := range slotset {\n\t\t\tsize += common.StorageSize(len(slot))\n\t\t}\n\t\tslotCount += len(slotset)\n\t}\n\tfor _, node := range proof {\n\t\tsize += common.StorageSize(len(node))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering ranges of storage slots\", \"accounts\", len(hashes), \"hashes\", hashCount, \"slots\", slotCount, \"proofs\", len(proof), \"size\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2888,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(node))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering set of healing trienodes\", \"trienodes\", len(trienodes), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2995,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(code))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering set of healing bytecodes\", \"bytecodes\", len(bytecodes), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0012",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 3100,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\ts.accountHealedBytes += common.StorageSize(1 + common.HashLength + len(blob))\n\t}\n\tif len(paths) == 2 {\n\t\trawdb.WriteStorageSnapshot(s.stateWriter, common.BytesToHash(paths[0]), common.BytesToHash(paths[1]), value)\n\t\ts.storageHealed += 1\n\t\ts.storageHealedBytes += common.StorageSize(1 + 2*common.HashLength + len(value))\n\t}\n\tif s.stateWriter.ValueSize() > ethdb.IdealBatchSize {\n\t\ts.stateWriter.Write() // It's fine to ignore the error here\n\t\ts.stateWriter.Reset()\n\t}\n\treturn nil\n}\n\n// hashSpace is the total size of the 256 bit hash space for accounts.\nvar hashSpace = new(big.Int).Exp(common.Big2, common.Big256, nil)\n\n// report calculates various status reports and provides it to the user.\nfunc (s *Syncer) report(force bool) {\n\tif len(s.tasks) > 0 {\n\t\ts.reportSyncProgress(force)\n\t\treturn\n\t}\n\ts.reportHealProgress(force)\n}\n\n// reportSyncProgress calculates various status reports and provides it to the user.\nfunc (s *Syncer) reportSyncProgress(force bool) {\n\t// Don't report all the events, just occasionally\n\tif !force && time.Since(s.logTime) < 8*time.Second {\n\t\treturn\n\t}\n\t// Don't report anything until we have a meaningful progress\n\tsynced := s.accountBytes + s.bytecodeBytes + s.storageBytes\n\tif synced == 0 {\n\t\treturn\n\t}\n\taccountGaps := new(big.Int)\n\tfor _, task := range s.tasks {\n\t\taccountGaps.Add(accountGaps, new(big.Int).Sub(task.Last.Big(), task.Next.Big()))\n\t}\n\taccountFills := new(big.Int).Sub(hashSpace, accountGaps)\n\tif accountFills.BitLen() == 0 {\n\t\treturn\n\t}\n\ts.logTime = time.Now()\n\testBytes := float64(new(big.Int).Div(\n\t\tnew(big.Int).Mul(new(big.Int).SetUint64(uint64(synced)), hashSpace),\n\t\taccountFills,\n\t).Uint64())\n\t// Don't report anything until we have a meaningful progress\n\tif estBytes < 1.0 {\n\t\treturn\n\t}\n\t// Cap the estimated state size using the synced size to avoid negative values\n\tif estBytes < float64(synced) {\n\t\testBytes = float64(synced)\n\t}\n\telapsed := time.Since(s.startTime)\n\testTime := elapsed / time.Duration(synced) * time.Duration(estBytes)\n\n\t// Create a mega progress report\n\tvar (\n\t\tprogress = fmt.Sprintf(\"%.2f%%\", float64(synced)*100/estBytes)\n\t\taccounts = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.accountSynced), s.accountBytes.TerminalString())\n\t\tstorage  = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.storageSynced), s.storageBytes.TerminalString())\n\t\tbytecode = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.bytecodeSynced), s.bytecodeBytes.TerminalString())\n\t)\n\tlog.Info(\"Syncing: state download in progress\", \"synced\", progress, \"state\", synced,\n\t\t\"accounts\", accounts, \"slots\", storage, \"codes\", bytecode, \"eta\", common.PrettyDuration(estTime-elapsed))\n}\n\n// reportHealProgress calculates various status reports and provides it to the user.\nfunc (s *Syncer) reportHealProgress(force bool) {\n\t// Don't report all the events, just occasionally\n\tif !force && time.Since(s.logTime) < 8*time.Second {\n\t\treturn\n\t}\n\ts.logTime = time.Now()\n\n\t// Create a mega progress report\n\tvar (\n\t\ttrienode = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.trienodeHealSynced), s.trienodeHealBytes.TerminalString())\n\t\tbytecode = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.bytecodeHealSynced), s.bytecodeHealBytes.TerminalString())\n\t\taccounts = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.accountHealed), s.accountHealedBytes.TerminalString())\n\t\tstorage  = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.storageHealed), s.storageHealedBytes.TerminalString())\n\t)\n\tlog.Info(\"Syncing: state healing in progress\", \"accounts\", accounts, \"slots\", storage,\n\t\t\"codes\", bytecode, \"nodes\", trienode, \"pending\", s.healer.scheduler.Pending())\n}\n\n// estimateRemainingSlots tries to determine roughly how many slots are left in\n// a contract storage, based on the number of keys and the last hash. This method\n// assumes that the hashes are lexicographically ordered and evenly distributed.\nfunc estimateRemainingSlots(hashes int, last common.Hash) (uint64, error) {\n\tif last == (common.Hash{}) {\n\t\treturn 0, errors.New(\"last hash empty\")\n\t}\n\tspace := new(big.Int).Mul(math.MaxBig256, big.NewInt(int64(hashes)))\n\tspace.Div(space, last.Big())\n\tif !space.IsUint64() {\n\t\t// Gigantic address space probably due to too few or malicious slots\n\t\treturn 0, errors.New(\"too few slots for estimation\")\n\t}\n\treturn space.Uint64() - uint64(hashes), nil\n}\n\n// capacitySort implements the Sort interface, allowing sorting by peer message\n// throughput. Note, callers should use sort.Reverse to get the desired effect\n// of highest capacity being at the front.\ntype capacitySort struct {\n\tids  []string\n\tcaps []int\n}\n\nfunc (s *capacitySort) Len() int {\n\treturn len(s.ids)\n}\n\nfunc (s *capacitySort) Less(i, j int) bool {\n\treturn s.caps[i] < s.caps[j]\n}\n\nfunc (s *capacitySort) Swap(i, j int) {\n\ts.ids[i], s.ids[j] = s.ids[j], s.ids[i]\n\ts.caps[i], s.caps[j] = s.caps[j], s.caps[i]\n}\n\n// healRequestSort implements the Sort interface, allowing sorting trienode\n// heal requests, which is a prerequisite for merging storage-requests.\ntype healRequestSort struct {\n\tpaths     []string\n\thashes    []common.Hash\n\tsyncPaths []trie.SyncPath\n}\n\nfunc (t *healRequestSort) Len() int {\n\treturn len(t.hashes)\n}\n\nfunc (t *healRequestSort) Less(i, j int) bool {\n\ta := t.syncPaths[i]\n\tb := t.syncPaths[j]\n\tswitch bytes.Compare(a[0], b[0]) {\n\tcase -1:\n\t\treturn true\n\tcase 1:\n\t\treturn false\n\t}\n\t// identical first part\n\tif len(a) < len(b) {\n\t\treturn true\n\t}\n\tif len(b) < len(a) {\n\t\treturn false\n\t}\n\tif len(a) == 2 {\n\t\treturn bytes.Compare(a[1], b[1]) < 0\n\t}\n\treturn false\n}\n\nfunc (t *healRequestSort) Swap(i, j int) {\n\tt.paths[i], t.paths[j] = t.paths[j], t.paths[i]\n\tt.hashes[i], t.hashes[j] = t.hashes[j], t.hashes[i]\n\tt.syncPaths[i], t.syncPaths[j] = t.syncPaths[j], t.syncPaths[i]\n}\n\n// Merge merges the pathsets, so that several storage requests concerning the\n// same account are merged into one, to reduce bandwidth.\n// OBS: This operation is moot if t has not first been sorted.\nfunc (t *healRequestSort) Merge() []TrieNodePathSet {\n\tvar result []TrieNodePathSet\n\tfor _, path := range t.syncPaths {\n\t\tpathset := TrieNodePathSet(path)\n\t\tif len(path) == 1 {\n\t\t\t// It's an account reference.\n\t\t\tresult = append(result, pathset)\n\t\t} else {\n\t\t\t// It's a storage reference.\n\t\t\tend := len(result) - 1\n\t\t\tif len(result) == 0 || !bytes.Equal(pathset[0], result[end][0]) {\n\t\t\t\t// The account doesn't match last, create a new entry.\n\t\t\t\tresult = append(result, pathset)\n\t\t\t} else {\n\t\t\t\t// It's the same account as the previous one, add to the storage\n\t\t\t\t// paths of that request.\n\t\t\t\tresult[end] = append(result[end], pathset[1])\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// sortByAccountPath takes hashes and paths, and sorts them. After that, it generates\n// the TrieNodePaths and merges paths which belongs to the same account path.\nfunc sortByAccountPath(paths []string, hashes []common.Hash) ([]string, []common.Hash, []trie.SyncPath, []TrieNodePathSet) {\n\tsyncPaths := make([]trie.SyncPath, len(paths))\n\tfor i, path := range paths {\n\t\tsyncPaths[i] = trie.NewSyncPath([]byte(path))\n\t}\n\tn := &healRequestSort{paths, hashes, syncPaths}\n\tsort.Sort(n)\n\tpathsets := n.Merge()\n\treturn n.paths, n.hashes, n.syncPaths, pathsets\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0013",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2361,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= trienodeHealThrottleIncrease\n\t\t} else {\n\t\t\ts.trienodeHealThrottle /= trienodeHealThrottleDecrease\n\t\t}\n\t\tif s.trienodeHealThrottle > maxTrienodeHealThrottle {\n\t\t\ts.trienodeHealThrottle = maxTrienodeHealThrottle\n\t\t} else if s.trienodeHealThrottle < minTrienodeHealThrottle {\n\t\t\ts.trienodeHealThrottle = minTrienodeHealThrottle\n\t\t}\n\t\ts.trienodeHealThrottled = time.Now()\n\n\t\tlog.Debug(\"Updated trie node heal throttler\", \"rate\", s.trienodeHealRate, \"pending\", pending, \"throttle\", s.trienodeHealThrottle)\n\t}\n}\n\nfunc (s *Syncer) commitHealer(force bool) {\n\tif !force && s.healer.scheduler.MemSize() < ethdb.IdealBatchSize {\n\t\treturn\n\t}\n\tbatch := s.db.NewBatch()\n\tvar stateBatch ethdb.Batch\n\tvar err error\n\tif s.db.HasSeparateStateStore() {\n\t\tstateBatch = s.db.GetStateStore().NewBatch()\n\t\terr = s.healer.scheduler.Commit(batch, stateBatch)\n\t} else {\n\t\terr = s.healer.scheduler.Commit(batch, nil)\n\t}\n\tif err != nil {\n\t\tlog.Crit(\"Failed to commit healing data\", \"err\", err)\n\t}\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0014",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 570,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0015",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 598,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/snap/gentrie_test.go",
          "line": 47,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\treturn\n\t}\n\tvar path []byte\n\tif account {\n\t\t_, path = rawdb.ResolveAccountTrieNodeKey(key)\n\t} else {\n\t\t_, owner, inner := rawdb.ResolveStorageTrieNode(key)\n\t\tpath = append(owner.Bytes(), inner...)\n\t}\n\tr.paths = append(r.paths, string(path))\n\n\tif len(value) == 0 {\n\t\tr.hashes = append(r.hashes, common.Hash{})\n\t} else {\n\t\tr.hashes = append(r.hashes, crypto.Keccak256Hash(value))\n\t}\n}\n\n// updates returns a set of effective mutations. Multiple mutations targeting\n// the same node path will be merged in FIFO order.\nfunc (r *replayer) modifies() map[string]common.Hash {\n\tset := make(map[string]common.Hash)\n\tfor i, path := range r.paths {\n\t\tset[path] = r.hashes[i]\n\t}\n\treturn set\n}\n\n// updates returns the number of updates.\nfunc (r *replayer) updates() int {\n\tvar count int\n\tfor _, hash := range r.modifies() {\n\t\tif hash == (common.Hash{}) {\n\t\t\tcontinue\n\t\t}\n\t\tcount++\n\t}\n\treturn count\n}\n\n// Put inserts the given value into the key-value data store.\nfunc (r *replayer) Put(key []byte, value []byte) error {\n\tr.decode(key, value)\n\treturn nil\n}\n\n// Delete removes the key from the key-value data store.\nfunc (r *replayer) Delete(key []byte) error {\n\tr.decode(key, nil)\n\treturn nil\n}\n\nfunc byteToHex(str []byte) []byte {\n\tl := len(str) * 2\n\tvar nibbles = make([]byte, l)\n\tfor i, b := range str {\n\t\tnibbles[i*2] = b / 16\n\t\tnibbles[i*2+1] = b % 16\n\t}\n\treturn nibbles\n}\n\n// innerNodes returns the internal nodes narrowed by two boundaries along with\n// the leftmost and rightmost sub-trie roots.\nfunc innerNodes(first, last []byte, includeLeft, includeRight bool, nodes map[string]common.Hash, t *testing.T) (map[string]common.Hash, []byte, []byte) {\n\tvar (\n\t\tleftRoot  []byte\n\t\trightRoot []byte\n\t\tfirstHex  = byteToHex(first)\n\t\tlastHex   = byteToHex(last)\n\t\tinner     = make(map[string]common.Hash)\n\t)\n\tfor path, hash := range nodes {\n\t\tif hash == (common.Hash{}) {\n\t\t\tt.Fatalf(\"Unexpected deletion, %v\", []byte(path))\n\t\t}\n\t\t// Filter out the siblings on the left side or the left boundary nodes.\n\t\tif !includeLeft && (bytes.Compare(firstHex, []byte(path)) > 0 || bytes.HasPrefix(firstHex, []byte(path))) {\n\t\t\tcontinue\n\t\t}\n\t\t// Filter out the siblings on the right side or the right boundary nodes.\n\t\tif !includeRight && (bytes.Compare(lastHex, []byte(path)) < 0 || bytes.HasPrefix(lastHex, []byte(path))) {\n\t\t\tcontinue\n\t\t}\n\t\tinner[path] = hash\n\n\t\t// Track the path of the leftmost sub trie root\n\t\tif leftRoot == nil || bytes.Compare(leftRoot, []byte(path)) > 0 {\n\t\t\tleftRoot = []byte(path)\n\t\t}\n\t\t// Track the path of the rightmost sub trie root\n\t\tif rightRoot == nil ||\n\t\t\t(bytes.Compare(rightRoot, []byte(path)) < 0) ||\n\t\t\t(bytes.Compare(rightRoot, []byte(path)) > 0 && bytes.HasPrefix(rightRoot, []byte(path))) {\n\t\t\trightRoot = []byte(path)\n\t\t}\n\t}\n\treturn inner, leftRoot, rightRoot\n}\n\nfunc buildPartial(owner common.Hash, db ethdb.KeyValueReader, batch ethdb.Batch, entries []*kv, first, last int) *replayer {\n\ttr := newPathTrie(owner, first != 0, db, batch)\n\tfor i := first",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/snap/sync_test.go",
          "line": 269,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(32 + len(entry.v))\n\t\t}\n\t\t// If we've exceeded the request threshold, abort\n\t\tif bytes.Compare(entry.k, limit[:]) >= 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\t// Unless we send the entire trie, we need to supply proofs\n\t// Actually, we need to supply proofs either way! This seems to be an implementation\n\t// quirk in go-ethereum\n\tproof := trienode.NewProofSet()\n\tif err := t.accountTrie.Prove(origin[:], proof)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/snap/sync_test.go",
          "line": 341,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(32 + len(entry.v))\n\t\t\tif bytes.Compare(entry.k, limitHash[:]) >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(keys) > 0 {\n\t\t\thashes = append(hashes, keys)\n\t\t\tslots = append(slots, vals)\n\t\t}\n\t\t// Generate the Merkle proofs for the first and last storage slot, but\n\t\t// only if the response was capped. If the entire storage trie included\n\t\t// in the response, no need for any proofs.\n\t\tif originHash != (common.Hash{}) || (abort && len(keys) > 0) {\n\t\t\t// If we're aborting, we need to prove the first and last item\n\t\t\t// This terminates the response (and thus the loop)\n\t\t\tproof := trienode.NewProofSet()\n\t\t\tstTrie := t.storageTries[account]\n\n\t\t\t// Here's a potential gotcha: when constructing the proof, we cannot\n\t\t\t// use the 'origin' slice directly, but must use the full 32-byte\n\t\t\t// hash form.\n\t\t\tif err := stTrie.Prove(originHash[:], proof)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/snap/sync_test.go",
          "line": 398,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(32 + len(entry.v))\n\t\t\tif size > max {\n\t\t\t\texit = true\n\t\t\t}\n\t\t}\n\t\tif i == len(accounts)-1 {\n\t\t\texit = true\n\t\t}\n\t\thashes = append(hashes, keys)\n\t\tslots = append(slots, vals)\n\n\t\tif exit {\n\t\t\t// If we're aborting, we need to prove the first and last item\n\t\t\t// This terminates the response (and thus the loop)\n\t\t\tproof := trienode.NewProofSet()\n\t\t\tstTrie := t.storageTries[account]\n\n\t\t\t// Here's a potential gotcha: when constructing the proof, we cannot\n\t\t\t// use the 'origin' slice directly, but must use the full 32-byte\n\t\t\t// hash form.\n\t\t\tif err := stTrie.Prove(origin[:], proof)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 80,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 99,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 114,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 127,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/handshake_test.go",
          "line": 81,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/handshake.go",
          "line": 69,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/eth/handshake.go",
          "line": 94,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/eth/handshake.go",
          "line": 154,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/dispatcher.go",
          "line": 200,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 97,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(rlpData))\n\t\t}\n\t\t// Advance to the next header of the query\n\t\tswitch {\n\t\tcase hashMode && query.Reverse:\n\t\t\t// Hash based traversal towards the genesis block\n\t\t\tancestor := query.Skip + 1\n\t\t\tif ancestor == 0 {\n\t\t\t\tunknown = true\n\t\t\t} else {\n\t\t\t\tquery.Origin.Hash, query.Origin.Number = chain.GetAncestor(query.Origin.Hash, query.Origin.Number, ancestor, &maxNonCanonical)\n\t\t\t\tunknown = (query.Origin.Hash == common.Hash{})\n\t\t\t}\n\t\tcase hashMode && !query.Reverse:\n\t\t\t// Hash based traversal towards the leaf block\n\t\t\tvar (\n\t\t\t\tcurrent = origin.Number.Uint64()\n\t\t\t\tnext    = current + query.Skip + 1\n\t\t\t)\n\t\t\tif next <= current {\n\t\t\t\tinfos, _ := json.MarshalIndent(peer.Peer.Info(), \"\", \"  \")\n\t\t\t\tpeer.Log().Warn(\"GetBlockHeaders skip overflow attack\", \"current\", current, \"skip\", query.Skip, \"next\", next, \"attacker\", infos)\n\t\t\t\tunknown = true\n\t\t\t} else {\n\t\t\t\tif header := chain.GetHeaderByNumber(next)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 259,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(enc)\n\t}\n\treturn bodies\n}\n\nfunc handleGetReceipts68(backend Backend, msg Decoder, peer *Peer) error {\n\t// Decode the block receipts retrieval message\n\tvar query GetReceiptsPacket\n\tif err := msg.Decode(&query)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 316,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(results)\n\t}\n\treturn receipts\n}\n\n// serviceGetReceiptsQuery69 assembles the response to a receipt query.\n// It does not send the bloom filters for the receipts\nfunc serviceGetReceiptsQuery69(chain *core.BlockChain, query GetReceiptsRequest) []rlp.RawValue {\n\t// Gather state data until the fetch or network limits is reached\n\tvar (\n\t\tbytes    int\n\t\treceipts []rlp.RawValue\n\t)\n\tfor lookups, hash := range query {\n\t\tif bytes >= softResponseLimit || len(receipts) >= maxReceiptsServe ||\n\t\t\tlookups >= 2*maxReceiptsServe {\n\t\t\tbreak\n\t\t}\n\t\t// Retrieve the requested block's receipts\n\t\tresults := chain.GetReceiptsRLP(hash)\n\t\tif results == nil {\n\t\t\tif header := chain.GetHeaderByHash(hash)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 353,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(results)\n\t}\n\treturn receipts\n}\n\nfunc handleNewBlockhashes(backend Backend, msg Decoder, peer *Peer) error {\n\t// A batch of new block announcements just arrived\n\tann := new(NewBlockHashesPacket)\n\tif err := msg.Decode(ann)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 530,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(encoded)\n\t}\n\treturn hashes, txs\n}\n\nfunc handleTransactions(backend Backend, msg Decoder, peer *Peer) error {\n\t// Transactions arrived, make sure we have a valid and fresh chain to handle them\n\tif !backend.AcceptTxs() {\n\t\treturn nil\n\t}\n\t// Transactions can be processed, parse all of them and deliver to the pool\n\tvar txs TransactionsPacket\n\tif err := msg.Decode(&txs)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 225,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 253,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 277,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 294,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 314,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 335,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 344,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 352,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 488,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 499,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 363,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 378,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 472,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 546,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 844,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/protocols/eth/broadcast.go",
          "line": 86,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(tx.Size())\n\t\t\t\t}\n\t\t\t\thashesCount++\n\t\t\t}\n\t\t\tqueue = queue[:copy(queue, queue[hashesCount:])]\n\n\t\t\t// If there's anything available to transfer, fire up an async writer\n\t\t\tif len(txs) > 0 {\n\t\t\t\tdone = make(chan struct{})\n\t\t\t\tgo func() {\n\t\t\t\t\tif err := p.SendTransactions(txs)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/protocols/eth/broadcast.go",
          "line": 160,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Shift and trim queue\n\t\t\tqueue = queue[:copy(queue, queue[count:])]\n\n\t\t\t// If there's anything available to transfer, fire up an async writer\n\t\t\tif len(pending) > 0 {\n\t\t\t\tdone = make(chan struct{})\n\t\t\t\tgopool.Submit(func() {\n\t\t\t\t\tif err := p.sendPooledTransactionHashes(pending, pendingTypes, pendingSizes)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/logger/logger.go",
          "line": 182,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32 {\n\t\t\tend := i + 32\n\t\t\tif end > len(s.Memory) {\n\t\t\t\tend = len(s.Memory)\n\t\t\t}\n\t\t\tmemory = append(memory, fmt.Sprintf(\"%x\", s.Memory[i:end]))\n\t\t}\n\t\tmsg.Memory = &memory\n\t}\n\tif len(s.Storage) > 0 {\n\t\tstorage := make(map[string]string)\n\t\tfor i, storageValue := range s.Storage {\n\t\t\tstorage[fmt.Sprintf(\"%x\", i)] = fmt.Sprintf(\"%x\", storageValue)\n\t\t}\n\t\tmsg.Storage = &storage\n\t}\n\telement, _ := json.Marshal(msg)\n\treturn element\n}\n\n// StructLogger is an EVM state logger and implements EVMLogger.\n//\n// StructLogger can capture state based on the given Log configuration and also keeps\n// a track record of modified storage which is used in reporting snapshots of the\n// contract their storage.\n//\n// A StructLogger can either yield it's output immediately (streaming) or store for\n// later output.\ntype StructLogger struct {\n\tcfg Config\n\tenv *tracing.VMContext\n\n\tstorage map[common.Address]Storage\n\toutput  []byte\n\terr     error\n\tusedGas uint64\n\n\twriter     io.Writer         // If set, the logger will stream instead of store logs\n\tlogs       []json.RawMessage // buffer of json-encoded logs\n\tresultSize int\n\n\tinterrupt atomic.Bool // Atomic flag to signal execution interruption\n\treason    error       // Textual reason for the interruption\n\tskip      bool        // skip processing hooks.\n}\n\n// NewStreamingStructLogger returns a new streaming logger.\nfunc NewStreamingStructLogger(cfg *Config, writer io.Writer) *StructLogger {\n\tl := NewStructLogger(cfg)\n\tl.writer = writer\n\treturn l\n}\n\n// NewStructLogger construct a new (non-streaming) struct logger.\nfunc NewStructLogger(cfg *Config) *StructLogger {\n\tlogger := &StructLogger{\n\t\tstorage: make(map[common.Address]Storage),\n\t\tlogs:    make([]json.RawMessage, 0),\n\t}\n\tif cfg != nil {\n\t\tlogger.cfg = *cfg\n\t}\n\treturn logger\n}\n\nfunc (l *StructLogger) Hooks() *tracing.Hooks {\n\treturn &tracing.Hooks{\n\t\tOnTxStart:                 l.OnTxStart,\n\t\tOnTxEnd:                   l.OnTxEnd,\n\t\tOnSystemCallStartV2:       l.OnSystemCallStart,\n\t\tOnSystemCallEnd:           l.OnSystemCallEnd,\n\t\tOnExit:                    l.OnExit,\n\t\tOnOpcode:                  l.OnOpcode,\n\t\tOnSystemTxFixIntrinsicGas: l.OnSystemTxFixIntrinsicGas,\n\t}\n}\n\n// OnOpcode logs a new structured log message and pushes it out to the environment\n//\n// OnOpcode also tracks SLOAD/SSTORE ops to track storage change.\nfunc (l *StructLogger) OnOpcode(pc uint64, opcode byte, gas, cost uint64, scope tracing.OpContext, rData []byte, depth int, err error) {\n\t// If tracing was interrupted, exit\n\tif l.interrupt.Load() {\n\t\treturn\n\t}\n\t// Processing a system call.\n\tif l.skip {\n\t\treturn\n\t}\n\t// check if already accumulated the size of the response.\n\tif l.cfg.Limit != 0 && l.resultSize > l.cfg.Limit {\n\t\treturn\n\t}\n\tvar (\n\t\top           = vm.OpCode(opcode)\n\t\tmemory       = scope.MemoryData()\n\t\tcontractAddr = scope.Address()\n\t\tstack        = scope.StackData()\n\t\tstackLen     = len(stack)\n\t)\n\tlog := StructLog{pc, op, gas, cost, nil, len(memory), nil, nil, nil, depth, l.env.StateDB.GetRefund(), err}\n\tif l.cfg.EnableMemory {\n\t\tlog.Memory = memory\n\t}\n\tif !l.cfg.DisableStack {\n\t\tlog.Stack = scope.StackData()\n\t}\n\tif l.cfg.EnableReturnData {\n\t\tlog.ReturnData = rData\n\t}\n\n\t// Copy a snapshot of the current storage to a new container\n\tvar storage Storage\n\tif !l.cfg.DisableStorage && (op == vm.SLOAD || op == vm.SSTORE) {\n\t\t// initialise new changed values storage container for this contract\n\t\t// if not present.\n\t\tif l.storage[contractAddr] == nil {\n\t\t\tl.storage[contractAddr] = make(Storage)\n\t\t}\n\t\t// capture SLOAD opcodes and record the read entry in the local storage\n\t\tif op == vm.SLOAD && stackLen >= 1 {\n\t\t\tvar (\n\t\t\t\taddress = common.Hash(stack[stackLen-1].Bytes32())\n\t\t\t\tvalue   = l.env.StateDB.GetState(contractAddr, address)\n\t\t\t)\n\t\t\tl.storage[contractAddr][address] = value\n\t\t\tstorage = maps.Clone(l.storage[contractAddr])\n\t\t} else if op == vm.SSTORE && stackLen >= 2 {\n\t\t\t// capture SSTORE opcodes and record the written entry in the local storage.\n\t\t\tvar (\n\t\t\t\tvalue   = common.Hash(stack[stackLen-2].Bytes32())\n\t\t\t\taddress = common.Hash(stack[stackLen-1].Bytes32())\n\t\t\t)\n\t\t\tl.storage[contractAddr][address] = value\n\t\t\tstorage = maps.Clone(l.storage[contractAddr])\n\t\t}\n\t}\n\tlog.Storage = storage\n\n\t// create a log\n\tif l.writer == nil {\n\t\tentry := log.toLegacyJSON()\n\t\tl.resultSize += len(entry)\n\t\tl.logs = append(l.logs, entry)\n\t\treturn\n\t}\n\tlog.WriteTo(l.writer)\n}\n\n// OnExit is called a call frame finishes processing.\nfunc (l *StructLogger) OnExit(depth int, output []byte, gasUsed uint64, err error, reverted bool) {\n\tif depth != 0 {\n\t\treturn\n\t}\n\tif l.skip {\n\t\treturn\n\t}\n\tl.output = output\n\tl.err = err\n\t// TODO @holiman, should we output the per-scope output?\n\t//if l.cfg.Debug {\n\t//\tfmt.Printf(\"%#x\\n\", output)\n\t//\tif err != nil {\n\t//\t\tfmt.Printf(\" error: %v\\n\", err)\n\t//\t}\n\t//}\n}\n\nfunc (l *StructLogger) GetResult() (json.RawMessage, error) {\n\t// Tracing aborted\n\tif l.reason != nil {\n\t\treturn nil, l.reason\n\t}\n\tfailed := l.err != nil\n\treturnData := common.CopyBytes(l.output)\n\t// Return data when successful and revert reason when reverted, otherwise empty.\n\tif failed && !errors.Is(l.err, vm.ErrExecutionReverted) {\n\t\treturnData = []byte{}\n\t}\n\treturn json.Marshal(&ExecutionResult{\n\t\tGas:         l.usedGas,\n\t\tFailed:      failed,\n\t\tReturnValue: returnData,\n\t\tStructLogs:  l.logs,\n\t})\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (l *StructLogger) Stop(err error) {\n\tl.reason = err\n\tl.interrupt.Store(true)\n}\n\nfunc (l *StructLogger) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tl.env = env\n}\nfunc (l *StructLogger) OnSystemCallStart(env *tracing.VMContext) {\n\tl.skip = true\n}\n\nfunc (l *StructLogger) OnSystemCallEnd() {\n\tl.skip = false\n}\n\nfunc (l *StructLogger) OnTxEnd(receipt *types.Receipt, err error) {\n\tif err != nil {\n\t\t// Don't override vm error\n\t\tif l.err == nil {\n\t\t\tl.err = err\n\t\t}\n\t\treturn\n\t}\n\tif receipt != nil {\n\t\tl.usedGas = receipt.GasUsed\n\t}\n}\n\nfunc (l *StructLogger) OnSystemTxFixIntrinsicGas(intrinsicGas uint64) {\n\tl.usedGas -= intrinsicGas\n}\n\n// Error returns the VM error captured by the trace.\nfunc (l *StructLogger) Error() error { return l.err }\n\n// Output returns the VM return value captured by the trace.\nfunc (l *StructLogger) Output() []byte { return l.output }\n\n// WriteTrace writes a formatted trace to the given writer\n// @deprecated\nfunc WriteTrace(writer io.Writer, logs []StructLog) {\n\tfor _, log := range logs {\n\t\tlog.WriteTo(writer)\n\t}\n}\n\ntype mdLogger struct {\n\tout  io.Writer\n\tcfg  *Config\n\tenv  *tracing.VMContext\n\tskip bool\n}\n\n// NewMarkdownLogger creates a logger which outputs information in a format adapted\n// for human readability, and is also a valid markdown table\nfunc NewMarkdownLogger(cfg *Config, writer io.Writer) *mdLogger {\n\tl := &mdLogger{out: writer, cfg: cfg}\n\tif l.cfg == nil {\n\t\tl.cfg = &Config{}\n\t}\n\treturn l\n}\n\nfunc (t *mdLogger) Hooks() *tracing.Hooks {\n\treturn &tracing.Hooks{\n\t\tOnTxStart:           t.OnTxStart,\n\t\tOnSystemCallStartV2: t.OnSystemCallStart,\n\t\tOnSystemCallEnd:     t.OnSystemCallEnd,\n\t\tOnEnter:             t.OnEnter,\n\t\tOnExit:              t.OnExit,\n\t\tOnOpcode:            t.OnOpcode,\n\t\tOnFault:             t.OnFault,\n\t}\n}\n\nfunc (t *mdLogger) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tt.env = env\n}\n\nfunc (t *mdLogger) OnSystemCallStart(env *tracing.VMContext) {\n\tt.skip = true\n}\n\nfunc (t *mdLogger) OnSystemCallEnd() {\n\tt.skip = false\n}\n\nfunc (t *mdLogger) OnEnter(depth int, typ byte, from common.Address, to common.Address, input []byte, gas uint64, value *big.Int) {\n\tif t.skip {\n\t\treturn\n\t}\n\tif depth != 0 {\n\t\treturn\n\t}\n\tif create := vm.OpCode(typ) == vm.CREATE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/tracers/logger/logger.go",
          "line": 400,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= intrinsicGas\n}\n\n// Error returns the VM error captured by the trace.\nfunc (l *StructLogger) Error() error { return l.err }\n\n// Output returns the VM return value captured by the trace.\nfunc (l *StructLogger) Output() []byte { return l.output }\n\n// WriteTrace writes a formatted trace to the given writer\n// @deprecated\nfunc WriteTrace(writer io.Writer, logs []StructLog) {\n\tfor _, log := range logs {\n\t\tlog.WriteTo(writer)\n\t}\n}\n\ntype mdLogger struct {\n\tout  io.Writer\n\tcfg  *Config\n\tenv  *tracing.VMContext\n\tskip bool\n}\n\n// NewMarkdownLogger creates a logger which outputs information in a format adapted\n// for human readability, and is also a valid markdown table\nfunc NewMarkdownLogger(cfg *Config, writer io.Writer) *mdLogger {\n\tl := &mdLogger{out: writer, cfg: cfg}\n\tif l.cfg == nil {\n\t\tl.cfg = &Config{}\n\t}\n\treturn l\n}\n\nfunc (t *mdLogger) Hooks() *tracing.Hooks {\n\treturn &tracing.Hooks{\n\t\tOnTxStart:           t.OnTxStart,\n\t\tOnSystemCallStartV2: t.OnSystemCallStart,\n\t\tOnSystemCallEnd:     t.OnSystemCallEnd,\n\t\tOnEnter:             t.OnEnter,\n\t\tOnExit:              t.OnExit,\n\t\tOnOpcode:            t.OnOpcode,\n\t\tOnFault:             t.OnFault,\n\t}\n}\n\nfunc (t *mdLogger) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tt.env = env\n}\n\nfunc (t *mdLogger) OnSystemCallStart(env *tracing.VMContext) {\n\tt.skip = true\n}\n\nfunc (t *mdLogger) OnSystemCallEnd() {\n\tt.skip = false\n}\n\nfunc (t *mdLogger) OnEnter(depth int, typ byte, from common.Address, to common.Address, input []byte, gas uint64, value *big.Int) {\n\tif t.skip {\n\t\treturn\n\t}\n\tif depth != 0 {\n\t\treturn\n\t}\n\tif create := vm.OpCode(typ) == vm.CREATE",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/native/erc7562.go",
          "line": 272,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\n\tif errors.Is(err, vm.ErrCodeStoreOutOfGas) || errors.Is(err, vm.ErrOutOfGas) {\n\t\tcall.OutOfGas = true\n\t}\n\tcall.GasUsed = gasUsed\n\tcall.processOutput(output, err, reverted)\n\t// Nest call into parent.\n\tt.callstackWithOpcodes[size-1].Calls = append(t.callstackWithOpcodes[size-1].Calls, call)\n}\n\nfunc (t *erc7562Tracer) OnTxEnd(receipt *types.Receipt, err error) {\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\t// Error happened during tx validation.\n\tif err != nil {\n\t\treturn\n\t}\n\tt.callstackWithOpcodes[0].GasUsed = receipt.GasUsed\n\tif t.config.WithLog {\n\t\t// Logs are not emitted when the call fails\n\t\tt.clearFailedLogs(&t.callstackWithOpcodes[0], false)\n\t}\n}\n\nfunc (t *erc7562Tracer) OnLog(log1 *types.Log) {\n\t// Only logs need to be captured via opcode processing\n\tif !t.config.WithLog {\n\t\treturn\n\t}\n\t// Skip if tracing was interrupted\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tl := callLog{\n\t\tAddress:  log1.Address,\n\t\tTopics:   log1.Topics,\n\t\tData:     log1.Data,\n\t\tPosition: hexutil.Uint(len(t.callstackWithOpcodes[len(t.callstackWithOpcodes)-1].Calls)),\n\t}\n\tt.callstackWithOpcodes[len(t.callstackWithOpcodes)-1].Logs = append(t.callstackWithOpcodes[len(t.callstackWithOpcodes)-1].Logs, l)\n}\n\n// GetResult returns the json-encoded nested list of call traces, and any\n// error arising from the encoding or forceful termination (via `Stop`).\nfunc (t *erc7562Tracer) GetResult() (json.RawMessage, error) {\n\tif t.interrupt.Load() {\n\t\treturn nil, t.reason\n\t}\n\tif len(t.callstackWithOpcodes) != 1 {\n\t\treturn nil, errors.New(\"incorrect number of top-level calls\")\n\t}\n\n\tkeccak := make([][]byte, 0, len(t.callstackWithOpcodes[0].KeccakPreimages))\n\tfor k := range t.keccakPreimages {\n\t\tkeccak = append(keccak, []byte(k))\n\t}\n\tt.callstackWithOpcodes[0].KeccakPreimages = keccak\n\tslices.SortFunc(keccak, func(a, b []byte) int {\n\t\treturn bytes.Compare(a, b)\n\t})\n\n\tenc, err := json.Marshal(t.callstackWithOpcodes[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn enc, t.reason\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (t *erc7562Tracer) Stop(err error) {\n\tt.reason = err\n\tt.interrupt.Store(true)\n}\n\n// clearFailedLogs clears the logs of a callframe and all its children\n// in case of execution failure.\nfunc (t *erc7562Tracer) clearFailedLogs(cf *callFrameWithOpcodes, parentFailed bool) {\n\tfailed := cf.failed() || parentFailed\n\t// Clear own logs\n\tif failed {\n\t\tcf.Logs = nil\n\t}\n\tfor i := range cf.Calls {\n\t\tt.clearFailedLogs(&cf.Calls[i], failed)\n\t}\n}\n\nfunc (t *erc7562Tracer) OnOpcode(pc uint64, op byte, gas, cost uint64, scope tracing.OpContext, rData []byte, depth int, err error) {\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tvar (\n\t\topcode          = vm.OpCode(op)\n\t\topcodeWithStack *opcodeWithPartialStack\n\t\tstackSize       = len(scope.StackData())\n\t\tstackLimit      = min(stackSize, t.config.StackTopItemsSize)\n\t\tstackTopItems   = make([]uint256.Int, stackLimit)\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/native/4byte.go",
          "line": 85,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n}\n\nfunc (t *fourByteTracer) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\t// Update list of precompiles based on current block\n\trules := t.chainConfig.Rules(env.BlockNumber, env.Random != nil, env.Time)\n\tt.activePrecompiles = vm.ActivePrecompiles(rules)\n}\n\n// OnEnter is called when EVM enters a new scope (via call, create or selfdestruct).\nfunc (t *fourByteTracer) OnEnter(depth int, opcode byte, from common.Address, to common.Address, input []byte, gas uint64, value *big.Int) {\n\t// Skip if tracing was interrupted\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tif len(input) < 4 {\n\t\treturn\n\t}\n\top := vm.OpCode(opcode)\n\t// primarily we want to avoid CREATE/CREATE2/SELFDESTRUCT\n\tif op != vm.DELEGATECALL && op != vm.STATICCALL &&\n\t\top != vm.CALL && op != vm.CALLCODE {\n\t\treturn\n\t}\n\t// Skip any pre-compile invocations, those are just fancy opcodes\n\tif t.isPrecompiled(to) {\n\t\treturn\n\t}\n\tt.store(input[0:4], len(input)-4)\n}\n\n// GetResult returns the json-encoded nested list of call traces, and any\n// error arising from the encoding or forceful termination (via `Stop`).\nfunc (t *fourByteTracer) GetResult() (json.RawMessage, error) {\n\tres, err := json.Marshal(t.ids)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn res, t.reason\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (t *fourByteTracer) Stop(err error) {\n\tt.reason = err\n\tt.interrupt.Store(true)\n}\n\nfunc bytesToHex(s []byte) string {\n\treturn \"0x\" + common.Bytes2Hex(s)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/native/call.go",
          "line": 204,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\n\tcall.GasUsed = gasUsed\n\tcall.processOutput(output, err, reverted)\n\t// Nest call into parent.\n\tt.callstack[size-1].Calls = append(t.callstack[size-1].Calls, call)\n}\n\nfunc (t *callTracer) captureEnd(output []byte, gasUsed uint64, err error, reverted bool) {\n\tif len(t.callstack) != 1 {\n\t\treturn\n\t}\n\tt.callstack[0].processOutput(output, err, reverted)\n}\n\nfunc (t *callTracer) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tt.gasLimit = tx.Gas()\n}\n\nfunc (t *callTracer) OnTxEnd(receipt *types.Receipt, err error) {\n\t// Error happened during tx validation.\n\tif err != nil {\n\t\treturn\n\t}\n\tif receipt != nil {\n\t\tt.callstack[0].GasUsed = receipt.GasUsed\n\t}\n\tif t.config.WithLog {\n\t\t// Logs are not emitted when the call fails\n\t\tclearFailedLogs(&t.callstack[0], false)\n\t}\n}\n\nfunc (t *callTracer) OnSystemTxFixIntrinsicGas(intrinsicGas uint64) {\n\tt.callstack[0].GasUsed -= intrinsicGas\n}\n\nfunc (t *callTracer) OnLog(log *types.Log) {\n\t// Only logs need to be captured via opcode processing\n\tif !t.config.WithLog {\n\t\treturn\n\t}\n\t// Avoid processing nested calls when only caring about top call\n\tif t.config.OnlyTopCall && t.depth > 0 {\n\t\treturn\n\t}\n\t// Skip if tracing was interrupted\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tl := callLog{\n\t\tAddress:  log.Address,\n\t\tTopics:   log.Topics,\n\t\tData:     log.Data,\n\t\tPosition: hexutil.Uint(len(t.callstack[len(t.callstack)-1].Calls)),\n\t}\n\tt.callstack[len(t.callstack)-1].Logs = append(t.callstack[len(t.callstack)-1].Logs, l)\n}\n\n// GetResult returns the json-encoded nested list of call traces, and any\n// error arising from the encoding or forceful termination (via `Stop`).\nfunc (t *callTracer) GetResult() (json.RawMessage, error) {\n\tif len(t.callstack) != 1 {\n\t\treturn nil, errors.New(\"incorrect number of top-level calls\")\n\t}\n\n\tres, err := json.Marshal(t.callstack[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn res, t.reason\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (t *callTracer) Stop(err error) {\n\tt.reason = err\n\tt.interrupt.Store(true)\n}\n\n// clearFailedLogs clears the logs of a callframe and all its children\n// in case of execution failure.\nfunc clearFailedLogs(cf *callFrame, parentFailed bool) {\n\tfailed := cf.failed() || parentFailed\n\t// Clear own logs\n\tif failed {\n\t\tcf.Logs = nil\n\t}\n\tfor i := range cf.Calls {\n\t\tclearFailedLogs(&cf.Calls[i], failed)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=1}while(carry>0){r[i++]=carry%base",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=base",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=base",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=carry}}trim(r)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=carry}}trim(r)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=quotientDigit*divisor[i]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=remainder[shift+i]-(carry-q*base)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=remainder[shift+i]-base+divisor[i]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=carry}result[shift]=quotientDigit}remainder=divModSmall(remainder,lambda)[0]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=zeros.slice(digit.length)+digit}var sign=this.sign?\"-\":\"\"",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=new Array(exp+1).join(\"0\")",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0012",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=powers2Length-1}return result.multiply(powersOfTwo[n])}",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0013",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=powers2Length-1}remQuo=divModAny(result,powersOfTwo[n])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0014",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=1){sum=sum.multiply(highestPower2).add(bigInt(result[i]))}return sum}BigInteger.prototype.not=function(){return this.negate().prev()}",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0015",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=text.length-decimalPlace-1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0016",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=l",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0017",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-=l}trim(r)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/js/tracer_test.go",
          "line": 113,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/eth/tracers/js/tracer_test.go",
          "line": 143,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/eth/tracers/live/supply.go",
          "line": 265,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\n\t// In case of a revert, we can drop the call and all its subcalls.\n\t// Caution, that this has to happen after popping the call from the stack.\n\tif reverted {\n\t\treturn\n\t}\n\ts.txCallstack[size-1].calls = append(s.txCallstack[size-1].calls, call)\n}\n\nfunc (s *supplyTracer) onClose() {\n\tif err := s.logger.Close()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/enr/enr.go",
          "line": 109,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rlp.BytesSize(r.signature)\n\tfor _, p := range r.pairs {\n\t\tsize += rlp.StringSize(p.k)\n\t\tsize += uint64(len(p.v))\n\t}\n\treturn rlp.ListSize(size)\n}\n\n// Seq returns the sequence number.\nfunc (r *Record) Seq() uint64 {\n\treturn r.seq\n}\n\n// SetSeq updates the record sequence number. This invalidates any signature on the record.\n// Calling SetSeq is usually not required because setting any key in a signed record\n// increments the sequence number.\nfunc (r *Record) SetSeq(s uint64) {\n\tr.signature = nil\n\tr.raw = nil\n\tr.seq = s\n}\n\n// Load retrieves the value of a key/value pair. The given Entry must be a pointer and will\n// be set to the value of the entry in the record.\n//\n// Errors returned by Load are wrapped in KeyError. You can distinguish decoding errors\n// from missing keys using the IsNotFound function.\nfunc (r *Record) Load(e Entry) error {\n\ti := sort.Search(len(r.pairs), func(i int) bool { return r.pairs[i].k >= e.ENRKey() })\n\tif i < len(r.pairs) && r.pairs[i].k == e.ENRKey() {\n\t\tif err := rlp.DecodeBytes(r.pairs[i].v, e)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/v5_udp_test.go",
          "line": 930,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/ntp.go",
          "line": 108,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= drifts[i]\n\t}\n\treturn drift / time.Duration(measurements), nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/v4_lookup_test.go",
          "line": 259,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(keys)\n\t}\n\treturn n\n}\n\nfunc (tn *preminedTestnet) nodes() []*enode.Node {\n\tresult := make([]*enode.Node, 0, tn.len())\n\tfor dist, keys := range tn.dists {\n\t\tfor index := range keys {\n\t\t\tresult = append(result, tn.node(dist, index))\n\t\t}\n\t}\n\tsortByID(result)\n\treturn result\n}\n\nfunc (tn *preminedTestnet) node(dist, index int) *enode.Node {\n\tkey := tn.dists[dist][index]\n\trec := new(enr.Record)\n\trec.Set(enr.IP{127, byte(dist >> 8), byte(dist), byte(index)})\n\trec.Set(enr.UDP(5000))\n\tenode.SignV4(rec, key)\n\tn, _ := enode.New(enode.ValidSchemes, rec)\n\treturn n\n}\n\nfunc (tn *preminedTestnet) nodeByAddr(addr netip.AddrPort) (*enode.Node, *ecdsa.PrivateKey) {\n\tip := addr.Addr().As4()\n\tdist := int(ip[1])<<8 + int(ip[2])\n\tindex := int(ip[3])\n\tkey := tn.dists[dist][index]\n\treturn tn.node(dist, index), key\n}\n\nfunc (tn *preminedTestnet) nodesAtDistance(dist int) []v4wire.Node {\n\tresult := make([]v4wire.Node, len(tn.dists[dist]))\n\tfor i := range result {\n\t\tresult[i] = nodeToRPC(tn.node(dist, i))\n\t}\n\treturn result\n}\n\nfunc (tn *preminedTestnet) neighborsAtDistances(base *enode.Node, distances []uint, elems int) []*enode.Node {\n\tvar result []*enode.Node\n\tfor d := range lookupTestnet.dists {\n\t\tfor i := range lookupTestnet.dists[d] {\n\t\t\tn := lookupTestnet.node(d, i)\n\t\t\td := enode.LogDist(base.ID(), n.ID())\n\t\t\tif slices.Contains(distances, uint(d)) {\n\t\t\t\tresult = append(result, n)\n\t\t\t\tif len(result) >= elems {\n\t\t\t\t\treturn result\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\nfunc (tn *preminedTestnet) closest(n int) (nodes []*enode.Node) {\n\tfor d := range tn.dists {\n\t\tfor i := range tn.dists[d] {\n\t\t\tnodes = append(nodes, tn.node(d, i))\n\t\t}\n\t}\n\tslices.SortFunc(nodes, func(a, b *enode.Node) int {\n\t\treturn enode.DistCmp(tn.target.ID(), a.ID(), b.ID())\n\t})\n\treturn nodes[:n]\n}\n\nvar _ = (*preminedTestnet).mine // avoid linter warning about mine being dead code.\n\n// mine generates a testnet struct literal with nodes at\n// various distances to the network's target.\nfunc (tn *preminedTestnet) mine() {\n\t// Clear existing slices first (useful when re-mining).\n\tfor i := range tn.dists {\n\t\ttn.dists[i] = nil\n\t}\n\n\ttargetSha := tn.target.ID()\n\tfound, need := 0, 40\n\tfor found < need {\n\t\tk := newkey()\n\t\tld := enode.LogDist(targetSha, v4wire.EncodePubkey(&k.PublicKey).ID())\n\t\tif len(tn.dists[ld]) < 8 {\n\t\t\ttn.dists[ld] = append(tn.dists[ld], k)\n\t\t\tfound++\n\t\t\tfmt.Printf(\"found ID with ld %d (%d/%d)\\n\", ld, found, need)\n\t\t}\n\t}\n\tfmt.Printf(\"&preminedTestnet{\\n\")\n\tfmt.Printf(\"\ttarget: hexEncPubkey(\\\"%x\\\"),\\n\", tn.target[:])\n\tfmt.Printf(\"\tdists: [%d][]*ecdsa.PrivateKey{\\n\", len(tn.dists))\n\tfor ld, ns := range tn.dists {\n\t\tif len(ns) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfmt.Printf(\"\t\t%d: {\\n\", ld)\n\t\tfor _, key := range ns {\n\t\t\tfmt.Printf(\"\t\t\thexEncPrivkey(\\\"%x\\\"),\\n\", crypto.FromECDSA(key))\n\t\t}\n\t\tfmt.Printf(\"\t\t},\\n\")\n\t}\n\tfmt.Printf(\"\t},\\n\")\n\tfmt.Printf(\"}\\n\")\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 974,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= r.Size()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 602,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 672,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 681,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/v4_udp_test.go",
          "line": 132,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 341,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 703,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 781,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 787,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 820,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/table.go",
          "line": 313,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(b.entries)\n\t}\n\treturn n\n}\n\n// addFoundNode adds a node which may not be live. If the bucket has space available,\n// adding the node succeeds immediately. Otherwise, the node is added to the replacements\n// list.\n//\n// The caller must not hold tab.mutex.\nfunc (tab *Table) addFoundNode(n *enode.Node, forceSetLive bool) bool {\n\top := addNodeOp{node: n, isInbound: false, forceSetLive: forceSetLive, syncExecution: true}\n\tselect {\n\tcase tab.addNodeCh <- op:\n\t\treturn <-tab.addNodeHandled\n\tcase <-tab.closeReq:\n\t\treturn false\n\t}\n}\n\n// addInboundNode adds a node from an inbound contact. If the bucket has no space, the\n// node is added to the replacements list.\n//\n// There is an additional safety measure: if the table is still initializing the node is\n// not added. This prevents an attack where the table could be filled by just sending ping\n// repeatedly.\n//\n// The caller must not hold tab.mutex.\nfunc (tab *Table) addInboundNode(n *enode.Node) {\n\top := addNodeOp{node: n, isInbound: true}\n\tselect {\n\tcase tab.addNodeCh <- op:\n\t\treturn\n\tcase <-tab.closeReq:\n\t\treturn\n\t}\n}\n\n// Only for testing purposes\nfunc (tab *Table) addInboundNodeSync(n *enode.Node) bool {\n\top := addNodeOp{node: n, isInbound: true, syncExecution: true}\n\tselect {\n\tcase tab.addNodeCh <- op:\n\t\treturn <-tab.addNodeHandled\n\tcase <-tab.closeReq:\n\t\treturn false\n\t}\n}\n\nfunc (tab *Table) trackRequest(n *enode.Node, success bool, foundNodes []*enode.Node) {\n\top := trackRequestOp{n, foundNodes, success}\n\tselect {\n\tcase tab.trackRequestCh <- op:\n\tcase <-tab.closeReq:\n\t}\n}\n\n// loop is the main loop of Table.\nfunc (tab *Table) loop() {\n\tvar (\n\t\trefresh         = time.NewTimer(tab.nextRefreshTime())\n\t\trefreshDone     = make(chan struct{})           // where doRefresh reports completion\n\t\twaiting         = []chan struct{}{tab.initDone} // holds waiting callers while doRefresh runs\n\t\trevalTimer      = mclock.NewAlarm(tab.cfg.Clock)\n\t\treseedRandTimer = time.NewTicker(10 * time.Minute)\n\t)\n\tdefer refresh.Stop()\n\tdefer revalTimer.Stop()\n\tdefer reseedRandTimer.Stop()\n\n\t// Start initial refresh.\n\tgopool.Submit(func() {\n\t\ttab.doRefresh(refreshDone)\n\t})\nloop:\n\tfor {\n\t\tnextTime := tab.revalidation.run(tab, tab.cfg.Clock.Now())\n\t\trevalTimer.Schedule(nextTime)\n\n\t\tselect {\n\t\tcase <-reseedRandTimer.C:\n\t\t\ttab.rand.seed()\n\n\t\tcase <-revalTimer.C():\n\n\t\tcase r := <-tab.revalResponseCh:\n\t\t\ttab.revalidation.handleResponse(tab, r)\n\n\t\tcase op := <-tab.addNodeCh:\n\t\t\t// only happens in tests\n\t\t\tif op.syncExecution {\n\t\t\t\tok := tab.handleAddNode(op)\n\t\t\t\ttab.addNodeHandled <- ok\n\t\t\t} else {\n\t\t\t\t// async execution as handleAddNode is blocking\n\t\t\t\tgo func() {\n\t\t\t\t\ttab.handleAddNode(op)\n\t\t\t\t}()\n\t\t\t}\n\n\t\tcase op := <-tab.trackRequestCh:\n\t\t\ttab.handleTrackRequest(op)\n\n\t\tcase <-refresh.C:\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tgopool.Submit(func() {\n\t\t\t\t\ttab.doRefresh(refreshDone)\n\t\t\t\t})\n\t\t\t}\n\n\t\tcase req := <-tab.refreshReq:\n\t\t\twaiting = append(waiting, req)\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tgopool.Submit(\n\t\t\t\t\tfunc() {\n\t\t\t\t\t\ttab.doRefresh(refreshDone)\n\t\t\t\t\t})\n\t\t\t}\n\n\t\tcase <-refreshDone:\n\t\t\tfor _, ch := range waiting {\n\t\t\t\tclose(ch)\n\t\t\t}\n\t\t\twaiting, refreshDone = nil, nil\n\t\t\trefresh.Reset(tab.nextRefreshTime())\n\n\t\tcase <-tab.closeReq:\n\t\t\tbreak loop\n\t\t}\n\t}\n\n\tif refreshDone != nil {\n\t\t<-refreshDone\n\t}\n\tfor _, ch := range waiting {\n\t\tclose(ch)\n\t}\n\tclose(tab.closed)\n}\n\n// doRefresh performs a lookup for a random target to keep buckets full. seed nodes are\n// inserted if the table is empty (initial bootstrap or discarded faulty peers).\nfunc (tab *Table) doRefresh(done chan struct{}) {\n\tdefer close(done)\n\n\t// Load nodes from the database and insert\n\t// them. This should yield a few previously seen nodes that are\n\t// (hopefully) still alive.\n\ttab.loadSeedNodes()\n\n\t// Run self lookup to discover new neighbor nodes.\n\ttab.net.lookupSelf()\n\n\t// The Kademlia paper specifies that the bucket refresh should\n\t// perform a lookup in the least recently used bucket. We cannot\n\t// adhere to this because the findnode target is a 512bit value\n\t// (not hash-sized) and it is not easily possible to generate a\n\t// sha3 preimage that falls into a chosen bucket.\n\t// We perform a few lookups with a random target instead.\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/rlpx/rlpx.go",
          "line": 182,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 16 - padding\n\t}\n\n\t// Read the frame content.\n\tframe, err := h.rbuf.read(conn, int(rsize))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Validate frame MAC.\n\tframeMAC, err := h.rbuf.read(conn, 16)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twantFrameMAC := h.ingressMAC.computeFrame(frame)\n\tif !hmac.Equal(wantFrameMAC, frameMAC) {\n\t\treturn nil, errors.New(\"bad frame MAC\")\n\t}\n\n\t// Decrypt the frame data.\n\th.dec.XORKeyStream(frame, frame)\n\treturn frame[:fsize], nil\n}\n\n// Write writes a message to the connection.\n//\n// Write returns the written size of the message data. This may be less than or equal to\n// len(data) depending on whether snappy compression is enabled.\nfunc (c *Conn) Write(code uint64, data []byte) (uint32, error) {\n\tif c.session == nil {\n\t\tpanic(\"can't WriteMsg before handshake\")\n\t}\n\tif len(data) > maxUint24 {\n\t\treturn 0, errPlainMessageTooLarge\n\t}\n\tif c.snappyWriteBuffer != nil {\n\t\t// Ensure the buffer has sufficient size.\n\t\t// Package snappy will allocate its own buffer if the provided\n\t\t// one is smaller than MaxEncodedLen.\n\t\tc.snappyWriteBuffer = growslice(c.snappyWriteBuffer, snappy.MaxEncodedLen(len(data)))\n\t\tdata = snappy.Encode(c.snappyWriteBuffer, data)\n\t}\n\n\twireSize := uint32(len(data))\n\terr := c.session.writeFrame(c.conn, code, data)\n\treturn wireSize, err\n}\n\nfunc (h *sessionState) writeFrame(conn io.Writer, code uint64, data []byte) error {\n\th.wbuf.reset()\n\n\t// Write header.\n\tfsize := rlp.IntSize(code) + len(data)\n\tif fsize > maxUint24 {\n\t\treturn errPlainMessageTooLarge\n\t}\n\theader := h.wbuf.appendZero(16)\n\tputUint24(uint32(fsize), header)\n\tcopy(header[3:], zeroHeader)\n\th.enc.XORKeyStream(header, header)\n\n\t// Write header MAC.\n\th.wbuf.Write(h.egressMAC.computeHeader(header))\n\n\t// Encode and encrypt the frame data.\n\toffset := len(h.wbuf.data)\n\th.wbuf.data = rlp.AppendUint64(h.wbuf.data, code)\n\th.wbuf.Write(data)\n\tif padding := fsize % 16",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/rlpx/buffer.go",
          "line": 68,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rn\n\tb.data = b.data[:offset+n]\n\treturn b.data[offset : offset+n], nil\n}\n\n// grow ensures the buffer has at least n bytes of unused space.\nfunc (b *readBuffer) grow(n int) {\n\tif cap(b.data)-b.end >= n {\n\t\treturn\n\t}\n\tneed := n - (cap(b.data) - b.end)\n\toffset := len(b.data)\n\tb.data = append(b.data[:cap(b.data)], make([]byte, need)...)\n\tb.data = b.data[:offset]\n}\n\n// writeBuffer implements buffering for network writes. This is essentially\n// a convenience wrapper around a byte slice.\ntype writeBuffer struct {\n\tdata []byte\n}\n\nfunc (b *writeBuffer) reset() {\n\tb.data = b.data[:0]\n}\n\nfunc (b *writeBuffer) appendZero(n int) []byte {\n\toffset := len(b.data)\n\tb.data = append(b.data, make([]byte, n)...)\n\treturn b.data[offset : offset+n]\n}\n\nfunc (b *writeBuffer) Write(data []byte) (int, error) {\n\tb.data = append(b.data, data...)\n\treturn len(data), nil\n}\n\nconst maxUint24 = int(^uint32(0) >> 8)\n\nfunc readUint24(b []byte) uint32 {\n\treturn uint32(b[2]) | uint32(b[1])<<8 | uint32(b[0])<<16\n}\n\nfunc putUint24(v uint32, b []byte) {\n\tb[0] = byte(v >> 16)\n\tb[1] = byte(v >> 8)\n\tb[2] = byte(v)\n}\n\n// growslice ensures b has the wanted length by either expanding it to its capacity\n// or allocating a new slice if b has insufficient capacity.\nfunc growslice(b []byte, wantLength int) []byte {\n\tif len(b) >= wantLength {\n\t\treturn b\n\t}\n\tif cap(b) >= wantLength {\n\t\treturn b[:cap(b)]\n\t}\n\treturn make([]byte, wantLength)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/nat/stun.go",
          "line": 110,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\":%d\", stunV2.DefaultPort)\n\t}\n\n\tlog.Trace(\"Attempting STUN binding request\", \"server\", server)\n\tconn, err := stunV2.Dial(\"udp4\", server)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\n\tmessage, err := stunV2.Build(stunV2.TransactionID, stunV2.BindingRequest)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar responseError error\n\tvar mappedAddr stunV2.XORMappedAddress\n\terr = conn.Do(message, func(event stunV2.Event) {\n\t\tif event.Error != nil {\n\t\t\tresponseError = event.Error\n\t\t\treturn\n\t\t}\n\t\tif err := mappedAddr.GetFrom(event.Message)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/enode/node.go",
          "line": 388,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 8\n\t\t} else {\n\t\t\tlz += bits.LeadingZeros8(x)\n\t\t\tbreak\n\t\t}\n\t}\n\treturn len(a)*8 - lz\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/msgrate/msgrate.go",
          "line": 337,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= val\n\t\t}\n\t\ttt.lock.RUnlock()\n\t}\n\tfor key, val := range capacities {\n\t\tcapacities[key] = val / float64(len(t.trackers))\n\t}\n\treturn capacities\n}\n\n// TargetRoundTrip returns the current target round trip time for a request to\n// complete in.The returned RTT is slightly under the estimated RTT. The reason\n// is that message rate estimation is a 2 dimensional problem which is solvable\n// for any RTT. The goal is to gravitate towards smaller RTTs instead of large\n// messages, to result in a stabler download stream.\nfunc (t *Trackers) TargetRoundTrip() time.Duration {\n\t// Recalculate the internal caches if it's been a while\n\tt.tune()\n\n\t// Caches surely recent, return target roundtrip\n\tt.lock.RLock()\n\tdefer t.lock.RUnlock()\n\n\treturn time.Duration(float64(t.roundtrip) * rttPushdownFactor)\n}\n\n// TargetTimeout returns the timeout allowance for a single request to finish\n// under. The timeout is proportional to the roundtrip, but also takes into\n// consideration the tracker's confidence in said roundtrip and scales it\n// accordingly. The final value is capped to avoid runaway requests.\nfunc (t *Trackers) TargetTimeout() time.Duration {\n\t// Recalculate the internal caches if it's been a while\n\tt.tune()\n\n\t// Caches surely recent, return target timeout\n\tt.lock.RLock()\n\tdefer t.lock.RUnlock()\n\n\treturn t.targetTimeout()\n}\n\n// targetTimeout is the internal lockless version of TargetTimeout to be used\n// during QoS tuning.\nfunc (t *Trackers) targetTimeout() time.Duration {\n\ttimeout := min(time.Duration(ttlScaling*float64(t.roundtrip)/t.confidence), t.OverrideTTLLimit)\n\treturn timeout\n}\n\n// tune gathers the individual tracker statistics and updates the estimated\n// request round trip time.\nfunc (t *Trackers) tune() {\n\t// Tune may be called concurrently all over the place, but we only want to\n\t// periodically update and even then only once. First check if it was updated\n\t// recently and abort if so.\n\tt.lock.RLock()\n\tdirty := time.Since(t.tuned) > t.roundtrip\n\tt.lock.RUnlock()\n\tif !dirty {\n\t\treturn\n\t}\n\t// If an update is needed, obtain a write lock but make sure we don't update\n\t// it on all concurrent threads one by one.\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tif dirty := time.Since(t.tuned) > t.roundtrip",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/netutil/net.go",
          "line": 306,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= i\n\t}\n\treturn int(n)\n}\n\n// key returns the map key for ip.\nfunc (s *DistinctNetSet) key(ip netip.Addr) netip.Prefix {\n\t// Lazily initialize storage.\n\tif s.members == nil {\n\t\ts.members = make(map[netip.Prefix]uint)\n\t}\n\tp, err := ip.Prefix(int(s.Subnet))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn p\n}\n\n// String implements fmt.Stringer\nfunc (s DistinctNetSet) String() string {\n\tkeys := slices.SortedFunc(maps.Keys(s.members), func(a, b netip.Prefix) int {\n\t\treturn strings.Compare(a.String(), b.String())\n\t})\n\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"{\")\n\tfor i, k := range keys {\n\t\tfmt.Fprintf(&buf, \"%v\u00d7%d\", k, s.members[k])\n\t\tif i != len(keys)-1 {\n\t\t\tbuf.WriteString(\" \")\n\t\t}\n\t}\n\tbuf.WriteString(\"}\")\n\treturn buf.String()\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/p2p/discover/v5wire/encoding.go",
          "line": 286,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= authsizeExtra\n\tif authsize > int(^uint16(0)) {\n\t\tpanic(fmt.Errorf(\"BUG: auth size %d overflows uint16\", authsize))\n\t}\n\treturn Header{\n\t\tStaticHeader: StaticHeader{\n\t\t\tProtocolID: c.protocolID,\n\t\t\tVersion:    version,\n\t\t\tFlag:       flag,\n\t\t\tAuthSize:   uint16(authsize),\n\t\t},\n\t}\n}\n\n// encodeRandom encodes a packet with random content.\nfunc (c *Codec) encodeRandom(toID enode.ID) (Header, []byte, error) {\n\thead := c.makeHeader(toID, flagMessage, 0)\n\n\t// Encode auth data.\n\tauth := messageAuthData{SrcID: c.localnode.ID()}\n\tif _, err := crand.Read(head.Nonce[:])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/usbwallet/trezor.go",
          "line": 272,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= byte(chainID.Uint64()*2 + 35)\n\t\t}\n\t}\n\n\t// Inject the final signature into the transaction and sanity check the sender\n\tsigned, err := tx.WithSignature(signer, signature)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\tsender, err := types.Sender(signer, signed)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\treturn sender, signed, nil\n}\n\n// trezorExchange performs a data exchange with the Trezor wallet, sending it a\n// message and retrieving the response. If multiple responses are possible, the\n// method will also return the index of the destination object used.\nfunc (w *trezorDriver) trezorExchange(req proto.Message, results ...proto.Message) (int, error) {\n\t// Construct the original message payload to chunk up\n\tdata, err := proto.Marshal(req)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tpayload := make([]byte, 8+len(data))\n\tcopy(payload, []byte{0x23, 0x23})\n\tbinary.BigEndian.PutUint16(payload[2:], trezor.Type(req))\n\tbinary.BigEndian.PutUint32(payload[4:], uint32(len(data)))\n\tcopy(payload[8:], data)\n\n\t// Stream all the chunks to the device\n\tchunk := make([]byte, 64)\n\tchunk[0] = 0x3f // Report ID magic number\n\n\tfor len(payload) > 0 {\n\t\t// Construct the new message to stream, padding with zeroes if needed\n\t\tif len(payload) > 63 {\n\t\t\tcopy(chunk[1:], payload[:63])\n\t\t\tpayload = payload[63:]\n\t\t} else {\n\t\t\tcopy(chunk[1:], payload)\n\t\t\tcopy(chunk[1+len(payload):], make([]byte, 63-len(payload)))\n\t\t\tpayload = nil\n\t\t}\n\t\t// Send over to the device\n\t\tw.log.Trace(\"Data chunk sent to the Trezor\", \"chunk\", hexutil.Bytes(chunk))\n\t\tif _, err := w.device.Write(chunk)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/usbwallet/hub.go",
          "line": 246,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/usbwallet/ledger.go",
          "line": 403,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= byte(chainID.Uint64()*2 + 35)\n\t\t}\n\t}\n\tsigned, err := tx.WithSignature(signer, signature)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\tsender, err := types.Sender(signer, signed)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\treturn sender, signed, nil\n}\n\n// ledgerSignTypedMessage sends the transaction to the Ledger wallet, and waits for the user\n// to confirm or deny the transaction.\n//\n// The signing protocol is defined as follows:\n//\n//\tCLA | INS | P1 | P2                          | Lc  | Le\n//\t----+-----+----+-----------------------------+-----+---\n//\t E0 | 0C  | 00 | implementation version : 00 | variable | variable\n//\n// Where the input is:\n//\n//\tDescription                                      | Length\n//\t-------------------------------------------------+----------\n//\tNumber of BIP 32 derivations to perform (max 10) | 1 byte\n//\tFirst derivation index (big endian)              | 4 bytes\n//\t...                                              | 4 bytes\n//\tLast derivation index (big endian)               | 4 bytes\n//\tdomain hash                                      | 32 bytes\n//\tmessage hash                                     | 32 bytes\n//\n// And the output data is:\n//\n//\tDescription | Length\n//\t------------+---------\n//\tsignature V | 1 byte\n//\tsignature R | 32 bytes\n//\tsignature S | 32 bytes\nfunc (w *ledgerDriver) ledgerSignTypedMessage(derivationPath []uint32, domainHash []byte, messageHash []byte) ([]byte, error) {\n\t// Flatten the derivation path into the Ledger request\n\tpath := make([]byte, 1+4*len(derivationPath))\n\tpath[0] = byte(len(derivationPath))\n\tfor i, component := range derivationPath {\n\t\tbinary.BigEndian.PutUint32(path[1+4*i:], component)\n\t}\n\t// Create the 712 message\n\tpayload := append(path, domainHash...)\n\tpayload = append(payload, messageHash...)\n\n\t// Send the request and wait for the response\n\tvar (\n\t\top    = ledgerP1InitTypedMessageData\n\t\treply []byte\n\t\terr   error\n\t)\n\n\t// Send the message over, ensuring it's processed correctly\n\treply, err = w.ledgerExchange(ledgerOpSignTypedMessage, op, 0, payload)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Extract the Ethereum signature and do a sanity validation\n\tif len(reply) != crypto.SignatureLength {\n\t\treturn nil, errors.New(\"reply lacks signature\")\n\t}\n\tsignature := append(reply[1:], reply[0])\n\treturn signature, nil\n}\n\n// ledgerExchange performs a data exchange with the Ledger wallet, sending it a\n// message and retrieving the response.\n//\n// The common transport header is defined as follows:\n//\n//\tDescription                           | Length\n//\t--------------------------------------+----------\n//\tCommunication channel ID (big endian) | 2 bytes\n//\tCommand tag                           | 1 byte\n//\tPacket sequence index (big endian)    | 2 bytes\n//\tPayload                               | arbitrary\n//\n// The Communication channel ID allows commands multiplexing over the same\n// physical link. It is not used for the time being, and should be set to 0101\n// to avoid compatibility issues with implementations ignoring a leading 00 byte.\n//\n// The Command tag describes the message content. Use TAG_APDU (0x05) for standard\n// APDU payloads, or TAG_PING (0x02) for a simple link test.\n//\n// The Packet sequence index describes the current sequence for fragmented payloads.\n// The first fragment index is 0x00.\n//\n// APDU Command payloads are encoded as follows:\n//\n//\tDescription              | Length\n//\t-----------------------------------\n//\tAPDU length (big endian) | 2 bytes\n//\tAPDU CLA                 | 1 byte\n//\tAPDU INS                 | 1 byte\n//\tAPDU P1                  | 1 byte\n//\tAPDU P2                  | 1 byte\n//\tAPDU length              | 1 byte\n//\tOptional APDU data       | arbitrary\nfunc (w *ledgerDriver) ledgerExchange(opcode ledgerOpcode, p1 ledgerParam1, p2 ledgerParam2, data []byte) ([]byte, error) {\n\t// Construct the message payload, possibly split into multiple chunks\n\tapdu := make([]byte, 2, 7+len(data))\n\n\tbinary.BigEndian.PutUint16(apdu, uint16(5+len(data)))\n\tapdu = append(apdu, []byte{0xe0, byte(opcode), byte(p1), byte(p2), byte(len(data))}...)\n\tapdu = append(apdu, data...)\n\n\t// Stream all the chunks to the device\n\theader := []byte{0x01, 0x01, 0x05, 0x00, 0x00} // Channel ID and command tag appended\n\tchunk := make([]byte, 64)\n\tspace := len(chunk) - len(header)\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/usbwallet/wallet.go",
          "line": 174,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/keystore/account_cache_test.go",
          "line": 222,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tcache.delete(wantAccounts[i])\n\t}\n\tcache.delete(accounts.Account{Address: common.HexToAddress(\"fd9bd350f08ee3c0c19b85a8e16114a11a60aa4e\"), URL: accounts.URL{Scheme: KeyStoreScheme, Path: \"something\"}})\n\n\t// Check content again after deletion.\n\twantAccountsAfterDelete := []accounts.Account{\n\t\twantAccounts[1],\n\t\twantAccounts[3],\n\t\twantAccounts[5],\n\t}\n\tlist = cache.accounts()\n\tif !reflect.DeepEqual(list, wantAccountsAfterDelete) {\n\t\tt.Fatalf(\"got accounts after delete: %s\\nwant %s\", spew.Sdump(list), spew.Sdump(wantAccountsAfterDelete))\n\t}\n\tfor _, a := range wantAccountsAfterDelete {\n\t\tif !cache.hasAddress(a.Address) {\n\t\t\tt.Errorf(\"expected hasAccount(%x) to return true\", a.Address)\n\t\t}\n\t}\n\tif cache.hasAddress(wantAccounts[0].Address) {\n\t\tt.Errorf(\"expected hasAccount(%x) to return false\", wantAccounts[0].Address)\n\t}\n}\n\nfunc TestCacheFind(t *testing.T) {\n\tt.Parallel()\n\tdir := filepath.Join(\"testdata\", \"dir\")\n\tcache, _ := newAccountCache(dir)\n\tcache.watcher.running = true // prevent unexpected reloads\n\n\taccs := []accounts.Account{\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"095e7baea6a6c7c4c2dfeb977efac326af552d87\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"a.key\")},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"2cac1adea150210703ba75ed097ddfe24e14f213\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"b.key\")},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"d49ff4eeb0b2686ed89c0fc0f2b6ea533ddbbd5e\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"c.key\")},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"d49ff4eeb0b2686ed89c0fc0f2b6ea533ddbbd5e\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"c2.key\")},\n\t\t},\n\t}\n\tfor _, a := range accs {\n\t\tcache.add(a)\n\t}\n\n\tnomatchAccount := accounts.Account{\n\t\tAddress: common.HexToAddress(\"f466859ead1932d743d622cb74fc058882e8648a\"),\n\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"something\")},\n\t}\n\ttests := []struct {\n\t\tQuery      accounts.Account\n\t\tWantResult accounts.Account\n\t\tWantError  error\n\t}{\n\t\t// by address\n\t\t{Query: accounts.Account{Address: accs[0].Address}, WantResult: accs[0]},\n\t\t// by file\n\t\t{Query: accounts.Account{URL: accs[0].URL}, WantResult: accs[0]},\n\t\t// by basename\n\t\t{Query: accounts.Account{URL: accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Base(accs[0].URL.Path)}}, WantResult: accs[0]},\n\t\t// by file and address\n\t\t{Query: accs[0], WantResult: accs[0]},\n\t\t// ambiguous address, tie resolved by file\n\t\t{Query: accs[2], WantResult: accs[2]},\n\t\t// ambiguous address error\n\t\t{\n\t\t\tQuery: accounts.Account{Address: accs[2].Address},\n\t\t\tWantError: &AmbiguousAddrError{\n\t\t\t\tAddr:    accs[2].Address,\n\t\t\t\tMatches: []accounts.Account{accs[2], accs[3]},\n\t\t\t},\n\t\t},\n\t\t// no match error\n\t\t{Query: nomatchAccount, WantError: ErrNoMatch},\n\t\t{Query: accounts.Account{URL: nomatchAccount.URL}, WantError: ErrNoMatch},\n\t\t{Query: accounts.Account{URL: accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Base(nomatchAccount.URL.Path)}}, WantError: ErrNoMatch},\n\t\t{Query: accounts.Account{Address: nomatchAccount.Address}, WantError: ErrNoMatch},\n\t}\n\tfor i, test := range tests {\n\t\ta, err := cache.find(test.Query)\n\t\tif !reflect.DeepEqual(err, test.WantError) {\n\t\t\tt.Errorf(\"test %d: error mismatch for query %v\\ngot %q\\nwant %q\", i, test.Query, err, test.WantError)\n\t\t\tcontinue\n\t\t}\n\t\tif a != test.WantResult {\n\t\t\tt.Errorf(\"test %d: result mismatch for query %v\\ngot %v\\nwant %v\", i, test.Query, a, test.WantResult)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\n// TestUpdatedKeyfileContents tests that updating the contents of a keystore file\n// is noticed by the watcher, and the account cache is updated accordingly\nfunc TestUpdatedKeyfileContents(t *testing.T) {\n\tt.Parallel()\n\n\t// Create a temporary keystore to test with\n\tdir := t.TempDir()\n\n\tks := NewKeyStore(dir, LightScryptN, LightScryptP)\n\n\tlist := ks.Accounts()\n\tif len(list) > 0 {\n\t\tt.Error(\"initial account list not empty:\", list)\n\t}\n\tif !waitWatcherStart(ks) {\n\t\tt.Fatal(\"keystore watcher didn't start in time\")\n\t}\n\t// Copy a key file into it\n\tfile := filepath.Join(dir, \"aaa\")\n\n\t// Place one of our testfiles in there\n\tif err := cp.CopyFile(file, cachetestAccounts[0].URL.Path)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/keystore/account_cache_test.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 2 {\n\t\tlist = ks.Accounts()\n\t\tif reflect.DeepEqual(list, wantAccounts) {\n\t\t\t// ks should have also received change notifications\n\t\t\tselect {\n\t\t\tcase <-ks.changes:\n\t\t\tdefault:\n\t\t\t\tt.Fatalf(\"wasn't notified of new accounts\")\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\ttime.Sleep(d)\n\t}\n\tt.Errorf(\"\\ngot  %v\\nwant %v\", list, wantAccounts)\n}\n\nfunc TestCacheInitialReload(t *testing.T) {\n\tt.Parallel()\n\tcache, _ := newAccountCache(cachetestDir)\n\taccounts := cache.accounts()\n\tif !reflect.DeepEqual(accounts, cachetestAccounts) {\n\t\tt.Fatalf(\"got initial accounts: %swant %s\", spew.Sdump(accounts), spew.Sdump(cachetestAccounts))\n\t}\n}\n\nfunc TestCacheAddDeleteOrder(t *testing.T) {\n\tt.Parallel()\n\tcache, _ := newAccountCache(\"testdata/no-such-dir\")\n\tcache.watcher.running = true // prevent unexpected reloads\n\n\taccs := []accounts.Account{\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"095e7baea6a6c7c4c2dfeb977efac326af552d87\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"-309830980\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"2cac1adea150210703ba75ed097ddfe24e14f213\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"ggg\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"8bda78331c916a08481428e4b07c96d3e916d165\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"zzzzzz-the-very-last-one.keyXXX\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"d49ff4eeb0b2686ed89c0fc0f2b6ea533ddbbd5e\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"SOMETHING.key\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"7ef5a6135f1fd6a02593eedc869c6d41d934aef8\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"UTC--2016-03-22T12-57-55.920751759Z--7ef5a6135f1fd6a02593eedc869c6d41d934aef8\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"f466859ead1932d743d622cb74fc058882e8648a\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"aaa\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"289d485d9771714cce91d3393d764e1311907acc\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"zzz\"},\n\t\t},\n\t}\n\tfor _, a := range accs {\n\t\tcache.add(a)\n\t}\n\t// Add some of them twice to check that they don't get reinserted.\n\tcache.add(accs[0])\n\tcache.add(accs[2])\n\n\t// Check that the account list is sorted by filename.\n\twantAccounts := make([]accounts.Account, len(accs))\n\tcopy(wantAccounts, accs)\n\tslices.SortFunc(wantAccounts, byURL)\n\tlist := cache.accounts()\n\tif !reflect.DeepEqual(list, wantAccounts) {\n\t\tt.Fatalf(\"got accounts: %s\\nwant %s\", spew.Sdump(accs), spew.Sdump(wantAccounts))\n\t}\n\tfor _, a := range accs {\n\t\tif !cache.hasAddress(a.Address) {\n\t\t\tt.Errorf(\"expected hasAccount(%x) to return true\", a.Address)\n\t\t}\n\t}\n\tif cache.hasAddress(common.HexToAddress(\"fd9bd350f08ee3c0c19b85a8e16114a11a60aa4e\")) {\n\t\tt.Errorf(\"expected hasAccount(%x) to return false\", common.HexToAddress(\"fd9bd350f08ee3c0c19b85a8e16114a11a60aa4e\"))\n\t}\n\n\t// Delete a few keys from the cache.\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/keystore/account_cache.go",
          "line": 57,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a.URL.Path\n\t\tif i < len(err.Matches)-1 {\n\t\t\tfiles += \", \"\n\t\t}\n\t}\n\treturn fmt.Sprintf(\"multiple keys match address (%s)\", files)\n}\n\n// accountCache is a live index of all accounts in the keystore.\ntype accountCache struct {\n\tkeydir   string\n\twatcher  *watcher\n\tmu       sync.Mutex\n\tall      []accounts.Account\n\tbyAddr   map[common.Address][]accounts.Account\n\tthrottle *time.Timer\n\tnotify   chan struct{}\n\tfileC    fileCache\n}\n\nfunc newAccountCache(keydir string) (*accountCache, chan struct{}) {\n\tac := &accountCache{\n\t\tkeydir: keydir,\n\t\tbyAddr: make(map[common.Address][]accounts.Account),\n\t\tnotify: make(chan struct{}, 1),\n\t\tfileC:  fileCache{all: mapset.NewThreadUnsafeSet[string]()},\n\t}\n\tac.watcher = newWatcher(ac)\n\treturn ac, ac.notify\n}\n\nfunc (ac *accountCache) accounts() []accounts.Account {\n\tac.maybeReload()\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\tcpy := make([]accounts.Account, len(ac.all))\n\tcopy(cpy, ac.all)\n\treturn cpy\n}\n\nfunc (ac *accountCache) hasAddress(addr common.Address) bool {\n\tac.maybeReload()\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\treturn len(ac.byAddr[addr]) > 0\n}\n\nfunc (ac *accountCache) add(newAccount accounts.Account) {\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\n\ti := sort.Search(len(ac.all), func(i int) bool { return ac.all[i].URL.Cmp(newAccount.URL) >= 0 })\n\tif i < len(ac.all) && ac.all[i] == newAccount {\n\t\treturn\n\t}\n\t// newAccount is not in the cache.\n\tac.all = append(ac.all, accounts.Account{})\n\tcopy(ac.all[i+1:], ac.all[i:])\n\tac.all[i] = newAccount\n\tac.byAddr[newAccount.Address] = append(ac.byAddr[newAccount.Address], newAccount)\n}\n\n// note: removed needs to be unique here (i.e. both File and Address must be set).\nfunc (ac *accountCache) delete(removed accounts.Account) {\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\n\tac.all = removeAccount(ac.all, removed)\n\tif ba := removeAccount(ac.byAddr[removed.Address], removed)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/keystore/keystore.go",
          "line": 170,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/keystore/passphrase_test.go",
          "line": 56,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"new data appended\" // nolint: gosec\n\t\tif keyjson, err = EncryptKey(key, password, veryLightScryptN, veryLightScryptP)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/argument.go",
          "line": 210,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= getTypeSize(arg.Type)/32 - 1\n\t\t} else if arg.Type.T == TupleTy && !isDynamicType(arg.Type) {\n\t\t\t// If we have a static tuple, like (uint256, bool, uint256), these are\n\t\t\t// coded as just like uint256,bool,uint256\n\t\t\tvirtualArgs += getTypeSize(arg.Type)/32 - 1\n\t\t}\n\t\tretval = append(retval, marshalledValue)\n\t\tindex++\n\t}\n\treturn retval, nil\n}\n\n// PackValues performs the operation Go format -> Hexdata.\n// It is the semantic opposite of UnpackValues.\nfunc (arguments Arguments) PackValues(args []any) ([]byte, error) {\n\treturn arguments.Pack(args...)\n}\n\n// Pack performs the operation Go format -> Hexdata.\nfunc (arguments Arguments) Pack(args ...any) ([]byte, error) {\n\t// Make sure arguments match up and pack them\n\tabiArgs := arguments\n\tif len(args) != len(abiArgs) {\n\t\treturn nil, fmt.Errorf(\"argument count mismatch: got %d for %d\", len(args), len(abiArgs))\n\t}\n\t// variable input is the output appended at the end of packed\n\t// output. This is used for strings and bytes types input.\n\tvar variableInput []byte\n\n\t// input offset is the bytes offset for packed output\n\tinputOffset := 0\n\tfor _, abiArg := range abiArgs {\n\t\tinputOffset += getTypeSize(abiArg.Type)\n\t}\n\tvar ret []byte\n\tfor i, a := range args {\n\t\tinput := abiArgs[i]\n\t\t// pack the input\n\t\tpacked, err := input.Type.pack(reflect.ValueOf(a))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// check for dynamic types\n\t\tif isDynamicType(input.Type) {\n\t\t\t// set the offset\n\t\t\tret = append(ret, packNum(reflect.ValueOf(inputOffset))...)\n\t\t\t// calculate next offset\n\t\t\tinputOffset += len(packed)\n\t\t\t// append to variable input\n\t\t\tvariableInput = append(variableInput, packed...)\n\t\t} else {\n\t\t\t// append the packed value to the input\n\t\t\tret = append(ret, packed...)\n\t\t}\n\t}\n\t// append the variable input at the end of the packed input\n\tret = append(ret, variableInput...)\n\n\treturn ret, nil\n}\n\n// ToCamelCase converts an under-score string to a camel-case string\nfunc ToCamelCase(input string) string {\n\tparts := strings.Split(input, \"_\")\n\tfor i, s := range parts {\n\t\tif len(s) > 0 {\n\t\t\tparts[i] = strings.ToUpper(s[:1]) + s[1:]\n\t\t}\n\t}\n\treturn strings.Join(parts, \"\")\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/abifuzzer_test.go",
          "line": 104,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(`, \"stateMutability\": \"%v\" `, *stateMutability)\n\t}\n\tif payable != nil {\n\t\tsig += fmt.Sprintf(`, \"payable\": %v `, *payable)\n\t}\n\tif len(inputs) > 0 {\n\t\tsig += `, \"inputs\" : [ {`\n\t\tfor i, inp := range inputs {\n\t\t\tsig += fmt.Sprintf(`\"name\" : \"%v\", \"type\" : \"%v\" `, inp.name, inp.typ)\n\t\t\tif i+1 < len(inputs) {\n\t\t\t\tsig += \",\"\n\t\t\t}\n\t\t}\n\t\tsig += \"} ]\"\n\t\tsig += `, \"outputs\" : [ {`\n\t\tfor i, inp := range inputs {\n\t\t\tsig += fmt.Sprintf(`\"name\" : \"%v\", \"type\" : \"%v\" `, inp.name, inp.typ)\n\t\t\tif i+1 < len(inputs) {\n\t\t\t\tsig += \",\"\n\t\t\t}\n\t\t}\n\t\tsig += \"} ]\"\n\t}\n\tsig += `}]`\n\t//fmt.Printf(\"sig: %s\\n\", sig)\n\treturn JSON(strings.NewReader(sig))\n}\n\nfunc fuzzAbi(input []byte) {\n\tvar (\n\t\tfuzzer    = fuzz.NewFromGoFuzz(input)\n\t\tname      = oneOf(fuzzer, names)\n\t\tstateM    = oneOfOrNil(fuzzer, stateMut)\n\t\tpayable   = oneOfOrNil(fuzzer, pays)\n\t\targuments []arg\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/abi/abifuzzer_test.go",
          "line": 145,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"[]\"\n\t\tcase 1: // 10% chance to make it an array\n\t\t\targTyp += fmt.Sprintf(\"[%d]\", 1+upTo(fuzzer, 30))\n\t\tdefault:\n\t\t}\n\t\targuments = append(arguments, arg{name: argName, typ: argTyp})\n\t}\n\tabi, err := createABI(name, stateM, payable, arguments)\n\tif err != nil {\n\t\t//fmt.Printf(\"err: %v\\n\", err)\n\t\tpanic(err)\n\t}\n\tstructs, _ := unpackPack(abi, name, input)\n\t_ = packUnpack(abi, name, &structs)\n}\n\nfunc upTo(fuzzer *fuzz.Fuzzer, max int) int {\n\tvar i int\n\tfuzzer.Fuzz(&i)\n\tif i < 0 {\n\t\treturn (-1 - i) % max\n\t}\n\treturn i % max\n}\n\nfunc oneOf(fuzzer *fuzz.Fuzzer, options []string) string {\n\treturn options[upTo(fuzzer, len(options))]\n}\n\nfunc oneOfOrNil(fuzzer *fuzz.Fuzzer, options []string) *string {\n\tif i := upTo(fuzzer, len(options)+1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/type.go",
          "line": 173,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"(\"\n\t\tfor idx, c := range components {\n\t\t\tcType, err := NewType(c.Type, c.InternalType, c.Components)\n\t\t\tif err != nil {\n\t\t\t\treturn Type{}, err\n\t\t\t}\n\t\t\tname := ToCamelCase(c.Name)\n\t\t\tif name == \"\" {\n\t\t\t\treturn Type{}, errors.New(\"abi: purely anonymous or underscored field is not supported\")\n\t\t\t}\n\t\t\tfieldName := ResolveNameConflict(name, func(s string) bool { return used[s] })\n\t\t\tused[fieldName] = true\n\t\t\tif !isValidFieldName(fieldName) {\n\t\t\t\treturn Type{}, fmt.Errorf(\"field %d has invalid name\", idx)\n\t\t\t}\n\t\t\tfields = append(fields, reflect.StructField{\n\t\t\t\tName: fieldName, // reflect.StructOf will panic for any exported field.\n\t\t\t\tType: cType.GetType(),\n\t\t\t\tTag:  reflect.StructTag(\"json:\\\"\" + c.Name + \"\\\"\"),\n\t\t\t})\n\t\t\telems = append(elems, &cType)\n\t\t\tnames = append(names, c.Name)\n\t\t\texpression += cType.stringKind\n\t\t\tif idx != len(components)-1 {\n\t\t\t\texpression += \",\"\n\t\t\t}\n\t\t}\n\t\texpression += \")\"\n\n\t\ttyp.TupleType = reflect.StructOf(fields)\n\t\ttyp.TupleElems = elems\n\t\ttyp.TupleRawNames = names\n\t\ttyp.T = TupleTy\n\t\ttyp.stringKind = expression\n\n\t\tconst structPrefix = \"struct \"\n\t\t// After solidity 0.5.10, a new field of abi \"internalType\"\n\t\t// is introduced. From that we can obtain the struct name\n\t\t// user defined in the source code.\n\t\tif internalType != \"\" && strings.HasPrefix(internalType, structPrefix) {\n\t\t\t// Foo.Bar type definition is not allowed in golang,\n\t\t\t// convert the format to FooBar\n\t\t\ttyp.TupleRawName = strings.ReplaceAll(internalType[len(structPrefix):], \".\", \"\")\n\t\t}\n\n\tcase \"function\":\n\t\ttyp.T = FunctionTy\n\t\ttyp.Size = 24\n\tdefault:\n\t\tif strings.HasPrefix(internalType, \"contract \") {\n\t\t\ttyp.Size = 20\n\t\t\ttyp.T = AddressTy\n\t\t} else {\n\t\t\treturn Type{}, fmt.Errorf(\"unsupported arg type: %s\", t)\n\t\t}\n\t}\n\n\treturn\n}\n\n// GetType returns the reflection type of the ABI type.\nfunc (t Type) GetType() reflect.Type {\n\tswitch t.T {\n\tcase IntTy:\n\t\treturn reflectIntType(false, t.Size)\n\tcase UintTy:\n\t\treturn reflectIntType(true, t.Size)\n\tcase BoolTy:\n\t\treturn reflect.TypeOf(false)\n\tcase StringTy:\n\t\treturn reflect.TypeOf(\"\")\n\tcase SliceTy:\n\t\treturn reflect.SliceOf(t.Elem.GetType())\n\tcase ArrayTy:\n\t\treturn reflect.ArrayOf(t.Size, t.Elem.GetType())\n\tcase TupleTy:\n\t\treturn t.TupleType\n\tcase AddressTy:\n\t\treturn reflect.TypeOf(common.Address{})\n\tcase FixedBytesTy:\n\t\treturn reflect.ArrayOf(t.Size, reflect.TypeOf(byte(0)))\n\tcase BytesTy:\n\t\treturn reflect.SliceOf(reflect.TypeOf(byte(0)))\n\tcase HashTy:\n\t\t// hashtype currently not used\n\t\treturn reflect.ArrayOf(32, reflect.TypeOf(byte(0)))\n\tcase FixedPointTy:\n\t\t// fixedpoint type currently not used\n\t\treturn reflect.ArrayOf(32, reflect.TypeOf(byte(0)))\n\tcase FunctionTy:\n\t\treturn reflect.ArrayOf(24, reflect.TypeOf(byte(0)))\n\tdefault:\n\t\tpanic(\"Invalid type\")\n\t}\n}\n\n// String implements Stringer.\nfunc (t Type) String() (out string) {\n\treturn t.stringKind\n}\n\nfunc (t Type) pack(v reflect.Value) ([]byte, error) {\n\t// dereference pointer first if it's a pointer\n\tv = indirect(v)\n\tif err := typeCheck(t, v)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/abi/type.go",
          "line": 307,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(val)\n\t\t\ttail = append(tail, val...)\n\t\t}\n\t\treturn append(ret, tail...), nil\n\tcase TupleTy:\n\t\t// (T1,...,Tk) for k >= 0 and any types T1, \u2026, Tk\n\t\t// enc(X) = head(X(1)) ... head(X(k)) tail(X(1)) ... tail(X(k))\n\t\t// where X = (X(1), ..., X(k)) and head and tail are defined for Ti being a static\n\t\t// type as\n\t\t//     head(X(i)) = enc(X(i)) and tail(X(i)) = \"\" (the empty string)\n\t\t// and as\n\t\t//     head(X(i)) = enc(len(head(X(1)) ... head(X(k)) tail(X(1)) ... tail(X(i-1))))\n\t\t//     tail(X(i)) = enc(X(i))\n\t\t// otherwise, i.e. if Ti is a dynamic type.\n\t\tfieldmap, err := mapArgNamesToStructFields(t.TupleRawNames, v)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Calculate prefix occupied size.\n\t\toffset := 0\n\t\tfor _, elem := range t.TupleElems {\n\t\t\toffset += getTypeSize(*elem)\n\t\t}\n\t\tvar ret, tail []byte\n\t\tfor i, elem := range t.TupleElems {\n\t\t\tfield := v.FieldByName(fieldmap[t.TupleRawNames[i]])\n\t\t\tif !field.IsValid() {\n\t\t\t\treturn nil, fmt.Errorf(\"field %s for tuple not found in the given struct\", t.TupleRawNames[i])\n\t\t\t}\n\t\t\tval, err := elem.pack(field)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif isDynamicType(*elem) {\n\t\t\t\tret = append(ret, packNum(reflect.ValueOf(offset))...)\n\t\t\t\ttail = append(tail, val...)\n\t\t\t\toffset += len(val)\n\t\t\t} else {\n\t\t\t\tret = append(ret, val...)\n\t\t\t}\n\t\t}\n\t\treturn append(ret, tail...), nil\n\n\tdefault:\n\t\treturn packElement(t, v)\n\t}\n}\n\n// requiresLengthPrefix returns whether the type requires any sort of length\n// prefixing.\nfunc (t Type) requiresLengthPrefix() bool {\n\treturn t.T == StringTy || t.T == BytesTy || t.T == SliceTy\n}\n\n// isDynamicType returns true if the type is dynamic.\n// The following types are called \u201cdynamic\u201d:\n// * bytes\n// * string\n// * T[] for any T\n// * T[k] for any dynamic T and any k >= 0\n// * (T1,...,Tk) if Ti is dynamic for some 1 <= i <= k\nfunc isDynamicType(t Type) bool {\n\tif t.T == TupleTy {\n\t\tfor _, elem := range t.TupleElems {\n\t\t\tif isDynamicType(*elem) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\treturn t.T == StringTy || t.T == BytesTy || t.T == SliceTy || (t.T == ArrayTy && isDynamicType(*t.Elem))\n}\n\n// getTypeSize returns the size that this type needs to occupy.\n// We distinguish static and dynamic types. Static types are encoded in-place\n// and dynamic types are encoded at a separately allocated location after the\n// current block.\n// So for a static variable, the size returned represents the size that the\n// variable actually occupies.\n// For a dynamic variable, the returned size is fixed 32 bytes, which is used\n// to store the location reference for actual value storage.\nfunc getTypeSize(t Type) int {\n\tif t.T == ArrayTy && !isDynamicType(*t.Elem) {\n\t\t// Recursively calculate type size if it is a nested array\n\t\tif t.Elem.T == ArrayTy || t.Elem.T == TupleTy {\n\t\t\treturn t.Size * getTypeSize(*t.Elem)\n\t\t}\n\t\treturn t.Size * 32\n\t} else if t.T == TupleTy && !isDynamicType(t) {\n\t\ttotal := 0\n\t\tfor _, elem := range t.TupleElems {\n\t\t\ttotal += getTypeSize(*elem)\n\t\t}\n\t\treturn total\n\t}\n\treturn 32\n}\n\n// isLetter reports whether a given 'rune' is classified as a Letter.\n// This method is copied from reflect/type.go\nfunc isLetter(ch rune) bool {\n\treturn 'a' <= ch && ch <= 'z' || 'A' <= ch && ch <= 'Z' || ch == '_' || ch >= utf8.RuneSelf && unicode.IsLetter(ch)\n}\n\n// isValidFieldName checks if a string is a valid (struct) field name or not.\n//\n// According to the language spec, a field name should be an identifier.\n//\n// identifier = letter { letter | unicode_digit } .\n// letter = unicode_letter | \"_\" .\n// This method is copied from reflect/type.go\nfunc isValidFieldName(fieldName string) bool {\n\tfor i, c := range fieldName {\n\t\tif i == 0 && !isLetter(c) {\n\t\t\treturn false\n\t\t}\n\n\t\tif !(isLetter(c) || unicode.IsDigit(c)) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn len(fieldName) > 0\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/unpack.go",
          "line": 211,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= getTypeSize(*elem)/32 - 1\n\t\t} else if elem.T == TupleTy && !isDynamicType(*elem) {\n\t\t\t// If we have a static tuple, like (uint256, bool, uint256), these are\n\t\t\t// coded as just like uint256,bool,uint256\n\t\t\tvirtualArgs += getTypeSize(*elem)/32 - 1\n\t\t}\n\t\tretval.Field(index).Set(reflect.ValueOf(marshalledValue))\n\t}\n\treturn retval.Interface(), nil\n}\n\n// toGoType parses the output bytes and recursively assigns the value of these bytes\n// into a go type with accordance with the ABI spec.\nfunc toGoType(index int, t Type, output []byte) (interface{}, error) {\n\tif index+32 > len(output) {\n\t\treturn nil, fmt.Errorf(\"abi: cannot marshal in to go type: length insufficient %d require %d\", len(output), index+32)\n\t}\n\n\tvar (\n\t\treturnOutput  []byte\n\t\tbegin, length int\n\t\terr           error\n\t)\n\n\t// if we require a length prefix, find the beginning word and size returned.\n\tif t.requiresLengthPrefix() {\n\t\tbegin, length, err = lengthPrefixPointsTo(index, output)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\treturnOutput = output[index : index+32]\n\t}\n\n\tswitch t.T {\n\tcase TupleTy:\n\t\tif isDynamicType(t) {\n\t\t\tbegin, err := tuplePointsTo(index, output)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn forTupleUnpack(t, output[begin:])\n\t\t}\n\t\treturn forTupleUnpack(t, output[index:])\n\tcase SliceTy:\n\t\treturn forEachUnpack(t, output[begin:], 0, length)\n\tcase ArrayTy:\n\t\tif isDynamicType(*t.Elem) {\n\t\t\toffset := binary.BigEndian.Uint64(returnOutput[len(returnOutput)-8:])\n\t\t\tif offset > uint64(len(output)) {\n\t\t\t\treturn nil, fmt.Errorf(\"abi: toGoType offset greater than output length: offset: %d, len(output): %d\", offset, len(output))\n\t\t\t}\n\t\t\treturn forEachUnpack(t, output[offset:], 0, t.Size)\n\t\t}\n\t\treturn forEachUnpack(t, output[index:], 0, t.Size)\n\tcase StringTy: // variable arrays are written at the end of the return bytes\n\t\treturn string(output[begin : begin+length]), nil\n\tcase IntTy, UintTy:\n\t\treturn ReadInteger(t, returnOutput)\n\tcase BoolTy:\n\t\treturn readBool(returnOutput)\n\tcase AddressTy:\n\t\treturn common.BytesToAddress(returnOutput), nil\n\tcase HashTy:\n\t\treturn common.BytesToHash(returnOutput), nil\n\tcase BytesTy:\n\t\treturn output[begin : begin+length], nil\n\tcase FixedBytesTy:\n\t\treturn ReadFixedBytes(t, returnOutput)\n\tcase FunctionTy:\n\t\treturn readFunctionType(t, returnOutput)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"abi: unknown type %v\", t.T)\n\t}\n}\n\n// lengthPrefixPointsTo interprets a 32 byte slice as an offset and then determines which indices to look to decode the type.\nfunc lengthPrefixPointsTo(index int, output []byte) (start int, length int, err error) {\n\tbigOffsetEnd := new(big.Int).SetBytes(output[index : index+32])\n\tbigOffsetEnd.Add(bigOffsetEnd, common.Big32)\n\toutputLength := big.NewInt(int64(len(output)))\n\n\tif bigOffsetEnd.Cmp(outputLength) > 0 {\n\t\treturn 0, 0, fmt.Errorf(\"abi: cannot marshal in to go slice: offset %v would go over slice boundary (len=%v)\", bigOffsetEnd, outputLength)\n\t}\n\n\tif bigOffsetEnd.BitLen() > 63 {\n\t\treturn 0, 0, fmt.Errorf(\"abi offset larger than int64: %v\", bigOffsetEnd)\n\t}\n\n\toffsetEnd := int(bigOffsetEnd.Uint64())\n\tlengthBig := new(big.Int).SetBytes(output[offsetEnd-32 : offsetEnd])\n\n\ttotalSize := new(big.Int).Add(bigOffsetEnd, lengthBig)\n\tif totalSize.BitLen() > 63 {\n\t\treturn 0, 0, fmt.Errorf(\"abi: length larger than int64: %v\", totalSize)\n\t}\n\n\tif totalSize.Cmp(outputLength) > 0 {\n\t\treturn 0, 0, fmt.Errorf(\"abi: cannot marshal in to go type: length insufficient %v require %v\", outputLength, totalSize)\n\t}\n\tstart = int(bigOffsetEnd.Uint64())\n\tlength = int(lengthBig.Uint64())\n\treturn\n}\n\n// tuplePointsTo resolves the location reference for dynamic tuple.\nfunc tuplePointsTo(index int, output []byte) (start int, err error) {\n\toffset := new(big.Int).SetBytes(output[index : index+32])\n\toutputLen := big.NewInt(int64(len(output)))\n\n\tif offset.Cmp(outputLen) > 0 {\n\t\treturn 0, fmt.Errorf(\"abi: cannot marshal in to go slice: offset %v would go over slice boundary (len=%v)\", offset, outputLen)\n\t}\n\tif offset.BitLen() > 63 {\n\t\treturn 0, fmt.Errorf(\"abi offset larger than int64: %v\", offset)\n\t}\n\treturn int(offset.Uint64()), nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/method.go",
          "line": 107,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\" %v\", output.Name)\n\t\t}\n\t}\n\t// calculate the signature and method id. Note only function\n\t// has meaningful signature and id.\n\tvar (\n\t\tsig string\n\t\tid  []byte\n\t)\n\tif funType == Function {\n\t\tsig = fmt.Sprintf(\"%v(%v)\", rawName, strings.Join(types, \",\"))\n\t\tid = crypto.Keccak256([]byte(sig))[:4]\n\t}\n\tidentity := fmt.Sprintf(\"function %v\", rawName)\n\tswitch funType {\n\tcase Fallback:\n\t\tidentity = \"fallback\"\n\tcase Receive:\n\t\tidentity = \"receive\"\n\tcase Constructor:\n\t\tidentity = \"constructor\"\n\t}\n\tvar str string\n\t// Extract meaningful state mutability of solidity method.\n\t// If it's empty string or default value \"nonpayable\", never print it.\n\tif mutability == \"\" || mutability == \"nonpayable\" {\n\t\tstr = fmt.Sprintf(\"%v(%v) returns(%v)\", identity, strings.Join(inputNames, \", \"), strings.Join(outputNames, \", \"))\n\t} else {\n\t\tstr = fmt.Sprintf(\"%v(%v) %s returns(%v)\", identity, strings.Join(inputNames, \", \"), mutability, strings.Join(outputNames, \", \"))\n\t}\n\n\treturn Method{\n\t\tName:            name,\n\t\tRawName:         rawName,\n\t\tType:            funType,\n\t\tStateMutability: mutability,\n\t\tConstant:        isConst,\n\t\tPayable:         isPayable,\n\t\tInputs:          inputs,\n\t\tOutputs:         outputs,\n\t\tstr:             str,\n\t\tSig:             sig,\n\t\tID:              id,\n\t}\n}\n\nfunc (method Method) String() string {\n\treturn method.str\n}\n\n// IsConstant returns the indicator whether the method is read-only.\nfunc (method Method) IsConstant() bool {\n\treturn method.StateMutability == \"view\" || method.StateMutability == \"pure\" || method.Constant\n}\n\n// IsPayable returns the indicator whether the method can process\n// plain ether transfers.\nfunc (method Method) IsPayable() bool {\n\treturn method.StateMutability == \"payable\" || method.Payable\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/scwallet/hub.go",
          "line": 261,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/scwallet/wallet.go",
          "line": 414,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/external/backend.go",
          "line": 168,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 27 // Transform V from 27/28 to 0/1 for Clique and Parlia use\n\t}\n\treturn res, nil\n}\n\nfunc (api *ExternalSigner) SignText(account accounts.Account, text []byte) ([]byte, error) {\n\tvar signature hexutil.Bytes\n\tvar signAddress = common.NewMixedcaseAddress(account.Address)\n\tif err := api.client.Call(&signature, \"account_signData\",\n\t\taccounts.MimetypeTextPlain,\n\t\t&signAddress, // Need to use the pointer here, because of how MarshalJSON is defined\n\t\thexutil.Encode(text))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/external/backend.go",
          "line": 185,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 27 // Transform V from Ethereum-legacy to 0/1\n\t}\n\treturn signature, nil\n}\n\n// signTransactionResult represents the signinig result returned by clef.\ntype signTransactionResult struct {\n\tRaw hexutil.Bytes      `json:\"raw\"`\n\tTx  *types.Transaction `json:\"tx\"`\n}\n\n// SignTx sends the transaction to the external signer.\n// If chainID is nil, or tx.ChainID is zero, the chain ID will be assigned\n// by the external signer. For non-legacy transactions, the chain ID of the\n// transaction overrides the chainID parameter.\nfunc (api *ExternalSigner) SignTx(account accounts.Account, tx *types.Transaction, chainID *big.Int) (*types.Transaction, error) {\n\tdata := hexutil.Bytes(tx.Data())\n\tvar to *common.MixedcaseAddress\n\tif tx.To() != nil {\n\t\tt := common.NewMixedcaseAddress(*tx.To())\n\t\tto = &t\n\t}\n\targs := &apitypes.SendTxArgs{\n\t\tInput: &data,\n\t\tNonce: hexutil.Uint64(tx.Nonce()),\n\t\tValue: hexutil.Big(*tx.Value()),\n\t\tGas:   hexutil.Uint64(tx.Gas()),\n\t\tTo:    to,\n\t\tFrom:  common.NewMixedcaseAddress(account.Address),\n\t}\n\tswitch tx.Type() {\n\tcase types.LegacyTxType, types.AccessListTxType:\n\t\targs.GasPrice = (*hexutil.Big)(tx.GasPrice())\n\tcase types.DynamicFeeTxType, types.BlobTxType, types.SetCodeTxType:\n\t\targs.MaxFeePerGas = (*hexutil.Big)(tx.GasFeeCap())\n\t\targs.MaxPriorityFeePerGas = (*hexutil.Big)(tx.GasTipCap())\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unsupported tx type %d\", tx.Type())\n\t}\n\t// We should request the default chain id that we're operating with\n\t// (the chain we're executing on)\n\tif chainID != nil && chainID.Sign() != 0 {\n\t\targs.ChainID = (*hexutil.Big)(chainID)\n\t}\n\tif tx.Type() != types.LegacyTxType {\n\t\t// However, if the user asked for a particular chain id, then we should\n\t\t// use that instead.\n\t\tif tx.ChainId().Sign() != 0 {\n\t\t\targs.ChainID = (*hexutil.Big)(tx.ChainId())\n\t\t}\n\t\taccessList := tx.AccessList()\n\t\targs.AccessList = &accessList\n\t}\n\tif tx.Type() == types.BlobTxType {\n\t\targs.BlobHashes = tx.BlobHashes()\n\t\tsidecar := tx.BlobTxSidecar()\n\t\tif sidecar == nil {\n\t\t\treturn nil, errors.New(\"blobs must be present for signing\")\n\t\t}\n\t\targs.Blobs = sidecar.Blobs\n\t\targs.Commitments = sidecar.Commitments\n\t\targs.Proofs = sidecar.Proofs\n\t}\n\n\tvar res signTransactionResult\n\tif err := api.client.Call(&res, \"account_signTransaction\", args)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/accounts/external/backend.go",
          "line": 160,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/accounts/external/backend.go",
          "line": 176,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/accounts/external/backend.go",
          "line": 250,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/accounts/external/backend.go",
          "line": 269,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/accounts/external/backend.go",
          "line": 277,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/abigen/bind_test.go",
          "line": 1862,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t}\n\t\t\tif count != 1 {\n\t\t\t\tt.Fatal(\"Unexpected contract event number\")\n\t\t\t}\n\t\t\t`,\n\t\tnil,\n\t\tnil,\n\t\tnil,\n\t\tnil,\n\t},\n\t// Test errors introduced in v0.8.4\n\t{\n\t\t`NewErrors`,\n\t\t`\n\t\tpragma solidity >0.8.4",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 160,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 170,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 180,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 570,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/bind/v2/dep_tree_test.go",
          "line": 186,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 20 {\n\t\t\t\tvar dep common.Address\n\t\t\t\tdep.SetBytes(deployer[i : i+20])\n\t\t\t\tif _, ok := overridesAddrs[dep]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/bind/v2/base.go",
          "line": 167,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/abi/bind/v2/base.go",
          "line": 184,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/accounts/abi/bind/v2/lib_test.go",
          "line": 111,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/accounts/abi/bind/v2/lib_test.go",
          "line": 179,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/trienode/proof.go",
          "line": 58,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(value)\n\n\treturn nil\n}\n\n// Delete removes a node from the set\nfunc (db *ProofSet) Delete(key []byte) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\tdelete(db.nodes, string(key))\n\treturn nil\n}\n\nfunc (db *ProofSet) DeleteRange(start, end []byte) error {\n\tpanic(\"not supported\")\n}\n\n// Get returns a stored node\nfunc (db *ProofSet) Get(key []byte) ([]byte, error) {\n\tdb.lock.RLock()\n\tdefer db.lock.RUnlock()\n\n\tif entry, ok := db.nodes[string(key)]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/trie/trienode/proof.go",
          "line": 163,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(node)\n\t}\n\treturn size\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/trie/trienode/node.go",
          "line": 96,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t} else {\n\t\tset.updates += 1\n\t}\n\tset.Nodes[string(path)] = n\n}\n\n// MergeSet merges this 'set' with 'other'. It assumes that the sets are disjoint,\n// and thus does not deduplicate data (count deletes, dedup leaves etc).\nfunc (set *NodeSet) MergeSet(other *NodeSet) error {\n\tif set.Owner != other.Owner {\n\t\treturn fmt.Errorf(\"nodesets belong to different owner are not mergeable %x-%x\", set.Owner, other.Owner)\n\t}\n\tmaps.Copy(set.Nodes, other.Nodes)\n\n\tset.deletes += other.deletes\n\tset.updates += other.updates\n\n\t// Since we assume the sets are disjoint, we can safely append leaves\n\t// like this without deduplication.\n\tset.Leaves = append(set.Leaves, other.Leaves...)\n\treturn nil\n}\n\n// Merge adds a set of nodes into the set.\nfunc (set *NodeSet) Merge(owner common.Hash, nodes map[string]*Node) error {\n\tif set.Owner != owner {\n\t\treturn fmt.Errorf(\"nodesets belong to different owner are not mergeable %x-%x\", set.Owner, owner)\n\t}\n\tfor path, node := range nodes {\n\t\tprev, ok := set.Nodes[path]\n\t\tif ok {\n\t\t\t// overwrite happens, revoke the counter\n\t\t\tif prev.IsDeleted() {\n\t\t\t\tset.deletes -= 1\n\t\t\t} else {\n\t\t\t\tset.updates -= 1\n\t\t\t}\n\t\t}\n\t\tif node.IsDeleted() {\n\t\t\tset.deletes += 1\n\t\t} else {\n\t\t\tset.updates += 1\n\t\t}\n\t\tset.Nodes[path] = node\n\t}\n\treturn nil\n}\n\n// AddLeaf adds the provided leaf node into set. TODO(rjl493456442) how can\n// we get rid of it?\nfunc (set *NodeSet) AddLeaf(parent common.Hash, blob []byte) {\n\tset.Leaves = append(set.Leaves, &leaf{Blob: blob, Parent: parent})\n}\n\n// Size returns the number of dirty nodes in set.\nfunc (set *NodeSet) Size() (int, int) {\n\treturn set.updates, set.deletes\n}\n\n// HashSet returns a set of trie nodes keyed by node hash.\nfunc (set *NodeSet) HashSet() map[common.Hash][]byte {\n\tret := make(map[common.Hash][]byte, len(set.Nodes))\n\tfor _, n := range set.Nodes {\n\t\tret[n.Hash] = n.Blob\n\t}\n\treturn ret\n}\n\n// Summary returns a string-representation of the NodeSet.\nfunc (set *NodeSet) Summary() string {\n\tvar out = new(strings.Builder)\n\tfmt.Fprintf(out, \"nodeset owner: %v\\n\", set.Owner)\n\tfor path, n := range set.Nodes {\n\t\t// Deletion\n\t\tif n.IsDeleted() {\n\t\t\tfmt.Fprintf(out, \"  [-]: %x\\n\", path)\n\t\t\tcontinue\n\t\t}\n\t\t// Insertion or update\n\t\tfmt.Fprintf(out, \"  [+/*]: %x -> %v \\n\", path, n.Hash)\n\t}\n\tfor _, n := range set.Leaves {\n\t\tfmt.Fprintf(out, \"[leaf]: %v\\n\", n)\n\t}\n\treturn out.String()\n}\n\n// MergedNodeSet represents a merged node set for a group of tries.\ntype MergedNodeSet struct {\n\tSets map[common.Hash]*NodeSet\n}\n\n// NewMergedNodeSet initializes an empty merged set.\nfunc NewMergedNodeSet() *MergedNodeSet {\n\treturn &MergedNodeSet{Sets: make(map[common.Hash]*NodeSet)}\n}\n\n// NewWithNodeSet constructs a merged nodeset with the provided single set.\nfunc NewWithNodeSet(set *NodeSet) *MergedNodeSet {\n\tmerged := NewMergedNodeSet()\n\tmerged.Merge(set)\n\treturn merged\n}\n\n// Merge merges the provided dirty nodes of a trie into the set. The assumption\n// is held that no duplicated set belonging to the same trie will be merged twice.\nfunc (set *MergedNodeSet) Merge(other *NodeSet) error {\n\tsubset, present := set.Sets[other.Owner]\n\tif present {\n\t\treturn subset.Merge(other.Owner, other.Nodes)\n\t}\n\tset.Sets[other.Owner] = other\n\treturn nil\n}\n\n// Flatten returns a two-dimensional map for internal nodes.\nfunc (set *MergedNodeSet) Flatten() map[common.Hash]map[string]*Node {\n\tnodes := make(map[common.Hash]map[string]*Node, len(set.Sets))\n\tfor owner, set := range set.Sets {\n\t\tnodes[owner] = set.Nodes\n\t}\n\treturn nodes\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/trie/trienode/node.go",
          "line": 130,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\t\t} else {\n\t\t\t\tset.updates -= 1\n\t\t\t}\n\t\t}\n\t\tif node.IsDeleted() {\n\t\t\tset.deletes += 1\n\t\t} else {\n\t\t\tset.updates += 1\n\t\t}\n\t\tset.Nodes[path] = node\n\t}\n\treturn nil\n}\n\n// AddLeaf adds the provided leaf node into set. TODO(rjl493456442) how can\n// we get rid of it?\nfunc (set *NodeSet) AddLeaf(parent common.Hash, blob []byte) {\n\tset.Leaves = append(set.Leaves, &leaf{Blob: blob, Parent: parent})\n}\n\n// Size returns the number of dirty nodes in set.\nfunc (set *NodeSet) Size() (int, int) {\n\treturn set.updates, set.deletes\n}\n\n// HashSet returns a set of trie nodes keyed by node hash.\nfunc (set *NodeSet) HashSet() map[common.Hash][]byte {\n\tret := make(map[common.Hash][]byte, len(set.Nodes))\n\tfor _, n := range set.Nodes {\n\t\tret[n.Hash] = n.Blob\n\t}\n\treturn ret\n}\n\n// Summary returns a string-representation of the NodeSet.\nfunc (set *NodeSet) Summary() string {\n\tvar out = new(strings.Builder)\n\tfmt.Fprintf(out, \"nodeset owner: %v\\n\", set.Owner)\n\tfor path, n := range set.Nodes {\n\t\t// Deletion\n\t\tif n.IsDeleted() {\n\t\t\tfmt.Fprintf(out, \"  [-]: %x\\n\", path)\n\t\t\tcontinue\n\t\t}\n\t\t// Insertion or update\n\t\tfmt.Fprintf(out, \"  [+/*]: %x -> %v \\n\", path, n.Hash)\n\t}\n\tfor _, n := range set.Leaves {\n\t\tfmt.Fprintf(out, \"[leaf]: %v\\n\", n)\n\t}\n\treturn out.String()\n}\n\n// MergedNodeSet represents a merged node set for a group of tries.\ntype MergedNodeSet struct {\n\tSets map[common.Hash]*NodeSet\n}\n\n// NewMergedNodeSet initializes an empty merged set.\nfunc NewMergedNodeSet() *MergedNodeSet {\n\treturn &MergedNodeSet{Sets: make(map[common.Hash]*NodeSet)}\n}\n\n// NewWithNodeSet constructs a merged nodeset with the provided single set.\nfunc NewWithNodeSet(set *NodeSet) *MergedNodeSet {\n\tmerged := NewMergedNodeSet()\n\tmerged.Merge(set)\n\treturn merged\n}\n\n// Merge merges the provided dirty nodes of a trie into the set. The assumption\n// is held that no duplicated set belonging to the same trie will be merged twice.\nfunc (set *MergedNodeSet) Merge(other *NodeSet) error {\n\tsubset, present := set.Sets[other.Owner]\n\tif present {\n\t\treturn subset.Merge(other.Owner, other.Nodes)\n\t}\n\tset.Sets[other.Owner] = other\n\treturn nil\n}\n\n// Flatten returns a two-dimensional map for internal nodes.\nfunc (set *MergedNodeSet) Flatten() map[common.Hash]map[string]*Node {\n\tnodes := make(map[common.Hash]map[string]*Node, len(set.Sets))\n\tfor owner, set := range set.Sets {\n\t\tnodes[owner] = set.Nodes\n\t}\n\treturn nodes\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/lru/blob_lru_test.go",
          "line": 40,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(v))\n\t\tif want > 100 {\n\t\t\twant = 100\n\t\t}\n\t\tif have := lru.size",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/lru/blob_lru.go",
          "line": 70,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(len(v))\n\t\t}\n\t\tc.size = targetSize\n\t}\n\tc.lru.Add(key, value)\n\treturn evicted\n}\n\n// Get looks up a key's value from the cache.\nfunc (c *SizeConstrainedCache[K, V]) Get(key K) (V, bool) {\n\tc.lock.Lock()\n\tdefer c.lock.Unlock()\n\n\treturn c.lru.Get(key)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/hexutil/hexutil.go",
          "line": 157,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tdec := new(big.Int).SetBits(words)\n\treturn dec, nil\n}\n\n// MustDecodeBig decodes a hex string with 0x prefix as a quantity.\n// It panics for invalid input.\nfunc MustDecodeBig(input string) *big.Int {\n\tdec, err := DecodeBig(input)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn dec\n}\n\n// EncodeBig encodes bigint as a hex string with 0x prefix.\nfunc EncodeBig(bigint *big.Int) string {\n\tif sign := bigint.Sign()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/common/hexutil/hexutil.go",
          "line": 156,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 16\n\t\t\twords[i] += big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tdec := new(big.Int).SetBits(words)\n\treturn dec, nil\n}\n\n// MustDecodeBig decodes a hex string with 0x prefix as a quantity.\n// It panics for invalid input.\nfunc MustDecodeBig(input string) *big.Int {\n\tdec, err := DecodeBig(input)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn dec\n}\n\n// EncodeBig encodes bigint as a hex string with 0x prefix.\nfunc EncodeBig(bigint *big.Int) string {\n\tif sign := bigint.Sign()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/hexutil/json.go",
          "line": 189,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tvar dec big.Int\n\tdec.SetBits(words)\n\t*b = (Big)(dec)\n\treturn nil\n}\n\n// ToInt converts b to a big.Int.\nfunc (b *Big) ToInt() *big.Int {\n\treturn (*big.Int)(b)\n}\n\n// String returns the hex encoding of b.\nfunc (b *Big) String() string {\n\treturn EncodeBig(b.ToInt())\n}\n\n// ImplementsGraphQLType returns true if Big implements the provided GraphQL type.\nfunc (b Big) ImplementsGraphQLType(name string) bool { return name == \"BigInt\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Big) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\tvar num big.Int\n\t\tnum.SetInt64(int64(input))\n\t\t*b = Big(num)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for BigInt\", input)\n\t}\n\treturn err\n}\n\n// U256 marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype U256 uint256.Int\n\n// MarshalText implements encoding.TextMarshaler\nfunc (b U256) MarshalText() ([]byte, error) {\n\tu256 := (*uint256.Int)(&b)\n\treturn []byte(u256.Hex()), nil\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *U256) UnmarshalJSON(input []byte) error {\n\t// The uint256.Int.UnmarshalJSON method accepts \"dec\", \"0xhex\"",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/common/hexutil/json.go",
          "line": 306,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nib\n\t}\n\t*b = Uint64(dec)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint64) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\n// ImplementsGraphQLType returns true if Uint64 implements the provided GraphQL type.\nfunc (b Uint64) ImplementsGraphQLType(name string) bool { return name == \"Long\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Uint64) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\t*b = Uint64(input)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for Long\", input)\n\t}\n\treturn err\n}\n\n// Uint marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype Uint uint\n\n// MarshalText implements encoding.TextMarshaler.\nfunc (b Uint) MarshalText() ([]byte, error) {\n\treturn Uint64(b).MarshalText()\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *Uint) UnmarshalJSON(input []byte) error {\n\tif !isString(input) {\n\t\treturn errNonString(uintT)\n\t}\n\treturn wrapTypeError(b.UnmarshalText(input[1:len(input)-1]), uintT)\n}\n\n// UnmarshalText implements encoding.TextUnmarshaler.\nfunc (b *Uint) UnmarshalText(input []byte) error {\n\tvar u64 Uint64\n\terr := u64.UnmarshalText(input)\n\tif u64 > Uint64(^uint(0)) || err == ErrUint64Range {\n\t\treturn ErrUintRange\n\t} else if err != nil {\n\t\treturn err\n\t}\n\t*b = Uint(u64)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\nfunc isString(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '\"' && input[len(input)-1] == '\"'\n}\n\nfunc bytesHave0xPrefix(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '0' && (input[1] == 'x' || input[1] == 'X')\n}\n\nfunc checkText(input []byte, wantPrefix bool) ([]byte, error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif bytesHave0xPrefix(input) {\n\t\tinput = input[2:]\n\t} else if wantPrefix {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tif len(input)%2 != 0 {\n\t\treturn nil, ErrOddLength\n\t}\n\treturn input, nil\n}\n\nfunc checkNumberText(input []byte) (raw []byte, err error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif !bytesHave0xPrefix(input) {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tinput = input[2:]\n\tif len(input) == 0 {\n\t\treturn nil, ErrEmptyNumber\n\t}\n\tif len(input) > 1 && input[0] == '0' {\n\t\treturn nil, ErrLeadingZero\n\t}\n\treturn input, nil\n}\n\nfunc wrapTypeError(err error, typ reflect.Type) error {\n\tif _, ok := err.(*decError)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/common/hexutil/json.go",
          "line": 188,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 16\n\t\t\twords[i] += big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tvar dec big.Int\n\tdec.SetBits(words)\n\t*b = (Big)(dec)\n\treturn nil\n}\n\n// ToInt converts b to a big.Int.\nfunc (b *Big) ToInt() *big.Int {\n\treturn (*big.Int)(b)\n}\n\n// String returns the hex encoding of b.\nfunc (b *Big) String() string {\n\treturn EncodeBig(b.ToInt())\n}\n\n// ImplementsGraphQLType returns true if Big implements the provided GraphQL type.\nfunc (b Big) ImplementsGraphQLType(name string) bool { return name == \"BigInt\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Big) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\tvar num big.Int\n\t\tnum.SetInt64(int64(input))\n\t\t*b = Big(num)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for BigInt\", input)\n\t}\n\treturn err\n}\n\n// U256 marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype U256 uint256.Int\n\n// MarshalText implements encoding.TextMarshaler\nfunc (b U256) MarshalText() ([]byte, error) {\n\tu256 := (*uint256.Int)(&b)\n\treturn []byte(u256.Hex()), nil\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *U256) UnmarshalJSON(input []byte) error {\n\t// The uint256.Int.UnmarshalJSON method accepts \"dec\", \"0xhex\"",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/common/hexutil/json.go",
          "line": 305,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 16\n\t\tdec += nib\n\t}\n\t*b = Uint64(dec)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint64) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\n// ImplementsGraphQLType returns true if Uint64 implements the provided GraphQL type.\nfunc (b Uint64) ImplementsGraphQLType(name string) bool { return name == \"Long\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Uint64) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\t*b = Uint64(input)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for Long\", input)\n\t}\n\treturn err\n}\n\n// Uint marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype Uint uint\n\n// MarshalText implements encoding.TextMarshaler.\nfunc (b Uint) MarshalText() ([]byte, error) {\n\treturn Uint64(b).MarshalText()\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *Uint) UnmarshalJSON(input []byte) error {\n\tif !isString(input) {\n\t\treturn errNonString(uintT)\n\t}\n\treturn wrapTypeError(b.UnmarshalText(input[1:len(input)-1]), uintT)\n}\n\n// UnmarshalText implements encoding.TextUnmarshaler.\nfunc (b *Uint) UnmarshalText(input []byte) error {\n\tvar u64 Uint64\n\terr := u64.UnmarshalText(input)\n\tif u64 > Uint64(^uint(0)) || err == ErrUint64Range {\n\t\treturn ErrUintRange\n\t} else if err != nil {\n\t\treturn err\n\t}\n\t*b = Uint(u64)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\nfunc isString(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '\"' && input[len(input)-1] == '\"'\n}\n\nfunc bytesHave0xPrefix(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '0' && (input[1] == 'x' || input[1] == 'X')\n}\n\nfunc checkText(input []byte, wantPrefix bool) ([]byte, error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif bytesHave0xPrefix(input) {\n\t\tinput = input[2:]\n\t} else if wantPrefix {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tif len(input)%2 != 0 {\n\t\treturn nil, ErrOddLength\n\t}\n\treturn input, nil\n}\n\nfunc checkNumberText(input []byte) (raw []byte, err error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif !bytesHave0xPrefix(input) {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tinput = input[2:]\n\tif len(input) == 0 {\n\t\treturn nil, ErrEmptyNumber\n\t}\n\tif len(input) > 1 && input[0] == '0' {\n\t\treturn nil, ErrLeadingZero\n\t}\n\treturn input, nil\n}\n\nfunc wrapTypeError(err error, typ reflect.Type) error {\n\tif _, ok := err.(*decError)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/prque/lazyqueue_test.go",
          "line": 100,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rand.Int63n(testPriorityStep*2-1) + 1\n\t\tif items[i].p > maxPri {\n\t\t\tmaxPri = items[i].p\n\t\t}\n\t\titems[i].last = clock.Now()\n\t\tif items[i].p > items[i].maxp {\n\t\t\tq.Update(items[i].index)\n\t\t}\n\t\tif rand.Intn(100) == 0 {\n\t\t\tp := q.PopItem().(*lazyItem)\n\t\t\tif p.p != maxPri {\n\t\t\t\tlock.Unlock()\n\t\t\t\tclose(stopCh)\n\t\t\t\tt.Fatalf(\"incorrect item (best known priority %d, popped %d)\", maxPri, p.p)\n\t\t\t}\n\t\t\tq.Push(p)\n\t\t}\n\t\tlock.Unlock()\n\t\tclock.Run(testStepPeriod)\n\t\tclock.WaitForTimers(1)\n\t}\n\n\tclose(stopCh)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/common/prque/sstack.go",
          "line": 58,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= blockSize\n\t\ts.offset = 0\n\t} else if s.offset == blockSize {\n\t\ts.active = s.blocks[s.size/blockSize]\n\t\ts.offset = 0\n\t}\n\tif s.setIndex != nil {\n\t\ts.setIndex(data.(*item[P, V]).value, s.size)\n\t}\n\ts.active[s.offset] = data.(*item[P, V])\n\ts.offset++\n\ts.size++\n}\n\n// Pop a value off the stack and returns it. Currently no shrinking is done.\n// Required by heap.Interface.\nfunc (s *sstack[P, V]) Pop() (res any) {\n\ts.size--\n\ts.offset--\n\tif s.offset < 0 {\n\t\ts.offset = blockSize - 1\n\t\ts.active = s.blocks[s.size/blockSize]\n\t}\n\tres, s.active[s.offset] = s.active[s.offset], nil\n\tif s.setIndex != nil {\n\t\ts.setIndex(res.(*item[P, V]).value, -1)\n\t}\n\treturn\n}\n\n// Len returns the length of the stack. Required by sort.Interface.\nfunc (s *sstack[P, V]) Len() int {\n\treturn s.size\n}\n\n// Less compares the priority of two elements of the stack (higher is first).\n// Required by sort.Interface.\nfunc (s *sstack[P, V]) Less(i, j int) bool {\n\treturn s.blocks[i/blockSize][i%blockSize].priority > s.blocks[j/blockSize][j%blockSize].priority\n}\n\n// Swap two elements in the stack. Required by sort.Interface.\nfunc (s *sstack[P, V]) Swap(i, j int) {\n\tib, io, jb, jo := i/blockSize, i%blockSize, j/blockSize, j%blockSize\n\ta, b := s.blocks[jb][jo], s.blocks[ib][io]\n\tif s.setIndex != nil {\n\t\ts.setIndex(a.value, i)\n\t\ts.setIndex(b.value, j)\n\t}\n\ts.blocks[ib][io], s.blocks[jb][jo] = a, b\n}\n\n// Reset the stack, effectively clearing its contents.\nfunc (s *sstack[P, V]) Reset() {\n\t*s = *newSstack[P, V](s.setIndex)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/rlp/rlpgen/gen.go",
          "line": 576,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" || \"\n\t\t\t}\n\t\t\tcond += zeroV[j]\n\t\t}\n\t\tfmt.Fprintf(b, \"if %s {\\n\", cond)\n\t\tfmt.Fprint(b, field.elem.genWrite(ctx, selector))\n\t\tfmt.Fprintf(b, \"}\\n\")\n\t}\n}\n\nfunc (op structOp) genDecode(ctx *genContext) (string, string) {\n\t// Get the string representation of the type.\n\t// Here, named types are handled separately because the output\n\t// would contain a copy of the struct definition otherwise.\n\tvar typeName string\n\tif op.named != nil {\n\t\ttypeName = types.TypeString(op.named, ctx.qualify)\n\t} else {\n\t\ttypeName = types.TypeString(op.typ, ctx.qualify)\n\t}\n\n\t// Create struct object.\n\tvar resultV = ctx.temp()\n\tvar b bytes.Buffer\n\tfmt.Fprintf(&b, \"var %s %s\\n\", resultV, typeName)\n\n\t// Decode fields.\n\tfmt.Fprintf(&b, \"{\\n\")\n\tfmt.Fprintf(&b, \"if _, err := dec.List()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/flags/helpers.go",
          "line": 207,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= wlen\n\n\t\tif sp == -1 {\n\t\t\tbreak\n\t\t}\n\t\ts = s[wlen+1:]\n\t}\n\n\treturn output.String()\n}\n\n// AutoEnvVars extends all the specific CLI flags with automatically generated\n// env vars by capitalizing the flag, replacing . with _ and prefixing it with\n// the specified string.\n//\n// Note, the prefix should *not* contain the separator underscore, that will be\n// added automatically.\nfunc AutoEnvVars(flags []cli.Flag, prefix string) {\n\tfor _, flag := range flags {\n\t\tenvvar := strings.ToUpper(prefix + \"_\" + strings.ReplaceAll(strings.ReplaceAll(flag.Names()[0], \".\", \"_\"), \"-\", \"_\"))\n\n\t\tswitch flag := flag.(type) {\n\t\tcase *cli.StringFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.StringSliceFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.BoolFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.IntFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.Int64Flag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.Uint64Flag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.Float64Flag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.DurationFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.PathFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *BigFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *DirectoryFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\t\t}\n\t}\n}\n\n// CheckEnvVars iterates over all the environment variables and checks if any of\n// them look like a CLI flag but is not consumed. This can be used to detect old\n// or mistyped names.\nfunc CheckEnvVars(ctx *cli.Context, flags []cli.Flag, prefix string) {\n\tknown := make(map[string]string)\n\tfor _, flag := range flags {\n\t\tdocflag, ok := flag.(cli.DocGenerationFlag)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, envvar := range docflag.GetEnvVars() {\n\t\t\tknown[envvar] = flag.Names()[0]\n\t\t}\n\t}\n\tkeyvals := os.Environ()\n\tsort.Strings(keyvals)\n\n\tfor _, keyval := range keyvals {\n\t\tkey := strings.Split(keyval, \"=\")[0]\n\t\tif !strings.HasPrefix(key, prefix) {\n\t\t\tcontinue\n\t\t}\n\t\tif flag, ok := known[key]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/flags/helpers.go",
          "line": 318,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"=\" + option\n\t\t\t\t\tset = append(set, \"--\"+name)\n\t\t\t\t}\n\t\t\t\t// shift arguments and continue\n\t\t\t\ti++\n\t\t\t\tcontinue\n\n\t\t\tcase cli.Flag:\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"invalid argument, not cli.Flag or string extension: %T\", args[i+1]))\n\t\t\t}\n\t\t}\n\t\t// Mark the flag if it's set\n\t\tif ctx.IsSet(flag.Names()[0]) {\n\t\t\tset = append(set, \"--\"+name)\n\t\t}\n\t}\n\tif len(set) > 1 {\n\t\tfmt.Fprintf(os.Stderr, \"Flags %v can't be used at the same time\", strings.Join(set, \", \"))\n\t\tos.Exit(1)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/utesting/utesting.go",
          "line": 205,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nn\n\t\t\treturn n, err\n\t\t}\n\n\t\tline := b[:end+1]\n\t\tnn, err := w.out.Write(line)\n\t\tn += nn\n\t\tif err != nil {\n\t\t\treturn n, err\n\t\t}\n\t\tb = b[end+1:]\n\t\tw.inLine = false\n\t}\n\treturn n, err\n}\n\n// flush ensures the current line is terminated.\nfunc (w *indentWriter) flush() {\n\tif w.inLine {\n\t\tfmt.Println(w.out)\n\t\tw.inLine = false\n\t}\n}\n\n// CountFailures returns the number of failed tests in the result slice.\nfunc CountFailures(rr []Result) int {\n\tcount := 0\n\tfor _, r := range rr {\n\t\tif r.Failed {\n\t\t\tcount++\n\t\t}\n\t}\n\treturn count\n}\n\n// Run executes a single test.\nfunc Run(test Test) (bool, string) {\n\toutput := new(bytes.Buffer)\n\tfailed := runTest(test, output)\n\treturn failed, output.String()\n}\n\nfunc runTest(test Test, output io.Writer) bool {\n\tt := &T{output: output}\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tdefer func() {\n\t\t\tif err := recover()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/utesting/utesting.go",
          "line": 312,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"\\n\"\n\t}\n\tfmt.Fprintf(t.output, format, vs...)\n}\n\n// Error is equivalent to Log followed by Fail.\nfunc (t *T) Error(vs ...interface{}) {\n\tt.Log(vs...)\n\tt.Fail()\n}\n\n// Errorf is equivalent to Logf followed by Fail.\nfunc (t *T) Errorf(format string, vs ...interface{}) {\n\tt.Logf(format, vs...)\n\tt.Fail()\n}\n\n// Fatal is equivalent to Log followed by FailNow.\nfunc (t *T) Fatal(vs ...interface{}) {\n\tt.Log(vs...)\n\tt.FailNow()\n}\n\n// Fatalf is equivalent to Logf followed by FailNow.\nfunc (t *T) Fatalf(format string, vs ...interface{}) {\n\tt.Logf(format, vs...)\n\tt.FailNow()\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/era/iterator.go",
          "line": 158,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif it.Body, n, it.err = newSnappyReader(it.e.s, TypeCompressedBody, off)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/era/iterator.go",
          "line": 163,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif it.Receipts, n, it.err = newSnappyReader(it.e.s, TypeCompressedReceipts, off)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/internal/era/iterator.go",
          "line": 168,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif it.TotalDifficulty, _, it.err = it.e.s.ReaderAt(TypeTotalDifficulty, off)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/internal/era/iterator.go",
          "line": 173,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\treturn true\n}\n\n// Number returns the current number block the iterator will return.\nfunc (it *RawIterator) Number() uint64 {\n\treturn it.next - 1\n}\n\n// Error returns the error status of the iterator. It should be called before\n// reading from any of the iterator's values.\nfunc (it *RawIterator) Error() error {\n\tif it.err == io.EOF {\n\t\treturn nil\n\t}\n\treturn it.err\n}\n\n// clear sets all the outputs to nil.\nfunc (it *RawIterator) clear() {\n\tit.Header = nil\n\tit.Body = nil\n\tit.Receipts = nil\n\tit.TotalDifficulty = nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/era/era.go",
          "line": 80,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\teras = append(eras, entry.Name())\n\t}\n\treturn eras, nil\n}\n\ntype ReadAtSeekCloser interface {\n\tio.ReaderAt\n\tio.Seeker\n\tio.Closer\n}\n\n// Era reads and Era1 file.\ntype Era struct {\n\tf   ReadAtSeekCloser // backing era1 file\n\ts   *e2store.Reader  // e2store reader over f\n\tm   metadata         // start, count, length info\n\tmu  *sync.Mutex      // lock for buf\n\tbuf [8]byte          // buffer reading entry offsets\n}\n\n// From returns an Era backed by f.\nfunc From(f ReadAtSeekCloser) (*Era, error) {\n\tm, err := readMetadata(f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Era{\n\t\tf:  f,\n\t\ts:  e2store.NewReader(f),\n\t\tm:  m,\n\t\tmu: new(sync.Mutex),\n\t}, nil\n}\n\n// Open returns an Era backed by the given filename.\nfunc Open(filename string) (*Era, error) {\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn From(f)\n}\n\nfunc (e *Era) Close() error {\n\treturn e.f.Close()\n}\n\n// GetBlockByNumber returns the block for the given block number.\nfunc (e *Era) GetBlockByNumber(num uint64) (*types.Block, error) {\n\tif e.m.start > num || e.m.start+e.m.count <= num {\n\t\treturn nil, fmt.Errorf(\"out-of-bounds: %d not in [%d, %d)\", num, e.m.start, e.m.start+e.m.count)\n\t}\n\toff, err := e.readOffset(num)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tr, n, err := newSnappyReader(e.s, TypeCompressedHeader, off)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar header types.Header\n\tif err := rlp.Decode(r, &header)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/era/era.go",
          "line": 145,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tr, _, err = newSnappyReader(e.s, TypeCompressedBody, off)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar body types.Body\n\tif err := rlp.Decode(r, &body)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/internal/era/era.go",
          "line": 231,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\n\t// Skip over header and body.\n\toff, err = e.s.SkipN(off, 2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Read total difficulty after first block.\n\tif r, _, err = e.s.ReaderAt(TypeTotalDifficulty, off)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/era/builder.go",
          "line": 127,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\t}\n\tif len(b.indexes) >= MaxEra1Size {\n\t\treturn fmt.Errorf(\"exceeds maximum batch size of %d\", MaxEra1Size)\n\t}\n\n\tb.indexes = append(b.indexes, uint64(b.written))\n\tb.hashes = append(b.hashes, hash)\n\tb.tds = append(b.tds, td)\n\n\t// Write block data.\n\tif err := b.snappyWrite(TypeCompressedHeader, header)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/era/builder.go",
          "line": 151,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Finalize computes the accumulator and block index values, then writes the\n// corresponding e2store entries.\nfunc (b *Builder) Finalize() (common.Hash, error) {\n\tif b.startNum == nil {\n\t\treturn common.Hash{}, errors.New(\"finalize called on empty builder\")\n\t}\n\t// Compute accumulator root and write entry.\n\troot, err := ComputeAccumulator(b.hashes, b.tds)\n\tif err != nil {\n\t\treturn common.Hash{}, fmt.Errorf(\"error calculating accumulator root: %w\", err)\n\t}\n\tn, err := b.w.Write(TypeAccumulator, root[:])\n\tb.written += n\n\tif err != nil {\n\t\treturn common.Hash{}, fmt.Errorf(\"error writing accumulator: %w\", err)\n\t}\n\t// Get beginning of index entry to calculate block relative offset.\n\tbase := int64(b.written)\n\n\t// Construct block index. Detailed format described in Builder\n\t// documentation, but it is essentially encoded as:\n\t// \"start | index | index | ... | count\"\n\tvar (\n\t\tcount = len(b.indexes)\n\t\tindex = make([]byte, 16+count*8)\n\t)\n\tbinary.LittleEndian.PutUint64(index, *b.startNum)\n\t// Each offset is relative from the position it is encoded in the\n\t// index. This means that even if the same block was to be included in\n\t// the index twice (this would be invalid anyways), the relative offset\n\t// would be different. The idea with this is that after reading a\n\t// relative offset, the corresponding block can be quickly read by\n\t// performing a seek relative to the current position.\n\tfor i, offset := range b.indexes {\n\t\trelative := int64(offset) - base\n\t\tbinary.LittleEndian.PutUint64(index[8+i*8:], uint64(relative))\n\t}\n\tbinary.LittleEndian.PutUint64(index[8+count*8:], uint64(count))\n\n\t// Finally, write the block index entry.\n\tif _, err := b.w.Write(TypeBlockIndex, index)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/internal/era/builder.go",
          "line": 221,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error writing e2store entry: %w\", err)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/version/version.go",
          "line": 41,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"-\" + version.Meta\n\t}\n\treturn v\n}()\n\nfunc WithCommit(gitCommit, gitDate string) string {\n\tvsn := WithMeta\n\tif len(gitCommit) >= 8 {\n\t\tvsn += \"-\" + gitCommit[:8]\n\t}\n\tif (version.Meta != \"stable\") && (gitDate != \"\") {\n\t\tvsn += \"-\" + gitDate\n\t}\n\treturn vsn\n}\n\n// Archive holds the textual version string used for Geth archives. e.g.\n// \"1.8.11-dea1ce05\" for stable releases, or \"1.8.13-unstable-21c059b6\" for unstable\n// releases.\nfunc Archive(gitCommit string) string {\n\tvsn := Semantic\n\tif version.Meta != \"stable\" {\n\t\tvsn += \"-\" + version.Meta\n\t}\n\tif len(gitCommit) >= 8 {\n\t\tvsn += \"-\" + gitCommit[:8]\n\t}\n\treturn vsn\n}\n\n// ClientName creates a software name/version identifier according to common\n// conventions in the Ethereum p2p network.\nfunc ClientName(clientIdentifier string) string {\n\tgit, _ := VCS()\n\treturn fmt.Sprintf(\"%s/v%v/%v-%v/%v\",\n\t\tstrings.Title(clientIdentifier),\n\t\tWithCommit(git.Commit, git.Date),\n\t\truntime.GOOS, runtime.GOARCH,\n\t\truntime.Version(),\n\t)\n}\n\n// Info returns build and platform information about the current binary.\n//\n// If the package that is currently executing is a prefixed by our go-ethereum\n// module path, it will print out commit and date VCS information. Otherwise,\n// it will assume it's imported by a third-party and will return the imported\n// version and whether it was replaced by another module.\nfunc Info() (version, vcs string) {\n\tversion = WithMeta\n\tbuildInfo, ok := debug.ReadBuildInfo()\n\tif !ok {\n\t\treturn version, \"\"\n\t}\n\tversion = versionInfo(buildInfo)\n\tif status, ok := VCS()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/version/version.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"%s@%s\", mod.Path, mod.Version)\n\tif mod.Replace != nil {\n\t\t// If our package was replaced by something else, also note that.\n\t\tversion += fmt.Sprintf(\" (replaced by %s@%s)\", mod.Replace.Path, mod.Replace.Version)\n\t}\n\treturn version\n}\n\n// findModule returns the module at path.\nfunc findModule(info *debug.BuildInfo, path string) *debug.Module {\n\tif info.Path == ourPath {\n\t\treturn &info.Main\n\t}\n\tfor _, mod := range info.Deps {\n\t\tif mod.Path == path {\n\t\t\treturn mod\n\t\t}\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/download/download.go",
          "line": 276,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(n)\n\tpct := w.written * 10 / w.size * 10\n\tif pct != w.lastpct {\n\t\tif w.lastpct != 0 {\n\t\t\tfmt.Print(\"...\")\n\t\t}\n\t\tfmt.Print(pct, \"%\")\n\t\tw.lastpct = pct\n\t}\n\treturn n, err\n}\n\nfunc (w *downloadWriter) Close() error {\n\tif w.lastpct > 0 {\n\t\tfmt.Println() // Finish the progress line.\n\t}\n\tflushErr := w.dstBuf.Flush()\n\tcloseErr := w.file.Close()\n\tif flushErr != nil {\n\t\treturn flushErr\n\t}\n\treturn closeErr\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/jsre/completion.go",
          "line": 84,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"(\"\n\t\t\t} else {\n\t\t\t\tresults[0] += \".\"\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Strings(results)\n\treturn results\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/cmdtest/test_cmd.go",
          "line": 174,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tsubmatch := string(output[matches[i]:matches[i+1]])\n\t\tsubmatches = append(submatches, submatch)\n\t}\n\treturn re, submatches\n}\n\n// ExpectExit expects the child process to exit within 5s without\n// printing any additional text on stdout.\nfunc (tt *TestCmd) ExpectExit() {\n\tvar output []byte\n\ttt.withKillTimeout(func() {\n\t\toutput, _ = io.ReadAll(tt.stdout)\n\t})\n\ttt.WaitExit()\n\tif tt.Cleanup != nil {\n\t\ttt.Cleanup()\n\t}\n\tif len(output) > 0 {\n\t\ttt.Errorf(\"Unmatched stdout text:\\n%s\", output)\n\t}\n}\n\nfunc (tt *TestCmd) WaitExit() {\n\ttt.Err = tt.cmd.Wait()\n}\n\nfunc (tt *TestCmd) Interrupt() {\n\ttt.Err = tt.cmd.Process.Signal(os.Interrupt)\n}\n\n// ExitStatus exposes the process' OS exit code\n// It will only return a valid value after the process has finished.\nfunc (tt *TestCmd) ExitStatus() int {\n\tif tt.Err != nil {\n\t\texitErr := tt.Err.(*exec.ExitError)\n\t\tif exitErr != nil {\n\t\t\tif status, ok := exitErr.Sys().(syscall.WaitStatus)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/ethapi/simulate.go",
          "line": 327,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= result.UsedGas\n\t\treceipts[i] = core.MakeReceipt(evm, result, sim.state, blockContext.BlockNumber, common.Hash{}, blockContext.Time, tx, gasUsed, root)\n\t\tblobGasUsed += receipts[i].BlobGasUsed\n\t\tlogs := tracer.Logs()\n\t\tcallRes := simCallResult{ReturnValue: result.Return(), Logs: logs, GasUsed: hexutil.Uint64(result.UsedGas)}\n\t\tif result.Failed() {\n\t\t\tcallRes.Status = hexutil.Uint64(types.ReceiptStatusFailed)\n\t\t\tif errors.Is(result.Err, vm.ErrExecutionReverted) {\n\t\t\t\t// If the result contains a revert reason, try to unpack it.\n\t\t\t\trevertErr := newRevertError(result.Revert())\n\t\t\t\tcallRes.Error = &callError{Message: revertErr.Error(), Code: errCodeReverted, Data: revertErr.ErrorData().(string)}\n\t\t\t} else {\n\t\t\t\tcallRes.Error = &callError{Message: result.Err.Error(), Code: errCodeVMError}\n\t\t\t}\n\t\t} else {\n\t\t\tcallRes.Status = hexutil.Uint64(types.ReceiptStatusSuccessful)\n\t\t\tallLogs = append(allLogs, callRes.Logs...)\n\t\t}\n\t\tcallResults[i] = callRes\n\t}\n\theader.GasUsed = gasUsed\n\tif sim.chainConfig.IsCancun(header.Number, header.Time) {\n\t\theader.BlobGasUsed = &blobGasUsed\n\t}\n\tvar requests [][]byte\n\t// Process EIP-7685 requests\n\tif sim.chainConfig.IsPrague(header.Number, header.Time) {\n\t\trequests = [][]byte{}\n\t\t// EIP-6110\n\t\tif err := core.ParseDepositLogs(&requests, allLogs, sim.chainConfig)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/ethapi/transaction_opts_utils.go",
          "line": 29,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t} else if account.StorageSlots != nil {\n\t\t\tcounter += len(account.StorageSlots)\n\t\t}\n\t}\n\tif counter > MaxNumberOfEntries {\n\t\treturn errors.New(\"knownAccounts too large\")\n\t}\n\treturn TxOptsCheckStorage(o, statedb)\n}\n\nfunc TxOptsCheckStorage(o types.TransactionOpts, statedb *state.StateDB) error {\n\tfor address, accountStorage := range o.KnownAccounts {\n\t\tif accountStorage.StorageRoot != nil {\n\t\t\trootHash := statedb.GetStorageRoot(address)\n\t\t\tif rootHash != *accountStorage.StorageRoot {\n\t\t\t\treturn errors.New(\"storage root hash condition not met\")\n\t\t\t}\n\t\t} else if len(accountStorage.StorageSlots) > 0 {\n\t\t\tfor slot, value := range accountStorage.StorageSlots {\n\t\t\t\tstored := statedb.GetState(address, slot)\n\t\t\t\tif !bytes.Equal(stored.Bytes(), value.Bytes()) {\n\t\t\t\t\treturn errors.New(\"storage slot value condition not met\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/ethapi/api.go",
          "line": 2098,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 27 // Transform V from 0/1 to 27/28 according to the yellow paper\n\t}\n\treturn signature, err\n}\n\n// SignTransactionResult represents a RLP encoded signed transaction.\ntype SignTransactionResult struct {\n\tRaw hexutil.Bytes      `json:\"raw\"`\n\tTx  *types.Transaction `json:\"tx\"`\n}\n\n// SignTransaction will sign the given transaction with the from account.\n// The node needs to have the private key of the account corresponding with\n// the given from address and it needs to be unlocked.\nfunc (api *TransactionAPI) SignTransaction(ctx context.Context, args TransactionArgs) (*SignTransactionResult, error) {\n\targs.blobSidecarAllowed = true\n\n\tif args.Gas == nil {\n\t\treturn nil, errors.New(\"gas not specified\")\n\t}\n\tif args.GasPrice == nil && (args.MaxPriorityFeePerGas == nil || args.MaxFeePerGas == nil) {\n\t\treturn nil, errors.New(\"missing gasPrice or maxFeePerGas/maxPriorityFeePerGas\")\n\t}\n\tif args.Nonce == nil {\n\t\treturn nil, errors.New(\"nonce not specified\")\n\t}\n\tif err := args.setDefaults(ctx, api.b, false)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/ethapi/api_test.go",
          "line": 1270,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/build/gotool.go",
          "line": 124,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \".zip\"\n\t} else {\n\t\tfile += \".tar.gz\"\n\t}\n\turl := \"https://golang.org/dl/\" + file\n\tdst := filepath.Join(ucache, file)\n\tif err := csdb.DownloadFile(url, dst)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/debug/flags.go",
          "line": 49,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=5,p2p=4)\",\n\t\tValue:    \"\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tvmoduleFlag = &cli.StringFlag{\n\t\tName:     \"vmodule\",\n\t\tUsage:    \"Per-module verbosity: comma-separated list of <pattern>=<level> (e.g. eth/*=5,p2p=4)\",\n\t\tValue:    \"\",\n\t\tHidden:   true,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogjsonFlag = &cli.BoolFlag{\n\t\tName:     \"log.json\",\n\t\tUsage:    \"Format logs with JSON\",\n\t\tHidden:   true,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogFormatFlag = &cli.StringFlag{\n\t\tName:     \"log.format\",\n\t\tUsage:    \"Log format to use (json|logfmt|terminal)\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogFileFlag = &cli.StringFlag{\n\t\tName:     \"log.file\",\n\t\tUsage:    \"Write logs to a file\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogRotateFlag = &cli.BoolFlag{\n\t\tName:     \"log.rotate\",\n\t\tUsage:    \"Enables log file rotation\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogMaxSizeMBsFlag = &cli.IntFlag{\n\t\tName:     \"log.maxsize\",\n\t\tUsage:    \"Maximum size in MBs of a single log file\",\n\t\tValue:    100,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogMaxBackupsFlag = &cli.IntFlag{\n\t\tName:     \"log.maxbackups\",\n\t\tUsage:    \"Maximum number of log files to retain\",\n\t\tValue:    10,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogMaxAgeFlag = &cli.IntFlag{\n\t\tName:     \"log.maxage\",\n\t\tUsage:    \"Maximum number of days to retain a log file\",\n\t\tValue:    30,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogCompressFlag = &cli.BoolFlag{\n\t\tName:     \"log.compress\",\n\t\tUsage:    \"Compress the log files\",\n\t\tValue:    false,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tpprofFlag = &cli.BoolFlag{\n\t\tName:     \"pprof\",\n\t\tUsage:    \"Enable the pprof HTTP server\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tpprofPortFlag = &cli.IntFlag{\n\t\tName:     \"pprof.port\",\n\t\tUsage:    \"pprof HTTP server listening port\",\n\t\tValue:    6060,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tpprofAddrFlag = &cli.StringFlag{\n\t\tName:     \"pprof.addr\",\n\t\tUsage:    \"pprof HTTP server listening interface\",\n\t\tValue:    \"127.0.0.1\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tmemprofilerateFlag = &cli.IntFlag{\n\t\tName:     \"pprof.memprofilerate\",\n\t\tUsage:    \"Turn on memory profiling with the given rate\",\n\t\tValue:    runtime.MemProfileRate,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tblockprofilerateFlag = &cli.IntFlag{\n\t\tName:     \"pprof.blockprofilerate\",\n\t\tUsage:    \"Turn on block profiling with the given rate\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tcpuprofileFlag = &cli.StringFlag{\n\t\tName:     \"pprof.cpuprofile\",\n\t\tUsage:    \"Write CPU profile to the given file\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\ttraceFlag = &cli.StringFlag{\n\t\tName:     \"go-execution-trace\",\n\t\tUsage:    \"Write Go execution trace to the given file\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n)\n\n// Flags holds all command-line flags required for debugging.\nvar Flags = []cli.Flag{\n\tverbosityFlag,\n\tlogVmoduleFlag,\n\tvmoduleFlag,\n\tlogjsonFlag,\n\tlogFormatFlag,\n\tlogFileFlag,\n\tlogRotateFlag,\n\tlogMaxSizeMBsFlag,\n\tlogMaxBackupsFlag,\n\tlogMaxAgeFlag,\n\tlogCompressFlag,\n\tpprofFlag,\n\tpprofAddrFlag,\n\tpprofPortFlag,\n\tmemprofilerateFlag,\n\tblockprofilerateFlag,\n\tcpuprofileFlag,\n\ttraceFlag,\n}\n\nvar (\n\tglogger       *log.GlogHandler\n\tlogOutputFile io.WriteCloser\n)\n\nfunc init() {\n\tglogger = log.NewGlogHandler(log.NewTerminalHandler(os.Stderr, false))\n}\n\n// Setup initializes profiling and logging based on the CLI flags.\n// It should be called as early as possible in the program.\nfunc Setup(ctx *cli.Context) error {\n\tvar (\n\t\thandler        slog.Handler\n\t\tterminalOutput = io.Writer(os.Stderr)\n\t\toutput         io.Writer\n\t\tlogFmtFlag     = ctx.String(logFormatFlag.Name)\n\t)\n\tvar (\n\t\tlogFile  = ctx.String(logFileFlag.Name)\n\t\trotation = ctx.Bool(logRotateFlag.Name)\n\t)\n\tif len(logFile) > 0 {\n\t\tif err := validateLogLocation(filepath.Dir(logFile))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 87,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(n)\n\treturn &e, nil\n}\n\n// ReadAt reads one Entry from r at the specified offset.\nfunc (r *Reader) ReadAt(entry *Entry, off int64) (int, error) {\n\ttyp, length, err := r.ReadMetadataAt(off)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tentry.Type = typ\n\n\t// Check length bounds.\n\tif length > valueSizeLimit {\n\t\treturn headerSize, fmt.Errorf(\"item larger than item size limit %d: have %d\", valueSizeLimit, length)\n\t}\n\tif length == 0 {\n\t\treturn headerSize, nil\n\t}\n\n\t// Read value.\n\tval := make([]byte, length)\n\tif n, err := r.r.ReadAt(val, off+headerSize)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 110,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= headerSize\n\t\t// An entry with a non-zero length should not return EOF when\n\t\t// reading the value.\n\t\tif err == io.EOF {\n\t\t\treturn n, io.ErrUnexpectedEOF\n\t\t}\n\t\treturn n, err\n\t}\n\tentry.Value = val\n\treturn int(headerSize + length), nil\n}\n\n// ReaderAt returns an io.Reader delivering value data for the entry at\n// the specified offset. If the entry type does not match the expected type, an\n// error is returned.\nfunc (r *Reader) ReaderAt(expectedType uint16, off int64) (io.Reader, int, error) {\n\t// problem = need to return length+headerSize not just value length via section reader\n\ttyp, length, err := r.ReadMetadataAt(off)\n\tif err != nil {\n\t\treturn nil, headerSize, err\n\t}\n\tif typ != expectedType {\n\t\treturn nil, headerSize, fmt.Errorf(\"wrong type, want %d have %d\", expectedType, typ)\n\t}\n\tif length > valueSizeLimit {\n\t\treturn nil, headerSize, fmt.Errorf(\"item larger than item size limit %d: have %d\", valueSizeLimit, length)\n\t}\n\treturn io.NewSectionReader(r.r, off+headerSize, int64(length)), headerSize + int(length), nil\n}\n\n// LengthAt reads the header at off and returns the total length of the entry,\n// including header.\nfunc (r *Reader) LengthAt(off int64) (int64, error) {\n\t_, length, err := r.ReadMetadataAt(off)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn int64(length) + headerSize, nil\n}\n\n// ReadMetadataAt reads the header metadata at the given offset.\nfunc (r *Reader) ReadMetadataAt(off int64) (typ uint16, length uint32, err error) {\n\tb := make([]byte, headerSize)\n\tif n, err := r.r.ReadAt(b, off)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 192,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(headerSize + length)\n\t}\n}\n\n// FindAll returns all entries with the matching type.\nfunc (r *Reader) FindAll(want uint16) ([]*Entry, error) {\n\tvar (\n\t\toff     int64\n\t\ttyp     uint16\n\t\tlength  uint32\n\t\tentries []*Entry\n\t\terr     error\n\t)\n\tfor {\n\t\ttyp, length, err = r.ReadMetadataAt(off)\n\t\tif err == io.EOF {\n\t\t\treturn entries, nil\n\t\t} else if err != nil {\n\t\t\treturn entries, err\n\t\t}\n\t\tif typ == want {\n\t\t\te := new(Entry)\n\t\t\tif _, err := r.ReadAt(e, off)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 219,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(headerSize + length)\n\t}\n}\n\n// SkipN skips `n` entries starting from `offset` and returns the new offset.\nfunc (r *Reader) SkipN(offset int64, n uint64) (int64, error) {\n\tfor i := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length\n\t}\n\treturn offset, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 170,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(common.HashLength + len(node))\n}\n\n// node retrieves an encoded cached trie node from memory. If it cannot be found\n// cached, the method queries the persistent database for the content.\nfunc (db *Database) node(hash common.Hash) ([]byte, error) {\n\t// It doesn't make sense to retrieve the metaroot\n\tif hash == (common.Hash{}) {\n\t\treturn nil, errors.New(\"not found\")\n\t}\n\t// Retrieve the node from the clean cache if available\n\tif db.cleans != nil {\n\t\tif enc := db.cleans.Get(nil, hash[:])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 236,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\treturn\n\t}\n\t// The reference is for external storage trie, don't duplicate if\n\t// the reference is already existent.\n\tif db.dirties[parent].external == nil {\n\t\tdb.dirties[parent].external = make(map[common.Hash]struct{})\n\t}\n\tif _, ok := db.dirties[parent].external[child]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 249,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength\n}\n\n// Dereference removes an existing reference from a root node.\nfunc (db *Database) Dereference(root common.Hash) {\n\t// Sanity check to ensure that the meta-root is not removed\n\tif root == (common.Hash{}) {\n\t\tlog.Error(\"Attempted to dereference the trie cache meta root\")\n\t\treturn\n\t}\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\tdb.dereference(root)\n\n\tdb.gcnodes += uint64(nodes - len(db.dirties))\n\tdb.gcsize += storage - db.dirtiesSize\n\tdb.gctime += time.Since(start)\n\n\tmemcacheGCTimeTimer.Update(time.Since(start))\n\tmemcacheGCBytesMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheGCNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Dereferenced trie from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"gcnodes\", db.gcnodes, \"gcsize\", db.gcsize, \"gctime\", db.gctime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n}\n\n// dereference is the private locked version of Dereference.\nfunc (db *Database) dereference(hash common.Hash) {\n\t// If the node does not exist, it's a previously committed node.\n\tnode, ok := db.dirties[hash]\n\tif !ok {\n\t\treturn\n\t}\n\t// If there are no more references to the node, delete it and cascade\n\tif node.parents > 0 {\n\t\t// This is a special cornercase where a node loaded from disk (i.e. not in the\n\t\t// memcache any more) gets reinjected as a new node (short node split into full,\n\t\t// then reverted into short), causing a cached node to have no parents. That is\n\t\t// no problem in itself, but don't make maxint parents out of it.\n\t\tnode.parents--\n\t}\n\tif node.parents == 0 {\n\t\t// Remove the node from the flush-list\n\t\tswitch hash {\n\t\tcase db.oldest:\n\t\t\tdb.oldest = node.flushNext\n\t\t\tif node.flushNext != (common.Hash{}) {\n\t\t\t\tdb.dirties[node.flushNext].flushPrev = common.Hash{}\n\t\t\t}\n\t\tcase db.newest:\n\t\t\tdb.newest = node.flushPrev\n\t\t\tif node.flushPrev != (common.Hash{}) {\n\t\t\t\tdb.dirties[node.flushPrev].flushNext = common.Hash{}\n\t\t\t}\n\t\tdefault:\n\t\t\tdb.dirties[node.flushPrev].flushNext = node.flushNext\n\t\t\tdb.dirties[node.flushNext].flushPrev = node.flushPrev\n\t\t}\n\t\t// Dereference all children and delete the node\n\t\tnode.forChildren(func(child common.Hash) {\n\t\t\tdb.dereference(child)\n\t\t})\n\t\tdelete(db.dirties, hash)\n\t\tdb.dirtiesSize -= common.StorageSize(common.HashLength + len(node.node))\n\t\tif node.external != nil {\n\t\t\tdb.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t}\n}\n\n// Cap iteratively flushes old but still referenced trie nodes until the total\n// memory usage goes below the given threshold.\nfunc (db *Database) Cap(limit common.StorageSize) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tbatch := db.diskdb.NewBatch()\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\n\t// db.dirtiesSize only contains the useful data in the cache, but when reporting\n\t// the total memory consumption, the maintenance metadata is also needed to be\n\t// counted.\n\tsize := db.dirtiesSize + common.StorageSize(len(db.dirties)*cachedNodeSize)\n\tsize += db.childrenSize\n\n\t// Keep committing nodes from the flush-list until we're below allowance\n\toldest := db.oldest\n\tfor size > limit && oldest != (common.Hash{}) {\n\t\t// Fetch the oldest referenced node and push into the batch\n\t\tnode := db.dirties[oldest]\n\t\trawdb.WriteLegacyTrieNode(batch, oldest, node.node)\n\n\t\t// If we exceeded the ideal batch size, commit and reset\n\t\tif batch.ValueSize() >= ethdb.IdealBatchSize {\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 383,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(nodes - len(db.dirties))\n\tdb.flushsize += storage - db.dirtiesSize\n\tdb.flushtime += time.Since(start)\n\n\tmemcacheFlushTimeTimer.Update(time.Since(start))\n\tmemcacheFlushBytesMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheFlushNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Persisted nodes from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"flushnodes\", db.flushnodes, \"flushsize\", db.flushsize, \"flushtime\", db.flushtime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n\n\treturn nil\n}\n\n// Commit iterates over all the children of a particular node, writes them out\n// to disk, forcefully tearing down all references in both directions. As a side\n// effect, all pre-images accumulated up to this point are also written.\nfunc (db *Database) Commit(node common.Hash, report bool) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tstart := time.Now()\n\tbatch := db.diskdb.NewBatch()\n\n\t// Move the trie itself into the batch, flushing if enough data is accumulated\n\tnodes, storage := len(db.dirties), db.dirtiesSize\n\n\tuncacher := &cleaner{db}\n\tif err := db.commit(node, batch, uncacher)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 314,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node))\n\t\tif node.external != nil {\n\t\t\tdb.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t}\n}\n\n// Cap iteratively flushes old but still referenced trie nodes until the total\n// memory usage goes below the given threshold.\nfunc (db *Database) Cap(limit common.StorageSize) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tbatch := db.diskdb.NewBatch()\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\n\t// db.dirtiesSize only contains the useful data in the cache, but when reporting\n\t// the total memory consumption, the maintenance metadata is also needed to be\n\t// counted.\n\tsize := db.dirtiesSize + common.StorageSize(len(db.dirties)*cachedNodeSize)\n\tsize += db.childrenSize\n\n\t// Keep committing nodes from the flush-list until we're below allowance\n\toldest := db.oldest\n\tfor size > limit && oldest != (common.Hash{}) {\n\t\t// Fetch the oldest referenced node and push into the batch\n\t\tnode := db.dirties[oldest]\n\t\trawdb.WriteLegacyTrieNode(batch, oldest, node.node)\n\n\t\t// If we exceeded the ideal batch size, commit and reset\n\t\tif batch.ValueSize() >= ethdb.IdealBatchSize {\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 358,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node) + cachedNodeSize)\n\t\tif node.external != nil {\n\t\t\tsize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t\toldest = node.flushNext\n\t}\n\t// Flush out any remainder data from the last batch\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 375,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node))\n\t\tif node.external != nil {\n\t\t\tdb.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t}\n\tif db.oldest != (common.Hash{}) {\n\t\tdb.dirties[db.oldest].flushPrev = common.Hash{}\n\t}\n\tdb.flushnodes += uint64(nodes - len(db.dirties))\n\tdb.flushsize += storage - db.dirtiesSize\n\tdb.flushtime += time.Since(start)\n\n\tmemcacheFlushTimeTimer.Update(time.Since(start))\n\tmemcacheFlushBytesMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheFlushNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Persisted nodes from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"flushnodes\", db.flushnodes, \"flushsize\", db.flushsize, \"flushtime\", db.flushtime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n\n\treturn nil\n}\n\n// Commit iterates over all the children of a particular node, writes them out\n// to disk, forcefully tearing down all references in both directions. As a side\n// effect, all pre-images accumulated up to this point are also written.\nfunc (db *Database) Commit(node common.Hash, report bool) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tstart := time.Now()\n\tbatch := db.diskdb.NewBatch()\n\n\t// Move the trie itself into the batch, flushing if enough data is accumulated\n\tnodes, storage := len(db.dirties), db.dirtiesSize\n\n\tuncacher := &cleaner{db}\n\tif err := db.commit(node, batch, uncacher)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 519,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node))\n\tif node.external != nil {\n\t\tc.db.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t}\n\t// Move the flushed node into the clean cache to prevent insta-reloads\n\tif c.db.cleans != nil {\n\t\tc.db.cleans.Set(hash[:], rlp)\n\t\tmemcacheCleanWriteMeter.Mark(int64(len(rlp)))\n\t}\n\treturn nil\n}\n\nfunc (c *cleaner) Delete(key []byte) error {\n\tpanic(\"not implemented\")\n}\n\n// Update inserts the dirty nodes in provided nodeset into database and link the\n// account trie with multiple storage tries if necessary.\nfunc (db *Database) Update(root common.Hash, parent common.Hash, block uint64, nodes *trienode.MergedNodeSet) error {\n\t// Ensure the parent state is present and signal a warning if not.\n\tif parent != types.EmptyRootHash {\n\t\tif blob, _ := db.node(parent)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 530,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t\t\t\tcontinue\n\t\t\t} else if cmp == 0 {\n\t\t\t\t// the snapshot key can be overwritten\n\t\t\t\tcreated--\n\t\t\t\tif write = !bytes.Equal(kvvals[0], iter.Value)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 549,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t}\n\tif iter.Err != nil {\n\t\t// Trie errors should never happen. Still, in case of a bug, expose the\n\t\t// error here, as the outer code will presume errors are interrupts, not\n\t\t// some deeper issues.\n\t\tlog.Error(\"State snapshotter failed to iterate trie\", \"err\", iter.Err)\n\t\treturn false, nil, iter.Err\n\t}\n\t// Delete all stale snapshot states remaining\n\tistart := time.Now()\n\tfor _, key := range kvkeys {\n\t\tif err := onState(key, nil, false, true)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 564,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tinternal += time.Since(istart)\n\n\t// Update metrics for counting trie iteration\n\tif kind == snapStorage {\n\t\tstorageTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t} else {\n\t\taccountTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t}\n\tlogger.Trace(\"Regenerated state range\", \"root\", trieId.Root, \"last\", hexutil.Encode(last),\n\t\t\"count\", count, \"created\", created, \"updated\", updated, \"untouched\", untouched, \"deleted\", deleted)\n\n\t// If there are either more trie items, or there are more snap items\n\t// (in the next segment), then we need to keep working\n\treturn !trieMore && !result.diskMore, last, nil\n}\n\n// checkAndFlush checks if an interruption signal is received or the\n// batch size has exceeded the allowance.\nfunc (g *generator) checkAndFlush(ctx *generatorContext, current []byte) error {\n\tvar abort chan struct{}\n\tselect {\n\tcase abort = <-g.abort:\n\tdefault:\n\t}\n\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize || abort != nil {\n\t\tif bytes.Compare(current, g.progress) < 0 {\n\t\t\tlog.Error(\"Snapshot generator went backwards\", \"current\", fmt.Sprintf(\"%x\", current), \"genMarker\", fmt.Sprintf(\"%x\", g.progress))\n\t\t}\n\t\t// Persist the progress marker regardless of whether the batch is empty or not.\n\t\t// It may happen that all the flat states in the database are correct, so the\n\t\t// generator indeed makes progress even if there is nothing to commit.\n\t\tjournalProgress(ctx.batch, current, g.stats)\n\n\t\t// Flush out the database writes atomically\n\t\tif err := ctx.batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 645,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + 2*common.HashLength + len(val))\n\t\tg.stats.slots++\n\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := g.checkAndFlush(ctx, append(account[:], key...))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 680,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= ctx.removeStorageBefore(account)\n\n\t\tstart := time.Now()\n\t\tif delete {\n\t\t\trawdb.DeleteAccountSnapshot(ctx.batch, account)\n\t\t\twipedAccountMeter.Mark(1)\n\t\t\taccountWriteCounter.Inc(time.Since(start).Nanoseconds())\n\n\t\t\tctx.removeStorageAt(account)\n\t\t\treturn nil\n\t\t}\n\t\t// Retrieve the current account and flatten it into the internal format\n\t\tvar acc types.StateAccount\n\t\tif err := rlp.DecodeBytes(val, &acc)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 713,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tg.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(g.progress) > common.HashLength {\n\t\t\tmarker = g.progress\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := g.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 757,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= ctx.removeRemainingStorage()\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nil\n}\n\n// generate is a background thread that iterates over the state and storage tries,\n// constructing the state snapshot. All the arguments are purely for statistics\n// gathering and logging, since the method surfs the blocks as they arrive, often\n// being restarted.\nfunc (g *generator) generate(ctx *generatorContext) {\n\tg.stats.log(\"Resuming snapshot generation\", ctx.root, g.progress)\n\tdefer ctx.close()\n\n\t// Persist the initial marker and state snapshot root if progress is none\n\tif len(g.progress) == 0 {\n\t\tbatch := g.db.NewBatch()\n\t\trawdb.WriteSnapshotRoot(batch, ctx.root)\n\t\tjournalProgress(batch, g.progress, g.stats)\n\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 701,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n\t\t\t\t}\n\t\t\t\tif acc.Root == types.EmptyRootHash {\n\t\t\t\t\tdataLen -= 32\n\t\t\t\t}\n\t\t\t\trecoveredAccountMeter.Mark(1)\n\t\t\t} else {\n\t\t\t\tdata := types.SlimAccountRLP(acc)\n\t\t\t\tdataLen = len(data)\n\t\t\t\trawdb.WriteAccountSnapshot(ctx.batch, account, data)\n\t\t\t\tgeneratedAccountMeter.Mark(1)\n\t\t\t}\n\t\t\tg.stats.storage += common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tg.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(g.progress) > common.HashLength {\n\t\t\tmarker = g.progress\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := g.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/database.go",
          "line": 679,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(diff.size())\n\t\t}\n\t\tif disk, ok := layer.(*diskLayer)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/history_index_block.go",
          "line": 207,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= x\n\t\t}\n\t\tif result > id {\n\t\t\treturn result, nil\n\t\t}\n\t\tpos += n\n\t}\n\t// The element which is greater than specified id is not found.\n\tif index == len(br.restarts) {\n\t\treturn math.MaxUint64, nil\n\t}\n\t// The element which is the first one greater than the specified id\n\t// is exactly the one located at the restart point.\n\titem, _ := binary.Uvarint(br.data[br.restarts[index]:])\n\treturn item, nil\n}\n\ntype blockWriter struct {\n\tdesc     *indexBlockDesc // Descriptor of the block\n\trestarts []uint16        // Offsets into the data slice, marking the start of each section\n\tscratch  []byte          // Buffer used for encoding full integers or value differences\n\tdata     []byte          // Aggregated encoded data slice\n}\n\nfunc newBlockWriter(blob []byte, desc *indexBlockDesc) (*blockWriter, error) {\n\tscratch := make([]byte, binary.MaxVarintLen64)\n\tif len(blob) == 0 {\n\t\treturn &blockWriter{\n\t\t\tdesc:    desc,\n\t\t\tscratch: scratch,\n\t\t\tdata:    make([]byte, 0, 1024),\n\t\t}, nil\n\t}\n\trestarts, data, err := parseIndexBlock(blob)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &blockWriter{\n\t\tdesc:     desc,\n\t\trestarts: restarts,\n\t\tscratch:  scratch,\n\t\tdata:     data, // safe to own the slice\n\t}, nil\n}\n\n// append adds a new element to the block. The new element must be greater than\n// the previous one. The provided ID is assumed to always be greater than 0.\nfunc (b *blockWriter) append(id uint64) error {\n\tif id == 0 {\n\t\treturn errors.New(\"invalid zero id\")\n\t}\n\tif id <= b.desc.max {\n\t\treturn fmt.Errorf(\"append element out of order, last: %d, this: %d\", b.desc.max, id)\n\t}\n\t// Rotate the current restart section if it's full\n\tif b.desc.entries%indexBlockRestartLen == 0 {\n\t\t// Save the offset within the data slice as the restart point\n\t\t// for the next section.\n\t\tb.restarts = append(b.restarts, uint16(len(b.data)))\n\n\t\t// The restart point item can either be encoded in variable\n\t\t// size or fixed size. Although variable-size encoding is\n\t\t// slightly slower (2ns per operation), it is still relatively\n\t\t// fast, therefore, it's picked for better space efficiency.\n\t\t//\n\t\t// The first element in a restart range is encoded using its\n\t\t// full value.\n\t\tn := binary.PutUvarint(b.scratch[0:], id)\n\t\tb.data = append(b.data, b.scratch[:n]...)\n\t} else {\n\t\t// The current section is not full, append the element.\n\t\t// The element which is not the first one in the section\n\t\t// is encoded using the value difference from the preceding\n\t\t// element.\n\t\tn := binary.PutUvarint(b.scratch[0:], id-b.desc.max)\n\t\tb.data = append(b.data, b.scratch[:n]...)\n\t}\n\tb.desc.entries++\n\n\t// The state history ID must be greater than 0.\n\t//if b.desc.min == 0 {\n\t//\tb.desc.min = id\n\t//}\n\tb.desc.max = id\n\treturn nil\n}\n\n// scanSection traverses the specified section and terminates if fn returns true.\nfunc (b *blockWriter) scanSection(section int, fn func(uint64, int) bool) {\n\tvar (\n\t\tvalue uint64\n\t\tstart = int(b.restarts[section])\n\t\tpos   = start\n\t\tlimit int\n\t)\n\tif section == len(b.restarts)-1 {\n\t\tlimit = len(b.data)\n\t} else {\n\t\tlimit = int(b.restarts[section+1])\n\t}\n\tfor pos < limit {\n\t\tx, n := binary.Uvarint(b.data[pos:])\n\t\tif pos == start {\n\t\t\tvalue = x\n\t\t} else {\n\t\t\tvalue += x\n\t\t}\n\t\tif fn(value, pos) {\n\t\t\treturn\n\t\t}\n\t\tpos += n\n\t}\n}\n\n// sectionLast returns the last element in the specified section.\nfunc (b *blockWriter) sectionLast(section int) uint64 {\n\tvar n uint64\n\tb.scanSection(section, func(v uint64, _ int) bool {\n\t\tn = v\n\t\treturn false\n\t})\n\treturn n\n}\n\n// sectionSearch looks up the specified value in the given section,\n// the position and the preceding value will be returned if found.\nfunc (b *blockWriter) sectionSearch(section int, n uint64) (found bool, prev uint64, pos int) {\n\tb.scanSection(section, func(v uint64, p int) bool {\n\t\tif n == v {\n\t\t\tpos = p\n\t\t\tfound = true\n\t\t\treturn true // terminate iteration\n\t\t}\n\t\tprev = v\n\t\treturn false // continue iteration\n\t})\n\treturn found, prev, pos\n}\n\n// pop removes the last element from the block. The assumption is held that block\n// writer must be non-empty.\nfunc (b *blockWriter) pop(id uint64) error {\n\tif id == 0 {\n\t\treturn errors.New(\"invalid zero id\")\n\t}\n\tif id != b.desc.max {\n\t\treturn fmt.Errorf(\"pop element out of order, last: %d, this: %d\", b.desc.max, id)\n\t}\n\t// If there is only one entry left, the entire block should be reset\n\tif b.desc.entries == 1 {\n\t\t//b.desc.min = 0\n\t\tb.desc.max = 0\n\t\tb.desc.entries = 0\n\t\tb.restarts = nil\n\t\tb.data = b.data[:0]\n\t\treturn nil\n\t}\n\t// Pop the last restart section if the section becomes empty after removing\n\t// one element.\n\tif b.desc.entries%indexBlockRestartLen == 1 {\n\t\tb.data = b.data[:b.restarts[len(b.restarts)-1]]\n\t\tb.restarts = b.restarts[:len(b.restarts)-1]\n\t\tb.desc.max = b.sectionLast(len(b.restarts) - 1)\n\t\tb.desc.entries -= 1\n\t\treturn nil\n\t}\n\t// Look up the element preceding the one to be popped, in order to update\n\t// the maximum element in the block.\n\tfound, prev, pos := b.sectionSearch(len(b.restarts)-1, id)\n\tif !found {\n\t\treturn fmt.Errorf(\"pop element is not found, last: %d, this: %d\", b.desc.max, id)\n\t}\n\tb.desc.max = prev\n\tb.data = b.data[:pos]\n\tb.desc.entries -= 1\n\treturn nil\n}\n\nfunc (b *blockWriter) empty() bool {\n\treturn b.desc.empty()\n}\n\nfunc (b *blockWriter) full() bool {\n\treturn b.desc.full()\n}\n\n// finish finalizes the index block encoding by appending the encoded restart points\n// and the restart counter to the end of the block.\n//\n// This function is safe to be called multiple times.\nfunc (b *blockWriter) finish() []byte {\n\tvar buf []byte\n\tfor _, number := range b.restarts {\n\t\tbinary.BigEndian.PutUint16(b.scratch[:2], number)\n\t\tbuf = append(buf, b.scratch[:2]...)\n\t}\n\tbuf = append(buf, byte(len(b.restarts)))\n\treturn append(b.data, buf...)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/history_index_block.go",
          "line": 370,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\treturn nil\n\t}\n\t// Look up the element preceding the one to be popped, in order to update\n\t// the maximum element in the block.\n\tfound, prev, pos := b.sectionSearch(len(b.restarts)-1, id)\n\tif !found {\n\t\treturn fmt.Errorf(\"pop element is not found, last: %d, this: %d\", b.desc.max, id)\n\t}\n\tb.desc.max = prev\n\tb.data = b.data[:pos]\n\tb.desc.entries -= 1\n\treturn nil\n}\n\nfunc (b *blockWriter) empty() bool {\n\treturn b.desc.empty()\n}\n\nfunc (b *blockWriter) full() bool {\n\treturn b.desc.full()\n}\n\n// finish finalizes the index block encoding by appending the encoded restart points\n// and the restart counter to the end of the block.\n//\n// This function is safe to be called multiple times.\nfunc (b *blockWriter) finish() []byte {\n\tvar buf []byte\n\tfor _, number := range b.restarts {\n\t\tbinary.BigEndian.PutUint16(b.scratch[:2], number)\n\t\tbuf = append(buf, b.scratch[:2]...)\n\t}\n\tbuf = append(buf, byte(len(b.restarts)))\n\treturn append(b.data, buf...)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/iterator_test.go",
          "line": 165,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t}\n\t\t}\n\t\tstorage[hash] = accStorage\n\t\tnilStorage[hash] = nilstorage\n\t}\n\tstates := newStates(accounts, storage, false)\n\tfor account := range accounts {\n\t\tit := newDiffStorageIterator(account, common.Hash{}, states.storageList(account), nil)\n\t\tverifyIterator(t, 100, it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n\n\tdb := rawdb.NewMemoryDatabase()\n\tbatch := db.NewBatch()\n\tstates.write(batch, nil, nil)\n\tbatch.Write()\n\tfor account := range accounts {\n\t\tit := newDiskStorageIterator(db, account, common.Hash{})\n\t\tverifyIterator(t, 100-nilStorage[account], it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n}\n\ntype testIterator struct {\n\tvalues []byte\n}\n\nfunc newTestIterator(values ...byte) *testIterator {\n\treturn &testIterator{values}\n}\n\nfunc (ti *testIterator) Seek(common.Hash) {\n\tpanic(\"implement me\")\n}\n\nfunc (ti *testIterator) Next() bool {\n\tti.values = ti.values[1:]\n\treturn len(ti.values) > 0\n}\n\nfunc (ti *testIterator) Error() error {\n\treturn nil\n}\n\nfunc (ti *testIterator) Hash() common.Hash {\n\treturn common.BytesToHash([]byte{ti.values[0]})\n}\n\nfunc (ti *testIterator) Account() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Slot() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Release() {}\n\nfunc TestFastIteratorBasics(t *testing.T) {\n\ttype testCase struct {\n\t\tlists   [][]byte\n\t\texpKeys []byte\n\t}\n\tfor i, tc := range []testCase{\n\t\t{lists: [][]byte{{0, 1, 8}, {1, 2, 8}, {2, 9}, {4},\n\t\t\t{7, 14, 15}, {9, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 4, 7, 8, 9, 13, 14, 15, 16}},\n\t\t{lists: [][]byte{{0, 8}, {1, 2, 8}, {7, 14, 15}, {8, 9},\n\t\t\t{9, 10}, {10, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 7, 8, 9, 10, 13, 14, 15, 16}},\n\t} {\n\t\tvar iterators []*weightedIterator\n\t\tfor i, data := range tc.lists {\n\t\t\tit := newTestIterator(data...)\n\t\t\titerators = append(iterators, &weightedIterator{it, i})\n\t\t}\n\t\tfi := &fastIterator{\n\t\t\titerators: iterators,\n\t\t\tinitiated: false,\n\t\t}\n\t\tcount := 0\n\t\tfor fi.Next() {\n\t\t\tif got, exp := fi.Hash()[31], tc.expKeys[count]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/states_test.go",
          "line": 442,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2*common.HashLength + 2 + 2                                                  /* storage a change */\n\tmergeSize += 2*common.HashLength + 2 - 1                                                  /* storage b change */\n\tmergeSize += 2*2*common.HashLength - 1                                                    /* storage data removal of 0xc */\n\n\tif a.size != uint64(mergeSize) {\n\t\tt.Fatalf(\"Unexpected size, want: %d, got: %d\", mergeSize, a.size)\n\t}\n\n\t// Revert the set to original status\n\ta.revertTo(\n\t\tmap[common.Hash][]byte{\n\t\t\t{0xa}: {0xa0},\n\t\t\t{0xb}: {0xb0},\n\t\t\t{0xc}: {0xc0},\n\t\t},\n\t\tmap[common.Hash]map[common.Hash][]byte{\n\t\t\t{0xa}: {\n\t\t\t\tcommon.Hash{0x1}: {0x10},\n\t\t\t\tcommon.Hash{0x2}: {0x20},\n\t\t\t\tcommon.Hash{0x3}: nil, // revert slot creation\n\t\t\t},\n\t\t\t{0xb}: {\n\t\t\t\tcommon.Hash{0x1}: {0x10, 0x11, 0x12},\n\t\t\t\tcommon.Hash{0x2}: nil, // revert slot creation\n\t\t\t},\n\t\t\t{0xc}: {\n\t\t\t\tcommon.Hash{0x1}: {0x10},\n\t\t\t\tcommon.Hash{0x2}: {0x20}, // resurrected slot\n\t\t\t\tcommon.Hash{0x3}: {0x30}, // resurrected slot\n\t\t\t},\n\t\t},\n\t)\n\trevertSize := expSizeA + 2*common.HashLength + 2*common.HashLength // delete-marker of a.3 and b.2 slot\n\trevertSize += 2 * (2*common.HashLength + 1)                        // resurrected slot, c.2, c.3\n\tif a.size != uint64(revertSize) {\n\t\tt.Fatalf(\"Unexpected size, want: %d, got: %d\", revertSize, a.size)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/flush.go",
          "line": 64,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(subset)\n\t}\n\treturn total\n}\n\n// writeStates flushes state mutations into the provided database batch as a whole.\n//\n// This function assumes the background generator is already terminated and states\n// before the supplied marker has been correctly generated.\n//\n// TODO(rjl493456442) do we really need this generation marker? The state updates\n// after the marker can also be written and will be fixed by generator later if\n// it's outdated.\nfunc writeStates(batch ethdb.Batch, genMarker []byte, accountData map[common.Hash][]byte, storageData map[common.Hash]map[common.Hash][]byte, clean *fastcache.Cache) (int, int) {\n\tvar (\n\t\taccounts int\n\t\tslots    int\n\t)\n\tfor addrHash, blob := range accountData {\n\t\t// Skip any account not yet covered by the snapshot. The account\n\t\t// at the generation marker position (addrHash == genMarker[:common.HashLength])\n\t\t// should still be updated, as it would be skipped in the next\n\t\t// generation cycle.\n\t\tif genMarker != nil && bytes.Compare(addrHash[:], genMarker) > 0 {\n\t\t\tcontinue\n\t\t}\n\t\taccounts += 1\n\t\tif len(blob) == 0 {\n\t\t\trawdb.DeleteAccountSnapshot(batch, addrHash)\n\t\t\tif clean != nil {\n\t\t\t\tclean.Set(addrHash[:], nil)\n\t\t\t}\n\t\t} else {\n\t\t\trawdb.WriteAccountSnapshot(batch, addrHash, blob)\n\t\t\tif clean != nil {\n\t\t\t\tclean.Set(addrHash[:], blob)\n\t\t\t}\n\t\t}\n\t}\n\tfor addrHash, storages := range storageData {\n\t\t// Skip any account not covered yet by the snapshot\n\t\tif genMarker != nil && bytes.Compare(addrHash[:], genMarker) > 0 {\n\t\t\tcontinue\n\t\t}\n\t\tmidAccount := genMarker != nil && bytes.Equal(addrHash[:], genMarker[:common.HashLength])\n\n\t\tfor storageHash, blob := range storages {\n\t\t\t// Skip any storage slot not yet covered by the snapshot. The storage slot\n\t\t\t// at the generation marker position (addrHash == genMarker[:common.HashLength]\n\t\t\t// and storageHash == genMarker[common.HashLength:]) should still be updated,\n\t\t\t// as it would be skipped in the next generation cycle.\n\t\t\tif midAccount && bytes.Compare(storageHash[:], genMarker[common.HashLength:]) > 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tslots += 1\n\t\t\tif len(blob) == 0 {\n\t\t\t\trawdb.DeleteStorageSnapshot(batch, addrHash, storageHash)\n\t\t\t\tif clean != nil {\n\t\t\t\t\tclean.Set(append(addrHash[:], storageHash[:]...), nil)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trawdb.WriteStorageSnapshot(batch, addrHash, storageHash, blob)\n\t\t\t\tif clean != nil {\n\t\t\t\t\tclean.Set(append(addrHash[:], storageHash[:]...), blob)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn accounts, slots\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/history.go",
          "line": 336,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint32(len(slots))\n\t\t}\n\t\taccountData = append(accountData, h.accounts[addr]...)\n\t\taccountIndexes = append(accountIndexes, accIndex.encode()...)\n\t}\n\treturn accountData, storageData, accountIndexes, storageIndexes\n}\n\n// decoder wraps the byte streams for decoding with extra meta fields.\ntype decoder struct {\n\taccountData    []byte // the buffer for concatenated account data\n\tstorageData    []byte // the buffer for concatenated storage data\n\taccountIndexes []byte // the buffer for concatenated account index\n\tstorageIndexes []byte // the buffer for concatenated storage index\n\n\tlastAccount       *common.Address // the address of last resolved account\n\tlastAccountRead   uint32          // the read-cursor position of account data\n\tlastSlotIndexRead uint32          // the read-cursor position of storage slot index\n\tlastSlotDataRead  uint32          // the read-cursor position of storage slot data\n}\n\n// verify validates the provided byte streams for decoding state history. A few\n// checks will be performed to quickly detect data corruption. The byte stream\n// is regarded as corrupted if:\n//\n// - account indexes buffer is empty(empty state set is invalid)\n// - account indexes/storage indexer buffer is not aligned\n//\n// note, these situations are allowed:\n//\n// - empty account data: all accounts were not present\n// - empty storage set: no slots are modified\nfunc (r *decoder) verify() error {\n\tif len(r.accountIndexes)%accountIndexSize != 0 || len(r.accountIndexes) == 0 {\n\t\treturn fmt.Errorf(\"invalid account index, len: %d\", len(r.accountIndexes))\n\t}\n\tif len(r.storageIndexes)%slotIndexSize != 0 {\n\t\treturn fmt.Errorf(\"invalid storage index, len: %d\", len(r.storageIndexes))\n\t}\n\treturn nil\n}\n\n// readAccount parses the account from the byte stream with specified position.\nfunc (r *decoder) readAccount(pos int) (accountIndex, []byte, error) {\n\t// Decode account index from the index byte stream.\n\tvar index accountIndex\n\tif (pos+1)*accountIndexSize > len(r.accountIndexes) {\n\t\treturn accountIndex{}, nil, errors.New(\"account data buffer is corrupted\")\n\t}\n\tindex.decode(r.accountIndexes[pos*accountIndexSize : (pos+1)*accountIndexSize])\n\n\t// Perform validation before parsing account data, ensure\n\t// - account is sorted in order in byte stream\n\t// - account data is strictly encoded with no gap inside\n\t// - account data is not out-of-slice\n\tif r.lastAccount != nil { // zero address is possible\n\t\tif bytes.Compare(r.lastAccount.Bytes(), index.address.Bytes()) >= 0 {\n\t\t\treturn accountIndex{}, nil, errors.New(\"account is not in order\")\n\t\t}\n\t}\n\tif index.offset != r.lastAccountRead {\n\t\treturn accountIndex{}, nil, errors.New(\"account data buffer is gaped\")\n\t}\n\tlast := index.offset + uint32(index.length)\n\tif uint32(len(r.accountData)) < last {\n\t\treturn accountIndex{}, nil, errors.New(\"account data buffer is corrupted\")\n\t}\n\tdata := r.accountData[index.offset:last]\n\n\tr.lastAccount = &index.address\n\tr.lastAccountRead = last\n\n\treturn index, data, nil\n}\n\n// readStorage parses the storage slots from the byte stream with specified account.\nfunc (r *decoder) readStorage(accIndex accountIndex) ([]common.Hash, map[common.Hash][]byte, error) {\n\tvar (\n\t\tlast    *common.Hash\n\t\tcount   = int(accIndex.storageSlots)\n\t\tlist    = make([]common.Hash, 0, count)\n\t\tstorage = make(map[common.Hash][]byte, count)\n\t)\n\tfor j := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/history.go",
          "line": 593,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(blobs))\n\t}\n\treturn nil\n}\n\n// truncateFromHead removes the extra state histories from the head with the given\n// parameters. It returns the number of items removed from the head.\nfunc truncateFromHead(db ethdb.Batcher, store ethdb.AncientStore, nhead uint64) (int, error) {\n\tohead, err := store.Ancients()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\totail, err := store.Tail()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\t// Ensure that the truncation target falls within the specified range.\n\tif ohead < nhead || nhead < otail {\n\t\treturn 0, fmt.Errorf(\"out of range, tail: %d, head: %d, target: %d\", otail, ohead, nhead)\n\t}\n\t// Short circuit if nothing to truncate.\n\tif ohead == nhead {\n\t\treturn 0, nil\n\t}\n\t// Load the meta objects in range [nhead+1, ohead]\n\tblobs, err := rawdb.ReadStateHistoryMetaList(store, nhead+1, ohead-nhead)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbatch := db.NewBatch()\n\tfor _, blob := range blobs {\n\t\tvar m meta\n\t\tif err := m.decode(blob)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/triedb/pathdb/history.go",
          "line": 592,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(len(blobs))\n\t\tstart += uint64(len(blobs))\n\t}\n\treturn nil\n}\n\n// truncateFromHead removes the extra state histories from the head with the given\n// parameters. It returns the number of items removed from the head.\nfunc truncateFromHead(db ethdb.Batcher, store ethdb.AncientStore, nhead uint64) (int, error) {\n\tohead, err := store.Ancients()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\totail, err := store.Tail()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\t// Ensure that the truncation target falls within the specified range.\n\tif ohead < nhead || nhead < otail {\n\t\treturn 0, fmt.Errorf(\"out of range, tail: %d, head: %d, target: %d\", otail, ohead, nhead)\n\t}\n\t// Short circuit if nothing to truncate.\n\tif ohead == nhead {\n\t\treturn 0, nil\n\t}\n\t// Load the meta objects in range [nhead+1, ohead]\n\tblobs, err := rawdb.ReadStateHistoryMetaList(store, nhead+1, ohead-nhead)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbatch := db.NewBatch()\n\tfor _, blob := range blobs {\n\t\tvar m meta\n\t\tif err := m.decode(blob)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/verifier.go",
          "line": 121,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tstat.head = account\n}\n\n// finishAccounts updates the generator stats for the finished account range.\nfunc (stat *generateStats) finishAccounts(done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.accounts += done\n}\n\n// progressContract updates the generator stats for a specific in-progress contract.\nfunc (stat *generateStats) progressContract(account common.Hash, slot common.Hash, done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.slots += done\n\tstat.slotsHead[account] = slot\n\tif _, ok := stat.slotsStart[account]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/verifier.go",
          "line": 150,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tdelete(stat.slotsHead, account)\n\tdelete(stat.slotsStart, account)\n}\n\n// report prints the cumulative progress statistic smartly.\nfunc (stat *generateStats) report() {\n\tstat.lock.RLock()\n\tdefer stat.lock.RUnlock()\n\n\tctx := []interface{}{\n\t\t\"accounts\", stat.accounts,\n\t\t\"slots\", stat.slots,\n\t\t\"elapsed\", common.PrettyDuration(time.Since(stat.start)),\n\t}\n\tif stat.accounts > 0 {\n\t\t// If there's progress on the account trie, estimate the time to finish crawling it\n\t\tif done := binary.BigEndian.Uint64(stat.head[:8]) / stat.accounts",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/history_indexer.go",
          "line": 99,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tb.accounts[addrHash] = append(b.accounts[addrHash], historyID)\n\n\t\tfor _, slotKey := range h.storageList[address] {\n\t\t\tb.counter += 1\n\t\t\tif _, ok := b.storages[addrHash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/history_indexer.go",
          "line": 172,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(slots)\n\t\tfor storageHash, idList := range slots {\n\t\t\teg.Go(func() error {\n\t\t\t\tif !b.delete {\n\t\t\t\t\tiw, err := newIndexWriter(b.db, newStorageIdent(addrHash, storageHash))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tfor _, n := range idList {\n\t\t\t\t\t\tif err := iw.append(n)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/triedb/pathdb/history_indexer.go",
          "line": 539,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\n\t\t\t// Occasionally report the indexing progress\n\t\t\tif time.Since(logged) > time.Second*8 {\n\t\t\t\tlogged = time.Now()\n\n\t\t\t\tvar (\n\t\t\t\t\tleft  = lastID - current + 1\n\t\t\t\t\tdone  = current - beginID\n\t\t\t\t\tspeed = done/uint64(time.Since(start)/time.Millisecond+1) + 1 // +1s to avoid division by zero\n\t\t\t\t)\n\t\t\t\t// Override the ETA if larger than the largest until now\n\t\t\t\teta := time.Duration(left/speed) * time.Millisecond\n\t\t\t\tlog.Info(\"Indexing state history\", \"processed\", done, \"left\", left, \"elapsed\", common.PrettyDuration(time.Since(start)), \"eta\", common.PrettyDuration(eta))\n\t\t\t}\n\t\t}\n\t\ti.indexed.Store(current - 1) // update indexing progress\n\n\t\t// Check interruption signal and abort process if it's fired\n\t\tif interrupt != nil {\n\t\t\tif signal := interrupt.Load()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 44,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n}\n\n// report uploads the cached statistics to meters.\nfunc (c *counter) report(count, size *metrics.Meter) {\n\tcount.Mark(int64(c.n))\n\tsize.Mark(int64(c.size))\n}\n\n// stateSet represents a collection of state modifications associated with a\n// transition (e.g., a block execution) or multiple aggregated transitions.\n//\n// A stateSet can only reside within a diffLayer or the buffer of a diskLayer,\n// serving as the envelope for the set. Lock protection is not required for\n// accessing or mutating the account set and storage set, as the associated\n// envelope is always marked as stale before any mutation is applied. Any\n// subsequent state access will be denied due to the stale flag. Therefore,\n// state access and mutation won't happen at the same time with guarantee.\ntype stateSet struct {\n\taccountData map[common.Hash][]byte                 // Keyed accounts for direct retrieval (nil means deleted)\n\tstorageData map[common.Hash]map[common.Hash][]byte // Keyed storage slots for direct retrieval. one per account (nil means deleted)\n\tsize        uint64                                 // Memory size of the state data (accountData and storageData)\n\n\taccountListSorted []common.Hash                 // List of account for iteration. If it exists, it's sorted, otherwise it's nil\n\tstorageListSorted map[common.Hash][]common.Hash // List of storage slots for iterated retrievals, one per account. Any existing lists are sorted if non-nil\n\n\trawStorageKey bool // indicates whether the storage set uses the raw slot key or the hash\n\n\t// Lock for guarding the two lists above. These lists might be accessed\n\t// concurrently and lock protection is essential to avoid concurrent\n\t// slice or map read/write.\n\tlistLock sync.RWMutex\n}\n\n// newStates constructs the state set with the provided account and storage data.\nfunc newStates(accounts map[common.Hash][]byte, storages map[common.Hash]map[common.Hash][]byte, rawStorageKey bool) *stateSet {\n\t// Don't panic for the lazy callers, initialize the nil maps instead.\n\tif accounts == nil {\n\t\taccounts = make(map[common.Hash][]byte)\n\t}\n\tif storages == nil {\n\t\tstorages = make(map[common.Hash]map[common.Hash][]byte)\n\t}\n\ts := &stateSet{\n\t\taccountData:       accounts,\n\t\tstorageData:       storages,\n\t\trawStorageKey:     rawStorageKey,\n\t\tstorageListSorted: make(map[common.Hash][]common.Hash),\n\t}\n\ts.size = s.check()\n\treturn s\n}\n\n// account returns the account data associated with the specified address hash.\nfunc (s *stateSet) account(hash common.Hash) ([]byte, bool) {\n\t// If the account is known locally, return it\n\tif data, ok := s.accountData[hash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 147,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength + len(blob)\n\t}\n\tfor accountHash, slots := range s.storageData {\n\t\tif slots == nil {\n\t\t\tpanic(fmt.Sprintf(\"storage %#x nil\", accountHash)) // nil slots is not permitted\n\t\t}\n\t\tfor _, blob := range slots {\n\t\t\tsize += 2*common.HashLength + len(blob)\n\t\t}\n\t}\n\treturn uint64(size)\n}\n\n// accountList returns a sorted list of all accounts in this state set, including\n// the deleted ones.\n//\n// Note, the returned slice is not a copy, so do not modify it.\nfunc (s *stateSet) accountList() []common.Hash {\n\t// If an old list already exists, return it\n\ts.listLock.RLock()\n\tlist := s.accountListSorted\n\ts.listLock.RUnlock()\n\n\tif list != nil {\n\t\treturn list\n\t}\n\t// No old sorted account list exists, generate a new one. It's possible that\n\t// multiple threads waiting for the write lock may regenerate the list\n\t// multiple times, which is acceptable.\n\ts.listLock.Lock()\n\tdefer s.listLock.Unlock()\n\n\tlist = slices.SortedFunc(maps.Keys(s.accountData), common.Hash.Cmp)\n\ts.accountListSorted = list\n\treturn list\n}\n\n// StorageList returns a sorted list of all storage slot hashes in this state set\n// for the given account. The returned list will include the hash of deleted\n// storage slot.\n//\n// Note, the returned slice is not a copy, so do not modify it.\nfunc (s *stateSet) storageList(accountHash common.Hash) []common.Hash {\n\ts.listLock.RLock()\n\tif _, ok := s.storageData[accountHash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(data) - len(origin)\n\t\t\taccountOverwrites.add(common.HashLength + len(origin))\n\t\t} else {\n\t\t\tdelta += common.HashLength + len(data)\n\t\t}\n\t\ts.accountData[accountHash] = data\n\t}\n\t// Apply all the updated storage slots (individually)\n\tfor accountHash, storage := range other.storageData {\n\t\t// If storage didn't exist in the set, overwrite blindly\n\t\tif _, ok := s.storageData[accountHash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 256,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2*common.HashLength + len(data)\n\t\t\t}\n\t\t\ts.storageData[accountHash] = slots\n\t\t\tcontinue\n\t\t}\n\t\t// Storage exists in both local and external set, merge the slots\n\t\tslots := s.storageData[accountHash]\n\t\tfor storageHash, data := range storage {\n\t\t\tif origin, ok := slots[storageHash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 265,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(data) - len(origin)\n\t\t\t\tstorageOverwrites.add(2*common.HashLength + len(origin))\n\t\t\t} else {\n\t\t\t\tdelta += 2*common.HashLength + len(data)\n\t\t\t}\n\t\t\tslots[storageHash] = data\n\t\t}\n\t}\n\taccountOverwrites.report(gcAccountMeter, gcAccountBytesMeter)\n\tstorageOverwrites.report(gcStorageMeter, gcStorageBytesMeter)\n\ts.clearLists()\n\ts.updateSize(delta)\n}\n\n// revertTo takes the original value of accounts and storages as input and reverts\n// the latest state transition applied on the state set.\n//\n// Notably, this operation may result in the set containing more entries after a\n// revert. For example, if account x did not exist and was created during transition\n// w, reverting w will retain an x=nil entry in the set. And also if account x along\n// with its storage slots was deleted in the transition w, reverting w will retain\n// a list of additional storage slots with their original value.\nfunc (s *stateSet) revertTo(accountOrigin map[common.Hash][]byte, storageOrigin map[common.Hash]map[common.Hash][]byte) {\n\tvar delta int // size tracking\n\tfor addrHash, blob := range accountOrigin {\n\t\tdata, ok := s.accountData[addrHash]\n\t\tif !ok {\n\t\t\tpanic(fmt.Sprintf(\"non-existent account for reverting, %x\", addrHash))\n\t\t}\n\t\tif len(data) == 0 && len(blob) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"invalid account mutation (null to null), %x\", addrHash))\n\t\t}\n\t\tdelta += len(blob) - len(data)\n\t\ts.accountData[addrHash] = blob\n\t}\n\t// Overwrite the storage data with original value blindly\n\tfor addrHash, storage := range storageOrigin {\n\t\tslots := s.storageData[addrHash]\n\t\tif len(slots) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"non-existent storage set for reverting, %x\", addrHash))\n\t\t}\n\t\tfor storageHash, blob := range storage {\n\t\t\tdata, ok := slots[storageHash]\n\t\t\tif !ok {\n\t\t\t\tpanic(fmt.Sprintf(\"non-existent storage slot for reverting, %x-%x\", addrHash, storageHash))\n\t\t\t}\n\t\t\tif len(blob) == 0 && len(data) == 0 {\n\t\t\t\tpanic(fmt.Sprintf(\"invalid storage slot mutation (null to null), %x-%x\", addrHash, storageHash))\n\t\t\t}\n\t\t\tdelta += len(blob) - len(data)\n\t\t\tslots[storageHash] = blob\n\t\t}\n\t}\n\ts.clearLists()\n\ts.updateSize(delta)\n}\n\n// updateSize updates the total cache size by the given delta.\nfunc (s *stateSet) updateSize(delta int) {\n\tsize := int64(s.size) + int64(delta)\n\tif size >= 0 {\n\t\ts.size = uint64(size)\n\t\treturn\n\t}\n\tlog.Error(\"Stateset size underflow\", \"prev\", common.StorageSize(s.size), \"delta\", common.StorageSize(delta))\n\ts.size = 0\n}\n\n// encode serializes the content of state set into the provided writer.\nfunc (s *stateSet) encode(w io.Writer) error {\n\t// Encode accounts\n\tif err := rlp.Encode(w, s.rawStorageKey)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 440,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(slots) * len(rawdb.SnapshotStoragePrefix)\n\t}\n\treturn m + int(s.size)\n}\n\n// StateSetWithOrigin wraps the state set with additional original values of the\n// mutated states.\ntype StateSetWithOrigin struct {\n\t*stateSet\n\n\t// accountOrigin represents the account data before the state transition,\n\t// corresponding to both the accountData and destructSet. It's keyed by the\n\t// account address. The nil value means the account was not present before.\n\taccountOrigin map[common.Address][]byte\n\n\t// storageOrigin represents the storage data before the state transition,\n\t// corresponding to storageData and deleted slots of destructSet. It's keyed\n\t// by the account address and slot key hash. The nil value means the slot was\n\t// not present.\n\tstorageOrigin map[common.Address]map[common.Hash][]byte\n\n\t// memory size of the state data (accountOrigin and storageOrigin)\n\tsize uint64\n}\n\n// NewStateSetWithOrigin constructs the state set with the provided data.\nfunc NewStateSetWithOrigin(accounts map[common.Hash][]byte, storages map[common.Hash]map[common.Hash][]byte, accountOrigin map[common.Address][]byte, storageOrigin map[common.Address]map[common.Hash][]byte, rawStorageKey bool) *StateSetWithOrigin {\n\t// Don't panic for the lazy callers, initialize the nil maps instead.\n\tif accountOrigin == nil {\n\t\taccountOrigin = make(map[common.Address][]byte)\n\t}\n\tif storageOrigin == nil {\n\t\tstorageOrigin = make(map[common.Address]map[common.Hash][]byte)\n\t}\n\t// Count the memory size occupied by the set. Note that each slot key here\n\t// uses 2*common.HashLength to keep consistent with the calculation method\n\t// of stateSet.\n\tvar size int\n\tfor _, data := range accountOrigin {\n\t\tsize += common.HashLength + len(data)\n\t}\n\tfor _, slots := range storageOrigin {\n\t\tfor _, data := range slots {\n\t\t\tsize += 2*common.HashLength + len(data)\n\t\t}\n\t}\n\tset := newStates(accounts, storages, rawStorageKey)\n\treturn &StateSetWithOrigin{\n\t\tstateSet:      set,\n\t\taccountOrigin: accountOrigin,\n\t\tstorageOrigin: storageOrigin,\n\t\tsize:          set.size + uint64(size),\n\t}\n}\n\n// encode serializes the content of state set into the provided writer.\nfunc (s *StateSetWithOrigin) encode(w io.Writer) error {\n\t// Encode state set\n\tif err := s.stateSet.encode(w)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 69,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(n.Blob) + len(path))\n\t}\n\tfor _, subset := range s.storageNodes {\n\t\tfor path, n := range subset {\n\t\t\tsize += uint64(common.HashLength + len(n.Blob) + len(path))\n\t\t}\n\t}\n\ts.size = size\n}\n\n// updateSize updates the total cache size by the given delta.\nfunc (s *nodeSet) updateSize(delta int64) {\n\tsize := int64(s.size) + delta\n\tif size >= 0 {\n\t\ts.size = uint64(size)\n\t\treturn\n\t}\n\tlog.Error(\"Nodeset size underflow\", \"prev\", common.StorageSize(s.size), \"delta\", common.StorageSize(delta))\n\ts.size = 0\n}\n\n// node retrieves the trie node with node path and its trie identifier.\nfunc (s *nodeSet) node(owner common.Hash, path []byte) (*trienode.Node, bool) {\n\t// Account trie node\n\tif owner == (common.Hash{}) {\n\t\tn, ok := s.accountNodes[string(path)]\n\t\treturn n, ok\n\t}\n\t// Storage trie node\n\tsubset, ok := s.storageNodes[owner]\n\tif !ok {\n\t\treturn nil, false\n\t}\n\tn, ok := subset[string(path)]\n\treturn n, ok\n}\n\n// merge integrates the provided dirty nodes into the set. The provided nodeset\n// will remain unchanged, as it may still be referenced by other layers.\nfunc (s *nodeSet) merge(set *nodeSet) {\n\tvar (\n\t\tdelta     int64   // size difference resulting from node merging\n\t\toverwrite counter // counter of nodes being overwritten\n\t)\n\n\t// Merge account nodes\n\tfor path, n := range set.accountNodes {\n\t\tif orig, exist := s.accountNodes[path]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 117,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(len(n.Blob) + len(path))\n\t\t} else {\n\t\t\tdelta += int64(len(n.Blob) - len(orig.Blob))\n\t\t\toverwrite.add(len(orig.Blob) + len(path))\n\t\t}\n\t\ts.accountNodes[path] = n\n\t}\n\n\t// Merge storage nodes\n\tfor owner, subset := range set.storageNodes {\n\t\tcurrent, exist := s.storageNodes[owner]\n\t\tif !exist {\n\t\t\tfor path, n := range subset {\n\t\t\t\tdelta += int64(common.HashLength + len(n.Blob) + len(path))\n\t\t\t}\n\t\t\t// Perform a shallow copy of the map for the subset instead of claiming it\n\t\t\t// directly from the provided nodeset to avoid potential concurrent map\n\t\t\t// read/write issues. The nodes belonging to the original diff layer remain\n\t\t\t// accessible even after merging. Therefore, ownership of the nodes map\n\t\t\t// should still belong to the original layer, and any modifications to it\n\t\t\t// should be prevented.\n\t\t\ts.storageNodes[owner] = maps.Clone(subset)\n\t\t\tcontinue\n\t\t}\n\t\tfor path, n := range subset {\n\t\t\tif orig, exist := current[path]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 143,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(common.HashLength + len(n.Blob) + len(path))\n\t\t\t} else {\n\t\t\t\tdelta += int64(len(n.Blob) - len(orig.Blob))\n\t\t\t\toverwrite.add(common.HashLength + len(orig.Blob) + len(path))\n\t\t\t}\n\t\t\tcurrent[path] = n\n\t\t}\n\t\ts.storageNodes[owner] = current\n\t}\n\toverwrite.report(gcTrieNodeMeter, gcTrieNodeBytesMeter)\n\ts.updateSize(delta)\n}\n\n// revertTo merges the provided trie nodes into the set. This should reverse the\n// changes made by the most recent state transition.\nfunc (s *nodeSet) revertTo(db ethdb.KeyValueReader, nodes map[common.Hash]map[string]*trienode.Node) {\n\tvar delta int64\n\tfor owner, subset := range nodes {\n\t\tif owner == (common.Hash{}) {\n\t\t\t// Account trie nodes\n\t\t\tfor path, n := range subset {\n\t\t\t\torig, ok := s.accountNodes[path]\n\t\t\t\tif !ok {\n\t\t\t\t\tblob := rawdb.ReadAccountTrieNode(db, []byte(path))\n\t\t\t\t\tif bytes.Equal(blob, n.Blob) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tpanic(fmt.Sprintf(\"non-existent account node (%v) blob: %v\", path, crypto.Keccak256Hash(n.Blob).Hex()))\n\t\t\t\t}\n\t\t\t\ts.accountNodes[path] = n\n\t\t\t\tdelta += int64(len(n.Blob)) - int64(len(orig.Blob))\n\t\t\t}\n\t\t} else {\n\t\t\t// Storage trie nodes\n\t\t\tcurrent, ok := s.storageNodes[owner]\n\t\t\tif !ok {\n\t\t\t\tpanic(fmt.Sprintf(\"non-existent subset (%x)\", owner))\n\t\t\t}\n\t\t\tfor path, n := range subset {\n\t\t\t\torig, ok := current[path]\n\t\t\t\tif !ok {\n\t\t\t\t\tblob := rawdb.ReadStorageTrieNode(db, owner, []byte(path))\n\t\t\t\t\tif bytes.Equal(blob, n.Blob) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tpanic(fmt.Sprintf(\"non-existent storage node (%x %v) blob: %v\", owner, path, crypto.Keccak256Hash(n.Blob).Hex()))\n\t\t\t\t}\n\t\t\t\tcurrent[path] = n\n\t\t\t\tdelta += int64(len(n.Blob)) - int64(len(orig.Blob))\n\t\t\t}\n\t\t}\n\t}\n\ts.updateSize(delta)\n}\n\n// journalNode represents a trie node persisted in the journal.\ntype journalNode struct {\n\tPath []byte // Path of the node in the trie\n\tBlob []byte // RLP-encoded trie node blob, nil means the node is deleted\n}\n\n// journalNodes represents a list trie nodes belong to a single account\n// or the main account trie.\ntype journalNodes struct {\n\tOwner common.Hash\n\tNodes []journalNode\n}\n\n// encode serializes the content of trie nodes into the provided writer.\nfunc (s *nodeSet) encode(w io.Writer) error {\n\tnodes := make([]journalNodes, 0, len(s.storageNodes)+1)\n\n\t// Encode account nodes\n\tif len(s.accountNodes) > 0 {\n\t\tentry := journalNodes{Owner: common.Hash{}}\n\t\tfor path, node := range s.accountNodes {\n\t\t\tentry.Nodes = append(entry.Nodes, journalNode{\n\t\t\t\tPath: []byte(path),\n\t\t\t\tBlob: node.Blob,\n\t\t\t})\n\t\t}\n\t\tnodes = append(nodes, entry)\n\t}\n\t// Encode storage nodes\n\tfor owner, subset := range s.storageNodes {\n\t\tentry := journalNodes{Owner: owner}\n\t\tfor path, node := range subset {\n\t\t\tentry.Nodes = append(entry.Nodes, journalNode{\n\t\t\t\tPath: []byte(path),\n\t\t\t\tBlob: node.Blob,\n\t\t\t})\n\t\t}\n\t\tnodes = append(nodes, entry)\n\t}\n\treturn rlp.Encode(w, nodes)\n}\n\n// decode deserializes the content from the rlp stream into the nodeset.\nfunc (s *nodeSet) decode(r *rlp.Stream) error {\n\tvar encoded []journalNodes\n\tif err := r.Decode(&encoded)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 298,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(s.accountNodes) * len(rawdb.TrieNodeAccountPrefix) // database key prefix\n\tfor _, nodes := range s.storageNodes {\n\t\tm += len(nodes) * (len(rawdb.TrieNodeStoragePrefix)) // database key prefix\n\t}\n\treturn m + int(s.size)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/triedb/pathdb/history_inspect.go",
          "line": 74,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 {\n\t\t// The entire history object is decoded, although it's unnecessary for\n\t\t// account inspection. TODO(rjl493456442) optimization is worthwhile.\n\t\th, err := readHistory(freezer, id)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif id == start {\n\t\t\tstats.Start = h.meta.block\n\t\t}\n\t\tif id == end {\n\t\t\tstats.End = h.meta.block\n\t\t}\n\t\tonHistory(h, stats)\n\n\t\tif time.Since(logged) > time.Second*8 {\n\t\t\tlogged = time.Now()\n\t\t\teta := float64(time.Since(init)) / float64(id-start+1) * float64(end-id)\n\t\t\tlog.Info(\"Inspecting state history\", \"checked\", id-start+1, \"left\", end-id, \"elapsed\", common.PrettyDuration(time.Since(init)), \"eta\", common.PrettyDuration(eta))\n\t\t}\n\t}\n\tlog.Info(\"Inspected state history\", \"total\", end-start+1, \"elapsed\", common.PrettyDuration(time.Since(init)))\n\treturn stats, nil\n}\n\n// accountHistory inspects the account history within the range.\nfunc accountHistory(freezer ethdb.AncientReader, address common.Address, start, end uint64) (*HistoryStats, error) {\n\treturn inspectHistory(freezer, start, end, func(h *history, stats *HistoryStats) {\n\t\tblob, exists := h.accounts[address]\n\t\tif !exists {\n\t\t\treturn\n\t\t}\n\t\tstats.Blocks = append(stats.Blocks, h.meta.block)\n\t\tstats.Origins = append(stats.Origins, blob)\n\t})\n}\n\n// storageHistory inspects the storage history within the range.\nfunc storageHistory(freezer ethdb.AncientReader, address common.Address, slot common.Hash, start uint64, end uint64) (*HistoryStats, error) {\n\tslotHash := crypto.Keccak256Hash(slot.Bytes())\n\treturn inspectHistory(freezer, start, end, func(h *history, stats *HistoryStats) {\n\t\tslots, exists := h.storages[address]\n\t\tif !exists {\n\t\t\treturn\n\t\t}\n\t\tkey := slotHash\n\t\tif h.meta.version != stateHistoryV0 {\n\t\t\tkey = slot\n\t\t}\n\t\tblob, exists := slots[key]\n\t\tif !exists {\n\t\t\treturn\n\t\t}\n\t\tstats.Blocks = append(stats.Blocks, h.meta.block)\n\t\tstats.Origins = append(stats.Origins, blob)\n\t})\n}\n\n// historyRange returns the block number range of local state histories.\nfunc historyRange(freezer ethdb.AncientReader) (uint64, uint64, error) {\n\t// Load the id of the first history object in local store.\n\ttail, err := freezer.Tail()\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tfirst := tail + 1\n\n\t// Load the id of the last history object in local store.\n\thead, err := freezer.Ancients()\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tlast := head - 1\n\n\tfh, err := readHistory(freezer, first)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tlh, err := readHistory(freezer, last)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\treturn fh.meta.block, lh.meta.block, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vote/vote_journal.go",
          "line": 82,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif err = walLog.Write(lastIndex, vote)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 178,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 284,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vote/vote_pool_test.go",
          "line": 342,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tif curNumber == 294 {\n\t\t\tfutureBlockHash = bs[0].Hash()\n\t\t\tfutureVotesMap := votePool.futureVotes\n\t\t\tvoteBox := futureVotesMap[common.Hash{}]\n\t\t\tfutureVotesMap[futureBlockHash] = voteBox\n\t\t\tdelete(futureVotesMap, common.Hash{})\n\t\t\tfutureVotesPq := votePool.futureVotesPq\n\t\t\tfutureVotesPq.Peek().TargetHash = futureBlockHash\n\t\t}\n\t\tif _, err := chain.InsertChain(bs)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/tracing/journal_test.go",
          "line": 273,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/tracing/gen_balance_change_reason_stringer.go",
          "line": 46,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 210\n\t\treturn _BalanceChangeReason_name_1[_BalanceChangeReason_index_1[i]:_BalanceChangeReason_index_1[i+1]]\n\tdefault:\n\t\treturn \"BalanceChangeReason(\" + strconv.FormatInt(int64(i), 10) + \")\"\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/deposit.go",
          "line": 49,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 48 + 16 + 32\n\t// WithdrawalCredentials is 32 bytes. Read that value then skip over next\n\t// length.\n\tcopy(request[withdrawalCredOffset:], data[b:b+32])\n\tb += 32 + 32\n\t// Amount is 8 bytes, but it is padded to 32. Skip over it and the next\n\t// length.\n\tcopy(request[amountOffset:], data[b:b+8])\n\tb += 8 + 24 + 32\n\t// Signature is 96 bytes. Skip over it and the next length.\n\tcopy(request[signatureOffset:], data[b:b+96])\n\tb += 96 + 32\n\t// Index is 8 bytes.\n\tcopy(request[indexOffset:], data[b:b+8])\n\treturn request, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/types_test.go",
          "line": 31,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(p)\n\treturn len(p), nil\n}\n\nfunc BenchmarkEncodeRLP(b *testing.B) {\n\tbenchRLP(b, true)\n}\n\nfunc BenchmarkDecodeRLP(b *testing.B) {\n\tbenchRLP(b, false)\n}\n\nfunc benchRLP(b *testing.B, encode bool) {\n\tkey, _ := crypto.HexToECDSA(\"b71c71a67e1177ad4e901695e1b4b9ee17ae16c6668d313eac2f96dbcda3f291\")\n\tto := common.HexToAddress(\"0x00000000000000000000000000000000deadbeef\")\n\tsigner := NewLondonSigner(big.NewInt(1337))\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tobj  interface{}\n\t}{\n\t\t{\n\t\t\t\"legacy-header\",\n\t\t\t&Header{\n\t\t\t\tDifficulty: big.NewInt(10000000000),\n\t\t\t\tNumber:     big.NewInt(1000),\n\t\t\t\tGasLimit:   8_000_000,\n\t\t\t\tGasUsed:    8_000_000,\n\t\t\t\tTime:       555,\n\t\t\t\tExtra:      make([]byte, 32),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"london-header\",\n\t\t\t&Header{\n\t\t\t\tDifficulty: big.NewInt(10000000000),\n\t\t\t\tNumber:     big.NewInt(1000),\n\t\t\t\tGasLimit:   8_000_000,\n\t\t\t\tGasUsed:    8_000_000,\n\t\t\t\tTime:       555,\n\t\t\t\tExtra:      make([]byte, 32),\n\t\t\t\tBaseFee:    big.NewInt(10000000000),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"receipt-for-storage\",\n\t\t\t&ReceiptForStorage{\n\t\t\t\tStatus:            ReceiptStatusSuccessful,\n\t\t\t\tCumulativeGasUsed: 0x888888888,\n\t\t\t\tLogs:              make([]*Log, 0),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"receipt-full\",\n\t\t\t&Receipt{\n\t\t\t\tStatus:            ReceiptStatusSuccessful,\n\t\t\t\tCumulativeGasUsed: 0x888888888,\n\t\t\t\tLogs:              make([]*Log, 0),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"legacy-transaction\",\n\t\t\tMustSignNewTx(key, signer,\n\t\t\t\t&LegacyTx{\n\t\t\t\t\tNonce:    1,\n\t\t\t\t\tGasPrice: big.NewInt(500),\n\t\t\t\t\tGas:      1000000,\n\t\t\t\t\tTo:       &to,\n\t\t\t\t\tValue:    big.NewInt(1),\n\t\t\t\t}),\n\t\t},\n\t\t{\n\t\t\t\"access-transaction\",\n\t\t\tMustSignNewTx(key, signer,\n\t\t\t\t&AccessListTx{\n\t\t\t\t\tNonce:    1,\n\t\t\t\t\tGasPrice: big.NewInt(500),\n\t\t\t\t\tGas:      1000000,\n\t\t\t\t\tTo:       &to,\n\t\t\t\t\tValue:    big.NewInt(1),\n\t\t\t\t}),\n\t\t},\n\t\t{\n\t\t\t\"1559-transaction\",\n\t\t\tMustSignNewTx(key, signer,\n\t\t\t\t&DynamicFeeTx{\n\t\t\t\t\tNonce:     1,\n\t\t\t\t\tGas:       1000000,\n\t\t\t\t\tTo:        &to,\n\t\t\t\t\tValue:     big.NewInt(1),\n\t\t\t\t\tGasTipCap: big.NewInt(500),\n\t\t\t\t\tGasFeeCap: big.NewInt(500),\n\t\t\t\t}),\n\t\t},\n\t} {\n\t\tif encode {\n\t\t\tb.Run(tc.name, func(b *testing.B) {\n\t\t\t\tb.ReportAllocs()\n\t\t\t\tvar null = &devnull{}\n\t\t\t\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/transaction.go",
          "line": 592,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rlp.ListSize(sc.encodedSize())\n\t}\n\n\t// For typed transactions, the encoding also includes the leading type byte.\n\tif tx.Type() != LegacyTxType {\n\t\tsize += 1\n\t}\n\n\ttx.size.Store(size)\n\treturn size\n}\n\n// WithSignature returns a new transaction with the given signature.\n// This signature needs to be in the [R || S || V] format where V is 0 or 1.\nfunc (tx *Transaction) WithSignature(signer Signer, sig []byte) (*Transaction, error) {\n\tr, s, v, err := signer.SignatureValues(tx, sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif r == nil || s == nil || v == nil {\n\t\treturn nil, fmt.Errorf(\"%w: r: %s, s: %s, v: %s\", ErrInvalidSig, r, s, v)\n\t}\n\tcpy := tx.inner.copy()\n\tcpy.setSignatureValues(signer.ChainID(), v, r, s)\n\treturn &Transaction{inner: cpy, time: tx.time}, nil\n}\n\n// Transactions implements DerivableList for transactions.\ntype Transactions []*Transaction\n\n// Len returns the length of s.\nfunc (s Transactions) Len() int { return len(s) }\n\n// EncodeIndex encodes the i'th transaction to w. Note that this does not check for errors\n// because we assume that *Transaction will only ever contain valid txs that were either\n// constructed by decoding or via public API in this package.\nfunc (s Transactions) EncodeIndex(i int, w *bytes.Buffer) {\n\ttx := s[i]\n\tif tx.Type() == LegacyTxType {\n\t\trlp.Encode(w, tx.inner)\n\t} else {\n\t\ttx.encodeTyped(w)\n\t}\n}\n\n// TxDifference returns a new set of transactions that are present in a but not in b.\nfunc TxDifference(a, b Transactions) Transactions {\n\tkeep := make(Transactions, 0, len(a))\n\n\tremove := make(map[common.Hash]struct{}, b.Len())\n\tfor _, tx := range b {\n\t\tremove[tx.Hash()] = struct{}{}\n\t}\n\n\tfor _, tx := range a {\n\t\tif _, ok := remove[tx.Hash()]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/receipt.go",
          "line": 260,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(r.Logs)) * common.StorageSize(unsafe.Sizeof(Log{}))\n\tfor _, log := range r.Logs {\n\t\tsize += common.StorageSize(len(log.Topics)*common.HashLength + len(log.Data))\n\t}\n\treturn size\n}\n\n// DeriveReceiptContext holds the contextual information needed to derive a receipt\ntype DeriveReceiptContext struct {\n\tBlockHash    common.Hash\n\tBlockNumber  uint64\n\tBlockTime    uint64\n\tBaseFee      *big.Int\n\tBlobGasPrice *big.Int\n\tGasUsed      uint64\n\tLogIndex     uint // Number of logs in the block until this receipt\n\tTx           *Transaction\n\tTxIndex      uint\n}\n\n// DeriveFields fills the receipt with computed fields based on consensus\n// data and contextual infos like containing block and transactions.\nfunc (r *Receipt) DeriveFields(signer Signer, context DeriveReceiptContext) {\n\t// The transaction type and hash can be retrieved from the transaction itself\n\tr.Type = context.Tx.Type()\n\tr.TxHash = context.Tx.Hash()\n\tr.GasUsed = context.GasUsed\n\tr.EffectiveGasPrice = context.Tx.inner.effectiveGasPrice(new(big.Int), context.BaseFee)\n\n\t// EIP-4844 blob transaction fields\n\tif context.Tx.Type() == BlobTxType {\n\t\tr.BlobGasUsed = context.Tx.BlobGas()\n\t\tr.BlobGasPrice = context.BlobGasPrice\n\t}\n\n\t// Block location fields\n\tr.BlockHash = context.BlockHash\n\tr.BlockNumber = new(big.Int).SetUint64(context.BlockNumber)\n\tr.TransactionIndex = context.TxIndex\n\n\t// The contract address can be derived from the transaction itself\n\tif context.Tx.To() == nil {\n\t\t// Deriving the signer is expensive, only do if it's actually needed\n\t\tfrom, _ := Sender(signer, context.Tx)\n\t\tr.ContractAddress = crypto.CreateAddress(from, context.Tx.Nonce())\n\t} else {\n\t\tr.ContractAddress = common.Address{}\n\t}\n\t// The derived log fields can simply be set from the block and transaction\n\tlogIndex := context.LogIndex\n\tfor j := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/types/receipt.go",
          "line": 411,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint(len(rs[i].Logs))\n\t}\n\treturn nil\n}\n\n// EncodeBlockReceiptLists encodes a list of block receipt lists into RLP.\nfunc EncodeBlockReceiptLists(receipts []Receipts) []rlp.RawValue {\n\tvar storageReceipts []*ReceiptForStorage\n\tresult := make([]rlp.RawValue, len(receipts))\n\tfor i, receipt := range receipts {\n\t\tstorageReceipts = storageReceipts[:0]\n\t\tfor _, r := range receipt {\n\t\t\tstorageReceipts = append(storageReceipts, (*ReceiptForStorage)(r))\n\t\t}\n\t\tbytes, err := rlp.EncodeToBytes(storageReceipts)\n\t\tif err != nil {\n\t\t\tlog.Crit(\"Failed to encode block receipts\", \"err\", err)\n\t\t}\n\t\tresult[i] = bytes\n\t}\n\treturn result\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/tx_blob.go",
          "line": 101,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rlp.BytesSize(sc.Blobs[i][:])\n\t}\n\tfor i := range sc.Commitments {\n\t\tcommitments += rlp.BytesSize(sc.Commitments[i][:])\n\t}\n\tfor i := range sc.Proofs {\n\t\tproofs += rlp.BytesSize(sc.Proofs[i][:])\n\t}\n\treturn rlp.ListSize(blobs) + rlp.ListSize(commitments) + rlp.ListSize(proofs)\n}\n\n// ValidateBlobCommitmentHashes checks whether the given hashes correspond to the\n// commitments in the sidecar\nfunc (sc *BlobTxSidecar) ValidateBlobCommitmentHashes(hashes []common.Hash) error {\n\tif len(sc.Commitments) != len(hashes) {\n\t\treturn fmt.Errorf(\"invalid number of %d blob commitments compared to %d blob hashes\", len(sc.Commitments), len(hashes))\n\t}\n\thasher := sha256.New()\n\tfor i, vhash := range hashes {\n\t\tcomputed := kzg4844.CalcBlobHashV1(hasher, &sc.Commitments[i])\n\t\tif vhash != computed {\n\t\t\treturn fmt.Errorf(\"blob %d: computed hash %#x mismatches transaction one %#x\", i, computed, vhash)\n\t\t}\n\t}\n\treturn nil\n}\n\n// blobTxWithBlobs represents blob tx with its corresponding sidecar.\n// This is an interface because sidecars are versioned.\ntype blobTxWithBlobs interface {\n\ttx() *BlobTx\n\tassign(*BlobTxSidecar) error\n}\n\ntype blobTxWithBlobsV0 struct {\n\tBlobTx      *BlobTx\n\tBlobs       []kzg4844.Blob\n\tCommitments []kzg4844.Commitment\n\tProofs      []kzg4844.Proof\n}\n\ntype blobTxWithBlobsV1 struct {\n\tBlobTx      *BlobTx\n\tVersion     byte\n\tBlobs       []kzg4844.Blob\n\tCommitments []kzg4844.Commitment\n\tProofs      []kzg4844.Proof\n}\n\nfunc (btx *blobTxWithBlobsV0) tx() *BlobTx {\n\treturn btx.BlobTx\n}\n\nfunc (btx *blobTxWithBlobsV0) assign(sc *BlobTxSidecar) error {\n\tsc.Version = 0\n\tsc.Blobs = btx.Blobs\n\tsc.Commitments = btx.Commitments\n\tsc.Proofs = btx.Proofs\n\treturn nil\n}\n\nfunc (btx *blobTxWithBlobsV1) tx() *BlobTx {\n\treturn btx.BlobTx\n}\n\nfunc (btx *blobTxWithBlobsV1) assign(sc *BlobTxSidecar) error {\n\t// NOTE(BSC): Upstream geth supports both Version 0 and 1 sidecars.\n\t// BSC only supports Version 0, as EIP-7594 (cell proofs) is not enabled yet.\n\tdisableEIP7594 := true\n\tif disableEIP7594 || btx.Version != 1 {\n\t\treturn fmt.Errorf(\"unsupported blob tx version %d\", btx.Version)\n\t}\n\tsc.Version = 1\n\tsc.Blobs = btx.Blobs\n\tsc.Commitments = btx.Commitments\n\tsc.Proofs = btx.Proofs\n\treturn nil\n}\n\n// copy creates a deep copy of the transaction data and initializes all fields.\nfunc (tx *BlobTx) copy() TxData {\n\tcpy := &BlobTx{\n\t\tNonce: tx.Nonce,\n\t\tTo:    tx.To,\n\t\tData:  common.CopyBytes(tx.Data),\n\t\tGas:   tx.Gas,\n\t\t// These are copied below.\n\t\tAccessList: make(AccessList, len(tx.AccessList)),\n\t\tBlobHashes: make([]common.Hash, len(tx.BlobHashes)),\n\t\tValue:      new(uint256.Int),\n\t\tChainID:    new(uint256.Int),\n\t\tGasTipCap:  new(uint256.Int),\n\t\tGasFeeCap:  new(uint256.Int),\n\t\tBlobFeeCap: new(uint256.Int),\n\t\tV:          new(uint256.Int),\n\t\tR:          new(uint256.Int),\n\t\tS:          new(uint256.Int),\n\t}\n\tcopy(cpy.AccessList, tx.AccessList)\n\tcopy(cpy.BlobHashes, tx.BlobHashes)\n\n\tif tx.Value != nil {\n\t\tcpy.Value.Set(tx.Value)\n\t}\n\tif tx.ChainID != nil {\n\t\tcpy.ChainID.Set(tx.ChainID)\n\t}\n\tif tx.GasTipCap != nil {\n\t\tcpy.GasTipCap.Set(tx.GasTipCap)\n\t}\n\tif tx.GasFeeCap != nil {\n\t\tcpy.GasFeeCap.Set(tx.GasFeeCap)\n\t}\n\tif tx.BlobFeeCap != nil {\n\t\tcpy.BlobFeeCap.Set(tx.BlobFeeCap)\n\t}\n\tif tx.V != nil {\n\t\tcpy.V.Set(tx.V)\n\t}\n\tif tx.R != nil {\n\t\tcpy.R.Set(tx.R)\n\t}\n\tif tx.S != nil {\n\t\tcpy.S.Set(tx.S)\n\t}\n\tif tx.Sidecar != nil {\n\t\tcpy.Sidecar = &BlobTxSidecar{\n\t\t\tVersion:     tx.Sidecar.Version,\n\t\t\tBlobs:       slices.Clone(tx.Sidecar.Blobs),\n\t\t\tCommitments: slices.Clone(tx.Sidecar.Commitments),\n\t\t\tProofs:      slices.Clone(tx.Sidecar.Proofs),\n\t\t}\n\t}\n\treturn cpy\n}\n\n// accessors for innerTx.\nfunc (tx *BlobTx) txType() byte           { return BlobTxType }\nfunc (tx *BlobTx) chainID() *big.Int      { return tx.ChainID.ToBig() }\nfunc (tx *BlobTx) accessList() AccessList { return tx.AccessList }\nfunc (tx *BlobTx) data() []byte           { return tx.Data }\nfunc (tx *BlobTx) gas() uint64            { return tx.Gas }\nfunc (tx *BlobTx) gasFeeCap() *big.Int    { return tx.GasFeeCap.ToBig() }\nfunc (tx *BlobTx) gasTipCap() *big.Int    { return tx.GasTipCap.ToBig() }\nfunc (tx *BlobTx) gasPrice() *big.Int     { return tx.GasFeeCap.ToBig() }\nfunc (tx *BlobTx) value() *big.Int        { return tx.Value.ToBig() }\nfunc (tx *BlobTx) nonce() uint64          { return tx.Nonce }\nfunc (tx *BlobTx) to() *common.Address    { tmp := tx.To",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/tx_access_list.go",
          "line": 42,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(tuple.StorageKeys)\n\t}\n\treturn sum\n}\n\n// AccessListTx is the data of EIP-2930 access list transactions.\ntype AccessListTx struct {\n\tChainID    *big.Int        // destination chain ID\n\tNonce      uint64          // nonce of sender account\n\tGasPrice   *big.Int        // wei per gas\n\tGas        uint64          // gas limit\n\tTo         *common.Address `rlp:\"nil\"` // nil means contract creation\n\tValue      *big.Int        // wei amount\n\tData       []byte          // contract invocation input data\n\tAccessList AccessList      // EIP-2930 access list\n\tV, R, S    *big.Int        // signature values\n}\n\n// copy creates a deep copy of the transaction data and initializes all fields.\nfunc (tx *AccessListTx) copy() TxData {\n\tcpy := &AccessListTx{\n\t\tNonce: tx.Nonce,\n\t\tTo:    copyAddressPtr(tx.To),\n\t\tData:  common.CopyBytes(tx.Data),\n\t\tGas:   tx.Gas,\n\t\t// These are copied below.\n\t\tAccessList: make(AccessList, len(tx.AccessList)),\n\t\tValue:      new(big.Int),\n\t\tChainID:    new(big.Int),\n\t\tGasPrice:   new(big.Int),\n\t\tV:          new(big.Int),\n\t\tR:          new(big.Int),\n\t\tS:          new(big.Int),\n\t}\n\tcopy(cpy.AccessList, tx.AccessList)\n\tif tx.Value != nil {\n\t\tcpy.Value.Set(tx.Value)\n\t}\n\tif tx.ChainID != nil {\n\t\tcpy.ChainID.Set(tx.ChainID)\n\t}\n\tif tx.GasPrice != nil {\n\t\tcpy.GasPrice.Set(tx.GasPrice)\n\t}\n\tif tx.V != nil {\n\t\tcpy.V.Set(tx.V)\n\t}\n\tif tx.R != nil {\n\t\tcpy.R.Set(tx.R)\n\t}\n\tif tx.S != nil {\n\t\tcpy.S.Set(tx.S)\n\t}\n\treturn cpy\n}\n\n// accessors for innerTx.\nfunc (tx *AccessListTx) txType() byte           { return AccessListTxType }\nfunc (tx *AccessListTx) chainID() *big.Int      { return tx.ChainID }\nfunc (tx *AccessListTx) accessList() AccessList { return tx.AccessList }\nfunc (tx *AccessListTx) data() []byte           { return tx.Data }\nfunc (tx *AccessListTx) gas() uint64            { return tx.Gas }\nfunc (tx *AccessListTx) gasPrice() *big.Int     { return tx.GasPrice }\nfunc (tx *AccessListTx) gasTipCap() *big.Int    { return tx.GasPrice }\nfunc (tx *AccessListTx) gasFeeCap() *big.Int    { return tx.GasPrice }\nfunc (tx *AccessListTx) value() *big.Int        { return tx.Value }\nfunc (tx *AccessListTx) nonce() uint64          { return tx.Nonce }\nfunc (tx *AccessListTx) to() *common.Address    { return tx.To }\n\nfunc (tx *AccessListTx) effectiveGasPrice(dst *big.Int, baseFee *big.Int) *big.Int {\n\treturn dst.Set(tx.GasPrice)\n}\n\nfunc (tx *AccessListTx) rawSignatureValues() (v, r, s *big.Int) {\n\treturn tx.V, tx.R, tx.S\n}\n\nfunc (tx *AccessListTx) setSignatureValues(chainID, v, r, s *big.Int) {\n\ttx.ChainID, tx.V, tx.R, tx.S = chainID, v, r, s\n}\n\nfunc (tx *AccessListTx) encode(b *bytes.Buffer) error {\n\treturn rlp.Encode(b, tx)\n}\n\nfunc (tx *AccessListTx) decode(input []byte) error {\n\treturn rlp.DecodeBytes(input, tx)\n}\n\nfunc (tx *AccessListTx) sigHash(chainID *big.Int) common.Hash {\n\treturn prefixedRlpHash(\n\t\tAccessListTxType,\n\t\t[]any{\n\t\t\tchainID,\n\t\t\ttx.Nonce,\n\t\t\ttx.GasPrice,\n\t\t\ttx.Gas,\n\t\t\ttx.To,\n\t\t\ttx.Value,\n\t\t\ttx.Data,\n\t\t\ttx.AccessList,\n\t\t})\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/bloom9_test.go",
          "line": 126,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tcopy(rLarge[i:], rSmall)\n\t}\n\tb.Run(\"small-createbloom\", func(b *testing.B) {\n\t\tb.ReportAllocs()\n\t\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/types/block.go",
          "line": 518,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeCounter(len(b))\n\treturn len(b), nil\n}\n\nfunc CalcUncleHash(uncles []*Header) common.Hash {\n\tif len(uncles) == 0 {\n\t\treturn EmptyUncleHash\n\t}\n\treturn rlpHash(uncles)\n}\n\n// CalcRequestsHash creates the block requestsHash value for a list of requests.\nfunc CalcRequestsHash(requests [][]byte) common.Hash {\n\th1, h2 := sha256.New(), sha256.New()\n\tvar buf common.Hash\n\tfor _, item := range requests {\n\t\tif len(item) > 1 { // skip items with only requestType and no data.\n\t\t\th1.Reset()\n\t\t\th1.Write(item)\n\t\t\th2.Write(h1.Sum(buf[:0]))\n\t\t}\n\t}\n\th2.Sum(buf[:0])\n\treturn buf\n}\n\n// NewBlockWithHeader creates a block with the given header data. The\n// header data is copied, changes to header and to the field values\n// will not affect the block.\nfunc NewBlockWithHeader(header *Header) *Block {\n\treturn &Block{header: CopyHeader(header)}\n}\n\n// WithSeal returns a new block with the data from b but the header replaced with\n// the sealed one.\nfunc (b *Block) WithSeal(header *Header) *Block {\n\t// fill sidecars metadata\n\tfor _, sidecar := range b.sidecars {\n\t\tsidecar.BlockNumber = header.Number\n\t\tsidecar.BlockHash = header.Hash()\n\t}\n\treturn &Block{\n\t\theader:       CopyHeader(header),\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twithdrawals:  b.withdrawals,\n\t\twitness:      b.witness,\n\t\tsidecars:     b.sidecars,\n\t}\n}\n\n// WithBody returns a new block with the original header and a deep copy of the\n// provided body.\nfunc (b *Block) WithBody(body Body) *Block {\n\tblock := &Block{\n\t\theader:       b.header,\n\t\ttransactions: slices.Clone(body.Transactions),\n\t\tuncles:       make([]*Header, len(body.Uncles)),\n\t\twithdrawals:  slices.Clone(body.Withdrawals),\n\t\twitness:      b.witness,\n\t\tsidecars:     b.sidecars,\n\t}\n\tfor i := range body.Uncles {\n\t\tblock.uncles[i] = CopyHeader(body.Uncles[i])\n\t}\n\treturn block\n}\n\n// WithWithdrawals returns a copy of the block containing the given withdrawals.\nfunc (b *Block) WithWithdrawals(withdrawals []*Withdrawal) *Block {\n\tblock := &Block{\n\t\theader:       b.header,\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twitness:      b.witness,\n\t\tsidecars:     b.sidecars,\n\t}\n\tif withdrawals != nil {\n\t\tblock.withdrawals = make([]*Withdrawal, len(withdrawals))\n\t\tcopy(block.withdrawals, withdrawals)\n\t}\n\treturn block\n}\n\n// WithSidecars returns a block containing the given blobs.\nfunc (b *Block) WithSidecars(sidecars BlobSidecars) *Block {\n\tblock := &Block{\n\t\theader:       b.header,\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twithdrawals:  b.withdrawals,\n\t\twitness:      b.witness,\n\t}\n\tif sidecars != nil {\n\t\tblock.sidecars = make(BlobSidecars, len(sidecars))\n\t\tcopy(block.sidecars, sidecars)\n\t}\n\treturn block\n}\n\nfunc (b *Block) WithWitness(witness *ExecutionWitness) *Block {\n\treturn &Block{\n\t\theader:       b.header,\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twithdrawals:  b.withdrawals,\n\t\twitness:      witness,\n\t\tsidecars:     b.sidecars,\n\t}\n}\n\n// Hash returns the keccak256 hash of b's header.\n// The hash is computed on the first call and cached thereafter.\nfunc (b *Block) Hash() common.Hash {\n\tif hash := b.hash.Load()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/table_test.go",
          "line": 111,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\tcount++\n\t\t}\n\t\tif count != expCount {\n\t\t\tt.Fatalf(\"Wrong number of elems, exp %d got %d\", expCount, count)\n\t\t}\n\t\titer.Release()\n\t}\n\t// Test iterators\n\tcheck(db.NewIterator(nil, nil), 6, 0)\n\t// Test iterators with prefix\n\tcheck(db.NewIterator([]byte{0xff, 0xff}, nil), 3, 3)\n\t// Test iterators with start point\n\tcheck(db.NewIterator(nil, []byte{0xff, 0xff, 0x02}), 2, 4)\n\t// Test iterators with prefix and start point\n\tcheck(db.NewIterator([]byte{0xee}, nil), 0, 0)\n\tcheck(db.NewIterator(nil, []byte{0x00}), 6, 0)\n\n\t// Test range deletion\n\tdb.DeleteRange(nil, nil)\n\tfor _, entry := range entries {\n\t\t_, err := db.Get(entry.key)\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Unexpected item after deletion\")\n\t\t}\n\t}\n\t// Test range deletion by batch\n\tbatch = db.NewBatch()\n\tfor _, entry := range entries {\n\t\tbatch.Put(entry.key, entry.value)\n\t}\n\tbatch.Write()\n\tbatch.Reset()\n\tbatch.DeleteRange(nil, nil)\n\tbatch.Write()\n\tfor _, entry := range entries {\n\t\t_, err := db.Get(entry.key)\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Unexpected item after deletion\")\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 72,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(data))\n\t\t// If we've spent too much time already, notify the user of what we're doing\n\t\tif time.Since(logged) > 8*time.Second {\n\t\t\tlog.Info(\"Initializing database from freezer\", \"total\", frozen, \"number\", i, \"hash\", hash, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\tlogged = time.Now()\n\t\t}\n\t}\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 229,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(delivery.hashes)\n\t\t\t// If enough data was accumulated in memory or we're at the last block, dump to disk\n\t\t\tif batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\t\tWriteTxIndexTail(batch, lastNum) // Also write the tail here\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 324,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(delivery.hashes)\n\t\t\tblocks++\n\n\t\t\t// If enough data was accumulated in memory or we're at the last block, dump to disk\n\t\t\t// A batch counts the size of deletion as '1', so we need to flush more\n\t\t\t// often than that.\n\t\t\tif blocks%1000 == 0 {\n\t\t\t\tWriteTxIndexTail(batch, nextNum)\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/freezer_test.go",
          "line": 146,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeBatchSize {\n\t\t\t_, err := f.ModifyAncients(func(op ethdb.AncientWriteOp) error {\n\t\t\t\tfor i := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/database.go",
          "line": 554,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\ts.count++\n}\n\nfunc (s *stat) Size() string {\n\treturn s.size.String()\n}\n\nfunc (s *stat) Count() string {\n\treturn s.count.String()\n}\n\nfunc AncientInspect(db ethdb.Database) error {\n\tancientTail, err := db.Tail()\n\tif err != nil {\n\t\treturn err\n\t}\n\tancientHead, err := db.Ancients()\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats := [][]string{\n\t\t{\"Offset/StartBlockNumber\", \"Offset/StartBlockNumber of ancientDB\", counter(ancientTail).String()},\n\t\t{\"Amount of remained items in AncientStore\", \"Remaining items of ancientDB\", counter(ancientHead - ancientTail).String()},\n\t\t{\"The last BlockNumber within ancientDB\", \"The last BlockNumber\", counter(ancientHead - 1).String()},\n\t}\n\ttable := tablewriter.NewWriter(os.Stdout)\n\ttable.SetHeader([]string{\"Database\", \"Category\", \"Items\"})\n\ttable.SetFooter([]string{\"\", \"AncientStore information\", \"\"})\n\ttable.AppendBulk(stats)\n\ttable.Render()\n\n\treturn nil\n}\n\ntype DataType int\n\nconst (\n\tStateDataType DataType = iota\n\tChainDataType\n\tUnknown\n)\n\nfunc DataTypeByKey(key []byte) DataType {\n\tswitch {\n\t// state\n\tcase IsLegacyTrieNode(key, key),\n\t\tbytes.HasPrefix(key, stateIDPrefix) && len(key) == len(stateIDPrefix)+common.HashLength,\n\t\tIsAccountTrieNode(key),\n\t\tIsStorageTrieNode(key):\n\t\treturn StateDataType\n\n\tdefault:\n\t\tfor _, meta := range [][]byte{\n\t\t\tfastTrieProgressKey, persistentStateIDKey, trieJournalKey, snapSyncStatusFlagKey} {\n\t\t\tif bytes.Equal(key, meta) {\n\t\t\t\treturn StateDataType\n\t\t\t}\n\t\t}\n\t\treturn ChainDataType\n\t}\n}\n\n// InspectDatabase traverses the entire database and checks the size\n// of all different categories of data.\nfunc InspectDatabase(db ethdb.Database, keyPrefix, keyStart []byte) error {\n\tit := db.NewIterator(keyPrefix, keyStart)\n\tdefer it.Release()\n\n\tvar trieIter ethdb.Iterator\n\tif db.HasSeparateStateStore() {\n\t\ttrieIter = db.GetStateStore().NewIterator(keyPrefix, nil)\n\t\tdefer trieIter.Release()\n\t}\n\n\tvar (\n\t\tcount  int64\n\t\tstart  = time.Now()\n\t\tlogged = time.Now()\n\n\t\t// Key-value store statistics\n\t\theaders            stat\n\t\tbodies             stat\n\t\treceipts           stat\n\t\ttds                stat\n\t\tnumHashPairings    stat\n\t\tblobSidecars       stat\n\t\thashNumPairings    stat\n\t\tlegacyTries        stat\n\t\tstateLookups       stat\n\t\taccountTries       stat\n\t\tstorageTries       stat\n\t\tcodes              stat\n\t\ttxLookups          stat\n\t\taccountSnaps       stat\n\t\tstorageSnaps       stat\n\t\tpreimages          stat\n\t\tcliqueSnaps        stat\n\t\tparliaSnaps        stat\n\t\tbloomBits          stat\n\t\tfilterMapRows      stat\n\t\tfilterMapLastBlock stat\n\t\tfilterMapBlockLV   stat\n\n\t\t// Path-mode archive data\n\t\tstateIndex stat\n\n\t\t// Verkle statistics\n\t\tverkleTries        stat\n\t\tverkleStateLookups stat\n\n\t\t// Meta- and unaccounted data\n\t\tmetadata    stat\n\t\tunaccounted stat\n\n\t\t// Totals\n\t\ttotal common.StorageSize\n\n\t\t// This map tracks example keys for unaccounted data.\n\t\t// For each unique two-byte prefix, the first unaccounted key encountered\n\t\t// by the iterator will be stored.\n\t\tunaccountedKeys = make(map[[2]byte][]byte)\n\t)\n\t// Inspect key-value database first.\n\tfor it.Next() {\n\t\tvar (\n\t\t\tkey  = it.Key()\n\t\t\tsize = common.StorageSize(len(key) + len(it.Value()))\n\t\t)\n\t\ttotal += size\n\t\tswitch {\n\t\tcase bytes.HasPrefix(key, headerPrefix) && len(key) == (len(headerPrefix)+8+common.HashLength):\n\t\t\theaders.Add(size)\n\t\tcase bytes.HasPrefix(key, blockBodyPrefix) && len(key) == (len(blockBodyPrefix)+8+common.HashLength):\n\t\t\tbodies.Add(size)\n\t\tcase bytes.HasPrefix(key, blockReceiptsPrefix) && len(key) == (len(blockReceiptsPrefix)+8+common.HashLength):\n\t\t\treceipts.Add(size)\n\t\tcase bytes.HasPrefix(key, headerPrefix) && bytes.HasSuffix(key, headerTDSuffix):\n\t\t\ttds.Add(size)\n\t\tcase bytes.HasPrefix(key, BlockBlobSidecarsPrefix):\n\t\t\tblobSidecars.Add(size)\n\t\tcase bytes.HasPrefix(key, headerPrefix) && bytes.HasSuffix(key, headerHashSuffix):\n\t\t\tnumHashPairings.Add(size)\n\t\tcase bytes.HasPrefix(key, headerNumberPrefix) && len(key) == (len(headerNumberPrefix)+common.HashLength):\n\t\t\thashNumPairings.Add(size)\n\t\tcase IsLegacyTrieNode(key, it.Value()):\n\t\t\tlegacyTries.Add(size)\n\t\tcase bytes.HasPrefix(key, stateIDPrefix) && len(key) == len(stateIDPrefix)+common.HashLength:\n\t\t\tstateLookups.Add(size)\n\t\tcase IsAccountTrieNode(key):\n\t\t\taccountTries.Add(size)\n\t\tcase IsStorageTrieNode(key):\n\t\t\tstorageTries.Add(size)\n\t\tcase bytes.HasPrefix(key, CodePrefix) && len(key) == len(CodePrefix)+common.HashLength:\n\t\t\tcodes.Add(size)\n\t\tcase bytes.HasPrefix(key, txLookupPrefix) && len(key) == (len(txLookupPrefix)+common.HashLength):\n\t\t\ttxLookups.Add(size)\n\t\tcase bytes.HasPrefix(key, SnapshotAccountPrefix) && len(key) == (len(SnapshotAccountPrefix)+common.HashLength):\n\t\t\taccountSnaps.Add(size)\n\t\tcase bytes.HasPrefix(key, SnapshotStoragePrefix) && len(key) == (len(SnapshotStoragePrefix)+2*common.HashLength):\n\t\t\tstorageSnaps.Add(size)\n\t\tcase bytes.HasPrefix(key, PreimagePrefix) && len(key) == (len(PreimagePrefix)+common.HashLength):\n\t\t\tpreimages.Add(size)\n\t\tcase bytes.HasPrefix(key, configPrefix) && len(key) == (len(configPrefix)+common.HashLength):\n\t\t\tmetadata.Add(size)\n\t\tcase bytes.HasPrefix(key, genesisPrefix) && len(key) == (len(genesisPrefix)+common.HashLength):\n\t\t\tmetadata.Add(size)\n\t\tcase bytes.HasPrefix(key, CliqueSnapshotPrefix) && len(key) == 7+common.HashLength:\n\t\t\tcliqueSnaps.Add(size)\n\t\tcase bytes.HasPrefix(key, ParliaSnapshotPrefix) && len(key) == 7+common.HashLength:\n\t\t\tparliaSnaps.Add(size)\n\n\t\t// new log index\n\t\tcase bytes.HasPrefix(key, filterMapRowPrefix) && len(key) <= len(filterMapRowPrefix)+9:\n\t\t\tfilterMapRows.Add(size)\n\t\tcase bytes.HasPrefix(key, filterMapLastBlockPrefix) && len(key) == len(filterMapLastBlockPrefix)+4:\n\t\t\tfilterMapLastBlock.Add(size)\n\t\tcase bytes.HasPrefix(key, filterMapBlockLVPrefix) && len(key) == len(filterMapBlockLVPrefix)+8:\n\t\t\tfilterMapBlockLV.Add(size)\n\n\t\t// old log index (deprecated)\n\t\tcase bytes.HasPrefix(key, bloomBitsPrefix) && len(key) == (len(bloomBitsPrefix)+10+common.HashLength):\n\t\t\tbloomBits.Add(size)\n\t\tcase bytes.HasPrefix(key, bloomBitsMetaPrefix) && len(key) < len(bloomBitsMetaPrefix)+8:\n\t\t\tbloomBits.Add(size)\n\n\t\t// Path-based historic state indexes\n\t\tcase bytes.HasPrefix(key, StateHistoryIndexPrefix) && len(key) >= len(StateHistoryIndexPrefix)+common.HashLength:\n\t\t\tstateIndex.Add(size)\n\n\t\t// Verkle trie data is detected, determine the sub-category\n\t\tcase bytes.HasPrefix(key, VerklePrefix):\n\t\t\tremain := key[len(VerklePrefix):]\n\t\t\tswitch {\n\t\t\tcase IsAccountTrieNode(remain):\n\t\t\t\tverkleTries.Add(size)\n\t\t\tcase bytes.HasPrefix(remain, stateIDPrefix) && len(remain) == len(stateIDPrefix)+common.HashLength:\n\t\t\t\tverkleStateLookups.Add(size)\n\t\t\tcase bytes.Equal(remain, persistentStateIDKey):\n\t\t\t\tmetadata.Add(size)\n\t\t\tcase bytes.Equal(remain, trieJournalKey):\n\t\t\t\tmetadata.Add(size)\n\t\t\tcase bytes.Equal(remain, snapSyncStatusFlagKey):\n\t\t\t\tmetadata.Add(size)\n\t\t\tdefault:\n\t\t\t\tunaccounted.Add(size)\n\t\t\t}\n\n\t\t// Metadata keys\n\t\tcase slices.ContainsFunc(knownMetadataKeys, func(x []byte) bool { return bytes.Equal(x, key) }):\n\t\t\tmetadata.Add(size)\n\n\t\tdefault:\n\t\t\tunaccounted.Add(size)\n\t\t\tif len(key) >= 2 {\n\t\t\t\tprefix := [2]byte(key[:2])\n\t\t\t\tif _, ok := unaccountedKeys[prefix]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/database.go",
          "line": 791,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\n\t\t\tswitch {\n\t\t\tcase IsLegacyTrieNode(key, value):\n\t\t\t\tlegacyTries.Add(size)\n\t\t\tcase bytes.HasPrefix(key, stateIDPrefix) && len(key) == len(stateIDPrefix)+common.HashLength:\n\t\t\t\tstateLookups.Add(size)\n\t\t\tcase IsAccountTrieNode(key):\n\t\t\t\taccountTries.Add(size)\n\t\t\tcase IsStorageTrieNode(key):\n\t\t\t\tstorageTries.Add(size)\n\t\t\tdefault:\n\t\t\t\tvar accounted bool\n\t\t\t\tfor _, meta := range [][]byte{\n\t\t\t\t\tfastTrieProgressKey, persistentStateIDKey, trieJournalKey, snapSyncStatusFlagKey} {\n\t\t\t\t\tif bytes.Equal(key, meta) {\n\t\t\t\t\t\tmetadata.Add(size)\n\t\t\t\t\t\taccounted = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !accounted {\n\t\t\t\t\tunaccounted.Add(size)\n\t\t\t\t}\n\t\t\t}\n\t\t\tcount++\n\t\t\tif count%1000 == 0 && time.Since(logged) > 8*time.Second {\n\t\t\t\tlog.Info(\"Inspecting separate state database\", \"count\", count, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\tlogged = time.Now()\n\t\t\t}\n\t\t}\n\t\tlog.Info(\"Inspecting separate state database\", \"count\", count, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t}\n\t// Display the database statistic of key-value store.\n\tstats := [][]string{\n\t\t{\"Key-Value store\", \"Headers\", headers.Size(), headers.Count()},\n\t\t{\"Key-Value store\", \"Bodies\", bodies.Size(), bodies.Count()},\n\t\t{\"Key-Value store\", \"Receipt lists\", receipts.Size(), receipts.Count()},\n\t\t{\"Key-Value store\", \"Difficulties\", tds.Size(), tds.Count()},\n\t\t{\"Key-Value store\", \"BlobSidecars\", blobSidecars.Size(), blobSidecars.Count()},\n\t\t{\"Key-Value store\", \"Block number->hash\", numHashPairings.Size(), numHashPairings.Count()},\n\t\t{\"Key-Value store\", \"Block hash->number\", hashNumPairings.Size(), hashNumPairings.Count()},\n\t\t{\"Key-Value store\", \"Transaction index\", txLookups.Size(), txLookups.Count()},\n\t\t{\"Key-Value store\", \"Log index filter-map rows\", filterMapRows.Size(), filterMapRows.Count()},\n\t\t{\"Key-Value store\", \"Log index last-block-of-map\", filterMapLastBlock.Size(), filterMapLastBlock.Count()},\n\t\t{\"Key-Value store\", \"Log index block-lv\", filterMapBlockLV.Size(), filterMapBlockLV.Count()},\n\t\t{\"Key-Value store\", \"Log bloombits (deprecated)\", bloomBits.Size(), bloomBits.Count()},\n\t\t{\"Key-Value store\", \"Contract codes\", codes.Size(), codes.Count()},\n\t\t{\"Key-Value store\", \"Hash trie nodes\", legacyTries.Size(), legacyTries.Count()},\n\t\t{\"Key-Value store\", \"Path trie state lookups\", stateLookups.Size(), stateLookups.Count()},\n\t\t{\"Key-Value store\", \"Path trie account nodes\", accountTries.Size(), accountTries.Count()},\n\t\t{\"Key-Value store\", \"Path trie storage nodes\", storageTries.Size(), storageTries.Count()},\n\t\t{\"Key-Value store\", \"Path state history indexes\", stateIndex.Size(), stateIndex.Count()},\n\t\t{\"Key-Value store\", \"Verkle trie nodes\", verkleTries.Size(), verkleTries.Count()},\n\t\t{\"Key-Value store\", \"Verkle trie state lookups\", verkleStateLookups.Size(), verkleStateLookups.Count()},\n\t\t{\"Key-Value store\", \"Trie preimages\", preimages.Size(), preimages.Count()},\n\t\t{\"Key-Value store\", \"Account snapshot\", accountSnaps.Size(), accountSnaps.Count()},\n\t\t{\"Key-Value store\", \"Storage snapshot\", storageSnaps.Size(), storageSnaps.Count()},\n\t\t{\"Key-Value store\", \"Clique snapshots\", cliqueSnaps.Size(), cliqueSnaps.Count()},\n\t\t{\"Key-Value store\", \"Parlia snapshots\", parliaSnaps.Size(), parliaSnaps.Count()},\n\t\t{\"Key-Value store\", \"Singleton metadata\", metadata.Size(), metadata.Count()},\n\t}\n\t// Inspect all registered append-only file store then.\n\tancients, err := inspectFreezers(db)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, ancient := range ancients {\n\t\tfor _, table := range ancient.sizes {\n\t\t\tstats = append(stats, []string{\n\t\t\t\tfmt.Sprintf(\"Ancient store (%s)\", strings.Title(ancient.name)),\n\t\t\t\tstrings.Title(table.name),\n\t\t\t\ttable.size.String(),\n\t\t\t\tfmt.Sprintf(\"%d\", ancient.count()),\n\t\t\t})\n\t\t}\n\t\ttotal += ancient.size()\n\t}\n\n\t// inspect ancient state in separate trie db if exist\n\tif trieIter != nil {\n\t\tstateAncients, err := inspectFreezers(db.GetStateStore())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, ancient := range stateAncients {\n\t\t\tfor _, table := range ancient.sizes {\n\t\t\t\tif ancient.name == \"chain\" {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tstats = append(stats, []string{\n\t\t\t\t\tfmt.Sprintf(\"Ancient store (%s)\", strings.Title(ancient.name)),\n\t\t\t\t\tstrings.Title(table.name),\n\t\t\t\t\ttable.size.String(),\n\t\t\t\t\tfmt.Sprintf(\"%d\", ancient.count()),\n\t\t\t\t})\n\t\t\t}\n\t\t\ttotal += ancient.size()\n\t\t}\n\t}\n\ttable := tablewriter.NewWriter(os.Stdout)\n\ttable.SetHeader([]string{\"Database\", \"Category\", \"Size\", \"Items\"})\n\ttable.SetFooter([]string{\"\", \"Total\", total.String(), \" \"})\n\ttable.AppendBulk(stats)\n\ttable.Render()\n\n\tif unaccounted.size > 0 {\n\t\tlog.Error(\"Database contains unaccounted data\", \"size\", unaccounted.size, \"count\", unaccounted.count)\n\t\tfor _, e := range slices.SortedFunc(maps.Values(unaccountedKeys), bytes.Compare) {\n\t\t\tlog.Error(fmt.Sprintf(\"   example key: %x\", e))\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc DeleteTrieState(db ethdb.Database) error {\n\tvar (\n\t\tit     ethdb.Iterator\n\t\tbatch  = db.NewBatch()\n\t\tstart  = time.Now()\n\t\tlogged = time.Now()\n\t\tcount  int64\n\t\tkey    []byte\n\t)\n\n\tprefixKeys := map[string]func([]byte) bool{\n\t\tstring(TrieNodeAccountPrefix): IsAccountTrieNode,\n\t\tstring(TrieNodeStoragePrefix): IsStorageTrieNode,\n\t\tstring(stateIDPrefix):         func(key []byte) bool { return len(key) == len(stateIDPrefix)+common.HashLength },\n\t}\n\n\tfor prefix, isValid := range prefixKeys {\n\t\tit = db.NewIterator([]byte(prefix), nil)\n\n\t\tfor it.Next() {\n\t\t\tkey = it.Key()\n\t\t\tif !isValid(key) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tbatch.Delete(it.Key())\n\t\t\tif batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 523,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= indexEntrySize {\n\t\tentry, err := read()\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tif offset == 0 {\n\t\t\thead = entry\n\t\t\tcontinue\n\t\t}\n\t\t// Ensure that the first non-head index refers to the earliest file,\n\t\t// or the next file if the earliest file has no space to place the\n\t\t// first item.\n\t\tif offset == indexEntrySize {\n\t\t\tif entry.filenum != head.filenum && entry.filenum != head.filenum+1 {\n\t\t\t\tlog.Error(\"Corrupted index item detected\", \"earliest\", head.filenum, \"filenumber\", entry.filenum)\n\t\t\t\treturn truncate(offset)\n\t\t\t}\n\t\t\tprev = entry\n\t\t\tcontinue\n\t\t}\n\t\t// ensure two consecutive index items are in order\n\t\tif err := t.checkIndexItems(prev, entry)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 955,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= indexEntrySize\n\t\tindices = append(indices, index)\n\t}\n\tif from == 0 {\n\t\t// Special case if we're reading the first item in the freezer. We assume that\n\t\t// the first item always start from zero(regarding the deletion, we\n\t\t// only support deletion by files, so that the assumption is held).\n\t\t// This means we can use the first item metadata to carry information about\n\t\t// the 'global' offset, for the deletion-case\n\t\tindices[0].offset = 0\n\t\tindices[0].filenum = indices[1].filenum\n\t}\n\treturn indices, nil\n}\n\n// Retrieve looks up the data offset of an item with the given number and retrieves\n// the raw binary blob from the data file.\nfunc (t *freezerTable) Retrieve(item uint64) ([]byte, error) {\n\titems, err := t.RetrieveItems(item, 1, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn items[0], nil\n}\n\n// RetrieveItems returns multiple items in sequence, starting from the index 'start'.\n// It will return at most 'max' items, but will abort earlier to respect the\n// 'maxBytes' argument. However, if the 'maxBytes' is smaller than the size of one\n// item, it _will_ return one element and possibly overflow the maxBytes.\nfunc (t *freezerTable) RetrieveItems(start, count, maxBytes uint64) ([][]byte, error) {\n\t// First we read the 'raw' data, which might be compressed.\n\tdiskData, sizes, err := t.retrieveItems(start, count, maxBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\toutput     = make([][]byte, 0, count)\n\t\toffset     int // offset for reading\n\t\toutputSize int // size of uncompressed data\n\t)\n\t// Now slice up the data and decompress.\n\tfor i, diskSize := range sizes {\n\t\titem := diskData[offset : offset+diskSize]\n\t\toffset += diskSize\n\t\tdecompressedSize := diskSize\n\t\tif !t.config.noSnappy {\n\t\t\tdecompressedSize, _ = snappy.DecodedLen(item)\n\t\t}\n\t\tif i > 0 && maxBytes != 0 && uint64(outputSize+decompressedSize) > maxBytes {\n\t\t\tbreak\n\t\t}\n\t\tif !t.config.noSnappy {\n\t\t\tdata, err := snappy.Decode(nil, item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\toutput = append(output, data)\n\t\t} else {\n\t\t\toutput = append(output, item)\n\t\t}\n\t\toutputSize += decompressedSize\n\t}\n\treturn output, nil\n}\n\n// retrieveItems reads up to 'count' items from the table. It reads at least\n// one item, but otherwise avoids reading more than maxBytes bytes. Freezer\n// will ignore the size limitation and continuously allocate memory to store\n// data if maxBytes is 0. It returns the (potentially compressed) data, and\n// the sizes.\nfunc (t *freezerTable) retrieveItems(start, count, maxBytes uint64) ([]byte, []int, error) {\n\tt.lock.RLock()\n\tdefer t.lock.RUnlock()\n\n\t// Ensure the table and the item are accessible\n\tif t.index == nil || t.head == nil || t.metadata.file == nil {\n\t\treturn nil, nil, errClosed\n\t}\n\tvar (\n\t\titems  = t.items.Load()      // the total items(head + 1)\n\t\thidden = t.itemHidden.Load() // the number of hidden items\n\t)\n\t// Ensure the start is written, not deleted from the tail, and that the\n\t// caller actually wants something\n\tif items <= start || hidden > start || count == 0 {\n\t\treturn nil, nil, errOutOfBounds\n\t}\n\tif start+count > items {\n\t\tcount = items - start\n\t}\n\tvar output []byte // Buffer to read data into\n\tif maxBytes != 0 {\n\t\toutput = make([]byte, 0, maxBytes)\n\t} else {\n\t\toutput = make([]byte, 0, 1024) // initial buffer cap\n\t}\n\t// readData is a helper method to read a single data item from disk.\n\treadData := func(fileId, start uint32, length int) error {\n\t\toutput = grow(output, length)\n\t\tdataFile, exist := t.files[fileId]\n\t\tif !exist {\n\t\t\treturn fmt.Errorf(\"missing data file %d\", fileId)\n\t\t}\n\t\tif _, err := dataFile.ReadAt(output[len(output)-length:], int64(start))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 1102,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\t\ttotalSize += size\n\t\tsizes = append(sizes, size)\n\t\tif i == len(indices)-2 || (uint64(totalSize) > maxBytes && maxBytes != 0) {\n\t\t\t// Last item, need to do the read now\n\t\t\tif err := readData(secondIndex.filenum, readStart, unreadSize)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 1337,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= t.itemHidden.Load()\n\n\t// overwrite metadata file\n\tt.metadata.setVirtualTail(legacyOffset, true)\n\tif t.metadata.flushOffset < int64(legacyOffset) {\n\t\tt.metadata.setFlushOffset(int64(legacyOffset), true)\n\t}\n\n\t// overwrite first index\n\tvar firstIndex indexEntry\n\tbuffer := make([]byte, indexEntrySize)\n\tt.index.ReadAt(buffer, 0)\n\tfirstIndex.unmarshalBinary(buffer)\n\tfirstIndex.offset = uint32(legacyOffset)\n\tif _, err := t.index.WriteAt(firstIndex.append(nil), 0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 326,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= indexEntrySize\n\n\t\t\t// If the index file is truncated beyond the flush offset, move the flush\n\t\t\t// offset back to the new end of the file. A crash may occur before the\n\t\t\t// offset is updated, leaving a dangling reference that points to a position\n\t\t\t// outside the file. If so, the offset will be reset to the new end of the\n\t\t\t// file during the next run.\n\t\t\tif t.metadata.flushOffset > newOffset {\n\t\t\t\tif err := t.metadata.setFlushOffset(newOffset, true)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 783,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1 {\n\t\tif _, err := t.index.ReadAt(buffer, int64((current-deleted+1)*indexEntrySize))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/ancient_utils.go",
          "line": 51,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= table.size\n\t}\n\treturn total\n}\n\nfunc inspect(name string, order map[string]freezerTableConfig, reader ethdb.AncientReader) (freezerInfo, error) {\n\tinfo := freezerInfo{name: name}\n\tfor t := range order {\n\t\tsize, err := reader.AncientSize(t)\n\t\tif err != nil {\n\t\t\treturn freezerInfo{}, err\n\t\t}\n\t\tinfo.sizes = append(info.sizes, tableSize{name: t, size: common.StorageSize(size)})\n\t}\n\t// Retrieve the number of last stored item\n\tancients, err := reader.Ancients()\n\tif err != nil {\n\t\treturn freezerInfo{}, err\n\t}\n\tinfo.head = ancients - 1\n\n\t// Retrieve the number of first stored item\n\ttail, err := reader.Tail()\n\tif err != nil {\n\t\treturn freezerInfo{}, err\n\t}\n\tinfo.tail = tail\n\treturn info, nil\n}\n\n// inspectFreezers inspects all freezers registered in the system.\nfunc inspectFreezers(db ethdb.Database) ([]freezerInfo, error) {\n\tvar infos []freezerInfo\n\tfor _, freezer := range freezers {\n\t\tswitch freezer {\n\t\tcase ChainFreezerName:\n\t\t\tinfo, err := inspect(ChainFreezerName, chainFreezerTableConfigs, db)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tinfos = append(infos, info)\n\n\t\tcase MerkleStateFreezerName, VerkleStateFreezerName:\n\t\t\tif db.HasSeparateStateStore() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdatadir, err := db.AncientDatadir()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tfile, err := os.Open(filepath.Join(datadir, MerkleStateFreezerName))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tdefer file.Close()\n\t\t\t// if state freezer folder has been pruned, there is no need for inspection\n\t\t\t_, err = file.Readdirnames(1)\n\t\t\tif err == io.EOF {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tf, err := NewStateFreezer(datadir, freezer == VerkleStateFreezerName, true)\n\t\t\tif err != nil {\n\t\t\t\tcontinue // might be possible the state freezer is not existent\n\t\t\t}\n\t\t\tdefer f.Close()\n\n\t\t\tinfo, err := inspect(freezer, stateFreezerTableConfigs, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tinfos = append(infos, info)\n\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unknown freezer, supported ones: %v\", freezers)\n\t\t}\n\t}\n\treturn infos, nil\n}\n\n// InspectFreezerTable dumps out the index of a specific freezer table. The passed\n// ancient indicates the path of root ancient directory where the chain freezer can\n// be opened. Start and end specify the range for dumping out indexes.\n// Note this function can only be used for debugging purposes.\nfunc InspectFreezerTable(ancient string, freezerName string, tableName string, start, end int64, multiDatabase bool) error {\n\tvar (\n\t\tpath   string\n\t\ttables map[string]freezerTableConfig\n\t)\n\tswitch freezerName {\n\tcase ChainFreezerName:\n\t\tpath, tables = resolveChainFreezerDir(ancient), chainFreezerTableConfigs\n\n\tcase MerkleStateFreezerName, VerkleStateFreezerName:\n\t\tif multiDatabase {\n\t\t\tpath, tables = filepath.Join(filepath.Dir(ancient)+\"/state/ancient\", freezerName), stateFreezerTableConfigs\n\t\t} else {\n\t\t\tpath, tables = filepath.Join(ancient, freezerName), stateFreezerTableConfigs\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown freezer, supported ones: %v\", freezers)\n\t}\n\tnoSnappy, exist := tables[tableName]\n\tif !exist {\n\t\tvar names []string\n\t\tfor name := range tables {\n\t\t\tnames = append(names, name)\n\t\t}\n\t\treturn fmt.Errorf(\"unknown table, supported ones: %v\", names)\n\t}\n\ttable, err := newFreezerTable(path, tableName, noSnappy, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttable.dumpIndexStdout(start, end)\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/accessors_metadata.go",
          "line": 132,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(numDel)\n\t}\n\t// And save it again\n\tdata, _ := rlp.EncodeToBytes(uncleanShutdowns)\n\tif err := db.Put(uncleanShutdownKey, data)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/schema.go",
          "line": 230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(buf[n:], accountHash.Bytes())\n\tcopy(buf[n:], storageHash.Bytes())\n\treturn buf\n}\n\n// storageSnapshotsKey = SnapshotStoragePrefix + account hash + storage hash\nfunc storageSnapshotsKey(accountHash common.Hash) []byte {\n\treturn append(SnapshotStoragePrefix, accountHash.Bytes()...)\n}\n\n// preimageKey = PreimagePrefix + hash\nfunc preimageKey(hash common.Hash) []byte {\n\treturn append(PreimagePrefix, hash.Bytes()...)\n}\n\n// codeKey = CodePrefix + hash\nfunc codeKey(hash common.Hash) []byte {\n\treturn append(CodePrefix, hash.Bytes()...)\n}\n\n// IsCodeKey reports whether the given byte slice is the key of contract code,\n// if so return the raw code hash as well.\nfunc IsCodeKey(key []byte) (bool, []byte) {\n\tif bytes.HasPrefix(key, CodePrefix) && len(key) == common.HashLength+len(CodePrefix) {\n\t\treturn true, key[len(CodePrefix):]\n\t}\n\treturn false, nil\n}\n\n// configKey = configPrefix + hash\nfunc configKey(hash common.Hash) []byte {\n\treturn append(configPrefix, hash.Bytes()...)\n}\n\n// genesisStateSpecKey = genesisPrefix + hash\nfunc genesisStateSpecKey(hash common.Hash) []byte {\n\treturn append(genesisPrefix, hash.Bytes()...)\n}\n\n// stateIDKey = stateIDPrefix + root (32 bytes)\nfunc stateIDKey(root common.Hash) []byte {\n\treturn append(stateIDPrefix, root.Bytes()...)\n}\n\n// accountTrieNodeKey = TrieNodeAccountPrefix + nodePath.\nfunc accountTrieNodeKey(path []byte) []byte {\n\treturn append(TrieNodeAccountPrefix, path...)\n}\n\n// storageTrieNodeKey = TrieNodeStoragePrefix + accountHash + nodePath.\nfunc storageTrieNodeKey(accountHash common.Hash, path []byte) []byte {\n\tbuf := make([]byte, len(TrieNodeStoragePrefix)+common.HashLength+len(path))\n\tn := copy(buf, TrieNodeStoragePrefix)\n\tn += copy(buf[n:], accountHash.Bytes())\n\tcopy(buf[n:], path)\n\treturn buf\n}\n\n// IsLegacyTrieNode reports whether a provided database entry is a legacy trie\n// node. The characteristics of legacy trie node are:\n// - the key length is 32 bytes\n// - the key is the hash of val\nfunc IsLegacyTrieNode(key []byte, val []byte) bool {\n\tif len(key) != common.HashLength {\n\t\treturn false\n\t}\n\treturn bytes.Equal(key, crypto.Keccak256(val))\n}\n\n// ResolveAccountTrieNodeKey reports whether a provided database entry is an\n// account trie node in path-based state scheme, and returns the resolved\n// node path if so.\nfunc ResolveAccountTrieNodeKey(key []byte) (bool, []byte) {\n\tif !bytes.HasPrefix(key, TrieNodeAccountPrefix) {\n\t\treturn false, nil\n\t}\n\t// The remaining key should only consist a hex node path\n\t// whose length is in the range 0 to 64 (64 is excluded\n\t// since leaves are always wrapped with shortNode).\n\tif len(key) >= len(TrieNodeAccountPrefix)+common.HashLength*2 {\n\t\treturn false, nil\n\t}\n\treturn true, key[len(TrieNodeAccountPrefix):]\n}\n\n// IsAccountTrieNode reports whether a provided database entry is an account\n// trie node in path-based state scheme.\nfunc IsAccountTrieNode(key []byte) bool {\n\tok, _ := ResolveAccountTrieNodeKey(key)\n\treturn ok\n}\n\n// ResolveStorageTrieNode reports whether a provided database entry is a storage\n// trie node in path-based state scheme, and returns the resolved account hash\n// and node path if so.\nfunc ResolveStorageTrieNode(key []byte) (bool, common.Hash, []byte) {\n\tif !bytes.HasPrefix(key, TrieNodeStoragePrefix) {\n\t\treturn false, common.Hash{}, nil\n\t}\n\t// The remaining key consists of 2 parts:\n\t// - 32 bytes account hash\n\t// - hex node path whose length is in the range 0 to 64\n\tif len(key) < len(TrieNodeStoragePrefix)+common.HashLength {\n\t\treturn false, common.Hash{}, nil\n\t}\n\tif len(key) >= len(TrieNodeStoragePrefix)+common.HashLength+common.HashLength*2 {\n\t\treturn false, common.Hash{}, nil\n\t}\n\taccountHash := common.BytesToHash(key[len(TrieNodeStoragePrefix) : len(TrieNodeStoragePrefix)+common.HashLength])\n\treturn true, accountHash, key[len(TrieNodeStoragePrefix)+common.HashLength:]\n}\n\n// IsStorageTrieNode reports whether a provided database entry is a storage\n// trie node in path-based state scheme.\nfunc IsStorageTrieNode(key []byte) bool {\n\tok, _, _ := ResolveStorageTrieNode(key)\n\treturn ok\n}\n\n// filterMapRowKey = filterMapRowPrefix + mapRowIndex (uint64 big endian)\nfunc filterMapRowKey(mapRowIndex uint64, base bool) []byte {\n\textLen := 8\n\tif base {\n\t\textLen = 9\n\t}\n\tl := len(filterMapRowPrefix)\n\tkey := make([]byte, l+extLen)\n\tcopy(key[:l], filterMapRowPrefix)\n\tbinary.BigEndian.PutUint64(key[l:l+8], mapRowIndex)\n\treturn key\n}\n\n// filterMapLastBlockKey = filterMapLastBlockPrefix + mapIndex (uint32 big endian)\nfunc filterMapLastBlockKey(mapIndex uint32) []byte {\n\tl := len(filterMapLastBlockPrefix)\n\tkey := make([]byte, l+4)\n\tcopy(key[:l], filterMapLastBlockPrefix)\n\tbinary.BigEndian.PutUint32(key[l:], mapIndex)\n\treturn key\n}\n\n// filterMapBlockLVKey = filterMapBlockLVPrefix + num (uint64 big endian)\nfunc filterMapBlockLVKey(number uint64) []byte {\n\tl := len(filterMapBlockLVPrefix)\n\tkey := make([]byte, l+8)\n\tcopy(key[:l], filterMapBlockLVPrefix)\n\tbinary.BigEndian.PutUint64(key[l:], number)\n\treturn key\n}\n\n// accountHistoryIndexKey = StateHistoryAccountMetadataPrefix + addressHash\nfunc accountHistoryIndexKey(addressHash common.Hash) []byte {\n\treturn append(StateHistoryAccountMetadataPrefix, addressHash.Bytes()...)\n}\n\n// storageHistoryIndexKey = StateHistoryStorageMetadataPrefix + addressHash + storageHash\nfunc storageHistoryIndexKey(addressHash common.Hash, storageHash common.Hash) []byte {\n\treturn append(append(StateHistoryStorageMetadataPrefix, addressHash.Bytes()...), storageHash.Bytes()...)\n}\n\n// accountHistoryIndexBlockKey = StateHistoryAccountBlockPrefix + addressHash + blockID\nfunc accountHistoryIndexBlockKey(addressHash common.Hash, blockID uint32) []byte {\n\tvar buf [4]byte\n\tbinary.BigEndian.PutUint32(buf[:], blockID)\n\treturn append(append(StateHistoryAccountBlockPrefix, addressHash.Bytes()...), buf[:]...)\n}\n\n// storageHistoryIndexBlockKey = StateHistoryStorageBlockPrefix + addressHash + storageHash + blockID\nfunc storageHistoryIndexBlockKey(addressHash common.Hash, storageHash common.Hash, blockID uint32) []byte {\n\tvar buf [4]byte\n\tbinary.BigEndian.PutUint32(buf[:], blockID)\n\treturn append(append(append(StateHistoryStorageBlockPrefix, addressHash.Bytes()...), storageHash.Bytes()...), buf[:]...)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/freezer_table_test.go",
          "line": 1265,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(index)\n\t\t\t\tstep.target = deleted\n\t\t\t}\n\t\tcase opTruncateTailAll:\n\t\t\tstep.target = deleted + uint64(len(items))\n\t\t\titems = items[:0]\n\t\t\tdeleted = step.target\n\t\t}\n\t\tsteps = append(steps, step)\n\t}\n\treturn reflect.ValueOf(steps)\n}\n\nfunc runRandTest(rt randTest) bool {\n\tfname := fmt.Sprintf(\"randtest-%d\", rand.Uint64())\n\tf, err := newTable(os.TempDir(), fname, metrics.NewMeter(), metrics.NewMeter(), metrics.NewGauge(), 50, freezerTableConfig{noSnappy: true}, false)\n\tif err != nil {\n\t\tpanic(\"failed to initialize table\")\n\t}\n\tvar values [][]byte\n\tfor i, step := range rt {\n\t\tswitch step.op {\n\t\tcase opReload:\n\t\t\tf.Close()\n\t\t\tf, err = newTable(os.TempDir(), fname, metrics.NewMeter(), metrics.NewMeter(), metrics.NewGauge(), 50, freezerTableConfig{noSnappy: true}, false)\n\t\t\tif err != nil {\n\t\t\t\trt[i].err = fmt.Errorf(\"failed to reload table %v\", err)\n\t\t\t}\n\t\tcase opCheckAll:\n\t\t\ttail := f.itemHidden.Load()\n\t\t\thead := f.items.Load()\n\n\t\t\tif tail == head {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tgot, err := f.RetrieveItems(f.itemHidden.Load(), head-tail, 100000)\n\t\t\tif err != nil {\n\t\t\t\trt[i].err = err\n\t\t\t} else {\n\t\t\t\tif !reflect.DeepEqual(got, values) {\n\t\t\t\t\trt[i].err = fmt.Errorf(\"mismatch on retrieved values %v %v\", got, values)\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase opAppend:\n\t\t\tbatch := f.newBatch()\n\t\t\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/accessors_indexes.go",
          "line": 304,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= logs\n\t\t}\n\t}\n\treturn nil, RawReceiptContext{}, fmt.Errorf(\"receipt not found, %d, %x, %d\", blockNumber, blockHash, txIndex)\n}\n\n// ReadFilterMapExtRow retrieves a filter map row at the given mapRowIndex\n// (see filtermaps.mapRowIndex for the storage index encoding).\n// Note that zero length rows are not stored in the database and therefore all\n// non-existent entries are interpreted as empty rows and return no error.\n// Also note that the mapRowIndex indexing scheme is the same as the one\n// proposed in EIP-7745 for tree-hashing the filter map structure and for the\n// same data proximity reasons it is also suitable for database representation.\n// See also:\n// https://eips.ethereum.org/EIPS/eip-7745#hash-tree-structure\nfunc ReadFilterMapExtRow(db ethdb.KeyValueReader, mapRowIndex uint64, bitLength uint) ([]uint32, error) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tkey := filterMapRowKey(mapRowIndex, false)\n\thas, err := db.Has(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !has {\n\t\treturn nil, nil\n\t}\n\tencRow, err := db.Get(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(encRow)%byteLength != 0 {\n\t\treturn nil, errors.New(\"invalid encoded extended filter row length\")\n\t}\n\trow := make([]uint32, len(encRow)/byteLength)\n\tvar b [4]byte\n\tfor i := range row {\n\t\tcopy(b[:byteLength], encRow[i*byteLength:(i+1)*byteLength])\n\t\trow[i] = binary.LittleEndian.Uint32(b[:])\n\t}\n\treturn row, nil\n}\n\nfunc ReadFilterMapBaseRows(db ethdb.KeyValueReader, mapRowIndex uint64, rowCount uint32, bitLength uint) ([][]uint32, error) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tkey := filterMapRowKey(mapRowIndex, true)\n\thas, err := db.Has(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trows := make([][]uint32, rowCount)\n\tif !has {\n\t\treturn rows, nil\n\t}\n\tencRows, err := db.Get(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tencLen := len(encRows)\n\tvar (\n\t\tentryCount, entriesInRow, rowIndex, headerLen, headerBits int\n\t\theaderByte                                                byte\n\t)\n\tfor headerLen+byteLength*entryCount < encLen {\n\t\tif headerBits == 0 {\n\t\t\theaderByte = encRows[headerLen]\n\t\t\theaderLen++\n\t\t\theaderBits = 8\n\t\t}\n\t\tif headerByte&1 > 0 {\n\t\t\tentriesInRow++\n\t\t\tentryCount++\n\t\t} else {\n\t\t\tif entriesInRow > 0 {\n\t\t\t\trows[rowIndex] = make([]uint32, entriesInRow)\n\t\t\t\tentriesInRow = 0\n\t\t\t}\n\t\t\trowIndex++\n\t\t}\n\t\theaderByte >>= 1\n\t\theaderBits--\n\t}\n\tif headerLen+byteLength*entryCount > encLen {\n\t\treturn nil, errors.New(\"invalid encoded base filter rows length\")\n\t}\n\tif entriesInRow > 0 {\n\t\trows[rowIndex] = make([]uint32, entriesInRow)\n\t}\n\tnextEntry := headerLen\n\tfor _, row := range rows {\n\t\tfor i := range row {\n\t\t\tvar b [4]byte\n\t\t\tcopy(b[:byteLength], encRows[nextEntry:nextEntry+byteLength])\n\t\t\trow[i] = binary.LittleEndian.Uint32(b[:])\n\t\t\tnextEntry += byteLength\n\t\t}\n\t}\n\treturn rows, nil\n}\n\n// WriteFilterMapExtRow stores an extended filter map row at the given mapRowIndex\n// or deletes any existing entry if the row is empty.\nfunc WriteFilterMapExtRow(db ethdb.KeyValueWriter, mapRowIndex uint64, row []uint32, bitLength uint) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tvar err error\n\tif len(row) > 0 {\n\t\tencRow := make([]byte, len(row)*byteLength)\n\t\tfor i, c := range row {\n\t\t\tvar b [4]byte\n\t\t\tbinary.LittleEndian.PutUint32(b[:], c)\n\t\t\tcopy(encRow[i*byteLength:(i+1)*byteLength], b[:byteLength])\n\t\t}\n\t\terr = db.Put(filterMapRowKey(mapRowIndex, false), encRow)\n\t} else {\n\t\terr = db.Delete(filterMapRowKey(mapRowIndex, false))\n\t}\n\tif err != nil {\n\t\tlog.Crit(\"Failed to store extended filter map row\", \"err\", err)\n\t}\n}\n\nfunc WriteFilterMapBaseRows(db ethdb.KeyValueWriter, mapRowIndex uint64, rows [][]uint32, bitLength uint) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tvar entryCount, zeroBits int\n\tfor i, row := range rows {\n\t\tif len(row) > 0 {\n\t\t\tentryCount += len(row)\n\t\t\tzeroBits = i\n\t\t}\n\t}\n\tvar err error\n\tif entryCount > 0 {\n\t\theaderLen := (zeroBits + entryCount + 7) / 8\n\t\tencRows := make([]byte, headerLen+entryCount*byteLength)\n\t\tnextEntry := headerLen\n\n\t\theaderPtr, headerByte := 0, byte(1)\n\t\taddHeaderBit := func(bit bool) {\n\t\t\tif bit {\n\t\t\t\tencRows[headerPtr] += headerByte\n\t\t\t}\n\t\t\tif headerByte += headerByte",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/accessors_indexes.go",
          "line": 466,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= byteLength\n\t\t\t\taddHeaderBit(true)\n\t\t\t}\n\t\t\tif zeroBits == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\taddHeaderBit(false)\n\t\t\tzeroBits--\n\t\t}\n\t\terr = db.Put(filterMapRowKey(mapRowIndex, true), encRows)\n\t} else {\n\t\terr = db.Delete(filterMapRowKey(mapRowIndex, true))\n\t}\n\tif err != nil {\n\t\tlog.Crit(\"Failed to store base filter map rows\", \"err\", err)\n\t}\n}\n\nfunc DeleteFilterMapRows(db ethdb.KeyValueStore, mapRows common.Range[uint64], hashScheme bool, stopCallback func(bool) bool) error {\n\treturn SafeDeleteRange(db, filterMapRowKey(mapRows.First(), false), filterMapRowKey(mapRows.AfterLast(), false), hashScheme, stopCallback)\n}\n\n// ReadFilterMapLastBlock retrieves the number of the block that generated the\n// last log value entry of the given map.\nfunc ReadFilterMapLastBlock(db ethdb.KeyValueReader, mapIndex uint32) (uint64, common.Hash, error) {\n\tenc, err := db.Get(filterMapLastBlockKey(mapIndex))\n\tif err != nil {\n\t\treturn 0, common.Hash{}, err\n\t}\n\tif len(enc) != 40 {\n\t\treturn 0, common.Hash{}, errors.New(\"invalid block number and id encoding\")\n\t}\n\tvar id common.Hash\n\tcopy(id[:], enc[8:])\n\treturn binary.BigEndian.Uint64(enc[:8]), id, nil\n}\n\n// WriteFilterMapLastBlock stores the number of the block that generated the\n// last log value entry of the given map.\nfunc WriteFilterMapLastBlock(db ethdb.KeyValueWriter, mapIndex uint32, blockNumber uint64, id common.Hash) {\n\tvar enc [40]byte\n\tbinary.BigEndian.PutUint64(enc[:8], blockNumber)\n\tcopy(enc[8:], id[:])\n\tif err := db.Put(filterMapLastBlockKey(mapIndex), enc[:])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 739,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= batchSize {\n\t\tlength := batchSize\n\t\tif i+batchSize > b.N {\n\t\t\tlength = b.N - i\n\t\t}\n\n\t\tblocks := allBlocks[i : i+length]\n\t\treceipts := batchReceipts[:length]\n\t\tfor j := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 754,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeSize\n\t}\n\n\t// Enable MB/s reporting.\n\tb.SetBytes(totalSize / int64(b.N))\n}\n\n// makeTestBlocks creates fake blocks for the ancient write benchmark.\nfunc makeTestBlocks(nblock int, txsPerBlock int) []*types.Block {\n\tkey, _ := crypto.HexToECDSA(\"b71c71a67e1177ad4e901695e1b4b9ee17ae16c6668d313eac2f96dbcda3f291\")\n\tsigner := types.LatestSignerForChainID(big.NewInt(8))\n\n\t// Create transactions.\n\ttxs := make([]*types.Transaction, txsPerBlock)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/freezer_batch.go",
          "line": 85,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= tb.totalBytes\n\t}\n\treturn item, writeSize, nil\n}\n\n// freezerTableBatch is a batch for a freezer table.\ntype freezerTableBatch struct {\n\tt *freezerTable\n\n\tsb          *snappyBuffer\n\tencBuffer   writeBuffer\n\tdataBuffer  []byte\n\tindexBuffer []byte\n\tcurItem     uint64 // expected index of next append\n\ttotalBytes  int64  // counts written bytes since reset\n}\n\n// newBatch creates a new batch for the freezer table.\nfunc (t *freezerTable) newBatch() *freezerTableBatch {\n\tbatch := &freezerTableBatch{t: t}\n\tif !t.config.noSnappy {\n\t\tbatch.sb = new(snappyBuffer)\n\t}\n\tbatch.reset()\n\treturn batch\n}\n\n// reset clears the batch for reuse.\nfunc (batch *freezerTableBatch) reset() {\n\tbatch.dataBuffer = batch.dataBuffer[:0]\n\tbatch.indexBuffer = batch.indexBuffer[:0]\n\tcurItem := batch.t.items.Load()\n\tbatch.curItem = atomic.LoadUint64(&curItem)\n\tbatch.totalBytes = 0\n}\n\n// Append rlp-encodes and adds data at the end of the freezer table. The item number is a\n// precautionary parameter to ensure data correctness, but the table will reject already\n// existing data.\nfunc (batch *freezerTableBatch) Append(item uint64, data interface{}) error {\n\tif item != batch.curItem {\n\t\treturn fmt.Errorf(\"%w: have %d want %d\", errOutOrderInsertion, item, batch.curItem)\n\t}\n\n\t// Encode the item.\n\tbatch.encBuffer.Reset()\n\tif err := rlp.Encode(&batch.encBuffer, data)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/freezer_batch.go",
          "line": 173,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= itemSize\n\n\t// Put index entry to buffer.\n\tentry := indexEntry{filenum: batch.t.headId, offset: uint32(itemOffset + itemSize)}\n\tbatch.indexBuffer = entry.append(batch.indexBuffer)\n\tbatch.curItem++\n\n\treturn batch.maybeCommit()\n}\n\n// maybeCommit writes the buffered data if the buffer is full enough.\nfunc (batch *freezerTableBatch) maybeCommit() error {\n\tif len(batch.dataBuffer) > freezerBatchBufferLimit {\n\t\treturn batch.commit()\n\t}\n\treturn nil\n}\n\n// commit writes the batched items to the backing freezerTable. Note index\n// file isn't fsync'd after the file write, the recent write can be lost\n// after the power failure.\nfunc (batch *freezerTableBatch) commit() error {\n\t_, err := batch.t.head.Write(batch.dataBuffer)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdataSize := int64(len(batch.dataBuffer))\n\tbatch.dataBuffer = batch.dataBuffer[:0]\n\n\t_, err = batch.t.index.Write(batch.indexBuffer)\n\tif err != nil {\n\t\treturn err\n\t}\n\tindexSize := int64(len(batch.indexBuffer))\n\tbatch.indexBuffer = batch.indexBuffer[:0]\n\n\t// Update headBytes of table.\n\tbatch.t.headBytes += dataSize\n\titems := batch.curItem\n\tbatch.t.items.Store(items)\n\n\t// Update metrics.\n\tbatch.t.sizeGauge.Inc(dataSize + indexSize)\n\tbatch.t.writeMeter.Mark(dataSize + indexSize)\n\n\t// Periodically sync the table, todo (rjl493456442) make it configurable?\n\tif time.Since(batch.t.lastSync) > 30*time.Second {\n\t\tbatch.t.lastSync = time.Now()\n\t\treturn batch.t.Sync()\n\t}\n\treturn nil\n}\n\n// snappyBuffer writes snappy in block format, and can be reused. It is\n// reset when WriteTo is called.\ntype snappyBuffer struct {\n\tdst []byte\n}\n\n// compress snappy-compresses the data.\nfunc (s *snappyBuffer) compress(data []byte) []byte {\n\t// The snappy library does not care what the capacity of the buffer is,\n\t// but only checks the length. If the length is too small, it will\n\t// allocate a brand new buffer.\n\t// To avoid that, we check the required size here, and grow the size of the\n\t// buffer to utilize the full capacity.\n\tif n := snappy.MaxEncodedLen(len(data))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/freezer_memory.go",
          "line": 78,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(t.data[index]))\n\t}\n\treturn batch, nil\n}\n\n// truncateHead discards any recent data above the provided threshold number.\nfunc (t *memoryTable) truncateHead(items uint64) error {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\t// Short circuit if nothing to delete.\n\tif t.items <= items {\n\t\treturn nil\n\t}\n\tif items < t.offset {\n\t\treturn errors.New(\"truncation below tail\")\n\t}\n\tt.data = t.data[:items-t.offset]\n\tt.items = items\n\treturn nil\n}\n\n// truncateTail discards any recent data before the provided threshold number.\nfunc (t *memoryTable) truncateTail(items uint64) error {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\t// Short circuit if nothing to delete.\n\tif t.offset >= items {\n\t\treturn nil\n\t}\n\tif t.items < items {\n\t\treturn errors.New(\"truncation above head\")\n\t}\n\tt.data = t.data[items-t.offset:]\n\tt.offset = items\n\treturn nil\n}\n\n// commit merges the given item batch into table. It's presumed that the\n// batch is ordered and continuous with table.\nfunc (t *memoryTable) commit(batch [][]byte) error {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tfor _, item := range batch {\n\t\tt.size += uint64(len(item))\n\t}\n\tt.data = append(t.data, batch...)\n\tt.items += uint64(len(batch))\n\treturn nil\n}\n\n// memoryBatch is the singleton batch used for ancient write.\ntype memoryBatch struct {\n\tdata map[string][][]byte\n\tnext map[string]uint64\n\tsize map[string]int64\n}\n\nfunc newMemoryBatch() *memoryBatch {\n\treturn &memoryBatch{\n\t\tdata: make(map[string][][]byte),\n\t\tnext: make(map[string]uint64),\n\t\tsize: make(map[string]int64),\n\t}\n}\n\nfunc (b *memoryBatch) reset(freezer *MemoryFreezer) {\n\tb.data = make(map[string][][]byte)\n\tb.next = make(map[string]uint64)\n\tb.size = make(map[string]int64)\n\n\tfor name, table := range freezer.tables {\n\t\tb.next[name] = table.items\n\t}\n}\n\n// Append adds an RLP-encoded item.\nfunc (b *memoryBatch) Append(kind string, number uint64, item interface{}) error {\n\tif b.next[kind] != number {\n\t\treturn errOutOrderInsertion\n\t}\n\tblob, err := rlp.EncodeToBytes(item)\n\tif err != nil {\n\t\treturn err\n\t}\n\tb.data[kind] = append(b.data[kind], blob)\n\tb.next[kind]++\n\tb.size[kind] += int64(len(blob))\n\treturn nil\n}\n\n// AppendRaw adds an item without RLP-encoding it.\nfunc (b *memoryBatch) AppendRaw(kind string, number uint64, blob []byte) error {\n\tif b.next[kind] != number {\n\t\treturn errOutOrderInsertion\n\t}\n\tb.data[kind] = append(b.data[kind], common.CopyBytes(blob))\n\tb.next[kind]++\n\tb.size[kind] += int64(len(blob))\n\treturn nil\n}\n\n// commit is called at the end of a write operation and writes all remaining\n// data to tables.\nfunc (b *memoryBatch) commit(freezer *MemoryFreezer) (items uint64, writeSize int64, err error) {\n\t// Check that count agrees on all batches.\n\titems = math.MaxUint64\n\tfor name, next := range b.next {\n\t\t// skip empty addition tables\n\t\tif slices.Contains(additionTables, name) && next == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif items < math.MaxUint64 && next != items {\n\t\t\treturn 0, 0, fmt.Errorf(\"table %s is at item %d, want %d\", name, next, items)\n\t\t}\n\t\titems = next\n\t}\n\t// Commit all table batches.\n\tfor name, batch := range b.data {\n\t\ttable := freezer.tables[name]\n\t\tif err := table.commit(batch)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/rawdb/freezer_memory.go",
          "line": 203,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= b.size[name]\n\t}\n\treturn items, writeSize, nil\n}\n\n// MemoryFreezer is an ephemeral ancient store. It implements the ethdb.AncientStore\n// interface and can be used along with ephemeral key-value store.\ntype MemoryFreezer struct {\n\titems      uint64                  // Number of items stored\n\ttail       uint64                  // Number of the first stored item in the freezer\n\treadonly   bool                    // Flag if the freezer is only for reading\n\tlock       sync.RWMutex            // Lock to protect fields\n\ttables     map[string]*memoryTable // Tables for storing everything\n\twriteBatch *memoryBatch            // Pre-allocated write batch\n}\n\n// NewMemoryFreezer initializes an in-memory freezer instance.\nfunc NewMemoryFreezer(readonly bool, tableName map[string]freezerTableConfig) *MemoryFreezer {\n\ttables := make(map[string]*memoryTable)\n\tfor name, cfg := range tableName {\n\t\ttables[name] = newMemoryTable(name, cfg)\n\t}\n\treturn &MemoryFreezer{\n\t\twriteBatch: newMemoryBatch(),\n\t\treadonly:   readonly,\n\t\ttables:     tables,\n\t}\n}\n\n// Ancient retrieves an ancient binary blob from the in-memory freezer.\nfunc (f *MemoryFreezer) Ancient(kind string, number uint64) ([]byte, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\tt := f.tables[kind]\n\tif t == nil {\n\t\treturn nil, errUnknownTable\n\t}\n\tdata, err := t.retrieve(number, 1, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn data[0], nil\n}\n\n// AncientRange retrieves multiple items in sequence, starting from the index 'start'.\n// It will return\n//   - at most 'count' items,\n//   - if maxBytes is specified: at least 1 item (even if exceeding the maxByteSize),\n//     but will otherwise return as many items as fit into maxByteSize.\n//   - if maxBytes is not specified, 'count' items will be returned if they are present\nfunc (f *MemoryFreezer) AncientRange(kind string, start, count, maxBytes uint64) ([][]byte, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\tt := f.tables[kind]\n\tif t == nil {\n\t\treturn nil, errUnknownTable\n\t}\n\treturn t.retrieve(start, count, maxBytes)\n}\n\n// Ancients returns the ancient item numbers in the freezer.\nfunc (f *MemoryFreezer) Ancients() (uint64, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\treturn f.items, nil\n}\n\n// Tail returns the number of first stored item in the freezer.\n// This number can also be interpreted as the total deleted item numbers.\nfunc (f *MemoryFreezer) Tail() (uint64, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\treturn f.tail, nil\n}\n\n// AncientSize returns the ancient size of the specified category.\nfunc (f *MemoryFreezer) AncientSize(kind string) (uint64, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\tif table := f.tables[kind]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/txpool.go",
          "line": 436,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= run\n\t\tblocked += block\n\t}\n\treturn runnable, blocked\n}\n\n// Content retrieves the data content of the transaction pool, returning all the\n// pending as well as queued transactions, grouped by account and sorted by nonce.\nfunc (p *TxPool) Content() (map[common.Address][]*types.Transaction, map[common.Address][]*types.Transaction) {\n\tvar (\n\t\trunnable = make(map[common.Address][]*types.Transaction)\n\t\tblocked  = make(map[common.Address][]*types.Transaction)\n\t)\n\tfor _, subpool := range p.subpools {\n\t\trun, block := subpool.Content()\n\n\t\tmaps.Copy(runnable, run)\n\t\tmaps.Copy(blocked, block)\n\t}\n\treturn runnable, blocked\n}\n\n// ContentFrom retrieves the data content of the transaction pool, returning the\n// pending as well as queued transactions of this address, grouped by nonce.\nfunc (p *TxPool) ContentFrom(addr common.Address) ([]*types.Transaction, []*types.Transaction) {\n\tfor _, subpool := range p.subpools {\n\t\trun, block := subpool.ContentFrom(addr)\n\t\tif len(run) != 0 || len(block) != 0 {\n\t\t\treturn run, block\n\t\t}\n\t}\n\treturn []*types.Transaction{}, []*types.Transaction{}\n}\n\n// Status returns the known status (unknown/pending/queued) of a transaction\n// identified by its hash.\nfunc (p *TxPool) Status(hash common.Hash) TxStatus {\n\tfor _, subpool := range p.subpools {\n\t\tif status := subpool.Status(hash)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/statedb_fuzz_test.go",
          "line": 128,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" \" + strings.Join(names, \", \")\n\treturn action\n}\n\n// Generate returns a new snapshot test of the given size. All randomness is\n// derived from r.\nfunc (*stateTest) Generate(r *rand.Rand, size int) reflect.Value {\n\taddrs := make([]common.Address, 5)\n\tfor i := range addrs {\n\t\taddrs[i][0] = byte(i)\n\t}\n\tactions := make([][]testAction, rand.Intn(5)+1)\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/statedb_test.go",
          "line": 476,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= strings.Join(nameargs, \", \")\n\treturn action\n}\n\n// Generate returns a new snapshot test of the given size. All randomness is\n// derived from r.\nfunc (*snapshotTest) Generate(r *rand.Rand, size int) reflect.Value {\n\t// Generate random actions.\n\taddrs := make([]common.Address, 50)\n\tfor i := range addrs {\n\t\taddrs[i][0] = byte(i)\n\t}\n\tactions := make([]testAction, size)\n\tfor i := range actions {\n\t\taddr := addrs[r.Intn(len(addrs))]\n\t\tactions[i] = newTestAction(addr, r)\n\t}\n\t// Generate snapshot indexes.\n\tnsnapshots := int(math.Sqrt(float64(size)))\n\tif size > 0 && nsnapshots == 0 {\n\t\tnsnapshots = 1\n\t}\n\tsnapshots := make([]int, nsnapshots)\n\tsnaplen := len(actions) / nsnapshots\n\tfor i := range snapshots {\n\t\t// Try to place the snapshots some number of actions apart from each other.\n\t\tsnapshots[i] = (i * snaplen) + r.Intn(snaplen)\n\t}\n\treturn reflect.ValueOf(&snapshotTest{addrs, actions, snapshots, nil})\n}\n\nfunc (test *snapshotTest) String() string {\n\tout := new(bytes.Buffer)\n\tsindex := 0\n\tfor i, action := range test.actions {\n\t\tif len(test.snapshots) > sindex && i == test.snapshots[sindex] {\n\t\t\tfmt.Fprintf(out, \"---- snapshot %d ----\\n\", sindex)\n\t\t\tsindex++\n\t\t}\n\t\tfmt.Fprintf(out, \"%4d: %s\\n\", i, action.name)\n\t}\n\treturn out.String()\n}\n\nfunc (test *snapshotTest) run() bool {\n\t// Run all actions and create snapshots.\n\tvar (\n\t\tstate, _     = New(types.EmptyRootHash, NewDatabaseForTesting())\n\t\tsnapshotRevs = make([]int, len(test.snapshots))\n\t\tsindex       = 0\n\t\tcheckstates  = make([]*StateDB, len(test.snapshots))\n\t)\n\tfor i, action := range test.actions {\n\t\tif len(test.snapshots) > sindex && i == test.snapshots[sindex] {\n\t\t\tsnapshotRevs[sindex] = state.Snapshot()\n\t\t\tcheckstates[sindex] = state.Copy()\n\t\t\tsindex++\n\t\t}\n\t\taction.fn(action, state)\n\t}\n\t// Revert all snapshots in reverse order. Each revert must yield a state\n\t// that is equivalent to fresh state with all actions up the snapshot applied.\n\tfor sindex--",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/trie_prefetcher.go",
          "line": 446,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= feedNum\n\t\t}\n\t}\n\t// Children did not consume all the keys, to create new subfetch to handle left keys.\n\tkeysLeft := keys[keyIndex:]\n\tkeysLeftSize := len(keysLeft)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/state_object.go",
          "line": 218,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t}\n\n\t// Schedule the resolved storage slots for prefetching if it's enabled.\n\tif s.db.prefetcher != nil && s.data.Root != types.EmptyRootHash {\n\t\tif err = s.db.prefetcher.prefetch(s.addrHash, s.origin.Root, s.address, nil, []common.Hash{key}, true)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/access_events.go",
          "line": 102,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= consumed\n\tconsumed, expected = ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, isWrite, availableGas-consumed)\n\tif consumed < expected {\n\t\treturn expected + gas\n\t}\n\tgas += expected\n\treturn gas\n}\n\n// MessageCallGas returns the gas to be charged for each of the currently\n// cold member fields of an account, that need to be touched when making a message\n// call to that account.\nfunc (ae *AccessEvents) MessageCallGas(destination common.Address, availableGas uint64) uint64 {\n\t_, expected := ae.touchAddressAndChargeGas(destination, zeroTreeIndex, utils.BasicDataLeafKey, false, availableGas)\n\tif expected == 0 {\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n\n// ValueTransferGas returns the gas to be charged for each of the currently\n// cold balance member fields of the caller and the callee accounts.\nfunc (ae *AccessEvents) ValueTransferGas(callerAddr, targetAddr common.Address, availableGas uint64) uint64 {\n\t_, expected1 := ae.touchAddressAndChargeGas(callerAddr, zeroTreeIndex, utils.BasicDataLeafKey, true, availableGas)\n\tif expected1 > availableGas {\n\t\treturn expected1\n\t}\n\t_, expected2 := ae.touchAddressAndChargeGas(targetAddr, zeroTreeIndex, utils.BasicDataLeafKey, true, availableGas-expected1)\n\tif expected1+expected2 == 0 {\n\t\treturn params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected1 + expected2\n}\n\n// ContractCreatePreCheckGas charges access costs before\n// a contract creation is initiated. It is just reads, because the\n// address collision is done before the transfer, and so no write\n// are guaranteed to happen at this point.\nfunc (ae *AccessEvents) ContractCreatePreCheckGas(addr common.Address, availableGas uint64) uint64 {\n\tconsumed, expected1 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, false, availableGas)\n\t_, expected2 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, false, availableGas-consumed)\n\treturn expected1 + expected2\n}\n\n// ContractCreateInitGas returns the access gas costs for the initialization of\n// a contract creation.\nfunc (ae *AccessEvents) ContractCreateInitGas(addr common.Address, availableGas uint64) (uint64, uint64) {\n\tvar gas uint64\n\tconsumed, expected1 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, true, availableGas)\n\tgas += consumed\n\tconsumed, expected2 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, true, availableGas-consumed)\n\tgas += consumed\n\treturn gas, expected1 + expected2\n}\n\n// AddTxOrigin adds the member fields of the sender account to the access event list,\n// so that cold accesses are not charged, since they are covered by the 21000 gas.\nfunc (ae *AccessEvents) AddTxOrigin(originAddr common.Address) {\n\tae.touchAddressAndChargeGas(originAddr, zeroTreeIndex, utils.BasicDataLeafKey, true, gomath.MaxUint64)\n\tae.touchAddressAndChargeGas(originAddr, zeroTreeIndex, utils.CodeHashLeafKey, false, gomath.MaxUint64)\n}\n\n// AddTxDestination adds the member fields of the sender account to the access event list,\n// so that cold accesses are not charged, since they are covered by the 21000 gas.\nfunc (ae *AccessEvents) AddTxDestination(addr common.Address, sendsValue, doesntExist bool) {\n\tae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, sendsValue, gomath.MaxUint64)\n\tae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, doesntExist, gomath.MaxUint64)\n}\n\n// SlotGas returns the amount of gas to be charged for a cold storage access.\nfunc (ae *AccessEvents) SlotGas(addr common.Address, slot common.Hash, isWrite bool, availableGas uint64, chargeWarmCosts bool) uint64 {\n\ttreeIndex, subIndex := utils.StorageIndex(slot.Bytes())\n\t_, expected := ae.touchAddressAndChargeGas(addr, *treeIndex, subIndex, isWrite, availableGas)\n\tif expected == 0 && chargeWarmCosts {\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n\n// touchAddressAndChargeGas adds any missing access event to the access event list, and returns the\n// consumed and required gas.\nfunc (ae *AccessEvents) touchAddressAndChargeGas(addr common.Address, treeIndex uint256.Int, subIndex byte, isWrite bool, availableGas uint64) (uint64, uint64) {\n\tbranchKey := newBranchAccessKey(addr, treeIndex)\n\tchunkKey := newChunkAccessKey(branchKey, subIndex)\n\n\t// Read access.\n\tvar branchRead, chunkRead bool\n\tif _, hasStem := ae.branches[branchKey]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/access_events.go",
          "line": 211,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.WitnessBranchReadCost\n\t}\n\tif chunkRead {\n\t\tgas += params.WitnessChunkReadCost\n\t}\n\tif branchWrite {\n\t\tgas += params.WitnessBranchWriteCost\n\t}\n\tif chunkWrite {\n\t\tgas += params.WitnessChunkWriteCost\n\t}\n\tif chunkFill {\n\t\tgas += params.WitnessChunkFillCost\n\t}\n\n\tif availableGas < gas {\n\t\t// consumed != expected\n\t\treturn availableGas, gas\n\t}\n\n\tif branchRead {\n\t\tae.branches[branchKey] = AccessWitnessReadFlag\n\t}\n\tif branchWrite {\n\t\tae.branches[branchKey] |= AccessWitnessWriteFlag\n\t}\n\tif chunkRead {\n\t\tae.chunks[chunkKey] = AccessWitnessReadFlag\n\t}\n\tif chunkWrite {\n\t\tae.chunks[chunkKey] |= AccessWitnessWriteFlag\n\t}\n\n\t// consumed == expected\n\treturn gas, gas\n}\n\ntype branchAccessKey struct {\n\taddr      common.Address\n\ttreeIndex uint256.Int\n}\n\nfunc newBranchAccessKey(addr common.Address, treeIndex uint256.Int) branchAccessKey {\n\tvar sk branchAccessKey\n\tsk.addr = addr\n\tsk.treeIndex = treeIndex\n\treturn sk\n}\n\ntype chunkAccessKey struct {\n\tbranchAccessKey\n\tleafKey byte\n}\n\nfunc newChunkAccessKey(branchKey branchAccessKey, leafKey byte) chunkAccessKey {\n\tvar lk chunkAccessKey\n\tlk.branchAccessKey = branchKey\n\tlk.leafKey = leafKey\n\treturn lk\n}\n\n// CodeChunksRangeGas is a helper function to touch every chunk in a code range and charge witness gas costs\nfunc (ae *AccessEvents) CodeChunksRangeGas(contractAddr common.Address, startPC, size uint64, codeLen uint64, isWrite bool, availableGas uint64) (uint64, uint64) {\n\t// note that in the case where the copied code is outside the range of the\n\t// contract code but touches the last leaf with contract code in it,\n\t// we don't include the last leaf of code in the AccessWitness.  The\n\t// reason that we do not need the last leaf is the account's code size\n\t// is already in the AccessWitness so a stateless verifier can see that\n\t// the code from the last leaf is not needed.\n\tif (codeLen == 0 && size == 0) || startPC > codeLen {\n\t\treturn 0, 0\n\t}\n\n\tendPC := min(startPC+size, codeLen)\n\tif endPC > 0 {\n\t\tendPC -= 1 // endPC is the last bytecode that will be touched.\n\t}\n\n\tvar statelessGasCharged uint64\n\tfor chunkNumber := startPC / 31",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/state/access_events.go",
          "line": 286,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1 // endPC is the last bytecode that will be touched.\n\t}\n\n\tvar statelessGasCharged uint64\n\tfor chunkNumber := startPC / 31",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/state/access_events.go",
          "line": 303,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= consumed\n\t}\n\treturn statelessGasCharged, statelessGasCharged\n}\n\n// BasicDataGas adds the account's basic data to the accessed data, and returns the\n// amount of gas that it costs.\n// Note that an access in write mode implies an access in read mode, whereas an\n// access in read mode does not imply an access in write mode.\nfunc (ae *AccessEvents) BasicDataGas(addr common.Address, isWrite bool, availableGas uint64, chargeWarmCosts bool) uint64 {\n\t_, expected := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, isWrite, availableGas)\n\tif expected == 0 && chargeWarmCosts {\n\t\tif availableGas < params.WarmStorageReadCostEIP2929 {\n\t\t\treturn availableGas\n\t\t}\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n\n// CodeHashGas adds the account's code hash to the accessed data, and returns the\n// amount of gas that it costs.\n// in write mode. If false, the charged gas corresponds to an access in read mode.\n// Note that an access in write mode implies an access in read mode, whereas an access in\n// read mode does not imply an access in write mode.\nfunc (ae *AccessEvents) CodeHashGas(addr common.Address, isWrite bool, availableGas uint64, chargeWarmCosts bool) uint64 {\n\t_, expected := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, isWrite, availableGas)\n\tif expected == 0 && chargeWarmCosts {\n\t\tif availableGas < params.WarmStorageReadCostEIP2929 {\n\t\t\treturn availableGas\n\t\t}\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/statedb.go",
          "line": 333,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gas\n}\n\n// SubRefund removes gas from the refund counter.\n// This method will panic if the refund counter goes below zero\nfunc (s *StateDB) SubRefund(gas uint64) {\n\ts.journal.refundChange(s.refund)\n\tif gas > s.refund {\n\t\tpanic(fmt.Sprintf(\"Refund counter below zero (gas: %d > refund: %d)\", gas, s.refund))\n\t}\n\ts.refund -= gas\n}\n\n// Exist reports whether the given account address exists in the state.\n// Notably this also returns true for self-destructed accounts within the current transaction.\nfunc (s *StateDB) Exist(addr common.Address) bool {\n\treturn s.getStateObject(addr) != nil\n}\n\n// Empty returns whether the state object is either non-existent\n// or empty according to the EIP161 specification (balance = nonce = code = 0)\nfunc (s *StateDB) Empty(addr common.Address) bool {\n\tso := s.getStateObject(addr)\n\treturn so == nil || so.empty()\n}\n\n// GetBalance retrieves the balance from the given address or 0 if object not found\nfunc (s *StateDB) GetBalance(addr common.Address) *uint256.Int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Balance()\n\t}\n\treturn common.U2560\n}\n\n// GetNonce retrieves the nonce from the given address or 0 if object not found\nfunc (s *StateDB) GetNonce(addr common.Address) uint64 {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Nonce()\n\t}\n\n\treturn 0\n}\n\n// GetStorageRoot retrieves the storage root from the given address or empty\n// if object not found.\nfunc (s *StateDB) GetStorageRoot(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Root()\n\t}\n\treturn common.Hash{}\n}\n\n// TxIndex returns the current transaction index set by SetTxContext.\nfunc (s *StateDB) TxIndex() int {\n\treturn s.txIndex\n}\n\nfunc (s *StateDB) GetCode(addr common.Address) []byte {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.Code()\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) GetCodeSize(addr common.Address) int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.CodeSize()\n\t}\n\treturn 0\n}\n\nfunc (s *StateDB) GetCodeHash(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn common.BytesToHash(stateObject.CodeHash())\n\t}\n\treturn common.Hash{}\n}\n\n// GetState retrieves the value associated with the specific key.\nfunc (s *StateDB) GetState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// GetCommittedState retrieves the value associated with the specific key\n// without any mutations caused in the current execution.\nfunc (s *StateDB) GetCommittedState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetCommittedState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// Database retrieves the low level database supporting the lower level trie ops.\nfunc (s *StateDB) Database() Database {\n\treturn s.db\n}\n\n// Reader retrieves the low level database reader supporting the\n// lower level operations.\nfunc (s *StateDB) Reader() Reader {\n\treturn s.reader\n}\n\nfunc (s *StateDB) HasSelfDestructed(addr common.Address) bool {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.selfDestructed\n\t}\n\treturn false\n}\n\n/*\n * SETTERS\n */\n\n// AddBalance adds amount to the account associated with addr.\nfunc (s *StateDB) AddBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\treturn stateObject.AddBalance(amount)\n}\n\n// SubBalance subtracts amount from the account associated with addr.\nfunc (s *StateDB) SubBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\tif amount.IsZero() {\n\t\treturn *(stateObject.Balance())\n\t}\n\treturn stateObject.SetBalance(new(uint256.Int).Sub(stateObject.Balance(), amount))\n}\n\nfunc (s *StateDB) SetBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetBalance(amount)\n\t}\n}\n\nfunc (s *StateDB) SetNonce(addr common.Address, nonce uint64, reason tracing.NonceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetNonce(nonce)\n\t}\n}\n\nfunc (s *StateDB) SetCode(addr common.Address, code []byte) (prev []byte) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.SetCode(crypto.Keccak256Hash(code), code)\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) SetState(addr common.Address, key, value common.Hash) common.Hash {\n\tif stateObject := s.getOrNewStateObject(addr)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/statedb.go",
          "line": 655,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t}\n\n\t// Short circuit if the account is not found\n\tif acct == nil {\n\t\treturn nil\n\t}\n\t// Schedule the resolved account for prefetching if it's enabled.\n\tif s.prefetcher != nil {\n\t\tif err = s.prefetcher.prefetch(common.Hash{}, s.originalRoot, common.Address{}, []common.Address{addr}, nil, true)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/state/statedb.go",
          "line": 958,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t}\n\n\t// Now we're about to start to write changes to the trie. The trie is so far\n\t// _untouched_. We can check with the prefetcher, if it can give us a trie\n\t// which has the same root, but also has some content loaded into it.\n\t//\n\t// Don't check prefetcher if verkle trie has been used. In the context of verkle,\n\t// only a single trie is used for state hashing. Replacing a non-nil verkle tree\n\t// here could result in losing uncommitted changes from storage.\n\tif metrics.EnabledExpensive() {\n\t\tstart = time.Now()\n\t}\n\tif s.prefetcher != nil {\n\t\tif trie := s.prefetcher.trie(common.Hash{}, s.originalRoot)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/state/statedb.go",
          "line": 1002,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t\tusedAddrs = append(usedAddrs, addr) // Copy needed for closure\n\t}\n\tfor _, deletedAddr := range deletedAddrs {\n\t\ts.deleteStateObject(deletedAddr)\n\t\ts.AccountDeleted += 1\n\t}\n\tif metrics.EnabledExpensive() {\n\t\ts.AccountUpdates += time.Since(start)\n\t}\n\n\tif s.prefetcher != nil && len(usedAddrs) > 0 {\n\t\ts.prefetcher.used(common.Hash{}, s.originalRoot, usedAddrs, nil)\n\t}\n\n\tif metrics.EnabledExpensive() {\n\t\t// Track the amount of time wasted on hashing the account trie\n\t\tdefer func(start time.Time) { s.AccountHashes += time.Since(start) }(time.Now())\n\t}\n\n\thash := s.trie.Hash()\n\n\t// If witness building is enabled, gather the account trie witness\n\tif s.witness != nil {\n\t\ts.witness.AddState(s.trie.Witness())\n\t}\n\n\tif s.db.NoTries() {\n\t\treturn s.expectedRoot\n\t} else {\n\t\treturn hash\n\t}\n}\n\n// SetTxContext sets the current transaction hash and index which are\n// used when the EVM emits new state logs. It should be invoked before\n// transaction execution.\nfunc (s *StateDB) SetTxContext(thash common.Hash, ti int) {\n\ts.thash = thash\n\ts.txIndex = ti\n}\n\n// StateDB.Prepare is not called before processing a system transaction, call ClearAccessList instead.\nfunc (s *StateDB) ClearAccessList() {\n\ts.accessList = newAccessList()\n}\n\nfunc (s *StateDB) clearJournalAndRefund() {\n\ts.journal.reset()\n\ts.refund = 0\n}\n\n// fastDeleteStorage is the function that efficiently deletes the storage trie\n// of a specific account. It leverages the associated state snapshot for fast\n// storage iteration and constructs trie node deletion markers by creating\n// stack trie with iterated slots.\nfunc (s *StateDB) fastDeleteStorage(snaps *snapshot.Tree, addrHash common.Hash, root common.Hash) (map[common.Hash][]byte, map[common.Hash][]byte, *trienode.NodeSet, error) {\n\titer, err := snaps.StorageIterator(s.originalRoot, addrHash, common.Hash{})\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tdefer iter.Release()\n\n\tvar (\n\t\tnodes          = trienode.NewNodeSet(addrHash) // the set for trie node mutations (value is nil)\n\t\tstorages       = make(map[common.Hash][]byte)  // the set for storage mutations (value is nil)\n\t\tstorageOrigins = make(map[common.Hash][]byte)  // the set for tracking the original value of slot\n\t)\n\tstack := trie.NewStackTrie(func(path []byte, hash common.Hash, blob []byte) {\n\t\tnodes.AddNode(path, trienode.NewDeleted())\n\t})\n\tfor iter.Next() {\n\t\tslot := common.CopyBytes(iter.Slot())\n\t\tif err := iter.Error()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/state/statedb.go",
          "line": 1274,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= updates\n\t\t\t\taccountTrieNodesDeleted += deletes\n\t\t\t} else {\n\t\t\t\tstorageTrieNodesUpdated += updates\n\t\t\t\tstorageTrieNodesDeleted += deletes\n\t\t\t}\n\t\t\tif s.NoTries() {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn nodes.Merge(set)\n\t\t}\n\t)\n\t// Given that some accounts could be destroyed and then recreated within\n\t// the same block, account deletions must be processed first. This ensures\n\t// that the storage trie nodes deleted during destruction and recreated\n\t// during subsequent resurrection can be combined correctly.\n\tdeletes, delNodes, err := s.handleDestruction(noStorageWiping)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, set := range delNodes {\n\t\tif err := merge(set)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/state/statedb.go",
          "line": 1437,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\t}\n\t\t}\n\t\t// If trie database is enabled, commit the state update as a new layer\n\t\tif db := s.db.TrieDB()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/core/state/statedb.go",
          "line": 1447,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\t}\n\t\t}\n\t}\n\ts.reader, _ = s.db.Reader(s.originalRoot)\n\treturn ret, err\n}\n\n// Commit writes the state mutations into the configured data stores.\n//\n// Once the state is committed, tries cached in stateDB (including account\n// trie, storage tries) will no longer be functional. A new state instance\n// must be created with new root and updated database for accessing post-\n// commit states.\n//\n// The associated block number of the state transition is also provided\n// for more chain context.\n//\n// noStorageWiping is a flag indicating whether storage wiping is permitted.\n// Since self-destruction was deprecated with the Cancun fork and there are\n// no empty accounts left that could be deleted by EIP-158, storage wiping\n// should not occur.\nfunc (s *StateDB) Commit(block uint64, deleteEmptyObjects bool, noStorageWiping bool) (common.Hash, error) {\n\tret, err := s.commitAndFlush(block, deleteEmptyObjects, noStorageWiping)\n\tif err != nil {\n\t\treturn common.Hash{}, err\n\t}\n\treturn ret.root, nil\n}\n\n// Prepare handles the preparatory steps for executing a state transition with.\n// This method must be invoked before state transition.\n//\n// Berlin fork:\n// - Add sender to access list (2929)\n// - Add destination to access list (2929)\n// - Add precompiles to access list (2929)\n// - Add the contents of the optional tx access list (2930)\n//\n// Potential EIPs:\n// - Reset access list (Berlin)\n// - Add coinbase to access list (EIP-3651)\n// - Reset transient storage (EIP-1153)\nfunc (s *StateDB) Prepare(rules params.Rules, sender, coinbase common.Address, dst *common.Address, precompiles []common.Address, list types.AccessList) {\n\tif rules.IsEIP2929 && rules.IsEIP4762 {\n\t\tpanic(\"eip2929 and eip4762 are both activated\")\n\t}\n\tif rules.IsEIP2929 {\n\t\t// Clear out any leftover from previous executions\n\t\tal := newAccessList()\n\t\ts.accessList = al\n\n\t\tal.AddAddress(sender)\n\t\tif dst != nil {\n\t\t\tal.AddAddress(*dst)\n\t\t\t// If it's a create-tx, the destination will be added inside evm.create\n\t\t}\n\t\tfor _, addr := range precompiles {\n\t\t\tal.AddAddress(addr)\n\t\t}\n\t\tfor _, el := range list {\n\t\t\tal.AddAddress(el.Address)\n\t\t\tfor _, key := range el.StorageKeys {\n\t\t\t\tal.AddSlot(el.Address, key)\n\t\t\t}\n\t\t}\n\t\tif rules.IsShanghai { // EIP-3651: warm coinbase\n\t\t\tal.AddAddress(coinbase)\n\t\t}\n\t}\n\t// Reset transient storage at the beginning of transaction execution\n\ts.transientStorage = newTransientStorage()\n}\n\n// AddAddressToAccessList adds the given address to the access list\nfunc (s *StateDB) AddAddressToAccessList(addr common.Address) {\n\tif s.accessList.AddAddress(addr) {\n\t\ts.journal.accessListAddAccount(addr)\n\t}\n}\n\n// AddSlotToAccessList adds the given (address, slot)-tuple to the access list\nfunc (s *StateDB) AddSlotToAccessList(addr common.Address, slot common.Hash) {\n\taddrMod, slotMod := s.accessList.AddSlot(addr, slot)\n\tif addrMod {\n\t\t// In practice, this should not happen, since there is no way to enter the\n\t\t// scope of 'address' without having the 'address' become already added\n\t\t// to the access list (via call-variant, create, etc).\n\t\t// Better safe than sorry, though\n\t\ts.journal.accessListAddAccount(addr)\n\t}\n\tif slotMod {\n\t\ts.journal.accessListAddSlot(addr, slot)\n\t}\n}\n\n// AddressInAccessList returns true if the given address is in the access list.\nfunc (s *StateDB) AddressInAccessList(addr common.Address) bool {\n\treturn s.accessList.ContainsAddress(addr)\n}\n\n// SlotInAccessList returns true if the given (address, slot)-tuple is in the access list.\nfunc (s *StateDB) SlotInAccessList(addr common.Address, slot common.Hash) (addressPresent bool, slotPresent bool) {\n\treturn s.accessList.Contains(addr, slot)\n}\n\nfunc (s *StateDB) GetSnap() snapshot.Snapshot {\n\tsnaps := s.db.Snapshot()\n\tif snaps != nil {\n\t\treturn snaps.Snapshot(s.originalRoot)\n\t}\n\treturn nil\n}\n\n// markDelete is invoked when an account is deleted but the deletion is\n// not yet committed. The pending mutation is cached and will be applied\n// all together\nfunc (s *StateDB) markDelete(addr common.Address) {\n\tif _, ok := s.mutations[addr]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/core/state/statedb.go",
          "line": 343,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas\n}\n\n// Exist reports whether the given account address exists in the state.\n// Notably this also returns true for self-destructed accounts within the current transaction.\nfunc (s *StateDB) Exist(addr common.Address) bool {\n\treturn s.getStateObject(addr) != nil\n}\n\n// Empty returns whether the state object is either non-existent\n// or empty according to the EIP161 specification (balance = nonce = code = 0)\nfunc (s *StateDB) Empty(addr common.Address) bool {\n\tso := s.getStateObject(addr)\n\treturn so == nil || so.empty()\n}\n\n// GetBalance retrieves the balance from the given address or 0 if object not found\nfunc (s *StateDB) GetBalance(addr common.Address) *uint256.Int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Balance()\n\t}\n\treturn common.U2560\n}\n\n// GetNonce retrieves the nonce from the given address or 0 if object not found\nfunc (s *StateDB) GetNonce(addr common.Address) uint64 {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Nonce()\n\t}\n\n\treturn 0\n}\n\n// GetStorageRoot retrieves the storage root from the given address or empty\n// if object not found.\nfunc (s *StateDB) GetStorageRoot(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Root()\n\t}\n\treturn common.Hash{}\n}\n\n// TxIndex returns the current transaction index set by SetTxContext.\nfunc (s *StateDB) TxIndex() int {\n\treturn s.txIndex\n}\n\nfunc (s *StateDB) GetCode(addr common.Address) []byte {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.Code()\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) GetCodeSize(addr common.Address) int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.CodeSize()\n\t}\n\treturn 0\n}\n\nfunc (s *StateDB) GetCodeHash(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn common.BytesToHash(stateObject.CodeHash())\n\t}\n\treturn common.Hash{}\n}\n\n// GetState retrieves the value associated with the specific key.\nfunc (s *StateDB) GetState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// GetCommittedState retrieves the value associated with the specific key\n// without any mutations caused in the current execution.\nfunc (s *StateDB) GetCommittedState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetCommittedState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// Database retrieves the low level database supporting the lower level trie ops.\nfunc (s *StateDB) Database() Database {\n\treturn s.db\n}\n\n// Reader retrieves the low level database reader supporting the\n// lower level operations.\nfunc (s *StateDB) Reader() Reader {\n\treturn s.reader\n}\n\nfunc (s *StateDB) HasSelfDestructed(addr common.Address) bool {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.selfDestructed\n\t}\n\treturn false\n}\n\n/*\n * SETTERS\n */\n\n// AddBalance adds amount to the account associated with addr.\nfunc (s *StateDB) AddBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\treturn stateObject.AddBalance(amount)\n}\n\n// SubBalance subtracts amount from the account associated with addr.\nfunc (s *StateDB) SubBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\tif amount.IsZero() {\n\t\treturn *(stateObject.Balance())\n\t}\n\treturn stateObject.SetBalance(new(uint256.Int).Sub(stateObject.Balance(), amount))\n}\n\nfunc (s *StateDB) SetBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetBalance(amount)\n\t}\n}\n\nfunc (s *StateDB) SetNonce(addr common.Address, nonce uint64, reason tracing.NonceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetNonce(nonce)\n\t}\n}\n\nfunc (s *StateDB) SetCode(addr common.Address, code []byte) (prev []byte) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.SetCode(crypto.Keccak256Hash(code), code)\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) SetState(addr common.Address, key, value common.Hash) common.Hash {\n\tif stateObject := s.getOrNewStateObject(addr)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/gas_table_test.go",
          "line": 100,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/gas_table_test.go",
          "line": 156,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/gas_table.go",
          "line": 381,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CallNewAccountGas\n\t\t}\n\t} else if !evm.StateDB.Exist(address) {\n\t\tgas += params.CallNewAccountGas\n\t}\n\tif transfersValue && !evm.chainRules.IsEIP4762 {\n\t\tgas += params.CallValueTransferGas\n\t}\n\tmemoryGas, err := memoryGasCost(mem, memorySize)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tvar overflow bool\n\tif gas, overflow = math.SafeAdd(gas, memoryGas)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/gas_table.go",
          "line": 419,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CallValueTransferGas\n\t}\n\tif gas, overflow = math.SafeAdd(gas, memoryGas)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/gas_table.go",
          "line": 476,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CreateBySelfdestructGas\n\t\t\t}\n\t\t} else if !evm.StateDB.Exist(address) {\n\t\t\tgas += params.CreateBySelfdestructGas\n\t\t}\n\t}\n\n\tif !evm.StateDB.HasSelfDestructed(contract.Address()) {\n\t\tevm.StateDB.AddRefund(params.SelfdestructRefundGas)\n\t}\n\treturn gas, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/evm.go",
          "line": 223,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wgas\n\t\t}\n\n\t\tif !isPrecompile && evm.chainRules.IsEIP158 && value.IsZero() {\n\t\t\t// Calling a non-existing account, don't do anything.\n\t\t\treturn nil, gas, nil\n\t\t}\n\t\tevm.StateDB.CreateAccount(addr)\n\t}\n\tevm.Context.Transfer(evm.StateDB, caller, addr, value)\n\n\tif isPrecompile {\n\t\tret, gas, err = RunPrecompiledContract(p, input, gas, evm.Config.Tracer)\n\t} else {\n\t\t// Initialise a new contract and set the code that is to be used by the EVM.\n\t\tcode := evm.resolveCode(addr)\n\t\tif len(code) == 0 {\n\t\t\tret, err = nil, nil // gas is unchanged\n\t\t} else {\n\t\t\t// If the account has no code, we can abort here\n\t\t\t// The depth-check is already done, and precompiles handled above\n\t\t\tcontract := GetContract(caller, addr, value, gas, evm.jumpDests)\n\t\t\tdefer ReturnContract(contract)\n\n\t\t\tcontract.IsSystemCall = isSystemCall(caller)\n\t\t\tcontract.SetCallCode(evm.resolveCodeHash(addr), code)\n\t\t\tret, err = evm.interpreter.Run(contract, input, false)\n\t\t\tgas = contract.Gas\n\t\t}\n\t}\n\t// When an error was returned by the EVM or when setting the creation code\n\t// above we revert to the snapshot and consume any gas remaining. Additionally,\n\t// when we're in homestead this also counts for code storage gas errors.\n\tif err != nil {\n\t\tevm.StateDB.RevertToSnapshot(snapshot)\n\t\tif err != ErrExecutionReverted {\n\t\t\tif evm.Config.Tracer != nil && evm.Config.Tracer.OnGasChange != nil {\n\t\t\t\tevm.Config.Tracer.OnGasChange(gas, 0, tracing.GasChangeCallFailedExecution)\n\t\t\t}\n\n\t\t\tgas = 0\n\t\t}\n\t\t// TODO: consider clearing up unused snapshots:\n\t\t// } else {\n\t\t//\tevm.StateDB.DiscardSnapshot(snapshot)\n\t}\n\treturn ret, gas, err\n}\n\n// CallCode executes the contract associated with the addr with the given input\n// as parameters. It also handles any necessary value transfer required and takes\n// the necessary steps to create accounts and reverses the state in case of an\n// execution error or failed value transfer.\n//\n// CallCode differs from Call in the sense that it executes the given address'\n// code with the caller as context.\nfunc (evm *EVM) CallCode(caller common.Address, addr common.Address, input []byte, gas uint64, value *uint256.Int) (ret []byte, leftOverGas uint64, err error) {\n\t// Invoke tracer hooks that signal entering/exiting a call frame\n\tif evm.Config.Tracer != nil {\n\t\tevm.captureBegin(evm.depth, CALLCODE, caller, addr, input, gas, value.ToBig())\n\t\tdefer func(startGas uint64) {\n\t\t\tevm.captureEnd(evm.depth, startGas, leftOverGas, ret, err)\n\t\t}(gas)\n\t}\n\t// Fail if we're trying to execute above the call depth limit\n\tif evm.depth > int(params.CallCreateDepth) {\n\t\treturn nil, gas, ErrDepth\n\t}\n\t// Fail if we're trying to transfer more than the available balance\n\t// Note although it's noop to transfer X ether to caller itself. But\n\t// if caller doesn't have enough balance, it would be an error to allow\n\t// over-charging itself. So the check here is necessary.\n\tif !evm.Context.CanTransfer(evm.StateDB, caller, value) {\n\t\treturn nil, gas, ErrInsufficientBalance\n\t}\n\tvar snapshot = evm.StateDB.Snapshot()\n\n\t// It is allowed to call precompiles, even via delegatecall\n\tif p, isPrecompile := evm.precompile(addr)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/operations_acl.go",
          "line": 185,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= coldCost\n\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, coldCost)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/operations_acl.go",
          "line": 237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CreateBySelfdestructGas\n\t\t}\n\t\tif refundsEnabled && !evm.StateDB.HasSelfDestructed(contract.Address()) {\n\t\t\tevm.StateDB.AddRefund(params.SelfdestructRefundGas)\n\t\t}\n\t\treturn gas, nil\n\t}\n\treturn gasFunc\n}\n\nvar (\n\tgasCallEIP7702         = makeCallVariantGasCallEIP7702(gasCall)\n\tgasDelegateCallEIP7702 = makeCallVariantGasCallEIP7702(gasDelegateCall)\n\tgasStaticCallEIP7702   = makeCallVariantGasCallEIP7702(gasStaticCall)\n\tgasCallCodeEIP7702     = makeCallVariantGasCallEIP7702(gasCallCode)\n)\n\nfunc makeCallVariantGasCallEIP7702(oldCalculator gasFunc) gasFunc {\n\treturn func(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n\t\tvar (\n\t\t\ttotal uint64 // total dynamic gas used\n\t\t\taddr  = common.Address(stack.Back(1).Bytes20())\n\t\t)\n\n\t\t// Check slot presence in the access list\n\t\tif !evm.StateDB.AddressInAccessList(addr) {\n\t\t\tevm.StateDB.AddAddressToAccessList(addr)\n\t\t\t// The WarmStorageReadCostEIP2929 (100) is already deducted in the form of a constant cost, so\n\t\t\t// the cost to charge for cold access, if any, is Cold - Warm\n\t\t\tcoldCost := params.ColdAccountAccessCostEIP2929 - params.WarmStorageReadCostEIP2929\n\t\t\t// Charge the remaining difference here already, to correctly calculate available\n\t\t\t// gas for call\n\t\t\tif !contract.UseGas(coldCost, evm.Config.Tracer, tracing.GasChangeCallStorageColdAccess) {\n\t\t\t\treturn 0, ErrOutOfGas\n\t\t\t}\n\t\t\ttotal += coldCost\n\t\t}\n\n\t\t// Check if code is a delegation and if so, charge for resolution.\n\t\tif target, ok := types.ParseDelegation(evm.StateDB.GetCode(addr))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/operations_acl.go",
          "line": 287,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= cost\n\t\t}\n\n\t\t// Now call the old calculator, which takes into account\n\t\t// - create new account\n\t\t// - transfer value\n\t\t// - memory expansion\n\t\t// - 63/64ths rule\n\t\told, err := oldCalculator(evm, contract, stack, mem, memorySize)\n\t\tif err != nil {\n\t\t\treturn old, err\n\t\t}\n\n\t\t// Temporarily add the gas charge back to the contract and return value. By\n\t\t// adding it to the return, it will be charged outside of this function, as\n\t\t// part of the dynamic gas. This will ensure it is correctly reported to\n\t\t// tracers.\n\t\tcontract.Gas += total\n\n\t\tvar overflow bool\n\t\tif total, overflow = math.SafeAdd(old, total)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/eips.go",
          "line": 360,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif *pc < codeLen {\n\t\tscope.Stack.push(integer.SetUint64(uint64(scope.Contract.Code[*pc])))\n\n\t\tif !scope.Contract.IsDeployment && !scope.Contract.IsSystemCall && *pc%31 == 0 {\n\t\t\t// touch next chunk if PUSH1 is at the boundary. if so, *pc has\n\t\t\t// advanced past this boundary.\n\t\t\tcontractAddr := scope.Contract.Address()\n\t\t\tconsumed, wanted := interpreter.evm.AccessEvents.CodeChunksRangeGas(contractAddr, *pc+1, uint64(1), uint64(len(scope.Contract.Code)), false, scope.Contract.Gas)\n\t\t\tscope.Contract.UseGas(wanted, interpreter.evm.Config.Tracer, tracing.GasChangeUnspecified)\n\t\t\tif consumed < wanted {\n\t\t\t\treturn nil, ErrOutOfGas\n\t\t\t}\n\t\t}\n\t} else {\n\t\tscope.Stack.push(integer.Clear())\n\t}\n\treturn nil, nil\n}\n\nfunc makePushEIP4762(size uint64, pushByteSize int) executionFunc {\n\treturn func(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t\tvar (\n\t\t\tcodeLen = len(scope.Contract.Code)\n\t\t\tstart   = min(codeLen, int(*pc+1))\n\t\t\tend     = min(codeLen, start+pushByteSize)\n\t\t)\n\t\tscope.Stack.push(new(uint256.Int).SetBytes(\n\t\t\tcommon.RightPadBytes(\n\t\t\t\tscope.Contract.Code[start:end],\n\t\t\t\tpushByteSize,\n\t\t\t)),\n\t\t)\n\n\t\tif !scope.Contract.IsDeployment && !scope.Contract.IsSystemCall {\n\t\t\tcontractAddr := scope.Contract.Address()\n\t\t\tconsumed, wanted := interpreter.evm.AccessEvents.CodeChunksRangeGas(contractAddr, uint64(start), uint64(pushByteSize), uint64(len(scope.Contract.Code)), false, scope.Contract.Gas)\n\t\t\tscope.Contract.UseGas(consumed, interpreter.evm.Config.Tracer, tracing.GasChangeUnspecified)\n\t\t\tif consumed < wanted {\n\t\t\t\treturn nil, ErrOutOfGas\n\t\t\t}\n\t\t}\n\n\t\t*pc += size\n\t\treturn nil, nil\n\t}\n}\n\nfunc enable4762(jt *JumpTable) {\n\tjt[SSTORE] = &operation{\n\t\tdynamicGas: gasSStore4762,\n\t\texecute:    opSstore,\n\t\tminStack:   minStack(2, 0),\n\t\tmaxStack:   maxStack(2, 0),\n\t}\n\tjt[SLOAD] = &operation{\n\t\tdynamicGas: gasSLoad4762,\n\t\texecute:    opSload,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[BALANCE] = &operation{\n\t\texecute:    opBalance,\n\t\tdynamicGas: gasBalance4762,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[EXTCODESIZE] = &operation{\n\t\texecute:    opExtCodeSize,\n\t\tdynamicGas: gasExtCodeSize4762,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[EXTCODEHASH] = &operation{\n\t\texecute:    opExtCodeHash,\n\t\tdynamicGas: gasExtCodeHash4762,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[EXTCODECOPY] = &operation{\n\t\texecute:    opExtCodeCopyEIP4762,\n\t\tdynamicGas: gasExtCodeCopyEIP4762,\n\t\tminStack:   minStack(4, 0),\n\t\tmaxStack:   maxStack(4, 0),\n\t\tmemorySize: memoryExtCodeCopy,\n\t}\n\n\tjt[CODECOPY] = &operation{\n\t\texecute:     opCodeCopy,\n\t\tconstantGas: GasFastestStep,\n\t\tdynamicGas:  gasCodeCopyEip4762,\n\t\tminStack:    minStack(3, 0),\n\t\tmaxStack:    maxStack(3, 0),\n\t\tmemorySize:  memoryCodeCopy,\n\t}\n\n\tjt[SELFDESTRUCT] = &operation{\n\t\texecute:     opSelfdestruct6780,\n\t\tdynamicGas:  gasSelfdestructEIP4762,\n\t\tconstantGas: params.SelfdestructGasEIP150,\n\t\tminStack:    minStack(1, 0),\n\t\tmaxStack:    maxStack(1, 0),\n\t}\n\n\tjt[CREATE] = &operation{\n\t\texecute:     opCreate,\n\t\tconstantGas: params.CreateNGasEip4762,\n\t\tdynamicGas:  gasCreateEip3860,\n\t\tminStack:    minStack(3, 1),\n\t\tmaxStack:    maxStack(3, 1),\n\t\tmemorySize:  memoryCreate,\n\t}\n\n\tjt[CREATE2] = &operation{\n\t\texecute:     opCreate2,\n\t\tconstantGas: params.CreateNGasEip4762,\n\t\tdynamicGas:  gasCreate2Eip3860,\n\t\tminStack:    minStack(4, 1),\n\t\tmaxStack:    maxStack(4, 1),\n\t\tmemorySize:  memoryCreate2,\n\t}\n\n\tjt[CALL] = &operation{\n\t\texecute:    opCall,\n\t\tdynamicGas: gasCallEIP4762,\n\t\tminStack:   minStack(7, 1),\n\t\tmaxStack:   maxStack(7, 1),\n\t\tmemorySize: memoryCall,\n\t}\n\n\tjt[CALLCODE] = &operation{\n\t\texecute:    opCallCode,\n\t\tdynamicGas: gasCallCodeEIP4762,\n\t\tminStack:   minStack(7, 1),\n\t\tmaxStack:   maxStack(7, 1),\n\t\tmemorySize: memoryCall,\n\t}\n\n\tjt[STATICCALL] = &operation{\n\t\texecute:    opStaticCall,\n\t\tdynamicGas: gasStaticCallEIP4762,\n\t\tminStack:   minStack(6, 1),\n\t\tmaxStack:   maxStack(6, 1),\n\t\tmemorySize: memoryStaticCall,\n\t}\n\n\tjt[DELEGATECALL] = &operation{\n\t\texecute:    opDelegateCall,\n\t\tdynamicGas: gasDelegateCallEIP4762,\n\t\tminStack:   minStack(6, 1),\n\t\tmaxStack:   maxStack(6, 1),\n\t\tmemorySize: memoryDelegateCall,\n\t}\n\n\tjt[PUSH1] = &operation{\n\t\texecute:     opPush1EIP4762,\n\t\tconstantGas: GasFastestStep,\n\t\tminStack:    minStack(0, 1),\n\t\tmaxStack:    maxStack(0, 1),\n\t}\n\tfor i := 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/instructions.go",
          "line": 751,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CallStipend\n\t}\n\tret, returnGas, err := interpreter.evm.Call(scope.Contract.Address(), toAddr, args, gas, &value)\n\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opCallCode(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, value, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tif !value.IsZero() {\n\t\tgas += params.CallStipend\n\t}\n\n\tret, returnGas, err := interpreter.evm.CallCode(scope.Contract.Address(), toAddr, args, gas, &value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opDelegateCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tstack := scope.Stack\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.DelegateCall(scope.Contract.Caller(), scope.Contract.Address(), toAddr, args, gas, scope.Contract.value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opStaticCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.StaticCall(scope.Contract.Address(), toAddr, args, gas)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opReturn(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\treturn ret, errStopToken\n}\n\nfunc opRevert(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\tinterpreter.returnData = ret\n\treturn ret, ErrExecutionReverted\n}\n\nfunc opUndefined(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, &ErrInvalidOpCode{opcode: OpCode(scope.Contract.Code[*pc])}\n}\n\nfunc opStop(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, errStopToken\n}\n\nfunc opSelfdestruct(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tif interpreter.readOnly {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tbeneficiary := scope.Stack.pop()\n\tbalance := interpreter.evm.StateDB.GetBalance(scope.Contract.Address())\n\tinterpreter.evm.StateDB.AddBalance(beneficiary.Bytes20(), balance, tracing.BalanceIncreaseSelfdestruct)\n\tinterpreter.evm.StateDB.SelfDestruct(scope.Contract.Address())\n\tif tracer := interpreter.evm.Config.Tracer",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/instructions.go",
          "line": 960,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif *pc < codeLen {\n\t\tscope.Stack.push(integer.SetUint64(uint64(scope.Contract.Code[*pc])))\n\t} else {\n\t\tscope.Stack.push(integer.Clear())\n\t}\n\treturn nil, nil\n}\n\n// opPush2 is a specialized version of pushN\nfunc opPush2(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tvar (\n\t\tcodeLen = uint64(len(scope.Contract.Code))\n\t\tinteger = new(uint256.Int)\n\t)\n\tif *pc+2 < codeLen {\n\t\tscope.Stack.push(integer.SetBytes2(scope.Contract.Code[*pc+1 : *pc+3]))\n\t} else if *pc+1 < codeLen {\n\t\tscope.Stack.push(integer.SetUint64(uint64(scope.Contract.Code[*pc+1]) << 8))\n\t} else {\n\t\tscope.Stack.push(integer.Clear())\n\t}\n\t*pc += 2\n\treturn nil, nil\n}\n\n// make push instruction function\nfunc makePush(size uint64, pushByteSize int) executionFunc {\n\treturn func(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t\tvar (\n\t\t\tcodeLen = len(scope.Contract.Code)\n\t\t\tstart   = min(codeLen, int(*pc+1))\n\t\t\tend     = min(codeLen, start+pushByteSize)\n\t\t)\n\t\ta := new(uint256.Int).SetBytes(scope.Contract.Code[start:end])\n\n\t\t// Missing bytes: pushByteSize - len(pushData)\n\t\tif missing := pushByteSize - (end - start)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/instructions.go",
          "line": 1001,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\t\treturn nil, nil\n\t}\n}\n\n// make dup instruction function\nfunc makeDup(size int64) executionFunc {\n\treturn func(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t\tscope.Stack.dup(int(size))\n\t\treturn nil, nil\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/vm/instructions.go",
          "line": 667,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas / 64\n\t}\n\n\t// reuse size int for stackvalue\n\tstackvalue := size\n\n\tscope.Contract.UseGas(gas, interpreter.evm.Config.Tracer, tracing.GasChangeCallContractCreation)\n\n\tres, addr, returnGas, suberr := interpreter.evm.Create(scope.Contract.Address(), input, gas, &value)\n\t// Push item on the stack based on the returned error. If the ruleset is\n\t// homestead we must check for CodeStoreOutOfGasError (homestead only\n\t// rule) and treat as an error, if the ruleset is frontier we must\n\t// ignore this error and pretend the operation was successful.\n\tif interpreter.evm.chainRules.IsHomestead && suberr == ErrCodeStoreOutOfGas {\n\t\tstackvalue.Clear()\n\t} else if suberr != nil && suberr != ErrCodeStoreOutOfGas {\n\t\tstackvalue.Clear()\n\t} else {\n\t\tstackvalue.SetBytes(addr.Bytes())\n\t}\n\tscope.Stack.push(&stackvalue)\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tif suberr == ErrExecutionReverted {\n\t\tinterpreter.returnData = res // set REVERT data to return data buffer\n\t\treturn res, nil\n\t}\n\tinterpreter.returnData = nil // clear dirty return data buffer\n\treturn nil, nil\n}\n\nfunc opCreate2(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tif interpreter.readOnly {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tvar (\n\t\tendowment    = scope.Stack.pop()\n\t\toffset, size = scope.Stack.pop(), scope.Stack.pop()\n\t\tsalt         = scope.Stack.pop()\n\t\tinput        = scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\t\tgas          = scope.Contract.Gas\n\t)\n\n\t// Apply EIP150\n\tgas -= gas / 64\n\tscope.Contract.UseGas(gas, interpreter.evm.Config.Tracer, tracing.GasChangeCallContractCreation2)\n\t// reuse size int for stackvalue\n\tstackvalue := size\n\tres, addr, returnGas, suberr := interpreter.evm.Create2(scope.Contract.Address(), input, gas,\n\t\t&endowment, &salt)\n\t// Push item on the stack based on the returned error.\n\tif suberr != nil {\n\t\tstackvalue.Clear()\n\t} else {\n\t\tstackvalue.SetBytes(addr.Bytes())\n\t}\n\tscope.Stack.push(&stackvalue)\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tif suberr == ErrExecutionReverted {\n\t\tinterpreter.returnData = res // set REVERT data to return data buffer\n\t\treturn res, nil\n\t}\n\tinterpreter.returnData = nil // clear dirty return data buffer\n\treturn nil, nil\n}\n\nfunc opCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tstack := scope.Stack\n\t// Pop gas. The actual gas in interpreter.evm.callGasTemp.\n\t// We can use this as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, value, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get the arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tif interpreter.readOnly && !value.IsZero() {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tif !value.IsZero() {\n\t\tgas += params.CallStipend\n\t}\n\tret, returnGas, err := interpreter.evm.Call(scope.Contract.Address(), toAddr, args, gas, &value)\n\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opCallCode(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, value, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tif !value.IsZero() {\n\t\tgas += params.CallStipend\n\t}\n\n\tret, returnGas, err := interpreter.evm.CallCode(scope.Contract.Address(), toAddr, args, gas, &value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opDelegateCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tstack := scope.Stack\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.DelegateCall(scope.Contract.Caller(), scope.Contract.Address(), toAddr, args, gas, scope.Contract.value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opStaticCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.StaticCall(scope.Contract.Address(), toAddr, args, gas)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opReturn(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\treturn ret, errStopToken\n}\n\nfunc opRevert(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\tinterpreter.returnData = ret\n\treturn ret, ErrExecutionReverted\n}\n\nfunc opUndefined(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, &ErrInvalidOpCode{opcode: OpCode(scope.Contract.Code[*pc])}\n}\n\nfunc opStop(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, errStopToken\n}\n\nfunc opSelfdestruct(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tif interpreter.readOnly {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tbeneficiary := scope.Stack.pop()\n\tbalance := interpreter.evm.StateDB.GetBalance(scope.Contract.Address())\n\tinterpreter.evm.StateDB.AddBalance(beneficiary.Bytes20(), balance, tracing.BalanceIncreaseSelfdestruct)\n\tinterpreter.evm.StateDB.SelfDestruct(scope.Contract.Address())\n\tif tracer := interpreter.evm.Config.Tracer",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/vm/instructions.go",
          "line": 753,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/vm/instructions.go",
          "line": 816,
          "category": "unchecked_calls",
          "pattern": "\\.delegatecall\\s*\\(",
          "match": ".DelegateCall(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/operations_verkle.go",
          "line": 94,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= witnessGas // restore witness gas so that it can be charged at the callsite\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, witnessGas)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/operations_verkle.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= wanted\n\t}\n\t// Charge write costs if it transfers value\n\tif !balanceIsZero {\n\t\twanted := evm.AccessEvents.BasicDataGas(contractAddr, true, contract.Gas-statelessGas, false)\n\t\tif wanted > contract.Gas-statelessGas {\n\t\t\treturn statelessGas + wanted, nil\n\t\t}\n\t\tstatelessGas += wanted\n\n\t\tif contractAddr != beneficiaryAddr {\n\t\t\tif evm.StateDB.Exist(beneficiaryAddr) {\n\t\t\t\twanted = evm.AccessEvents.BasicDataGas(beneficiaryAddr, true, contract.Gas-statelessGas, false)\n\t\t\t} else {\n\t\t\t\twanted = evm.AccessEvents.AddAccount(beneficiaryAddr, true, contract.Gas-statelessGas)\n\t\t\t}\n\t\t\tif wanted > contract.Gas-statelessGas {\n\t\t\t\treturn statelessGas + wanted, nil\n\t\t\t}\n\t\t\tstatelessGas += wanted\n\t\t}\n\t}\n\treturn statelessGas, nil\n}\n\nfunc gasCodeCopyEip4762(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n\tgas, err := gasCodeCopy(evm, contract, stack, mem, memorySize)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif !contract.IsDeployment && !contract.IsSystemCall {\n\t\tvar (\n\t\t\tcodeOffset = stack.Back(1)\n\t\t\tlength     = stack.Back(2)\n\t\t)\n\t\tuint64CodeOffset, overflow := codeOffset.Uint64WithOverflow()\n\t\tif overflow {\n\t\t\tuint64CodeOffset = gomath.MaxUint64\n\t\t}\n\n\t\t_, copyOffset, nonPaddedCopyLength := getDataAndAdjustedBounds(contract.Code, uint64CodeOffset, length.Uint64())\n\t\t_, wanted := evm.AccessEvents.CodeChunksRangeGas(contract.Address(), copyOffset, nonPaddedCopyLength, uint64(len(contract.Code)), false, contract.Gas-gas)\n\t\tgas += wanted\n\t}\n\treturn gas, nil\n}\n\nfunc gasExtCodeCopyEIP4762(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n\t// memory expansion first (dynamic part of pre-2929 implementation)\n\tgas, err := gasExtCodeCopy(evm, contract, stack, mem, memorySize)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\taddr := common.Address(stack.peek().Bytes20())\n\t_, isPrecompile := evm.precompile(addr)\n\tif isPrecompile || addr == params.HistoryStorageAddress {\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, params.WarmStorageReadCostEIP2929)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/operations_verkle.go",
          "line": 91,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= witnessGas\n\t\t// if the operation fails, adds witness gas to the gas before returning the error\n\t\tgas, err := oldCalculator(evm, contract, stack, mem, memorySize)\n\t\tcontract.Gas += witnessGas // restore witness gas so that it can be charged at the callsite\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, witnessGas)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/contracts.go",
          "line": 892,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 192 {\n\t\tc, err := newCurvePoint(input[i : i+64])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tt, err := newTwistPoint(input[i+64 : i+192])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcs = append(cs, c)\n\t\tts = append(ts, t)\n\t}\n\t// Execute the pairing checks and return the results\n\tif bn256.PairingCheck(cs, ts) {\n\t\treturn true32Byte, nil\n\t}\n\treturn false32Byte, nil\n}\n\n// bn256PairingIstanbul implements a pairing pre-compile for the bn256 curve\n// conforming to Istanbul consensus rules.\ntype bn256PairingIstanbul struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\nfunc (c *bn256PairingIstanbul) RequiredGas(input []byte) uint64 {\n\treturn params.Bn256PairingBaseGasIstanbul + uint64(len(input)/192)*params.Bn256PairingPerPointGasIstanbul\n}\n\nfunc (c *bn256PairingIstanbul) Run(input []byte) ([]byte, error) {\n\treturn runBn256Pairing(input)\n}\n\n// bn256PairingByzantium implements a pairing pre-compile for the bn256 curve\n// conforming to Byzantium consensus rules.\ntype bn256PairingByzantium struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\nfunc (c *bn256PairingByzantium) RequiredGas(input []byte) uint64 {\n\treturn params.Bn256PairingBaseGasByzantium + uint64(len(input)/192)*params.Bn256PairingPerPointGasByzantium\n}\n\nfunc (c *bn256PairingByzantium) Run(input []byte) ([]byte, error) {\n\treturn runBn256Pairing(input)\n}\n\ntype blake2F struct{}\n\nfunc (c *blake2F) RequiredGas(input []byte) uint64 {\n\t// If the input is malformed, we can't calculate the gas, return 0 and let the\n\t// actual call choke and fault.\n\tif len(input) != blake2FInputLength {\n\t\treturn 0\n\t}\n\treturn uint64(binary.BigEndian.Uint32(input[0:4]))\n}\n\nconst (\n\tblake2FInputLength        = 213\n\tblake2FFinalBlockBytes    = byte(1)\n\tblake2FNonFinalBlockBytes = byte(0)\n)\n\nvar (\n\terrBlake2FInvalidInputLength = errors.New(\"invalid input length\")\n\terrBlake2FInvalidFinalFlag   = errors.New(\"invalid final flag\")\n)\n\nfunc (c *blake2F) Run(input []byte) ([]byte, error) {\n\t// Make sure the input is valid (correct length and final flag)\n\tif len(input) != blake2FInputLength {\n\t\treturn nil, errBlake2FInvalidInputLength\n\t}\n\tif input[212] != blake2FNonFinalBlockBytes && input[212] != blake2FFinalBlockBytes {\n\t\treturn nil, errBlake2FInvalidFinalFlag\n\t}\n\t// Parse the input into the Blake2b call parameters\n\tvar (\n\t\trounds = binary.BigEndian.Uint32(input[0:4])\n\t\tfinal  = input[212] == blake2FFinalBlockBytes\n\n\t\th [8]uint64\n\t\tm [16]uint64\n\t\tt [2]uint64\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/contracts.go",
          "line": 496,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gasCost\n\toutput, err := p.Run(input)\n\treturn output, suppliedGas, err\n}\n\n// ecrecover implemented as a native contract.\ntype ecrecover struct{}\n\nfunc (c *ecrecover) RequiredGas(input []byte) uint64 {\n\treturn params.EcrecoverGas\n}\n\nfunc (c *ecrecover) Run(input []byte) ([]byte, error) {\n\tconst ecRecoverInputLength = 128\n\n\tinput = common.RightPadBytes(input, ecRecoverInputLength)\n\t// \"input\" is (hash, v, r, s), each 32 bytes\n\t// but for ecrecover we want (r, s, v)\n\n\tr := new(big.Int).SetBytes(input[64:96])\n\ts := new(big.Int).SetBytes(input[96:128])\n\tv := input[63] - 27\n\n\t// tighter sig s values input homestead only apply to tx sigs\n\tif !allZero(input[32:63]) || !crypto.ValidateSignatureValues(v, r, s, false) {\n\t\treturn nil, nil\n\t}\n\t// We must make sure not to modify the 'input', so placing the 'v' along with\n\t// the signature needs to be done on a new allocation\n\tsig := make([]byte, 65)\n\tcopy(sig, input[64:128])\n\tsig[64] = v\n\t// v needs to be at the end for libsecp256k1\n\tpubKey, err := crypto.Ecrecover(input[:32], sig)\n\t// make sure the public key is a valid one\n\tif err != nil {\n\t\treturn nil, nil\n\t}\n\n\t// the first byte of pubkey is bitcoin heritage\n\treturn common.LeftPadBytes(crypto.Keccak256(pubKey[1:])[12:], 32), nil\n}\n\n// SHA256 implemented as a native contract.\ntype sha256hash struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\n//\n// This method does not require any overflow checking as the input size gas costs\n// required for anything significant is so high it's impossible to pay for.\nfunc (c *sha256hash) RequiredGas(input []byte) uint64 {\n\treturn uint64(len(input)+31)/32*params.Sha256PerWordGas + params.Sha256BaseGas\n}\n\nfunc (c *sha256hash) Run(input []byte) ([]byte, error) {\n\th := sha256.Sum256(input)\n\treturn h[:], nil\n}\n\n// RIPEMD160 implemented as a native contract.\ntype ripemd160hash struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\n//\n// This method does not require any overflow checking as the input size gas costs\n// required for anything significant is so high it's impossible to pay for.\nfunc (c *ripemd160hash) RequiredGas(input []byte) uint64 {\n\treturn uint64(len(input)+31)/32*params.Ripemd160PerWordGas + params.Ripemd160BaseGas\n}\n\nfunc (c *ripemd160hash) Run(input []byte) ([]byte, error) {\n\tripemd := ripemd160.New()\n\tripemd.Write(input)\n\treturn common.LeftPadBytes(ripemd.Sum(nil), 32), nil\n}\n\n// data copy implemented as a native contract.\ntype dataCopy struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\n//\n// This method does not require any overflow checking as the input size gas costs\n// required for anything significant is so high it's impossible to pay for.\nfunc (c *dataCopy) RequiredGas(input []byte) uint64 {\n\treturn uint64(len(input)+31)/32*params.IdentityPerWordGas + params.IdentityBaseGas\n}\n\nfunc (c *dataCopy) Run(in []byte) ([]byte, error) {\n\treturn common.CopyBytes(in), nil\n}\n\n// bigModExp implements a native big integer exponential modular operation.\ntype bigModExp struct {\n\teip2565 bool\n\teip7823 bool\n\teip7883 bool\n}\n\nvar (\n\tbig1      = big.NewInt(1)\n\tbig3      = big.NewInt(3)\n\tbig7      = big.NewInt(7)\n\tbig20     = big.NewInt(20)\n\tbig32     = big.NewInt(32)\n\tbig64     = big.NewInt(64)\n\tbig96     = big.NewInt(96)\n\tbig480    = big.NewInt(480)\n\tbig1024   = big.NewInt(1024)\n\tbig3072   = big.NewInt(3072)\n\tbig199680 = big.NewInt(199680)\n)\n\n// modexpMultComplexity implements bigModexp multComplexity formula, as defined in EIP-198\n//\n//\tdef mult_complexity(x):\n//\t\tif x <= 64: return x ** 2\n//\t\telif x <= 1024: return x ** 2 // 4 + 96 * x - 3072\n//\t\telse: return x ** 2 // 16 + 480 * x - 199680\n//\n// where is x is max(length_of_MODULUS, length_of_BASE)\nfunc modexpMultComplexity(x *big.Int) *big.Int {\n\tswitch {\n\tcase x.Cmp(big64) <= 0:\n\t\tx.Mul(x, x) // x ** 2\n\tcase x.Cmp(big1024) <= 0:\n\t\t// (x ** 2 // 4 ) + ( 96 * x - 3072)\n\t\tx = new(big.Int).Add(\n\t\t\tnew(big.Int).Rsh(new(big.Int).Mul(x, x), 2),\n\t\t\tnew(big.Int).Sub(new(big.Int).Mul(big96, x), big3072),\n\t\t)\n\tdefault:\n\t\t// (x ** 2 // 16) + (480 * x - 199680)\n\t\tx = new(big.Int).Add(\n\t\t\tnew(big.Int).Rsh(new(big.Int).Mul(x, x), 4),\n\t\t\tnew(big.Int).Sub(new(big.Int).Mul(big480, x), big199680),\n\t\t)\n\t}\n\treturn x\n}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\nfunc (c *bigModExp) RequiredGas(input []byte) uint64 {\n\tvar (\n\t\tbaseLen = new(big.Int).SetBytes(getData(input, 0, 32))\n\t\texpLen  = new(big.Int).SetBytes(getData(input, 32, 32))\n\t\tmodLen  = new(big.Int).SetBytes(getData(input, 64, 32))\n\t)\n\tif len(input) > 96 {\n\t\tinput = input[96:]\n\t} else {\n\t\tinput = input[:0]\n\t}\n\t// Retrieve the head 32 bytes of exp for the adjusted exponent length\n\tvar expHead *big.Int\n\tif big.NewInt(int64(len(input))).Cmp(baseLen) <= 0 {\n\t\texpHead = new(big.Int)\n\t} else {\n\t\tif expLen.Cmp(big32) > 0 {\n\t\t\texpHead = new(big.Int).SetBytes(getData(input, baseLen.Uint64(), 32))\n\t\t} else {\n\t\t\texpHead = new(big.Int).SetBytes(getData(input, baseLen.Uint64(), expLen.Uint64()))\n\t\t}\n\t}\n\t// Calculate the adjusted exponent length\n\tvar msb int\n\tif bitlen := expHead.BitLen()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/interpreter.go",
          "line": 289,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= dynamicCost // for tracing\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"%w: %v\", ErrOutOfGas, err)\n\t\t\t}\n\t\t\t// for tracing: this gas consumption event is emitted below in the debug section.\n\t\t\tif contract.Gas < dynamicCost {\n\t\t\t\treturn nil, ErrOutOfGas\n\t\t\t} else {\n\t\t\t\tcontract.Gas -= dynamicCost\n\t\t\t}\n\t\t}\n\n\t\t// Do tracing before potential memory expansion\n\t\tif debug {\n\t\t\tif in.evm.Config.Tracer.OnGasChange != nil {\n\t\t\t\tin.evm.Config.Tracer.OnGasChange(gasCopy, gasCopy-cost, tracing.GasChangeCallOpCode)\n\t\t\t}\n\t\t\tif in.evm.Config.Tracer.OnOpcode != nil {\n\t\t\t\tin.evm.Config.Tracer.OnOpcode(pc, byte(op), gasCopy, cost, callContext, in.returnData, in.evm.depth, VMErrorFromErr(err))\n\t\t\t\tlogged = true\n\t\t\t}\n\t\t}\n\t\tif memorySize > 0 {\n\t\t\tmem.Resize(memorySize)\n\t\t}\n\n\t\t// execute the operation\n\t\tres, err = operation.execute(&pc, in, callContext)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tpc++\n\t}\n\n\tif err == errStopToken {\n\t\terr = nil // clear stop token error\n\t}\n\n\treturn res, err\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/interpreter.go",
          "line": 263,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= cost\n\t\t}\n\n\t\t// All ops with a dynamic memory usage also has a dynamic gas cost.\n\t\tvar memorySize uint64\n\t\tif operation.dynamicGas != nil {\n\t\t\t// calculate the new memory size and expand the memory to fit\n\t\t\t// the operation\n\t\t\t// Memory check needs to be done prior to evaluating the dynamic gas portion,\n\t\t\t// to detect calculation overflows\n\t\t\tif operation.memorySize != nil {\n\t\t\t\tmemSize, overflow := operation.memorySize(stack)\n\t\t\t\tif overflow {\n\t\t\t\t\treturn nil, ErrGasUintOverflow\n\t\t\t\t}\n\t\t\t\t// memory is expanded in words of 32 bytes. Gas\n\t\t\t\t// is also calculated in words.\n\t\t\t\tif memorySize, overflow = math.SafeMul(toWordSize(memSize), 32)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/interpreter.go",
          "line": 297,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= dynamicCost\n\t\t\t}\n\t\t}\n\n\t\t// Do tracing before potential memory expansion\n\t\tif debug {\n\t\t\tif in.evm.Config.Tracer.OnGasChange != nil {\n\t\t\t\tin.evm.Config.Tracer.OnGasChange(gasCopy, gasCopy-cost, tracing.GasChangeCallOpCode)\n\t\t\t}\n\t\t\tif in.evm.Config.Tracer.OnOpcode != nil {\n\t\t\t\tin.evm.Config.Tracer.OnOpcode(pc, byte(op), gasCopy, cost, callContext, in.returnData, in.evm.depth, VMErrorFromErr(err))\n\t\t\t\tlogged = true\n\t\t\t}\n\t\t}\n\t\tif memorySize > 0 {\n\t\t\tmem.Resize(memorySize)\n\t\t}\n\n\t\t// execute the operation\n\t\tres, err = operation.execute(&pc, in, callContext)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tpc++\n\t}\n\n\tif err == errStopToken {\n\t\terr = nil // clear stop token error\n\t}\n\n\treturn res, err\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/interpreter_test.go",
          "line": 56,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/contract.go",
          "line": 151,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gas\n}\n\n// Address returns the contracts address\nfunc (c *Contract) Address() common.Address {\n\treturn c.address\n}\n\n// Value returns the contract's value (sent to it from it's caller)\nfunc (c *Contract) Value() *uint256.Int {\n\treturn c.value\n}\n\n// SetCallCode sets the code of the contract,\nfunc (c *Contract) SetCallCode(hash common.Hash, code []byte) {\n\tc.Code = code\n\tc.CodeHash = hash\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/contract.go",
          "line": 139,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas\n\treturn true\n}\n\n// RefundGas refunds gas to the contract\nfunc (c *Contract) RefundGas(gas uint64, logger *tracing.Hooks, reason tracing.GasChangeReason) {\n\tif gas == 0 {\n\t\treturn\n\t}\n\tif logger != nil && logger.OnGasChange != nil && reason != tracing.GasChangeIgnored {\n\t\tlogger.OnGasChange(c.Gas, c.Gas+gas, reason)\n\t}\n\tc.Gas += gas\n}\n\n// Address returns the contracts address\nfunc (c *Contract) Address() common.Address {\n\treturn c.address\n}\n\n// Value returns the contract's value (sent to it from it's caller)\nfunc (c *Contract) Value() *uint256.Int {\n\treturn c.value\n}\n\n// SetCallCode sets the code of the contract,\nfunc (c *Contract) SetCallCode(hash common.Hash, code []byte) {\n\tc.Code = code\n\tc.CodeHash = hash\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 86,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 16\n\t\t\t}\n\t\t\tfor ",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 90,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 8\n\t\t\t}\n\t\t}\n\t\tswitch numbits {\n\t\tcase 1:\n\t\t\tbits.set1(pc)\n\t\t\tpc += 1\n\t\tcase 2:\n\t\t\tbits.setN(set2BitsMask, pc)\n\t\t\tpc += 2\n\t\tcase 3:\n\t\t\tbits.setN(set3BitsMask, pc)\n\t\t\tpc += 3\n\t\tcase 4:\n\t\t\tbits.setN(set4BitsMask, pc)\n\t\t\tpc += 4\n\t\tcase 5:\n\t\t\tbits.setN(set5BitsMask, pc)\n\t\t\tpc += 5\n\t\tcase 6:\n\t\t\tbits.setN(set6BitsMask, pc)\n\t\t\tpc += 6\n\t\tcase 7:\n\t\t\tbits.setN(set7BitsMask, pc)\n\t\t\tpc += 7\n\t\t}\n\t}\n\treturn bits\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 84,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 16 {\n\t\t\t\tbits.set16(pc)\n\t\t\t\tpc += 16\n\t\t\t}\n\t\t\tfor ",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 88,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 8 {\n\t\t\t\tbits.set8(pc)\n\t\t\t\tpc += 8\n\t\t\t}\n\t\t}\n\t\tswitch numbits {\n\t\tcase 1:\n\t\t\tbits.set1(pc)\n\t\t\tpc += 1\n\t\tcase 2:\n\t\t\tbits.setN(set2BitsMask, pc)\n\t\t\tpc += 2\n\t\tcase 3:\n\t\t\tbits.setN(set3BitsMask, pc)\n\t\t\tpc += 3\n\t\tcase 4:\n\t\t\tbits.setN(set4BitsMask, pc)\n\t\t\tpc += 4\n\t\tcase 5:\n\t\t\tbits.setN(set5BitsMask, pc)\n\t\t\tpc += 5\n\t\tcase 6:\n\t\t\tbits.setN(set6BitsMask, pc)\n\t\t\tpc += 6\n\t\tcase 7:\n\t\t\tbits.setN(set7BitsMask, pc)\n\t\t\tpc += 7\n\t\t}\n\t}\n\treturn bits\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/filtermaps/matcher.go",
          "line": 417,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= groupLength\n\t}\n\tm.cleanMapIndices()\n\tm.stats.setState(&st, stNone)\n\treturn results, nil\n}\n\n// dropIndices implements matcherInstance.\nfunc (m *singleMatcherInstance) dropIndices(dropIndices []uint32) {\n\tfor _, mapIndex := range dropIndices {\n\t\tdelete(m.filterRows, mapIndex)\n\t}\n\tm.cleanMapIndices()\n}\n\n// cleanMapIndices removes map indices from the list if there is no matching\n// filterRows entry because a result has been returned or the index has been\n// dropped.\nfunc (m *singleMatcherInstance) cleanMapIndices() {\n\tvar j int\n\tfor i, mapIndex := range m.mapIndices {\n\t\tif _, ok := m.filterRows[mapIndex]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/filtermaps/matcher.go",
          "line": 555,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(res)\n\t}\n\tmerged := make(potentialMatches, 0, sumLen)\n\tfor {\n\t\tbest := -1\n\t\tfor i, res := range results {\n\t\t\tif len(res) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif best < 0 || res[0] < results[best][0] {\n\t\t\t\tbest = i\n\t\t\t}\n\t\t}\n\t\tif best < 0 {\n\t\t\treturn merged\n\t\t}\n\t\tif len(merged) == 0 || results[best][0] > merged[len(merged)-1] {\n\t\t\tmerged = append(merged, results[best][0])\n\t\t}\n\t\tresults[best] = results[best][1:]\n\t}\n}\n\n// matchSequence combines two matchers, a \"base\" and a \"next\" matcher with a\n// positive integer offset so that the resulting matcher signals a match at log\n// value index X when the base matcher returns a match at X and the next matcher\n// gives a match at X+offset. Note that matchSequence can be used recursively to\n// detect any log value sequence.\ntype matchSequence struct {\n\tparams               *Params\n\tbase, next           matcher\n\toffset               uint64\n\tstatsLock            sync.Mutex\n\tbaseStats, nextStats matchOrderStats\n}\n\n// newInstance creates a new instance of matchSequence.\nfunc (m *matchSequence) newInstance(mapIndices []uint32) matcherInstance {\n\t// determine set of indices to request from next matcher\n\tneedMatched := make(map[uint32]struct{})\n\tbaseRequested := make(map[uint32]struct{})\n\tnextRequested := make(map[uint32]struct{})\n\tfor _, mapIndex := range mapIndices {\n\t\tneedMatched[mapIndex] = struct{}{}\n\t\tbaseRequested[mapIndex] = struct{}{}\n\t\tnextRequested[mapIndex] = struct{}{}\n\t}\n\treturn &matchSequenceInstance{\n\t\tmatchSequence: m,\n\t\tbaseInstance:  m.base.newInstance(mapIndices),\n\t\tnextInstance:  m.next.newInstance(mapIndices),\n\t\tneedMatched:   needMatched,\n\t\tbaseRequested: baseRequested,\n\t\tnextRequested: nextRequested,\n\t\tbaseResults:   make(map[uint32]potentialMatches),\n\t\tnextResults:   make(map[uint32]potentialMatches),\n\t}\n}\n\n// matchOrderStats collects statistics about the evaluating cost and the\n// occurrence of empty result sets from both base and next child matchers.\n// This allows the optimization of the evaluation order by evaluating the\n// child first that is cheaper and/or gives empty results more often and not\n// evaluating the other child in most cases.\n// Note that matchOrderStats is specific to matchSequence and the results are\n// carried over to future instances as the results are mostly useful when\n// evaluating layer zero of each instance. For this reason it should be used\n// in a thread safe way as is may be accessed from multiple worker goroutines.\ntype matchOrderStats struct {\n\ttotalCount, nonEmptyCount, totalCost uint64\n}\n\n// add collects statistics after a child has been evaluated for a certain layer.\nfunc (ms *matchOrderStats) add(empty bool, layerIndex uint32) {\n\tif empty && layerIndex != 0 {\n\t\t// matchers may be evaluated for higher layers after all results have\n\t\t// been returned. Also, empty results are not relevant when previous\n\t\t// layers yielded matches already, so these cases can be ignored.\n\t\treturn\n\t}\n\tms.totalCount++\n\tif !empty {\n\t\tms.nonEmptyCount++\n\t}\n\tms.totalCost += uint64(layerIndex + 1)\n}\n\n// mergeStats merges two sets of matchOrderStats.\nfunc (ms *matchOrderStats) mergeStats(add matchOrderStats) {\n\tms.totalCount += add.totalCount\n\tms.nonEmptyCount += add.nonEmptyCount\n\tms.totalCost += add.totalCost\n}\n\n// baseFirst returns true if the base child matcher should be evaluated first.\nfunc (m *matchSequence) baseFirst() bool {\n\tm.statsLock.Lock()\n\tbf := float64(m.baseStats.totalCost)*float64(m.nextStats.totalCount)+\n\t\tfloat64(m.baseStats.nonEmptyCount)*float64(m.nextStats.totalCost) <\n\t\tfloat64(m.baseStats.totalCost)*float64(m.nextStats.nonEmptyCount)+\n\t\t\tfloat64(m.nextStats.totalCost)*float64(m.baseStats.totalCount)\n\tm.statsLock.Unlock()\n\treturn bf\n}\n\n// mergeBaseStats merges a set of matchOrderStats into the base matcher stats.\nfunc (m *matchSequence) mergeBaseStats(stats matchOrderStats) {\n\tm.statsLock.Lock()\n\tm.baseStats.mergeStats(stats)\n\tm.statsLock.Unlock()\n}\n\n// mergeNextStats merges a set of matchOrderStats into the next matcher stats.\nfunc (m *matchSequence) mergeNextStats(stats matchOrderStats) {\n\tm.statsLock.Lock()\n\tm.nextStats.mergeStats(stats)\n\tm.statsLock.Unlock()\n}\n\n// newMatchSequence creates a recursive sequence matcher from a list of underlying\n// matchers. The resulting matcher signals a match at log value index X when each\n// underlying matcher matchers[i] returns a match at X+i.\nfunc newMatchSequence(params *Params, matchers []matcher) matcher {\n\tif len(matchers) == 0 {\n\t\tpanic(\"zero length sequence matchers are not allowed\")\n\t}\n\tif len(matchers) == 1 {\n\t\treturn matchers[0]\n\t}\n\treturn &matchSequence{\n\t\tparams: params,\n\t\tbase:   newMatchSequence(params, matchers[:len(matchers)-1]),\n\t\tnext:   matchers[len(matchers)-1],\n\t\toffset: uint64(len(matchers) - 1),\n\t}\n}\n\n// matchSequenceInstance is an instance of matchSequence.\ntype matchSequenceInstance struct {\n\t*matchSequence\n\tbaseInstance, nextInstance                matcherInstance\n\tbaseRequested, nextRequested, needMatched map[uint32]struct{}\n\tbaseResults, nextResults                  map[uint32]potentialMatches\n}\n\n// getMatchesForLayer implements matcherInstance.\nfunc (m *matchSequenceInstance) getMatchesForLayer(ctx context.Context, layerIndex uint32) (matchedResults []matcherResult, err error) {\n\t// decide whether to evaluate base or next matcher first\n\tbaseFirst := m.baseFirst()\n\tif baseFirst {\n\t\tif err := m.evalBase(ctx, layerIndex)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/filtermaps/indexer_test.go",
          "line": 114,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rand.Intn(1000-head) + 1\n\t\t\t\tts.fm.testSnapshotUsed = false\n\t\t\t\tts.chain.setCanonicalChain(forks[fork][:head+1])\n\t\t\t}\n\t\t}\n\t\tts.fm.WaitIdle()\n\t\tif checkSnapshot {\n\t\t\tif ts.fm.testSnapshotUsed == ts.fm.testDisableSnapshots {\n\t\t\t\tts.t.Fatalf(\"Invalid snapshot used state after head extension (used: %v, disabled: %v)\", ts.fm.testSnapshotUsed, ts.fm.testDisableSnapshots)\n\t\t\t}\n\t\t\tcheckSnapshot = false\n\t\t}\n\t\tif noHistory {\n\t\t\tif ts.fm.indexedRange.initialized {\n\t\t\t\tt.Fatalf(\"filterMapsRange initialized while indexing is disabled\")\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif !ts.fm.indexedRange.initialized {\n\t\t\tt.Fatalf(\"filterMapsRange not initialized while indexing is enabled\")\n\t\t}\n\t\tvar tailBlock uint64\n\t\tif history > 0 && history <= head {\n\t\t\ttailBlock = uint64(head + 1 - history)\n\t\t}\n\t\tvar tailEpoch uint32\n\t\tif tailBlock > 0 {\n\t\t\ttailLvPtr := expspos(tailBlock) - 1\n\t\t\ttailEpoch = uint32(tailLvPtr >> (testParams.logValuesPerMap + testParams.logMapsPerEpoch))\n\t\t}\n\t\ttailLvPtr := uint64(tailEpoch) << (testParams.logValuesPerMap + testParams.logMapsPerEpoch) // first available lv ptr\n\t\tvar expTailBlock uint64\n\t\tif tailEpoch > 0 {\n\t\t\tfor expspos(expTailBlock) <= tailLvPtr {\n\t\t\t\texpTailBlock++\n\t\t\t}\n\t\t}\n\t\tif ts.fm.indexedRange.blocks.Last() != uint64(head) {\n\t\t\tts.t.Fatalf(\"Invalid index head (expected #%d, got #%d)\", head, ts.fm.indexedRange.blocks.Last())\n\t\t}\n\t\texpHeadDelimiter := expdpos(uint64(head))\n\t\tif ts.fm.indexedRange.headDelimiter != expHeadDelimiter {\n\t\t\tts.t.Fatalf(\"Invalid index head delimiter pointer (expected %d, got %d)\", expHeadDelimiter, ts.fm.indexedRange.headDelimiter)\n\t\t}\n\n\t\tif ts.fm.indexedRange.blocks.First() != expTailBlock {\n\t\t\tts.t.Fatalf(\"Invalid index tail block (expected #%d, got #%d)\", expTailBlock, ts.fm.indexedRange.blocks.First())\n\t\t}\n\t}\n}\n\nfunc TestIndexerMatcherView(t *testing.T) {\n\ttestIndexerMatcherView(t, false)\n}\n\nfunc TestIndexerMatcherViewWithConcurrentRead(t *testing.T) {\n\ttestIndexerMatcherView(t, true)\n}\n\nfunc testIndexerMatcherView(t *testing.T, concurrentRead bool) {\n\tts := newTestSetup(t)\n\tdefer ts.close()\n\n\tforks := make([][]common.Hash, 20)\n\thashes := make([]common.Hash, 20)\n\tts.chain.addBlocks(100, 5, 2, 4, true)\n\tts.setHistory(0, false)\n\tfor i := range forks {\n\t\tif i != 0 {\n\t\t\tts.chain.setHead(100 - i)\n\t\t\tts.chain.addBlocks(i, 5, 2, 4, true)\n\t\t}\n\t\tts.fm.WaitIdle()\n\t\tforks[i] = ts.chain.getCanonicalChain()\n\t\thashes[i] = ts.matcherViewHash()\n\t}\n\tfork := len(forks) - 1\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/filtermaps/filtermaps.go",
          "line": 557,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= r // skip to map boundary\n\t\t\t}\n\t\t\tif lvPointer > lvIndex {\n\t\t\t\t// lvIndex does not point to the first log value (address value)\n\t\t\t\t// generated by a log as true matches should always do, so it\n\t\t\t\t// is considered a false positive (no log and no error returned).\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tif lvPointer == lvIndex {\n\t\t\t\treturn log, nil // potential match\n\t\t\t}\n\t\t\tlvPointer += l\n\t\t}\n\t}\n\treturn nil, nil\n}\n\n// getFilterMap fetches an entire filter map from the database.\nfunc (f *FilterMaps) getFilterMap(mapIndex uint32) (filterMap, error) {\n\tif fm, ok := f.filterMapCache.Get(mapIndex)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/filtermaps/filtermaps.go",
          "line": 608,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= groupLength\n\t}\n\treturn rows, nil\n}\n\n// getFilterMapRowsOfGroup fetches a set of filter map rows at map indices\n// belonging to the same base row group.\nfunc (f *FilterMaps) getFilterMapRowsOfGroup(target []FilterRow, mapIndices []uint32, rowIndex uint32, baseLayerOnly bool) error {\n\tvar (\n\t\tgroupIndex  = f.mapGroupIndex(mapIndices[0])\n\t\tmapRowIndex = f.mapRowIndex(groupIndex, rowIndex)\n\t)\n\tbaseRows, err := rawdb.ReadFilterMapBaseRows(f.db, mapRowIndex, f.baseRowGroupSize, f.logMapWidth)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to retrieve base row group %d of row %d: %v\", groupIndex, rowIndex, err)\n\t}\n\tfor i, mapIndex := range mapIndices {\n\t\tif f.mapGroupIndex(mapIndex) != groupIndex {\n\t\t\treturn fmt.Errorf(\"maps are not in the same base row group, index: %d, group: %d\", mapIndex, groupIndex)\n\t\t}\n\t\trow := baseRows[f.mapGroupOffset(mapIndex)]\n\t\tif !baseLayerOnly {\n\t\t\textRow, err := rawdb.ReadFilterMapExtRow(f.db, f.mapRowIndex(mapIndex, rowIndex), f.logMapWidth)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to retrieve filter map %d extended row %d: %v\", mapIndex, rowIndex, err)\n\t\t\t}\n\t\t\trow = append(row, extRow...)\n\t\t}\n\t\ttarget[i] = row\n\t}\n\treturn nil\n}\n\n// storeFilterMapRows stores a set of filter map rows at the corresponding map\n// indices and a shared row index.\nfunc (f *FilterMaps) storeFilterMapRows(batch ethdb.Batch, mapIndices []uint32, rowIndex uint32, rows []FilterRow) error {\n\tfor len(mapIndices) > 0 {\n\t\tvar (\n\t\t\tpos        = 1\n\t\t\tgroupIndex = f.mapGroupIndex(mapIndices[0])\n\t\t)\n\t\tfor pos < len(mapIndices) && f.mapGroupIndex(mapIndices[pos]) == groupIndex {\n\t\t\tpos++\n\t\t}\n\t\tif err := f.storeFilterMapRowsOfGroup(batch, mapIndices[:pos], rowIndex, rows[:pos])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/filtermaps/math_test.go",
          "line": 130,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(matches) - testPmLen\n\t\t\t}\n\t\t}\n\t}\n\t// Whenever looking for a certain log value hash, each entry in the row that\n\t// was generated by another log value hash (a \"foreign entry\") has a\n\t// valuesPerMap // 2^32 chance of yielding a false positive if the reverse\n\t// transformed 32 bit integer is by random chance less than valuesPerMap and\n\t// is therefore considered a potentially valid match.\n\t// We have testPmLen unique hash entries and a testPmLen long series of entries\n\t// for the same hash. For each of the testPmLen unique hash entries there are\n\t// testPmLen*2-1 foreign entries while for the long series there are testPmLen\n\t// foreign entries. This means that after performing all these filtering runs,\n\t// we have processed 2*testPmLen^2 foreign entries, which given us an estimate\n\t// of how many false positives to expect.\n\texpFalse := int(uint64(testPmCount*testPmLen*testPmLen*2) * params.valuesPerMap >> params.logMapWidth)\n\tif falsePositives < expFalse/2 || falsePositives > expFalse*3/2 {\n\t\tt.Fatalf(\"False positive rate out of expected range (got %d, expected %d +-50%%)\", falsePositives, expFalse)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 329,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\tif stopCb() {\n\t\t\t\treturn false, nil\n\t\t\t}\n\t\t\tstart = time.Now()\n\t\t\tif !r.iterator.updateChainView(r.f.targetView) {\n\t\t\t\treturn false, errChainUpdate\n\t\t\t}\n\t\t\twaitCnt = 0\n\t\t}\n\t\tif logValue := r.iterator.getValueHash()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 375,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\tmapRenderTimer.Update(totalTime)\n\tmapLogValueMeter.Mark(logValuesProcessed)\n\tmapBlockMeter.Mark(blocksProcessed)\n\treturn true, nil\n}\n\n// writeFinishedMaps writes rendered maps to the database and updates\n// filterMapsRange and indexedView accordingly.\nfunc (r *mapRenderer) writeFinishedMaps(pauseCb func() bool) error {\n\tvar totalTime time.Duration\n\tstart := time.Now()\n\tif len(r.finishedMaps) == 0 {\n\t\treturn nil\n\t}\n\tr.f.indexLock.Lock()\n\tdefer r.f.indexLock.Unlock()\n\n\toldRange := r.f.indexedRange\n\ttempRange, err := r.getTempRange()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get temporary rendered range: %v\", err)\n\t}\n\tnewRange, err := r.getUpdatedRange()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get updated rendered range: %v\", err)\n\t}\n\trenderedView := r.f.targetView // pauseCb callback might still change targetView while writing finished maps\n\n\tbatch := r.f.db.NewBatch()\n\tvar writeCnt int\n\tcheckWriteCnt := func() {\n\t\twriteCnt++\n\t\tif writeCnt == rowsPerBatch {\n\t\t\twriteCnt = 0\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 416,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\tpauseCb()\n\t\t\tstart = time.Now()\n\t\t\tr.f.indexLock.Lock()\n\t\t\tbatch = r.f.db.NewBatch()\n\t\t}\n\t}\n\n\tif tempRange != r.f.indexedRange {\n\t\tr.f.setRange(batch, r.f.indexedView, tempRange, true)\n\t}\n\t// add or update filter rows\n\tfor rowIndex := uint32(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 512,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\tmapWriteTimer.Update(totalTime)\n\treturn nil\n}\n\n// getTempRange returns a temporary filterMapsRange that is committed to the\n// database while the newly rendered maps are partially written. Writing all\n// processed maps in a single database batch would be a serious hit on db\n// performance so instead safety is ensured by first reverting the valid map\n// range to the unchanged region until all new map data is committed.\nfunc (r *mapRenderer) getTempRange() (filterMapsRange, error) {\n\ttempRange := r.f.indexedRange\n\tif err := tempRange.addRenderedRange(r.finished.First(), r.finished.First(), r.renderBefore, r.f.mapsPerEpoch)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 626,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= e.d\n\t\tif i < len(endpoints)-1 && endpoints[i+1].m == e.m {\n\t\t\tcontinue\n\t\t}\n\t\tif (sum > 0) != last {\n\t\t\tmerged = append(merged, e.m)\n\t\t\tlast = !last\n\t\t}\n\t}\n\n\tswitch len(merged) {\n\tcase 0:\n\t\t// Initialized database, but no finished maps yet.\n\t\tfmr.tailPartialEpoch = 0\n\t\tfmr.maps = common.NewRange(firstRendered, 0)\n\n\tcase 2:\n\t\t// One rendered section (no partial tail epoch).\n\t\tfmr.tailPartialEpoch = 0\n\t\tfmr.maps = common.NewRange(merged[0], merged[1]-merged[0])\n\n\tcase 4:\n\t\t// Two rendered sections (with a gap).\n\t\t// First section (merged[0]-merged[1]) is for the partial tail epoch,\n\t\t// and it has to start exactly one epoch before the main section.\n\t\tif merged[2] != merged[0]+mapsPerEpoch {\n\t\t\treturn fmt.Errorf(\"invalid tail partial epoch: %v\", merged)\n\t\t}\n\t\tfmr.tailPartialEpoch = merged[1] - merged[0]\n\t\tfmr.maps = common.NewRange(merged[2], merged[3]-merged[2])\n\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid number of rendered sections: %v\", merged)\n\t}\n\treturn nil\n}\n\n// logIterator iterates on the linear log value index range.\ntype logIterator struct {\n\tparams                                          *Params\n\tchainView                                       *ChainView\n\tblockNumber                                     uint64\n\treceipts                                        types.Receipts\n\tblockStart, delimiter, skipToBoundary, finished bool\n\ttxIndex, logIndex, topicIndex                   int\n\tlvIndex                                         uint64\n}\n\nvar errUnindexedRange = errors.New(\"unindexed range\")\n\n// newLogIteratorFromBlockDelimiter creates a logIterator starting at the\n// given block's first log value entry (the block delimiter), according to the\n// current targetView.\nfunc (f *FilterMaps) newLogIteratorFromBlockDelimiter(blockNumber, lvIndex uint64) (*logIterator, error) {\n\tif blockNumber > f.targetView.HeadNumber() {\n\t\treturn nil, fmt.Errorf(\"iterator entry point %d after target chain head block %d\", blockNumber, f.targetView.HeadNumber())\n\t}\n\tif !f.indexedRange.blocks.Includes(blockNumber) {\n\t\treturn nil, errUnindexedRange\n\t}\n\tfinished := blockNumber == f.targetView.HeadNumber()\n\tl := &logIterator{\n\t\tchainView:   f.targetView,\n\t\tparams:      &f.Params,\n\t\tblockNumber: blockNumber,\n\t\tfinished:    finished,\n\t\tdelimiter:   !finished,\n\t\tlvIndex:     lvIndex,\n\t}\n\tl.enforceValidState()\n\treturn l, nil\n}\n\n// newLogIteratorFromMapBoundary creates a logIterator starting at the given\n// map boundary, according to the current targetView.\nfunc (f *FilterMaps) newLogIteratorFromMapBoundary(mapIndex uint32, startBlock, startLvPtr uint64) (*logIterator, error) {\n\tif startBlock > f.targetView.HeadNumber() {\n\t\treturn nil, fmt.Errorf(\"iterator entry point %d after target chain head block %d\", startBlock, f.targetView.HeadNumber())\n\t}\n\t// get block receipts\n\treceipts := f.targetView.RawReceipts(startBlock)\n\tif receipts == nil {\n\t\treturn nil, fmt.Errorf(\"receipts not found for start block %d\", startBlock)\n\t}\n\t// initialize iterator at block start\n\tl := &logIterator{\n\t\tchainView:   f.targetView,\n\t\tparams:      &f.Params,\n\t\tblockNumber: startBlock,\n\t\treceipts:    receipts,\n\t\tblockStart:  true,\n\t\tlvIndex:     startLvPtr,\n\t}\n\tl.enforceValidState()\n\ttargetIndex := uint64(mapIndex) << f.logValuesPerMap\n\tif l.lvIndex > targetIndex {\n\t\treturn nil, fmt.Errorf(\"log value pointer %d of last block of map is after map boundary %d\", l.lvIndex, targetIndex)\n\t}\n\t// iterate to map boundary\n\tfor l.lvIndex < targetIndex {\n\t\tif l.finished {\n\t\t\treturn nil, fmt.Errorf(\"iterator already finished at %d before map boundary target %d\", l.lvIndex, targetIndex)\n\t\t}\n\t\tif err := l.next()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/runtime/runtime.go",
          "line": 144,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/runtime/runtime.go",
          "line": 213,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 742,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 758,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 754,
          "category": "unchecked_calls",
          "pattern": "\\.delegatecall\\s*\\(",
          "match": ".DelegateCall(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 63,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 70,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 263,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 303,
          "category": "unchecked_calls",
          "pattern": "\\.delegatecall\\s*\\(",
          "match": ".DelegateCall(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/program/program.go",
          "line": 271,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32 {\n\t\tchunk := data[idx : idx+32]\n\t\t// push the value\n\t\tp.Push(chunk)\n\t\t// push the memory index\n\t\tp.Push(uint32(idx) + memStart)\n\t\tp.Op(vm.MSTORE)\n\t}\n\t// Remainders become stored using MSTORE8\n\tfor ",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/program/program.go",
          "line": 322,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32 {\n\t\tdataStart := idx\n\t\t// Mload the chunk\n\t\tp.Push(dataStart)\n\t\tp.Op(vm.MLOAD)\n\t\t// Value is now on stack,\n\t\tp.Push(startSlot)\n\t\tp.Op(vm.SSTORE)\n\t\tstartSlot++\n\t}\n\treturn p\n}\n\n// ReturnViaCodeCopy utilises CODECOPY to place the given data in the bytecode of\n// p, loads into memory (offset 0) and returns the code.\n// This is a typical \"constructor\".\n// Note: since all indexing is calculated immediately, the preceding bytecode\n// must not be expanded or shortened.\nfunc (p *Program) ReturnViaCodeCopy(data []byte) *Program {\n\tp.Push(len(data))\n\t// For convenience, we'll use PUSH2 for the offset. Then we know we can always\n\t// fit, since code is limited to 0xc000\n\tp.Op(vm.PUSH2)\n\toffsetPos := p.Size()  // Need to update this position later on\n\tp.Append([]byte{0, 0}) // Offset of the code to be copied\n\tp.Push(0)              // Offset in memory (destination)\n\tp.Op(vm.CODECOPY)      // Copy from code[offset:offset+len] to memory[0:]\n\tp.Return(0, len(data)) // Return memory[0:len]\n\toffset := p.Size()\n\tp.Append(data) // And add the data\n\n\t// Now, go back and fix the offset\n\tp.code[offsetPos] = byte(offset >> 8)\n\tp.code[offsetPos+1] = byte(offset)\n\treturn p\n}\n\n// Sstore stores the given byte array to the given slot.\n// OBS! Does not verify that the value indeed fits into 32 bytes.\n// If it does not, it will panic later on via doPush.\nfunc (p *Program) Sstore(slot any, value any) *Program {\n\tp.Push(value)\n\tp.Push(slot)\n\treturn p.Op(vm.SSTORE)\n}\n\n// Tstore stores the given byte array to the given t-slot.\n// OBS! Does not verify that the value indeed fits into 32 bytes.\n// If it does not, it will panic later on via doPush.\nfunc (p *Program) Tstore(slot any, value any) *Program {\n\tp.Push(value)\n\tp.Push(slot)\n\treturn p.Op(vm.TSTORE)\n}\n\n// Return implements RETURN\nfunc (p *Program) Return(offset, len int) *Program {\n\tp.Push(len)\n\tp.Push(offset)\n\treturn p.Op(vm.RETURN)\n}\n\n// ReturnData loads the given data into memory, and does a return with it\nfunc (p *Program) ReturnData(data []byte) *Program {\n\tp.Mstore(data, 0)\n\treturn p.Return(0, len(data))\n}\n\n// Create2 uses create2 to construct a contract with the given bytecode.\n// This operation leaves either '0' or address on the stack.\nfunc (p *Program) Create2(code []byte, salt any) *Program {\n\tvar (\n\t\tvalue  = 0\n\t\toffset = 0\n\t\tsize   = len(code)\n\t)\n\t// Load the code into mem\n\tp.Mstore(code, 0)\n\t// Create it\n\treturn p.Push(salt).\n\t\tPush(size).\n\t\tPush(offset).\n\t\tPush(value).\n\t\tOp(vm.CREATE2)\n\t// On the stack now, is either\n\t// - zero: in case of failure, OR\n\t// - address: in case of success\n}\n\n// Create2ThenCall calls create2 with the given initcode and salt, and then calls\n// into the created contract (or calls into zero, if the creation failed).\nfunc (p *Program) Create2ThenCall(code []byte, salt any) *Program {\n\tp.Create2(code, salt)\n\t// If there happen to be a zero on the stack, it doesn't matter, we're\n\t// not sending any value anyway\n\tp.Push(0).Push(0) // mem out\n\tp.Push(0).Push(0) // mem in\n\tp.Push(0)         // value\n\tp.Op(vm.DUP6)     // address\n\tp.Op(vm.GAS)\n\tp.Op(vm.CALL)\n\tp.Op(vm.POP)        // pop the retval\n\treturn p.Op(vm.POP) // pop the address\n}\n\n// Selfdestruct pushes beneficiary and invokes selfdestruct.\nfunc (p *Program) Selfdestruct(beneficiary any) *Program {\n\tp.Push(beneficiary)\n\treturn p.Op(vm.SELFDESTRUCT)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 50,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\theight := binary.BigEndian.Uint64(input[pos : pos+heightLength])\n\tpos += heightLength\n\n\tappHash := input[pos : pos+appHashLength]\n\tpos += appHashLength\n\n\tcurValidatorSetHash := input[pos : pos+validatorSetHashLength]\n\tpos += validatorSetHashLength\n\n\tnextValidatorSetLength := (inputLen - minimumLength) / singleValidatorBytesLength\n\tvalidatorSetBytes := input[pos:]\n\tvar validatorSet []*tmtypes.Validator\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 104,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+heightLength], cs.Height)\n\tpos += heightLength\n\n\tcopy(encodingBytes[pos:pos+appHashLength], cs.AppHash)\n\tpos += appHashLength\n\n\tcopy(encodingBytes[pos:pos+validatorSetHashLength], cs.CurValidatorSetHash)\n\tpos += validatorSetHashLength\n\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 123,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= validatorPubkeyLength\n\n\t\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+validatorVotingPowerLength], uint64(validator.VotingPower))\n\t\tpos += validatorVotingPowerLength\n\t}\n\n\treturn encodingBytes, nil\n}\n\nfunc (cs *ConsensusState) ApplyHeader(header *Header) (bool, error) {\n\tif uint64(header.Height) <= cs.Height {\n\t\treturn false, fmt.Errorf(\"header height <= consensus height (%d <= %d)\", header.Height, cs.Height)\n\t}\n\n\tif err := header.Validate(cs.ChainID)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 278,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= storeNameLengthBytesLength\n\n\tkeyLength := binary.BigEndian.Uint64(input[pos+keyLengthBytesLength-8 : pos+keyLengthBytesLength])\n\tpos += keyLengthBytesLength\n\n\tfixedSize = storeNameLengthBytesLength + keyLengthBytesLength + valueLengthBytesLength\n\tif inputLength <= fixedSize+keyLength || fixedSize+keyLength < fixedSize {\n\t\treturn nil, fmt.Errorf(\"invalid input, keyLength %d is too long\", keyLength)\n\t}\n\tkey := input[pos : pos+keyLength]\n\tpos += keyLength\n\n\tvalueLength := binary.BigEndian.Uint64(input[pos+valueLengthBytesLength-8 : pos+valueLengthBytesLength])\n\tpos += valueLengthBytesLength\n\n\tfixedSize = storeNameLengthBytesLength + keyLengthBytesLength + valueLengthBytesLength + appHashLength\n\tif inputLength <= fixedSize+keyLength+valueLength ||\n\t\tfixedSize+keyLength < fixedSize ||\n\t\tfixedSize+keyLength+valueLength < valueLength {\n\t\treturn nil, fmt.Errorf(\"invalid input, valueLength %d is too long\", valueLength)\n\t}\n\tvalue := input[pos : pos+valueLength]\n\tpos += valueLength\n\n\tappHash := input[pos : pos+appHashLength]\n\tpos += appHashLength\n\n\tproofBytes := input[pos:]\n\tvar merkleProof merkle.Proof\n\terr := merkleProof.Unmarshal(proofBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tkeyValueMerkleProof := &KeyValueMerkleProof{\n\t\tKey:       key,\n\t\tValue:     value,\n\t\tStoreName: storeName,\n\t\tAppHash:   appHash,\n\t\tProof:     &merkleProof,\n\t}\n\n\treturn keyValueMerkleProof, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 56,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+heightLength], cs.Height)\n\tpos += heightLength\n\n\tcopy(encodingBytes[pos:pos+validatorSetHashLength], cs.NextValidatorSetHash)\n\tpos += validatorSetHashLength\n\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 72,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= validatorPubkeyLength\n\n\t\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+validatorVotingPowerLength], uint64(validator.VotingPower))\n\t\tpos += validatorVotingPowerLength\n\n\t\tcopy(encodingBytes[pos:pos+relayerAddressLength], validator.RelayerAddress)\n\t\tpos += relayerAddressLength\n\n\t\tcopy(encodingBytes[pos:pos+relayerBlsKeyLength], validator.BlsKey)\n\t\tpos += relayerBlsKeyLength\n\t}\n\n\treturn encodingBytes, nil\n}\n\nfunc (cs *ConsensusState) ApplyLightBlock(block *types.LightBlock, isHertz bool) (bool, error) {\n\tif uint64(block.Height) <= cs.Height {\n\t\treturn false, fmt.Errorf(\"block height <= consensus height (%d < %d)\", block.Height, cs.Height)\n\t}\n\n\tif err := block.ValidateBasic(cs.ChainID)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 149,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\theight := binary.BigEndian.Uint64(input[pos : pos+heightLength])\n\tpos += heightLength\n\n\tnextValidatorSetHash := input[pos : pos+validatorSetHashLength]\n\tpos += validatorSetHashLength\n\n\tvalidatorSetLength := (inputLen - minimumLength) / singleValidatorBytesLength\n\tvalidatorSetBytes := input[pos:]\n\tvalidatorSet := make([]*types.Validator, 0, validatorSetLength)\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 166,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= validatorPubkeyLength\n\n\t\tvotingPower := int64(binary.BigEndian.Uint64(validatorBytes[pos : pos+validatorVotingPowerLength]))\n\t\tpos += validatorVotingPowerLength\n\n\t\trelayerAddress := make([]byte, relayerAddressLength)\n\t\tcopy(relayerAddress[:], validatorBytes[pos:pos+relayerAddressLength])\n\t\tpos += relayerAddressLength\n\n\t\trelayerBlsKey := make([]byte, relayerBlsKeyLength)\n\t\tcopy(relayerBlsKey[:], validatorBytes[pos:])\n\n\t\tvalidator := types.NewValidator(pubkey, votingPower)\n\t\tvalidator.SetRelayerAddress(relayerAddress)\n\t\tvalidator.SetBlsKey(relayerBlsKey)\n\t\tvalidatorSet = append(validatorSet, validator)\n\t}\n\n\tconsensusState := ConsensusState{\n\t\tChainID:              chainID,\n\t\tHeight:               height,\n\t\tNextValidatorSetHash: nextValidatorSetHash,\n\t\tValidatorSet: &types.ValidatorSet{\n\t\t\tValidators: validatorSet,\n\t\t},\n\t}\n\n\treturn consensusState, nil\n}\n\n// input:\n// consensus state length | consensus state | light block |\n// 32 bytes               |                 |             |\nfunc DecodeLightBlockValidationInput(input []byte) (*ConsensusState, *types.LightBlock, error) {\n\tif uint64(len(input)) <= consensusStateLengthBytesLength {\n\t\treturn nil, nil, errors.New(\"invalid input\")\n\t}\n\n\tcsLen := binary.BigEndian.Uint64(input[consensusStateLengthBytesLength-uint64TypeLength : consensusStateLengthBytesLength])\n\n\tif consensusStateLengthBytesLength+csLen < consensusStateLengthBytesLength {\n\t\treturn nil, nil, fmt.Errorf(\"integer overflow, csLen: %d\", csLen)\n\t}\n\n\tif uint64(len(input)) <= consensusStateLengthBytesLength+csLen {\n\t\treturn nil, nil, fmt.Errorf(\"expected payload size %d, actual size: %d\", consensusStateLengthBytesLength+csLen, len(input))\n\t}\n\n\tcs, err := DecodeConsensusState(input[consensusStateLengthBytesLength : consensusStateLengthBytesLength+csLen])\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar lbpb tmproto.LightBlock\n\terr = lbpb.Unmarshal(input[consensusStateLengthBytesLength+csLen:])\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tblock, err := types.LightBlockFromProto(&lbpb)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn &cs, block, nil\n}\n\n// output:\n// | validatorSetChanged | empty      | consensusStateBytesLength |  new consensusState |\n// | 1 byte              | 23 bytes   | 8 bytes                   |                     |\nfunc EncodeLightBlockValidationResult(validatorSetChanged bool, consensusStateBytes []byte) []byte {\n\tlengthBytes := make([]byte, validateResultMetaDataLength)\n\tif validatorSetChanged {\n\t\tcopy(lengthBytes[:1], []byte{0x01})\n\t}\n\n\tconsensusStateBytesLength := uint64(len(consensusStateBytes))\n\tbinary.BigEndian.PutUint64(lengthBytes[validateResultMetaDataLength-uint64TypeLength:], consensusStateBytesLength)\n\n\tresult := append(lengthBytes, consensusStateBytes...)\n\treturn result\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 415,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t\t\t\tcontinue\n\t\t\t} else if cmp == 0 {\n\t\t\t\t// the snapshot key can be overwritten\n\t\t\t\tcreated--\n\t\t\t\tif write = !bytes.Equal(kvvals[0], iter.Value)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 434,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t}\n\tif iter.Err != nil {\n\t\t// Trie errors should never happen. Still, in case of a bug, expose the\n\t\t// error here, as the outer code will presume errors are interrupts, not\n\t\t// some deeper issues.\n\t\tlog.Error(\"State snapshotter failed to iterate trie\", \"err\", iter.Err)\n\t\treturn false, nil, iter.Err\n\t}\n\t// Delete all stale snapshot states remaining\n\tistart := time.Now()\n\tfor _, key := range kvkeys {\n\t\tif err := onState(key, nil, false, true)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 449,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tinternal += time.Since(istart)\n\n\t// Update metrics for counting trie iteration\n\tif kind == snapStorage {\n\t\tsnapStorageTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t} else {\n\t\tsnapAccountTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t}\n\tlogger.Debug(\"Regenerated state range\", \"root\", trieId.Root, \"last\", hexutil.Encode(last),\n\t\t\"count\", count, \"created\", created, \"updated\", updated, \"untouched\", untouched, \"deleted\", deleted)\n\n\t// If there are either more trie items, or there are more snap items\n\t// (in the next segment), then we need to keep working\n\treturn !trieMore && !result.diskMore, last, nil\n}\n\n// checkAndFlush checks if an interruption signal is received or the\n// batch size has exceeded the allowance.\nfunc (dl *diskLayer) checkAndFlush(ctx *generatorContext, current []byte) error {\n\tvar abort chan *generatorStats\n\tselect {\n\tcase abort = <-dl.genAbort:\n\tdefault:\n\t}\n\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize || abort != nil {\n\t\tif bytes.Compare(current, dl.genMarker) < 0 {\n\t\t\tlog.Error(\"Snapshot generator went backwards\", \"current\", fmt.Sprintf(\"%x\", current), \"genMarker\", fmt.Sprintf(\"%x\", dl.genMarker))\n\t\t}\n\t\t// Flush out the batch anyway no matter it's empty or not.\n\t\t// It's possible that all the states are recovered and the\n\t\t// generation indeed makes progress.\n\t\tjournalProgress(ctx.batch, current, ctx.stats)\n\n\t\tif err := ctx.batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 527,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + 2*common.HashLength + len(val))\n\t\tctx.stats.slots++\n\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := dl.checkAndFlush(ctx, append(account[:], key...))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 595,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tctx.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(dl.genMarker) > common.HashLength {\n\t\t\tmarker = dl.genMarker[:]\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := dl.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 583,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n\t\t\t\t}\n\t\t\t\tif acc.Root == types.EmptyRootHash {\n\t\t\t\t\tdataLen -= 32\n\t\t\t\t}\n\t\t\t\tsnapRecoveredAccountMeter.Mark(1)\n\t\t\t} else {\n\t\t\t\tdata := types.SlimAccountRLP(acc)\n\t\t\t\tdataLen = len(data)\n\t\t\t\trawdb.WriteAccountSnapshot(ctx.batch, account, data)\n\t\t\t\tsnapGeneratedAccountMeter.Mark(1)\n\t\t\t}\n\t\t\tctx.stats.storage += common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tctx.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(dl.genMarker) > common.HashLength {\n\t\t\tmarker = dl.genMarker[:]\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := dl.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/difflayer.go",
          "line": 146,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(common.HashLength + len(blob))\n\t\tsnapshotDirtyAccountWriteMeter.Mark(int64(len(blob)))\n\t}\n\tfor accountHash, slots := range storage {\n\t\tif slots == nil {\n\t\t\tpanic(fmt.Sprintf(\"storage %#x nil\", accountHash))\n\t\t}\n\t\t// Determine memory size and track the dirty writes\n\t\tfor _, data := range slots {\n\t\t\tdl.memory += uint64(common.HashLength + len(data))\n\t\t\tsnapshotDirtyStorageWriteMeter.Mark(int64(len(data)))\n\t\t}\n\t}\n\treturn dl\n}\n\n// rebloom discards the layer's current bloom and rebuilds it from scratch based\n// on the parent's and the local diffs.\nfunc (dl *diffLayer) rebloom(origin *diskLayer) {\n\tdl.lock.Lock()\n\tdefer dl.lock.Unlock()\n\n\tdefer func(start time.Time) {\n\t\tsnapshotBloomIndexTimer.Update(time.Since(start))\n\t}(time.Now())\n\n\t// Inject the new origin that triggered the rebloom\n\tdl.origin = origin\n\n\t// Retrieve the parent bloom or create a fresh empty one\n\tif parent, ok := dl.parent.(*diffLayer)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/snapshot/difflayer.go",
          "line": 453,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(dl.accountList) * common.HashLength)\n\treturn dl.accountList\n}\n\n// StorageList returns a sorted list of all storage slot hashes in this diffLayer\n// for the given account. If the whole storage is destructed in this layer, then\n// an additional flag *destructed = true* will be returned, otherwise the flag is\n// false. Besides, the returned list will include the hash of deleted storage slot.\n// Note a special case is an account is deleted in a prior tx but is recreated in\n// the following tx with some storage slots set. In this case the returned list is\n// not empty but the flag is true.\n//\n// Note, the returned slice is not a copy, so do not modify it.\nfunc (dl *diffLayer) StorageList(accountHash common.Hash) []common.Hash {\n\tdl.lock.RLock()\n\tif _, ok := dl.storageData[accountHash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/state/snapshot/difflayer.go",
          "line": 486,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(dl.storageList)*common.HashLength + common.HashLength)\n\treturn storageList\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/conversion.go",
          "line": 128,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tstat.head = account\n}\n\n// finishAccounts updates the generator stats for the finished account range.\nfunc (stat *generateStats) finishAccounts(done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.accounts += done\n}\n\n// progressContract updates the generator stats for a specific in-progress contract.\nfunc (stat *generateStats) progressContract(account common.Hash, slot common.Hash, done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.slots += done\n\tstat.slotsHead[account] = slot\n\tif _, ok := stat.slotsStart[account]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/snapshot/conversion.go",
          "line": 157,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tdelete(stat.slotsHead, account)\n\tdelete(stat.slotsStart, account)\n}\n\n// report prints the cumulative progress statistic smartly.\nfunc (stat *generateStats) report() {\n\tstat.lock.RLock()\n\tdefer stat.lock.RUnlock()\n\n\tctx := []interface{}{\n\t\t\"accounts\", stat.accounts,\n\t\t\"slots\", stat.slots,\n\t\t\"elapsed\", common.PrettyDuration(time.Since(stat.start)),\n\t}\n\tif stat.accounts > 0 {\n\t\t// If there's progress on the account trie, estimate the time to finish crawling it\n\t\tif done := binary.BigEndian.Uint64(stat.head[:8]) / stat.accounts",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/iterator_test.go",
          "line": 87,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t}\n\t\t}\n\t\tstorage[h] = accStorage\n\t\tnilStorage[h] = nilstorage\n\t}\n\t// Add some (identical) layers on top\n\tdiffLayer := newDiffLayer(emptyLayer(), common.Hash{}, copyAccounts(accounts), copyStorage(storage))\n\tfor account := range accounts {\n\t\tit := diffLayer.StorageIterator(account, common.Hash{})\n\t\tverifyIterator(t, 100, it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n\n\tdiskLayer := diffToDisk(diffLayer)\n\tfor account := range accounts {\n\t\tit := diskLayer.StorageIterator(account, common.Hash{})\n\t\tverifyIterator(t, 100-nilStorage[account], it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n}\n\ntype testIterator struct {\n\tvalues []byte\n}\n\nfunc newTestIterator(values ...byte) *testIterator {\n\treturn &testIterator{values}\n}\n\nfunc (ti *testIterator) Seek(common.Hash) {\n\tpanic(\"implement me\")\n}\n\nfunc (ti *testIterator) Next() bool {\n\tti.values = ti.values[1:]\n\treturn len(ti.values) > 0\n}\n\nfunc (ti *testIterator) Error() error {\n\treturn nil\n}\n\nfunc (ti *testIterator) Hash() common.Hash {\n\treturn common.BytesToHash([]byte{ti.values[0]})\n}\n\nfunc (ti *testIterator) Account() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Slot() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Release() {}\n\nfunc TestFastIteratorBasics(t *testing.T) {\n\ttype testCase struct {\n\t\tlists   [][]byte\n\t\texpKeys []byte\n\t}\n\tfor i, tc := range []testCase{\n\t\t{lists: [][]byte{{0, 1, 8}, {1, 2, 8}, {2, 9}, {4},\n\t\t\t{7, 14, 15}, {9, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 4, 7, 8, 9, 13, 14, 15, 16}},\n\t\t{lists: [][]byte{{0, 8}, {1, 2, 8}, {7, 14, 15}, {8, 9},\n\t\t\t{9, 10}, {10, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 7, 8, 9, 10, 13, 14, 15, 16}},\n\t} {\n\t\tvar iterators []*weightedIterator\n\t\tfor i, data := range tc.lists {\n\t\t\tit := newTestIterator(data...)\n\t\t\titerators = append(iterators, &weightedIterator{it, i})\n\t\t}\n\t\tfi := &fastIterator{\n\t\t\titerators: iterators,\n\t\t\tinitiated: false,\n\t\t}\n\t\tcount := 0\n\t\tfor fi.Next() {\n\t\t\tif got, exp := fi.Hash()[31], tc.expKeys[count]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/context.go",
          "line": 186,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= count\n\tsnapStorageCleanCounter.Inc(time.Since(start).Nanoseconds())\n}\n\n// removeStorageAt deletes all storage entries which are located in the specified\n// account. When the iterator touches the storage entry which is outside the given\n// account, it stops and holds the current iterated element locally. An error will\n// be returned if the initial position of iterator is not in the given account.\nfunc (ctx *generatorContext) removeStorageAt(account common.Hash) error {\n\tvar (\n\t\tcount int64\n\t\tstart = time.Now()\n\t\titer  = ctx.storage\n\t)\n\tfor iter.Next() {\n\t\tkey := iter.Key()\n\t\tcmp := bytes.Compare(key[1:1+common.HashLength], account.Bytes())\n\t\tif cmp < 0 {\n\t\t\treturn errors.New(\"invalid iterator position\")\n\t\t}\n\t\tif cmp > 0 {\n\t\t\titer.Hold()\n\t\t\tbreak\n\t\t}\n\t\tcount++\n\t\tctx.batch.Delete(key)\n\t\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\tctx.batch.Write()\n\t\t\tctx.batch.Reset()\n\t\t}\n\t}\n\tsnapWipedStorageMeter.Mark(count)\n\tsnapStorageCleanCounter.Inc(time.Since(start).Nanoseconds())\n\treturn nil\n}\n\n// removeStorageLeft deletes all storage entries which are located after\n// the current iterator position.\nfunc (ctx *generatorContext) removeStorageLeft() {\n\tvar (\n\t\tcount uint64\n\t\tstart = time.Now()\n\t\titer  = ctx.storage\n\t)\n\tfor iter.Next() {\n\t\tcount++\n\t\tctx.batch.Delete(iter.Key())\n\t\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\tctx.batch.Write()\n\t\t\tctx.batch.Reset()\n\t\t}\n\t}\n\tctx.stats.dangling += count\n\tsnapDanglingStorageMeter.Mark(int64(count))\n\tsnapStorageCleanCounter.Inc(time.Since(start).Nanoseconds())\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/disklayer_test.go",
          "line": 535,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tacc := common.Hash{byte(i)}\n\t\trawdb.WriteAccountSnapshot(db, acc, acc[:])\n\t}\n\t// Add an 'higher' key, with incorrect (higher) prefix\n\thighKey := []byte{rawdb.SnapshotAccountPrefix[0] + 1}\n\tdb.Put(highKey, []byte{0xff, 0xff})\n\n\tbaseRoot := randomHash()\n\trawdb.WriteSnapshotRoot(db, baseRoot)\n\n\tsnaps := &Tree{\n\t\tlayers: map[common.Hash]snapshot{\n\t\t\tbaseRoot: &diskLayer{\n\t\t\t\tdiskdb: db,\n\t\t\t\tcache:  fastcache.New(500 * 1024),\n\t\t\t\troot:   baseRoot,\n\t\t\t},\n\t\t},\n\t}\n\t// Test some different seek positions\n\ttype testcase struct {\n\t\tpos    byte\n\t\texpkey byte\n\t}\n\tvar cases = []testcase{\n\t\t{0xff, 0x55}, // this should exit immediately without checking key\n\t\t{0x01, 0x02},\n\t\t{0xfe, 0xfe},\n\t\t{0xfd, 0xfe},\n\t\t{0x00, 0x00},\n\t}\n\tfor i, tc := range cases {\n\t\tit, err := snaps.AccountIterator(baseRoot, common.Hash{tc.pos})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"case %d, error: %v\", i, err)\n\t\t}\n\t\tcount := 0\n\t\tfor it.Next() {\n\t\t\tk, v, err := it.Hash()[0], it.Account()[0], it.Error()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"test %d, item %d, error: %v\", i, count, err)\n\t\t\t}\n\t\t\t// First item in iterator should have the expected key\n\t\t\tif count == 0 && k != tc.expkey {\n\t\t\t\tt.Fatalf(\"test %d, item %d, got %v exp %v\", i, count, k, tc.expkey)\n\t\t\t}\n\t\t\tcount++\n\t\t\tif v != k {\n\t\t\t\tt.Fatalf(\"test %d, item %d, value wrong, got %v exp %v\", i, count, v, k)\n\t\t\t}\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/snapshot/snapshot.go",
          "line": 868,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(layer.memory)\n\t\t}\n\t}\n\treturn size, 0, 0\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/snapshot/snapshot.go",
          "line": 330,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\tif limits == 0 {\n\t\t\tbreak\n\t\t}\n\t\tparent := layer.Parent()\n\t\tif parent == nil {\n\t\t\tbreak\n\t\t}\n\t\tlayer = parent\n\t}\n\treturn ret\n}\n\n// Update adds a new snapshot into the tree, if that can be linked to an existing\n// old parent. It is disallowed to insert a disk layer (the origin of all).\nfunc (t *Tree) Update(blockRoot common.Hash, parentRoot common.Hash, accounts map[common.Hash][]byte, storage map[common.Hash]map[common.Hash][]byte) error {\n\t// Reject noop updates to avoid self-loops in the snapshot tree. This is a\n\t// special case that can only happen for Clique networks where empty blocks\n\t// don't modify the state (0 block subsidy).\n\t//\n\t// Although we could silently ignore this internally, it should be the caller's\n\t// responsibility to avoid even attempting to insert such a snapshot.\n\tif blockRoot == parentRoot {\n\t\treturn errSnapshotCycle\n\t}\n\t// Generate a new snapshot on top of the parent\n\tparent := t.Snapshot(parentRoot)\n\tif parent == nil {\n\t\treturn fmt.Errorf(\"parent [%#x] snapshot missing\", parentRoot)\n\t}\n\tsnap := parent.(snapshot).Update(blockRoot, accounts, storage)\n\n\t// Save the new snapshot for later\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tt.layers[snap.root] = snap\n\tlog.Debug(\"Snapshot updated\", \"blockRoot\", blockRoot)\n\treturn nil\n}\n\nfunc (t *Tree) CapLimit() int {\n\treturn t.capLimit\n}\n\n// Cap traverses downwards the snapshot tree from a head block hash until the\n// number of allowed layers are crossed. All layers beyond the permitted number\n// are flattened downwards.\n//\n// Note, the final diff layer count in general will be one more than the amount\n// requested. This happens because the bottom-most diff layer is the accumulator\n// which may or may not overflow and cascade to disk. Since this last layer's\n// survival is only known *after* capping, we need to omit it from the count if\n// we want to ensure that *at least* the requested number of diff layers remain.\nfunc (t *Tree) Cap(root common.Hash, layers int) error {\n\t// Retrieve the head snapshot to cap from\n\tsnap := t.Snapshot(root)\n\tif snap == nil {\n\t\treturn fmt.Errorf(\"snapshot [%#x] missing\", root)\n\t}\n\tdiff, ok := snap.(*diffLayer)\n\tif !ok {\n\t\treturn fmt.Errorf(\"snapshot [%#x] is disk layer\", root)\n\t}\n\t// If the generator is still running, use a more aggressive cap\n\tdiff.origin.lock.RLock()\n\tif diff.origin.genMarker != nil && layers > 8 {\n\t\tlayers = 8\n\t}\n\tdiff.origin.lock.RUnlock()\n\n\t// Run the internal capping and discard all stale layers\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\t// Flattening the bottom-most diff layer requires special casing since there's\n\t// no child to rewire to the grandparent. In that case we can fake a temporary\n\t// child for the capping and then remove it.\n\tif layers == 0 {\n\t\t// If full commit was requested, flatten the diffs and merge onto disk\n\t\tdiff.lock.RLock()\n\t\tbase := diffToDisk(diff.flatten().(*diffLayer))\n\t\tdiff.lock.RUnlock()\n\n\t\t// Replace the entire snapshot tree with the flat base\n\t\tt.layers = map[common.Hash]snapshot{base.root: base}\n\t\treturn nil\n\t}\n\tpersisted := t.cap(diff, layers)\n\n\t// Remove any layer that is stale or links into a stale layer\n\tchildren := make(map[common.Hash][]common.Hash)\n\tfor root, snap := range t.layers {\n\t\tif diff, ok := snap.(*diffLayer)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/state/pruner/pruner.go",
          "line": 161,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tcount += 1\n\t\t\tsize += common.StorageSize(len(key) + len(iter.Value()))\n\t\t\tbatch.Delete(key)\n\n\t\t\tvar eta time.Duration // Realistically will never remain uninited\n\t\t\tif done := binary.BigEndian.Uint64(key[:8])",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/state/pruner/pruner.go",
          "line": 225,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 0x10 {\n\t\t\tvar (\n\t\t\t\tstart = []byte{byte(b)}\n\t\t\t\tend   = []byte{byte(b + 0x10)}\n\t\t\t)\n\t\t\tif b == 0xf0 {\n\t\t\t\tend = nil\n\t\t\t}\n\t\t\tlog.Info(\"Compacting database\", \"range\", fmt.Sprintf(\"%#x-%#x\", start, end), \"elapsed\", common.PrettyDuration(time.Since(cstart)))\n\t\t\tif err := pruneDB.Compact(start, end)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/blobpool/slotter.go",
          "line": 33,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= blobSize\n\t\tfinished := slotsize > uint32(maxBlobsPerTransaction)*blobSize+txMaxSize\n\n\t\treturn slotsize, finished\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/txpool/blobpool/slotter.go",
          "line": 30,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint32(blobSize) // underflows, it's ok, will overflow back in the first return\n\n\treturn func() (size uint32, done bool) {\n\t\tslotsize += blobSize\n\t\tfinished := slotsize > uint32(maxBlobsPerTransaction)*blobSize+txMaxSize\n\n\t\treturn slotsize, finished\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 523,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize)\n\treturn nil\n}\n\n// recheck verifies the pool's content for a specific account and drops anything\n// that does not fit anymore (dangling or filled nonce, overdraft).\nfunc (p *BlobPool) recheck(addr common.Address, inclusions map[common.Hash]uint64) {\n\t// Sort the transactions belonging to the account so reinjects can be simpler\n\ttxs := p.index[addr]\n\tif inclusions != nil && txs == nil { // during reorgs, we might find new accounts\n\t\treturn\n\t}\n\tsort.Slice(txs, func(i, j int) bool {\n\t\treturn txs[i].nonce < txs[j].nonce\n\t})\n\t// If there is a gap between the chain state and the blob pool, drop\n\t// all the transactions as they are non-executable. Similarly, if the\n\t// entire tx range was included, drop all.\n\tvar (\n\t\tnext   = p.state.GetNonce(addr)\n\t\tgapped = txs[0].nonce > next\n\t\tfilled = txs[len(txs)-1].nonce < next\n\t)\n\tif gapped || filled {\n\t\tvar (\n\t\t\tids    []uint64\n\t\t\tnonces []uint64\n\t\t)\n\t\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1032,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize)\n\treturn nil\n}\n\n// SetGasTip implements txpool.SubPool, allowing the blob pool's gas requirements\n// to be kept in sync with the main transaction pool's gas requirements.\nfunc (p *BlobPool) SetGasTip(tip *big.Int) {\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\t// Store the new minimum gas tip\n\told := p.gasTip\n\tp.gasTip = uint256.MustFromBig(tip)\n\n\t// If the min miner fee increased, remove transactions below the new threshold\n\tif old == nil || p.gasTip.Cmp(old) > 0 {\n\t\tfor addr, txs := range p.index {\n\t\t\tfor i, tx := range txs {\n\t\t\t\tif tx.execTipCap.Cmp(p.gasTip) < 0 {\n\t\t\t\t\t// Drop the offending transaction\n\t\t\t\t\tvar (\n\t\t\t\t\t\tids    = []uint64{tx.id}\n\t\t\t\t\t\tnonces = []uint64{tx.nonce}\n\t\t\t\t\t)\n\t\t\t\t\tp.spent[addr] = new(uint256.Int).Sub(p.spent[addr], txs[i].costCap)\n\t\t\t\t\tp.stored -= uint64(tx.storageSize)\n\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\ttxs[i] = nil\n\n\t\t\t\t\t// Drop everything afterwards, no gaps allowed\n\t\t\t\t\tfor j, tx := range txs[i+1:] {\n\t\t\t\t\t\tids = append(ids, tx.id)\n\t\t\t\t\t\tnonces = append(nonces, tx.nonce)\n\n\t\t\t\t\t\tp.spent[addr] = new(uint256.Int).Sub(p.spent[addr], tx.costCap)\n\t\t\t\t\t\tp.stored -= uint64(tx.storageSize)\n\t\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\t\ttxs[i+1+j] = nil\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the dropped transactions from the index\n\t\t\t\t\tif i > 0 {\n\t\t\t\t\t\tp.index[addr] = txs[:i]\n\t\t\t\t\t\theap.Fix(p.evict, p.evict.index[addr])\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdelete(p.index, addr)\n\t\t\t\t\t\tdelete(p.spent, addr)\n\n\t\t\t\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t\t\t\t\tp.reserver.Release(addr)\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the transactions from the data store\n\t\t\t\t\tlog.Warn(\"Dropping underpriced blob transaction\", \"from\", addr, \"rejected\", tx.nonce, \"tip\", tx.execTipCap, \"want\", tip, \"drop\", nonces, \"ids\", ids)\n\t\t\t\t\tdropUnderpricedMeter.Mark(int64(len(ids)))\n\n\t\t\t\t\tfor _, id := range ids {\n\t\t\t\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1486,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize) - uint64(prev.storageSize)\n\t} else {\n\t\t// Transaction extends previously scheduled ones\n\t\tp.index[from] = append(p.index[from], meta)\n\t\tif _, ok := p.spent[from]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1496,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize)\n\t}\n\t// Recompute the rolling eviction fields. In case of a replacement, this will\n\t// recompute all subsequent fields. In case of an append, this will only do\n\t// the fresh calculation.\n\ttxs := p.index[from]\n\n\tfor i := offset",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1694,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= slotDataused\n\t\tdatareal += slotDataused + slotDatagaps\n\t\tslotused += shelf.FilledSlots\n\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfDatausedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDataused))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfDatagapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDatagaps))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfSlotusedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.FilledSlots))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfSlotgapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.GappedSlots))\n\n\t\tmaxBlobs := eip4844.LatestMaxBlobsPerBlock(p.chain.Config())\n\t\tif shelf.SlotSize/blobSize > uint32(maxBlobs) {\n\t\t\toversizedDataused += slotDataused\n\t\t\toversizedDatagaps += slotDatagaps\n\t\t\toversizedSlotused += shelf.FilledSlots\n\t\t\toversizedSlotgaps += shelf.GappedSlots\n\t\t}\n\t}\n\tdatausedGauge.Update(int64(dataused))\n\tdatarealGauge.Update(int64(datareal))\n\tslotusedGauge.Update(int64(slotused))\n\n\toversizedDatausedGauge.Update(int64(oversizedDataused))\n\toversizedDatagapsGauge.Update(int64(oversizedDatagaps))\n\toversizedSlotusedGauge.Update(int64(oversizedSlotused))\n\toversizedSlotgapsGauge.Update(int64(oversizedSlotgaps))\n\n\tp.updateLimboMetrics()\n}\n\n// updateLimboMetrics retrieves a bunch of stats from the limbo store and pushes\n// them out as metrics.\nfunc (p *BlobPool) updateLimboMetrics() {\n\tstats := p.limbo.store.Infos()\n\n\tvar (\n\t\tdataused uint64\n\t\tdatareal uint64\n\t\tslotused uint64\n\t)\n\tfor _, shelf := range stats.Shelves {\n\t\tslotDataused := shelf.FilledSlots * uint64(shelf.SlotSize)\n\t\tslotDatagaps := shelf.GappedSlots * uint64(shelf.SlotSize)\n\n\t\tdataused += slotDataused\n\t\tdatareal += slotDataused + slotDatagaps\n\t\tslotused += shelf.FilledSlots\n\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfDatausedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDataused))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfDatagapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDatagaps))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfSlotusedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.FilledSlots))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfSlotgapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.GappedSlots))\n\t}\n\tlimboDatausedGauge.Update(int64(dataused))\n\tlimboDatarealGauge.Update(int64(datareal))\n\tlimboSlotusedGauge.Update(int64(slotused))\n}\n\n// SubscribeTransactions registers a subscription for new transaction events,\n// supporting feeding only newly seen or also resurrected transactions.\nfunc (p *BlobPool) SubscribeTransactions(ch chan<- core.NewTxsEvent, reorgs bool) event.Subscription {\n\tif reorgs {\n\t\treturn p.insertFeed.Subscribe(ch)\n\t} else {\n\t\treturn p.discoverFeed.Subscribe(ch)\n\t}\n}\n\n// SubscribeReannoTxsEvent registers a subscription of ReannoTxsEvent and\n// starts sending event to the given channel.\nfunc (p *BlobPool) SubscribeReannoTxsEvent(ch chan<- core.ReannoTxsEvent) event.Subscription {\n\treturn p.scope.Track(p.reannoTxFeed.Subscribe(ch))\n}\n\n// Nonce returns the next nonce of an account, with all transactions executable\n// by the pool already applied on top.\nfunc (p *BlobPool) Nonce(addr common.Address) uint64 {\n\t// We need a write lock here, since state.GetNonce might write the cache.\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\tif txs, ok := p.index[addr]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1788,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(txs)\n\t}\n\treturn pending, 0 // No non-executable txs in the blob pool\n}\n\n// Content retrieves the data content of the transaction pool, returning all the\n// pending as well as queued transactions, grouped by account and sorted by nonce.\n//\n// For the blob pool, this method will return nothing for now.\n// TODO(karalabe): Abstract out the returned metadata.\nfunc (p *BlobPool) Content() (map[common.Address][]*types.Transaction, map[common.Address][]*types.Transaction) {\n\treturn make(map[common.Address][]*types.Transaction), make(map[common.Address][]*types.Transaction)\n}\n\n// ContentFrom retrieves the data content of the transaction pool, returning the\n// pending as well as queued transactions of this address, grouped by nonce.\n//\n// For the blob pool, this method will return nothing for now.\n// TODO(karalabe): Abstract out the returned metadata.\nfunc (p *BlobPool) ContentFrom(addr common.Address) ([]*types.Transaction, []*types.Transaction) {\n\treturn []*types.Transaction{}, []*types.Transaction{}\n}\n\n// Status returns the known status (unknown/pending/queued) of a transaction\n// identified by their hashes.\nfunc (p *BlobPool) Status(hash common.Hash) txpool.TxStatus {\n\tif p.Has(hash) {\n\t\treturn txpool.TxStatusPending\n\t}\n\treturn txpool.TxStatusUnknown\n}\n\nfunc (p *BlobPool) SetMaxGas(maxGas uint64) {\n\tp.maxGas.Store(maxGas)\n}\n\nfunc (p *BlobPool) GetMaxGas() uint64 {\n\treturn p.maxGas.Load()\n}\n\n// Clear implements txpool.SubPool, removing all tracked transactions\n// from the blob pool and persistent store.\n//\n// Note, do not use this in production / live code. In live code, the pool is\n// meant to reset on a separate thread to avoid DoS vectors.\nfunc (p *BlobPool) Clear() {\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\t// manually iterating and deleting every entry is super sub-optimal\n\t// However, Clear is not currently used in production so\n\t// performance is not critical at the moment.\n\tfor hash := range p.lookup.txIndex {\n\t\tid, _ := p.lookup.storeidOfTx(hash)\n\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 555,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[i].storageSize)\n\t\t\tp.lookup.untrack(txs[i])\n\n\t\t\t// Included transactions blobs need to be moved to the limbo\n\t\t\tif filled && inclusions != nil {\n\t\t\t\tp.offload(addr, txs[i].nonce, txs[i].id, inclusions)\n\t\t\t}\n\t\t}\n\t\tdelete(p.index, addr)\n\t\tdelete(p.spent, addr)\n\t\tif inclusions != nil { // only during reorgs will the heap be initialized\n\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t}\n\t\tp.reserver.Release(addr)\n\n\t\tif gapped {\n\t\t\tlog.Warn(\"Dropping dangling blob transactions\", \"from\", addr, \"missing\", next, \"drop\", nonces, \"ids\", ids)\n\t\t\tdropDanglingMeter.Mark(int64(len(ids)))\n\t\t} else {\n\t\t\tlog.Trace(\"Dropping filled blob transactions\", \"from\", addr, \"filled\", nonces, \"ids\", ids)\n\t\t\tdropFilledMeter.Mark(int64(len(ids)))\n\t\t}\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 596,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[0].storageSize)\n\t\t\tp.lookup.untrack(txs[0])\n\n\t\t\t// Included transactions blobs need to be moved to the limbo\n\t\t\tif inclusions != nil {\n\t\t\t\tp.offload(addr, txs[0].nonce, txs[0].id, inclusions)\n\t\t\t}\n\t\t\ttxs = txs[1:]\n\t\t}\n\t\tlog.Trace(\"Dropping overlapped blob transactions\", \"from\", addr, \"overlapped\", nonces, \"ids\", ids, \"left\", len(txs))\n\t\tdropOverlappedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 652,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[i].storageSize)\n\t\t\tp.lookup.untrack(txs[i])\n\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 674,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[j].storageSize)\n\t\t\tp.lookup.untrack(txs[j])\n\t\t}\n\t\ttxs = txs[:i]\n\n\t\tlog.Error(\"Dropping gapped blob transactions\", \"from\", addr, \"missing\", txs[i-1].nonce+1, \"drop\", nonces, \"ids\", ids)\n\t\tdropGappedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 712,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(last.storageSize)\n\t\t\tp.lookup.untrack(last)\n\t\t}\n\t\tif len(txs) == 0 {\n\t\t\tdelete(p.index, addr)\n\t\t\tdelete(p.spent, addr)\n\t\t\tif inclusions != nil { // only during reorgs will the heap be initialized\n\t\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t\t}\n\t\t\tp.reserver.Release(addr)\n\t\t} else {\n\t\t\tp.index[addr] = txs\n\t\t}\n\t\tlog.Warn(\"Dropping overdrafted blob transactions\", \"from\", addr, \"balance\", balance, \"spent\", spent, \"drop\", nonces, \"ids\", ids)\n\t\tdropOverdraftedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 752,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(last.storageSize)\n\t\t\tp.lookup.untrack(last)\n\t\t}\n\t\tp.index[addr] = txs\n\n\t\tlog.Warn(\"Dropping overcapped blob transactions\", \"from\", addr, \"kept\", len(txs), \"drop\", nonces, \"ids\", ids)\n\t\tdropOvercappedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0012",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1057,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(tx.storageSize)\n\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\ttxs[i] = nil\n\n\t\t\t\t\t// Drop everything afterwards, no gaps allowed\n\t\t\t\t\tfor j, tx := range txs[i+1:] {\n\t\t\t\t\t\tids = append(ids, tx.id)\n\t\t\t\t\t\tnonces = append(nonces, tx.nonce)\n\n\t\t\t\t\t\tp.spent[addr] = new(uint256.Int).Sub(p.spent[addr], tx.costCap)\n\t\t\t\t\t\tp.stored -= uint64(tx.storageSize)\n\t\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\t\ttxs[i+1+j] = nil\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the dropped transactions from the index\n\t\t\t\t\tif i > 0 {\n\t\t\t\t\t\tp.index[addr] = txs[:i]\n\t\t\t\t\t\theap.Fix(p.evict, p.evict.index[addr])\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdelete(p.index, addr)\n\t\t\t\t\t\tdelete(p.spent, addr)\n\n\t\t\t\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t\t\t\t\tp.reserver.Release(addr)\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the transactions from the data store\n\t\t\t\t\tlog.Warn(\"Dropping underpriced blob transaction\", \"from\", addr, \"rejected\", tx.nonce, \"tip\", tx.execTipCap, \"want\", tip, \"drop\", nonces, \"ids\", ids)\n\t\t\t\t\tdropUnderpricedMeter.Mark(int64(len(ids)))\n\n\t\t\t\t\tfor _, id := range ids {\n\t\t\t\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0013",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1583,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(drop.storageSize)\n\tp.lookup.untrack(drop)\n\n\t// Remove the transaction from the pool's eviction heap:\n\t//   - If the entire account was dropped, pop off the address\n\t//   - Otherwise, if the new tail has better eviction caps, fix the heap\n\tif last {\n\t\theap.Pop(p.evict)\n\t} else {\n\t\ttail := txs[len(txs)-1] // new tail, surely exists\n\n\t\tevictionExecFeeDiff := tail.evictionExecFeeJumps - drop.evictionExecFeeJumps\n\t\tevictionBlobFeeDiff := tail.evictionBlobFeeJumps - drop.evictionBlobFeeJumps\n\n\t\tif evictionExecFeeDiff > 0.001 || evictionBlobFeeDiff > 0.001 { // no need for math.Abs, monotonic decreasing\n\t\t\theap.Fix(p.evict, 0)\n\t\t}\n\t}\n\t// Remove the transaction from the data store\n\tlog.Debug(\"Evicting overflown blob transaction\", \"from\", from, \"evicted\", drop.nonce, \"id\", drop.id)\n\tdropOverflownMeter.Mark(1)\n\n\tif err := p.store.Delete(drop.id)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0014",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 838,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0015",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1383,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0016",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1384,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/blobpool/blobpool_test.go",
          "line": 390,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(tx.storageSize)\n\t\t}\n\t}\n\tif pool.stored != stored {\n\t\tt.Errorf(\"pool storage mismatch: have %d, want %d\", pool.stored, stored)\n\t}\n\t// Verify the price heap internals\n\tverifyHeapInternals(t, pool.evict)\n\n\t// Verify that all the blobs can be retrieved\n\tverifyBlobRetrievals(t, pool)\n}\n\n// verifyBlobRetrievals attempts to retrieve all testing blobs and checks that\n// whatever is in the pool, it can be retrieved correctly.\nfunc verifyBlobRetrievals(t *testing.T, pool *BlobPool) {\n\t// Collect all the blobs tracked by the pool\n\tknown := make(map[common.Hash]struct{})\n\tfor _, txs := range pool.index {\n\t\tfor _, tx := range txs {\n\t\t\tfor _, vhash := range tx.vhashes {\n\t\t\t\tknown[vhash] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\t// Attempt to retrieve all test blobs\n\thashes := make([]common.Hash, len(testBlobVHashes))\n\tfor i := range testBlobVHashes {\n\t\tcopy(hashes[i][:], testBlobVHashes[i][:])\n\t}\n\tsidecars := pool.GetBlobs(hashes)\n\tvar blobs []*kzg4844.Blob\n\tvar proofs []*kzg4844.Proof\n\tfor idx, sidecar := range sidecars {\n\t\tif sidecar == nil {\n\t\t\tblobs = append(blobs, nil)\n\t\t\tproofs = append(proofs, nil)\n\t\t\tcontinue\n\t\t}\n\t\tblobHashes := sidecar.BlobHashes()\n\t\tfor i, hash := range blobHashes {\n\t\t\tif hash == hashes[idx] {\n\t\t\t\tblobs = append(blobs, &sidecar.Blobs[i])\n\t\t\t\tproofs = append(proofs, &sidecar.Proofs[i])\n\t\t\t}\n\t\t}\n\t}\n\t// Cross validate what we received vs what we wanted\n\tif len(blobs) != len(hashes) || len(proofs) != len(hashes) {\n\t\tt.Errorf(\"retrieved blobs/proofs size mismatch: have %d/%d, want %d\", len(blobs), len(proofs), len(hashes))\n\t\treturn\n\t}\n\tfor i, hash := range hashes {\n\t\t// If an item is missing, but shouldn't, error\n\t\tif blobs[i] == nil || proofs[i] == nil {\n\t\t\tif _, ok := known[hash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/legacypool/list.go",
          "line": 663,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= numSlots(tx)\n\t\t}\n\t}\n\t// If we still can't make enough room for the new transaction\n\tif slots > 0 {\n\t\tfor _, tx := range drop {\n\t\t\theap.Push(&l.urgent, tx)\n\t\t}\n\t\treturn nil, false\n\t}\n\treturn drop, true\n}\n\n// Reheap forcibly rebuilds the heap based on the current remote transaction set.\nfunc (l *pricedList) Reheap() {\n\tl.reheapMu.Lock()\n\tdefer l.reheapMu.Unlock()\n\tstart := time.Now()\n\tl.stales.Store(0)\n\tl.urgent.list = make([]*types.Transaction, 0, l.all.Count())\n\tl.all.Range(func(hash common.Hash, tx *types.Transaction) bool {\n\t\tl.urgent.list = append(l.urgent.list, tx)\n\t\treturn true\n\t})\n\theap.Init(&l.urgent)\n\n\t// balance out the two heaps by moving the worse half of transactions into the\n\t// floating heap\n\t// Note: Discard would also do this before the first eviction but Reheap can do\n\t// is more efficiently. Also, Underpriced would work suboptimally the first time\n\t// if the floating queue was empty.\n\tfloatingCount := len(l.urgent.list) * floatingRatio / (urgentRatio + floatingRatio)\n\tl.floating.list = make([]*types.Transaction, floatingCount)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 123,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= txSlots\n\tOverflowPoolGauge.Inc(1)\n\n\treturn true\n}\n\nfunc (tp *TxOverflowPool) Get(hash common.Hash) (*types.Transaction, bool) {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\tif item, ok := tp.index[hash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 112,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(numSlots(oldestItem.tx))\n\t\tOverflowPoolGauge.Dec(1)\n\t}\n\n\t// Add the new transaction\n\titem := &txHeapItem{\n\t\ttx:        tx,\n\t\ttimestamp: time.Now().UnixNano(),\n\t}\n\theap.Push(&tp.txHeap, item)\n\ttp.index[tx.Hash()] = item\n\ttp.totalSize += txSlots\n\tOverflowPoolGauge.Inc(1)\n\n\treturn true\n}\n\nfunc (tp *TxOverflowPool) Get(hash common.Hash) (*types.Transaction, bool) {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\tif item, ok := tp.index[hash]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 144,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(numSlots(item.tx))\n\t\tOverflowPoolGauge.Dec(1)\n\t}\n}\n\nfunc (tp *TxOverflowPool) Flush(n int) []*types.Transaction {\n\ttp.mu.Lock()\n\tdefer tp.mu.Unlock()\n\tif n > tp.txHeap.Len() {\n\t\tn = tp.txHeap.Len()\n\t}\n\ttxs := make([]*types.Transaction, n)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 163,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(numSlots(item.tx))\n\t}\n\n\tOverflowPoolGauge.Dec(int64(n))\n\treturn txs\n}\n\nfunc (tp *TxOverflowPool) Len() int {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\treturn tp.txHeap.Len()\n}\n\nfunc (tp *TxOverflowPool) Size() uint64 {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\treturn tp.totalSize\n}\n\nfunc (tp *TxOverflowPool) PrintTxStats() {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\tfor _, item := range tp.txHeap {\n\t\ttx := item.tx\n\t\tfmt.Printf(\"Hash: %s, Timestamp: %d, GasFeeCap: %s, GasTipCap: %s\\n\",\n\t\t\ttx.Hash().String(), item.timestamp, tx.GasFeeCap().String(), tx.GasTipCap().String())\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1028,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= list.Len()\n\t}\n\tif queued > int(config.GlobalQueue) {\n\t\tt.Fatalf(\"total transactions overflow allowance: %d > %d\", queued, config.GlobalQueue)\n\t}\n}\n\n// Tests that if an account remains idle for a prolonged amount of time, any\n// non-executable transactions queued up are dropped to prevent wasting resources\n// on shuffling them around.\nfunc TestQueueTimeLimiting(t *testing.T) {\n\t// Reduce the eviction interval to a testable amount\n\tdefer func(old time.Duration) { evictionInterval = old }(evictionInterval)\n\tevictionInterval = time.Millisecond * 100\n\n\t// Create the pool to test the non-expiration enforcement\n\tstatedb, _ := state.New(types.EmptyRootHash, state.NewDatabaseForTesting())\n\tblockchain := newTestBlockChain(params.TestChainConfig, 1000000, statedb, new(event.Feed))\n\n\tconfig := testTxPoolConfig\n\tconfig.Lifetime = time.Second\n\n\tpool := New(config, blockchain)\n\tpool.Init(config.PriceLimit, blockchain.CurrentBlock(), newReserver())\n\tdefer pool.Close()\n\n\t// Create a test account to ensure remotes expire\n\tremote, _ := crypto.GenerateKey()\n\n\ttestAddBalance(pool, crypto.PubkeyToAddress(remote.PublicKey), big.NewInt(1000000000))\n\n\t// Add the transaction and ensure it is queued up\n\tif err := pool.addRemote(pricedTransaction(1, 100000, big.NewInt(1), remote))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= list.Len()\n\t}\n\tif pending > int(config.GlobalSlots) {\n\t\tt.Fatalf(\"total pending transactions overflow allowance: %d > %d\", pending, config.GlobalSlots)\n\t}\n\tif err := validatePoolInternals(pool)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1926,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 100 {\n\t\tpool.priced.SetBaseFee(big.NewInt(int64(baseFee)))\n\t\tadd(true)\n\t\tcheck(highCap, \"fee cap\")\n\t\tadd(false)\n\t\tcheck(highTip, \"effective tip\")\n\t}\n\n\tif err := validatePoolInternals(pool)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1961,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tfirsts = append(firsts, txs[i])\n\t}\n\terrs := pool.addRemotesSync(firsts)\n\tif len(errs) != len(firsts) {\n\t\tt.Fatalf(\"first add mismatching result count: have %d, want %d\", len(errs), len(firsts))\n\t}\n\tfor i, err := range errs {\n\t\tif err != nil {\n\t\t\tt.Errorf(\"add %d failed: %v\", i, err)\n\t\t}\n\t}\n\tpending, queued := pool.Stats()\n\tif pending != 1 {\n\t\tt.Fatalf(\"pending transactions mismatched: have %d, want %d\", pending, 1)\n\t}\n\tif queued != len(txs)/2-1 {\n\t\tt.Fatalf(\"queued transactions mismatched: have %d, want %d\", queued, len(txs)/2-1)\n\t}\n\t// Try to add all of them now and ensure previous ones error out as knowns\n\terrs = pool.addRemotesSync(txs)\n\tif len(errs) != len(txs) {\n\t\tt.Fatalf(\"all add mismatching result count: have %d, want %d\", len(errs), len(txs))\n\t}\n\tfor i, err := range errs {\n\t\tif i%2 == 0 && err == nil {\n\t\t\tt.Errorf(\"add %d succeeded, should have failed as known\", i)\n\t\t}\n\t\tif i%2 == 1 && err != nil {\n\t\t\tt.Errorf(\"add %d failed: %v\", i, err)\n\t\t}\n\t}\n\tpending, queued = pool.Stats()\n\tif pending != len(txs) {\n\t\tt.Fatalf(\"pending transactions mismatched: have %d, want %d\", pending, len(txs))\n\t}\n\tif queued != 0 {\n\t\tt.Fatalf(\"queued transactions mismatched: have %d, want %d\", queued, 0)\n\t}\n\tif err := validatePoolInternals(pool)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 2628,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 3 {\n\t\t\t\t\tif err := pool.addRemoteSync(pricedSetCodeTx(0, 250000, uint256.NewInt(10), uint256.NewInt(3), keys[i], []unsignedAuth{{0, keys[i]}, {0, keys[i+1]}, {0, keys[i+2]}}))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 522,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= list.Len()\n\t}\n\tqueued := 0\n\tfor _, list := range pool.queue {\n\t\tqueued += list.Len()\n\t}\n\treturn pending, queued\n}\n\n// Content retrieves the data content of the transaction pool, returning all the\n// pending as well as queued transactions, grouped by account and sorted by nonce.\nfunc (pool *LegacyPool) Content() (map[common.Address][]*types.Transaction, map[common.Address][]*types.Transaction) {\n\tpool.mu.Lock()\n\tdefer pool.mu.Unlock()\n\n\tpending := make(map[common.Address][]*types.Transaction, len(pool.pending))\n\tfor addr, list := range pool.pending {\n\t\tpending[addr] = list.Flatten()\n\t}\n\tqueued := make(map[common.Address][]*types.Transaction, len(pool.queue))\n\tfor addr, list := range pool.queue {\n\t\tqueued[addr] = list.Flatten()\n\t}\n\treturn pending, queued\n}\n\n// ContentFrom retrieves the data content of the transaction pool, returning the\n// pending as well as queued transactions of this address, grouped by nonce.\nfunc (pool *LegacyPool) ContentFrom(addr common.Address) ([]*types.Transaction, []*types.Transaction) {\n\tpool.mu.RLock()\n\tdefer pool.mu.RUnlock()\n\n\tvar pending []*types.Transaction\n\tif list, ok := pool.pending[addr]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 733,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= pending.Len()\n\t\t\t}\n\t\t\tif queue := pool.queue[auth]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 736,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= queue.Len()\n\t\t\t}\n\t\t\tif count > 1 {\n\t\t\t\treturn ErrAuthorityReserved\n\t\t\t}\n\t\t\t// Because there is no exclusive lock held between different subpools\n\t\t\t// when processing transactions, the SetCode transaction may be accepted\n\t\t\t// while other transactions with the same sender address are also\n\t\t\t// accepted simultaneously in the other pools.\n\t\t\t//\n\t\t\t// This scenario is considered acceptable, as the rule primarily ensures\n\t\t\t// that attackers cannot easily stack a SetCode transaction when the sender\n\t\t\t// is reserved by other pools.\n\t\t\tif pool.reserver.Has(auth) {\n\t\t\t\treturn ErrAuthorityReserved\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// add validates a transaction and inserts it into the non-executable queue for later\n// pending promotion and execution. If the transaction is a replacement for an already\n// pending or queued one, it overwrites the previous transaction if its price is higher.\nfunc (pool *LegacyPool) add(tx *types.Transaction) (replaced bool, err error) {\n\t// If the transaction is already known, discard it\n\thash := tx.Hash()\n\tif pool.all.Get(hash) != nil {\n\t\tlog.Trace(\"Discarding already known transaction\", \"hash\", hash)\n\t\tknownTxMeter.Mark(1)\n\t\treturn false, txpool.ErrAlreadyKnown\n\t}\n\n\t// If the transaction fails basic validation, discard it\n\tif err := pool.validateTx(tx)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 859,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= dropped\n\t\t}\n\t}\n\n\t// Try to replace an existing transaction in the pending pool\n\tif list := pool.pending[from]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1598,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length\n\t\tif length > pool.config.AccountSlots {\n\t\t\tspammers.Push(addr, length)\n\t\t}\n\t}\n\tif pending <= pool.config.GlobalSlots {\n\t\treturn\n\t}\n\tpendingBeforeCap := pending\n\n\t// Gradually drop transactions from offenders\n\toffenders := []common.Address{}\n\tfor pending > pool.config.GlobalSlots && !spammers.Empty() {\n\t\t// Retrieve the next offender\n\t\toffender, _ := spammers.Pop()\n\t\toffenders = append(offenders, offender)\n\n\t\t// Equalize balances until all the same or below threshold\n\t\tif len(offenders) > 1 {\n\t\t\t// Calculate the equalization threshold for all current offenders\n\t\t\tthreshold := pool.pending[offender].Len()\n\n\t\t\t// Iteratively reduce all offenders until below limit or threshold reached\n\t\t\tfor pending > pool.config.GlobalSlots && pool.pending[offenders[len(offenders)-2]].Len() > threshold {\n\t\t\t\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1673,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(list.Len())\n\t}\n\tif queued <= pool.config.GlobalQueue {\n\t\treturn\n\t}\n\n\t// Sort all accounts with queued transactions by heartbeat\n\taddresses := make(addressesByHeartbeat, 0, len(pool.queue))\n\tfor addr := range pool.queue {\n\t\taddresses = append(addresses, addressByHeartbeat{addr, pool.beats[addr]})\n\t}\n\tsort.Sort(sort.Reverse(addresses))\n\n\t// Drop transactions until the total is below the limit\n\tfor drop := queued - pool.config.GlobalQueue",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1909,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= numSlots(tx)\n\tslotsGauge.Update(int64(t.slots))\n\n\tt.txs[tx.Hash()] = tx\n\tt.addAuthorities(tx)\n}\n\n// Remove removes a transaction from the lookup.\nfunc (t *lookup) Remove(hash common.Hash) {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\ttx, ok := t.txs[hash]\n\tif !ok {\n\t\tlog.Error(\"No transaction found to be deleted\", \"hash\", hash)\n\t\treturn\n\t}\n\tt.removeAuthorities(tx)\n\tt.slots -= numSlots(tx)\n\tslotsGauge.Update(int64(t.slots))\n\n\tdelete(t.txs, hash)\n}\n\n// Clear resets the lookup structure, removing all stored entries.\nfunc (t *lookup) Clear() {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tt.slots = 0\n\tt.txs = make(map[common.Hash]*types.Transaction)\n\tt.auths = make(map[common.Address][]common.Hash)\n}\n\n// TxsBelowTip finds all remote transactions below the given tip threshold.\nfunc (t *lookup) TxsBelowTip(threshold *big.Int) types.Transactions {\n\tfound := make(types.Transactions, 0, 128)\n\tt.Range(func(hash common.Hash, tx *types.Transaction) bool {\n\t\tif tx.GasTipCapIntCmp(threshold) < 0 {\n\t\t\tfound = append(found, tx)\n\t\t}\n\t\treturn true\n\t})\n\treturn found\n}\n\n// addAuthorities tracks the supplied tx in relation to each authority it\n// specifies.\nfunc (t *lookup) addAuthorities(tx *types.Transaction) {\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist, ok := t.auths[addr]\n\t\tif !ok {\n\t\t\tlist = []common.Hash{}\n\t\t}\n\t\tif slices.Contains(list, tx.Hash()) {\n\t\t\t// Don't add duplicates.\n\t\t\tcontinue\n\t\t}\n\t\tlist = append(list, tx.Hash())\n\t\tt.auths[addr] = list\n\t}\n}\n\n// removeAuthorities stops tracking the supplied tx in relation to its\n// authorities.\nfunc (t *lookup) removeAuthorities(tx *types.Transaction) {\n\thash := tx.Hash()\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist := t.auths[addr]\n\t\t// Remove tx from tracker.\n\t\tif i := slices.Index(list, hash)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1698,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= size\n\t\t\tqueuedRateLimitMeter.Mark(int64(size))\n\t\t\tcontinue\n\t\t}\n\t\t// Otherwise drop only last few transactions\n\t\ttxs := list.Flatten()\n\t\tfor i := len(txs) - 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1927,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= numSlots(tx)\n\tslotsGauge.Update(int64(t.slots))\n\n\tdelete(t.txs, hash)\n}\n\n// Clear resets the lookup structure, removing all stored entries.\nfunc (t *lookup) Clear() {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tt.slots = 0\n\tt.txs = make(map[common.Hash]*types.Transaction)\n\tt.auths = make(map[common.Address][]common.Hash)\n}\n\n// TxsBelowTip finds all remote transactions below the given tip threshold.\nfunc (t *lookup) TxsBelowTip(threshold *big.Int) types.Transactions {\n\tfound := make(types.Transactions, 0, 128)\n\tt.Range(func(hash common.Hash, tx *types.Transaction) bool {\n\t\tif tx.GasTipCapIntCmp(threshold) < 0 {\n\t\t\tfound = append(found, tx)\n\t\t}\n\t\treturn true\n\t})\n\treturn found\n}\n\n// addAuthorities tracks the supplied tx in relation to each authority it\n// specifies.\nfunc (t *lookup) addAuthorities(tx *types.Transaction) {\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist, ok := t.auths[addr]\n\t\tif !ok {\n\t\t\tlist = []common.Hash{}\n\t\t}\n\t\tif slices.Contains(list, tx.Hash()) {\n\t\t\t// Don't add duplicates.\n\t\t\tcontinue\n\t\t}\n\t\tlist = append(list, tx.Hash())\n\t\tt.auths[addr] = list\n\t}\n}\n\n// removeAuthorities stops tracking the supplied tx in relation to its\n// authorities.\nfunc (t *lookup) removeAuthorities(tx *types.Transaction) {\n\thash := tx.Hash()\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist := t.auths[addr]\n\t\t// Remove tx from tracker.\n\t\tif i := slices.Index(list, hash)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 426,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1420,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/locals/journal.go",
          "line": 154,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(txs)\n\t}\n\treplacement.Close()\n\n\t// Replace the live journal with the newly generated one\n\tif err = os.Rename(journal.path+\".new\", journal.path)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/txpool/locals/tx_tracker.go",
          "line": 131,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(stales)\n\n\t\t// Check the non-stale\n\t\tfor _, tx := range txs.Flatten() {\n\t\t\tif tracker.pool.Has(tx.Hash()) {\n\t\t\t\tnumOk++\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tresubmits = append(resubmits, tx)\n\t\t}\n\t}\n\n\tif journalCheck { // rejournal\n\t\trejournal = make(map[common.Address]types.Transactions)\n\t\tfor _, tx := range tracker.all {\n\t\t\taddr, _ := types.Sender(tracker.signer, tx)\n\t\t\trejournal[addr] = append(rejournal[addr], tx)\n\t\t}\n\t\t// Sort them\n\t\tfor _, list := range rejournal {\n\t\t\t// cmp(a, b) should return a negative number when a < b,\n\t\t\tslices.SortFunc(list, func(a, b *types.Transaction) int {\n\t\t\t\treturn int(a.Nonce() - b.Nonce())\n\t\t\t})\n\t\t}\n\t}\n\tlocalGauge.Update(int64(len(tracker.all)))\n\tlog.Debug(\"Tx tracker status\", \"need-resubmit\", len(resubmits), \"stale\", numStales, \"ok\", numOk)\n\treturn resubmits, rejournal\n}\n\n// Start implements node.Lifecycle interface\n// Start is called after all services have been constructed and the networking\n// layer was also initialized to spawn any goroutines required by the service.\nfunc (tracker *TxTracker) Start() error {\n\ttracker.wg.Add(1)\n\tgo tracker.loop()\n\treturn nil\n}\n\n// Stop implements node.Lifecycle interface\n// Stop terminates all goroutines belonging to the service, blocking until they\n// are all terminated.\nfunc (tracker *TxTracker) Stop() error {\n\tclose(tracker.shutdownCh)\n\ttracker.wg.Wait()\n\treturn nil\n}\n\nfunc (tracker *TxTracker) loop() {\n\tdefer tracker.wg.Done()\n\n\tif tracker.journal != nil {\n\t\ttracker.journal.load(func(transactions []*types.Transaction) []error {\n\t\t\ttracker.TrackAll(transactions)\n\t\t\treturn nil\n\t\t})\n\t\tdefer tracker.journal.close()\n\t}\n\tvar (\n\t\tlastJournal = time.Now()\n\t\ttimer       = time.NewTimer(10 * time.Second) // Do initial check after 10 seconds, do rechecks more seldom.\n\t)\n\tfor {\n\t\tselect {\n\t\tcase <-tracker.shutdownCh:\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t\tcheckJournal := tracker.journal != nil && time.Since(lastJournal) > tracker.rejournal\n\t\t\tresubmits, rejournal := tracker.recheck(checkJournal)\n\t\t\tif len(resubmits) > 0 {\n\t\t\t\ttracker.pool.Add(resubmits, false)\n\t\t\t}\n\t\t\tif checkJournal {\n\t\t\t\t// Lock to prevent journal.rotate <-> journal.insert (via TrackAll) conflicts\n\t\t\t\ttracker.mu.Lock()\n\t\t\t\tlastJournal = time.Now()\n\t\t\t\tif err := tracker.journal.rotate(rejournal)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/core/rawdb/ancienttest/testsuite.go",
          "line": 300,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 {\n\t\tvals = append(vals, testrand.Bytes(value))\n\t}\n\treturn vals\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/blake2b/blake2x.go",
          "line": 132,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(p, x.block[x.offset:])\n\t\t\tx.remaining -= uint64(n)\n\t\t\treturn\n\t\t}\n\t\tcopy(p, x.block[x.offset:])\n\t\tp = p[blockRemaining:]\n\t\tx.offset = 0\n\t\tx.remaining -= uint64(blockRemaining)\n\t}\n\n\tfor len(p) >= Size {\n\t\tbinary.LittleEndian.PutUint32(x.cfg[8:], x.nodeOffset)\n\t\tx.nodeOffset++\n\n\t\tx.d.initConfig(&x.cfg)\n\t\tx.d.Write(x.root[:])\n\t\tx.d.finalize(&x.block)\n\n\t\tcopy(p, x.block[:])\n\t\tp = p[Size:]\n\t\tx.remaining -= uint64(Size)\n\t}\n\n\tif todo := len(p)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/crypto/blake2b/blake2x.go",
          "line": 133,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(n)\n\t\t\treturn\n\t\t}\n\t\tcopy(p, x.block[x.offset:])\n\t\tp = p[blockRemaining:]\n\t\tx.offset = 0\n\t\tx.remaining -= uint64(blockRemaining)\n\t}\n\n\tfor len(p) >= Size {\n\t\tbinary.LittleEndian.PutUint32(x.cfg[8:], x.nodeOffset)\n\t\tx.nodeOffset++\n\n\t\tx.d.initConfig(&x.cfg)\n\t\tx.d.Write(x.root[:])\n\t\tx.d.finalize(&x.block)\n\n\t\tcopy(p, x.block[:])\n\t\tp = p[Size:]\n\t\tx.remaining -= uint64(Size)\n\t}\n\n\tif todo := len(p)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/crypto/blake2b/blake2x.go",
          "line": 167,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(todo)\n\t}\n\treturn\n}\n\nfunc (d *digest) initConfig(cfg *[Size]byte) {\n\td.offset, d.c[0], d.c[1] = 0, 0, 0\n\tfor i := range d.h {\n\t\td.h[i] = iv[i] ^ binary.LittleEndian.Uint64(cfg[i*8:])\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/blake2b/blake2b_generic.go",
          "line": 34,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= BlockSize\n\t\tif c0 < BlockSize {\n\t\t\tc1++\n\t\t}\n\t\tfor j := range m {\n\t\t\tm[j] = binary.LittleEndian.Uint64(blocks[i:])\n\t\t\ti += 8\n\t\t}\n\t\tfGeneric(h, &m, c0, c1, flag, 12)\n\t}\n\tc[0], c[1] = c0, c1\n}\n\nfunc fGeneric(h *[8]uint64, m *[16]uint64, c0, c1 uint64, flag uint64, rounds uint64) {\n\tv0, v1, v2, v3, v4, v5, v6, v7 := h[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7]\n\tv8, v9, v10, v11, v12, v13, v14, v15 := iv[0], iv[1], iv[2], iv[3], iv[4], iv[5], iv[6], iv[7]\n\tv12 ^= c0\n\tv13 ^= c1\n\tv14 ^= flag\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/crypto/blake2b/blake2b_generic.go",
          "line": 57,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= m[s[0]]\n\t\tv0 += v4\n\t\tv12 ^= v0\n\t\tv12 = bits.RotateLeft64(v12, -32)\n\t\tv8 += v12\n\t\tv4 ^= v8\n\t\tv4 = bits.RotateLeft64(v4, -24)\n\t\tv1 += m[s[1]]\n\t\tv1 += v5\n\t\tv13 ^= v1\n\t\tv13 = bits.RotateLeft64(v13, -32)\n\t\tv9 += v13\n\t\tv5 ^= v9\n\t\tv5 = bits.RotateLeft64(v5, -24)\n\t\tv2 += m[s[2]]\n\t\tv2 += v6\n\t\tv14 ^= v2\n\t\tv14 = bits.RotateLeft64(v14, -32)\n\t\tv10 += v14\n\t\tv6 ^= v10\n\t\tv6 = bits.RotateLeft64(v6, -24)\n\t\tv3 += m[s[3]]\n\t\tv3 += v7\n\t\tv15 ^= v3\n\t\tv15 = bits.RotateLeft64(v15, -32)\n\t\tv11 += v15\n\t\tv7 ^= v11\n\t\tv7 = bits.RotateLeft64(v7, -24)\n\n\t\tv0 += m[s[4]]\n\t\tv0 += v4\n\t\tv12 ^= v0\n\t\tv12 = bits.RotateLeft64(v12, -16)\n\t\tv8 += v12\n\t\tv4 ^= v8\n\t\tv4 = bits.RotateLeft64(v4, -63)\n\t\tv1 += m[s[5]]\n\t\tv1 += v5\n\t\tv13 ^= v1\n\t\tv13 = bits.RotateLeft64(v13, -16)\n\t\tv9 += v13\n\t\tv5 ^= v9\n\t\tv5 = bits.RotateLeft64(v5, -63)\n\t\tv2 += m[s[6]]\n\t\tv2 += v6\n\t\tv14 ^= v2\n\t\tv14 = bits.RotateLeft64(v14, -16)\n\t\tv10 += v14\n\t\tv6 ^= v10\n\t\tv6 = bits.RotateLeft64(v6, -63)\n\t\tv3 += m[s[7]]\n\t\tv3 += v7\n\t\tv15 ^= v3\n\t\tv15 = bits.RotateLeft64(v15, -16)\n\t\tv11 += v15\n\t\tv7 ^= v11\n\t\tv7 = bits.RotateLeft64(v7, -63)\n\n\t\tv0 += m[s[8]]\n\t\tv0 += v5\n\t\tv15 ^= v0\n\t\tv15 = bits.RotateLeft64(v15, -32)\n\t\tv10 += v15\n\t\tv5 ^= v10\n\t\tv5 = bits.RotateLeft64(v5, -24)\n\t\tv1 += m[s[9]]\n\t\tv1 += v6\n\t\tv12 ^= v1\n\t\tv12 = bits.RotateLeft64(v12, -32)\n\t\tv11 += v12\n\t\tv6 ^= v11\n\t\tv6 = bits.RotateLeft64(v6, -24)\n\t\tv2 += m[s[10]]\n\t\tv2 += v7\n\t\tv13 ^= v2\n\t\tv13 = bits.RotateLeft64(v13, -32)\n\t\tv8 += v13\n\t\tv7 ^= v8\n\t\tv7 = bits.RotateLeft64(v7, -24)\n\t\tv3 += m[s[11]]\n\t\tv3 += v4\n\t\tv14 ^= v3\n\t\tv14 = bits.RotateLeft64(v14, -32)\n\t\tv9 += v14\n\t\tv4 ^= v9\n\t\tv4 = bits.RotateLeft64(v4, -24)\n\n\t\tv0 += m[s[12]]\n\t\tv0 += v5\n\t\tv15 ^= v0\n\t\tv15 = bits.RotateLeft64(v15, -16)\n\t\tv10 += v15\n\t\tv5 ^= v10\n\t\tv5 = bits.RotateLeft64(v5, -63)\n\t\tv1 += m[s[13]]\n\t\tv1 += v6\n\t\tv12 ^= v1\n\t\tv12 = bits.RotateLeft64(v12, -16)\n\t\tv11 += v12\n\t\tv6 ^= v11\n\t\tv6 = bits.RotateLeft64(v6, -63)\n\t\tv2 += m[s[14]]\n\t\tv2 += v7\n\t\tv13 ^= v2\n\t\tv13 = bits.RotateLeft64(v13, -16)\n\t\tv8 += v13\n\t\tv7 ^= v8\n\t\tv7 = bits.RotateLeft64(v7, -63)\n\t\tv3 += m[s[15]]\n\t\tv3 += v4\n\t\tv14 ^= v3\n\t\tv14 = bits.RotateLeft64(v14, -16)\n\t\tv9 += v14\n\t\tv4 ^= v9\n\t\tv4 = bits.RotateLeft64(v4, -63)\n\t}\n\th[0] ^= v0 ^ v8\n\th[1] ^= v1 ^ v9\n\th[2] ^= v2 ^ v10\n\th[3] ^= v3 ^ v11\n\th[4] ^= v4 ^ v12\n\th[5] ^= v5 ^ v13\n\th[6] ^= v6 ^ v14\n\th[7] ^= v7 ^ v15\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 161,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= BlockSize\n\t\tif c0 < BlockSize {\n\t\t\tc1++\n\t\t}\n\t\tfor j := range m {\n\t\t\tm[j] = binary.LittleEndian.Uint64(blocks[i:])\n\t\t\ti += 8\n\t\t}\n\t\tf(h, &m, c0, c1, flag, 12)\n\t}\n\tc[0], c[1] = c0, c1\n}\n\ntype digest struct {\n\th      [8]uint64\n\tc      [2]uint64\n\tsize   int\n\tblock  [BlockSize]byte\n\toffset int\n\n\tkey    [BlockSize]byte\n\tkeyLen int\n}\n\nconst (\n\tmagic         = \"b2b\"\n\tmarshaledSize = len(magic) + 8*8 + 2*8 + 1 + BlockSize + 1\n)\n\nfunc (d *digest) MarshalBinary() ([]byte, error) {\n\tif d.keyLen != 0 {\n\t\treturn nil, errors.New(\"crypto/blake2b: cannot marshal MACs\")\n\t}\n\tb := make([]byte, 0, marshaledSize)\n\tb = append(b, magic...)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 249,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(d.block[d.offset:], p)\n\t\t\treturn\n\t\t}\n\t\tcopy(d.block[d.offset:], p[:remaining])\n\t\thashBlocks(&d.h, &d.c, 0, d.block[:])\n\t\td.offset = 0\n\t\tp = p[remaining:]\n\t}\n\n\tif length := len(p)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 268,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(d.block[:], p)\n\t}\n\n\treturn\n}\n\nfunc (d *digest) Sum(sum []byte) []byte {\n\tvar hash [Size]byte\n\td.finalize(&hash)\n\treturn append(sum, hash[:d.size]...)\n}\n\nfunc (d *digest) finalize(hash *[Size]byte) {\n\tvar block [BlockSize]byte\n\tcopy(block[:], d.block[:d.offset])\n\tremaining := uint64(BlockSize - d.offset)\n\n\tc := d.c\n\tif c[0] < remaining {\n\t\tc[1]--\n\t}\n\tc[0] -= remaining\n\n\th := d.h\n\thashBlocks(&h, &c, 0xFFFFFFFFFFFFFFFF, block[:])\n\n\tfor i, v := range h {\n\t\tbinary.LittleEndian.PutUint64(hash[8*i:], v)\n\t}\n}\n\nfunc appendUint64(b []byte, x uint64) []byte {\n\tvar a [8]byte\n\tbinary.BigEndian.PutUint64(a[:], x)\n\treturn append(b, a[:]...)\n}\n\n//nolint:unused,deadcode\nfunc appendUint32(b []byte, x uint32) []byte {\n\tvar a [4]byte\n\tbinary.BigEndian.PutUint32(a[:], x)\n\treturn append(b, a[:]...)\n}\n\nfunc consumeUint64(b []byte) ([]byte, uint64) {\n\tx := binary.BigEndian.Uint64(b)\n\treturn b[8:], x\n}\n\n//nolint:unused,deadcode\nfunc consumeUint32(b []byte) ([]byte, uint32) {\n\tx := binary.BigEndian.Uint32(b)\n\treturn b[4:], x\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 135,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= BlockSize\n\t\t}\n\t\thashBlocks(&h, &c, 0, data[:n])\n\t\tdata = data[n:]\n\t}\n\n\tvar block [BlockSize]byte\n\toffset := copy(block[:], data)\n\tremaining := uint64(BlockSize - offset)\n\tif c[0] < remaining {\n\t\tc[1]--\n\t}\n\tc[0] -= remaining\n\n\thashBlocks(&h, &c, 0xFFFFFFFFFFFFFFFF, block[:])\n\n\tfor i, v := range h[:(hashSize+7)/8] {\n\t\tbinary.LittleEndian.PutUint64(sum[8*i:], v)\n\t}\n}\n\nfunc hashBlocks(h *[8]uint64, c *[2]uint64, flag uint64, blocks []byte) {\n\tvar m [16]uint64\n\tc0, c1 := c[0], c[1]\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 261,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= BlockSize\n\t\t}\n\t\thashBlocks(&d.h, &d.c, 0, p[:nn])\n\t\tp = p[nn:]\n\t}\n\n\tif len(p) > 0 {\n\t\td.offset += copy(d.block[:], p)\n\t}\n\n\treturn\n}\n\nfunc (d *digest) Sum(sum []byte) []byte {\n\tvar hash [Size]byte\n\td.finalize(&hash)\n\treturn append(sum, hash[:d.size]...)\n}\n\nfunc (d *digest) finalize(hash *[Size]byte) {\n\tvar block [BlockSize]byte\n\tcopy(block[:], d.block[:d.offset])\n\tremaining := uint64(BlockSize - d.offset)\n\n\tc := d.c\n\tif c[0] < remaining {\n\t\tc[1]--\n\t}\n\tc[0] -= remaining\n\n\th := d.h\n\thashBlocks(&h, &c, 0xFFFFFFFFFFFFFFFF, block[:])\n\n\tfor i, v := range h {\n\t\tbinary.LittleEndian.PutUint64(hash[8*i:], v)\n\t}\n}\n\nfunc appendUint64(b []byte, x uint64) []byte {\n\tvar a [8]byte\n\tbinary.BigEndian.PutUint64(a[:], x)\n\treturn append(b, a[:]...)\n}\n\n//nolint:unused,deadcode\nfunc appendUint32(b []byte, x uint32) []byte {\n\tvar a [4]byte\n\tbinary.BigEndian.PutUint32(a[:], x)\n\treturn append(b, a[:]...)\n}\n\nfunc consumeUint64(b []byte) ([]byte, uint64) {\n\tx := binary.BigEndian.Uint64(b)\n\treturn b[8:], x\n}\n\n//nolint:unused,deadcode\nfunc consumeUint32(b []byte) ([]byte, uint32) {\n\tx := binary.BigEndian.Uint32(b)\n\treturn b[4:], x\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/kzg4844/kzg4844_test.go",
          "line": 41,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gokzg4844.SerializedScalarSize {\n\t\tfieldElementBytes := randFieldElement()\n\t\tcopy(blob[i:i+gokzg4844.SerializedScalarSize], fieldElementBytes[:])\n\t}\n\treturn &blob\n}\n\nfunc TestCKZGWithPoint(t *testing.T)  { testKZGWithPoint(t, true) }\nfunc TestGoKZGWithPoint(t *testing.T) { testKZGWithPoint(t, false) }\nfunc testKZGWithPoint(t *testing.T, ckzg bool) {\n\tif ckzg && !ckzgAvailable {\n\t\tt.Skip(\"CKZG unavailable in this test build\")\n\t}\n\tdefer func(old bool) { useCKZG.Store(old) }(useCKZG.Load())\n\tuseCKZG.Store(ckzg)\n\n\tblob := randBlob()\n\n\tcommitment, err := BlobToCommitment(blob)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create KZG commitment from blob: %v\", err)\n\t}\n\tpoint := randFieldElement()\n\tproof, claim, err := ComputeProof(blob, point)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create KZG proof at point: %v\", err)\n\t}\n\tif err := VerifyProof(commitment, point, claim, proof)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/bn256/cloudflare/gfp_generic.go",
          "line": 84,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a0 * b0\n\t\t\tbuff[off+1] += a1 * b0\n\t\t\tbuff[off+2] += a2*b0 + a0*b2\n\t\t\tbuff[off+3] += a3*b0 + a1*b2\n\t\t\tbuff[off+4] += a2 * b2\n\t\t\tbuff[off+5] += a3 * b2\n\t\t}\n\t}\n\n\tfor i := uint(1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/crypto/bn256/cloudflare/gfp_generic.go",
          "line": 130,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a0 * b0\n\t\t\tbuff[off+1] += a1 * b0\n\t\t\tbuff[off+2] += a2*b0 + a0*b2\n\t\t\tbuff[off+3] += a3*b0 + a1*b2\n\t\t\tbuff[off+4] += a2 * b2\n\t\t\tbuff[off+5] += a3 * b2\n\t\t}\n\t}\n\n\tfor i := uint(1)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/bn256/cloudflare/lattice.go",
          "line": 100,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint8(x.Bit(i)) << uint(j)\n\t\t}\n\t}\n\n\treturn out\n}\n\n// round sets num to num/denom rounded to the nearest integer.\nfunc round(num, denom *big.Int) {\n\tr := new(big.Int)\n\tnum.DivMod(num, denom, r)\n\n\tif r.Cmp(half) == 1 {\n\t\tnum.Add(num, big.NewInt(1))\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/crypto/bn256/cloudflare/gfp.go",
          "line": 66,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(in[8*w+b]) << (56 - 8*b)\n\t\t}\n\t}\n\t// Ensure the point respects the curve modulus\n\tfor i := 3",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/misc/gaslimit.go",
          "line": 35,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= %d\", headerGasLimit, parentGasLimit, limit-1)\n\t}\n\tif headerGasLimit < params.MinGasLimit {\n\t\treturn fmt.Errorf(\"invalid gas limit below %d\", params.MinGasLimit)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/consensus/misc/gaslimit.go",
          "line": 31,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= -1\n\t}\n\tlimit := parentGasLimit / params.GasLimitBoundDivisor\n\tif uint64(diff) >= limit {\n\t\treturn fmt.Errorf(\"invalid gas limit: have %d, want %d +-= %d\", headerGasLimit, parentGasLimit, limit-1)\n\t}\n\tif headerGasLimit < params.MinGasLimit {\n\t\treturn fmt.Errorf(\"invalid gas limit below %d\", params.MinGasLimit)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/lubanFork.go",
          "line": 37,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 397,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= turnLengthSize\n\t}\n\tif num == 0 || len(header.Extra) < extraMinLen {\n\t\treturn nil\n\t}\n\treturn header.Extra[start:end]\n}\n\n// getVoteAttestationFromHeader returns the vote attestation extracted from the header's extra field if exists.\nfunc getVoteAttestationFromHeader(header *types.Header, chainConfig *params.ChainConfig, epochLength uint64) (*types.VoteAttestation, error) {\n\tif len(header.Extra) <= extraVanity+extraSeal {\n\t\treturn nil, nil\n\t}\n\n\tif !chainConfig.IsLuban(header.Number) {\n\t\treturn nil, nil\n\t}\n\n\tvar attestationBytes []byte\n\tif header.Number.Uint64()%epochLength != 0 {\n\t\tattestationBytes = header.Extra[extraVanity : len(header.Extra)-extraSeal]\n\t} else {\n\t\tnum := int(header.Extra[extraVanity])\n\t\tstart := extraVanity + validatorNumberSize + num*validatorBytesLength\n\t\tif chainConfig.IsBohr(header.Number, header.Time) {\n\t\t\tstart += turnLengthSize\n\t\t}\n\t\tend := len(header.Extra) - extraSeal\n\t\tif end <= start {\n\t\t\treturn nil, nil\n\t\t}\n\t\tattestationBytes = header.Extra[start:end]\n\t}\n\n\tvar attestation types.VoteAttestation\n\tif err := rlp.Decode(bytes.NewReader(attestationBytes), &attestation)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 719,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= %d\", header.GasLimit, parent.GasLimit, limit-1)\n\t}\n\n\t// Verify vote attestation for fast finality.\n\tif err := p.verifyVoteAttestation(chain, header, parents)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1286,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\tvalidVoteCount += 1\n\t\t\t}\n\t\t}\n\t\tquorum := cmath.CeilDiv(len(snap.Validators)*2, 3)\n\t\tif validVoteCount > quorum {\n\t\t\taccumulatedWeights[head.Coinbase] += uint64((validVoteCount - quorum) * collectAdditionalVotesRewardRatio / 100)\n\t\t}\n\t}\n\n\tvalidators := make([]common.Address, 0, len(accumulatedWeights))\n\tweights := make([]*big.Int, 0, len(accumulatedWeights))\n\tfor val := range accumulatedWeights {\n\t\tvalidators = append(validators, val)\n\t}\n\tsort.Sort(validatorsAscending(validators))\n\tfor _, val := range validators {\n\t\tweights = append(weights, big.NewInt(int64(accumulatedWeights[val])))\n\t}\n\n\t// generate system transaction\n\tmethod := \"distributeFinalityReward\"\n\tdata, err := p.validatorSetABI.Pack(method, validators, weights)\n\tif err != nil {\n\t\tlog.Error(\"Unable to pack tx for distributeFinalityReward\", \"error\", err)\n\t\treturn err\n\t}\n\tmsg := p.getSystemMessage(header.Coinbase, common.HexToAddress(systemcontracts.ValidatorContract), data, common.Big0)\n\treturn p.applyTransaction(msg, state, header, cx, txs, receipts, systemTxs, usedGas, mining, tracer)\n}\n\nfunc (p *Parlia) EstimateGasReservedForSystemTxs(chain consensus.ChainHeaderReader, header *types.Header) uint64 {\n\tparent := chain.GetHeaderByHash(header.ParentHash)\n\tif parent != nil {\n\t\t// Mainnet and Chapel have both passed Feynman. Now, simplify the logic before and during the Feynman hard fork.\n\t\tif p.chainConfig.IsFeynman(header.Number, header.Time) &&\n\t\t\t!p.chainConfig.IsOnFeynman(header.Number, parent.Time, header.Time) {\n\t\t\t// const (\n\t\t\t// \tthe following values represent the maximum values found in the most recent blocks on the mainnet\n\t\t\t// \tdepositTxGas         = uint64(60_000)\n\t\t\t// \tslashTxGas           = uint64(140_000)\n\t\t\t// \tfinalityRewardTxGas  = uint64(350_000)\n\t\t\t// \tupdateValidatorTxGas = uint64(12_160_000)\n\t\t\t// )\n\t\t\t// suggestReservedGas := depositTxGas\n\t\t\t// if header.Difficulty.Cmp(diffInTurn) != 0 {\n\t\t\t// \tsnap, err := p.snapshot(chain, header.Number.Uint64()-1, header.ParentHash, nil)\n\t\t\t// \tif err != nil || !snap.SignRecently(snap.inturnValidator()) {\n\t\t\t// \t\tsuggestReservedGas += slashTxGas\n\t\t\t// \t}\n\t\t\t// }\n\t\t\t// if header.Number.Uint64()%p.config.Epoch == 0 {\n\t\t\t// \tsuggestReservedGas += finalityRewardTxGas\n\t\t\t// }\n\t\t\t// if isBreatheBlock(parent.Time, header.Time) {\n\t\t\t// \tsuggestReservedGas += updateValidatorTxGas\n\t\t\t// }\n\t\t\t// return suggestReservedGas * 150 / 100\n\t\t\tif !isBreatheBlock(parent.Time, header.Time) {\n\t\t\t\t// params.SystemTxsGasSoftLimit > (depositTxGas+slashTxGas+finalityRewardTxGas)*150/100\n\t\t\t\treturn params.SystemTxsGasSoftLimit\n\t\t\t}\n\t\t}\n\t}\n\n\t// params.SystemTxsGasHardLimit > (depositTxGas+slashTxGas+finalityRewardTxGas+updateValidatorTxGas)*150/100\n\treturn params.SystemTxsGasHardLimit\n}\n\n// Finalize implements consensus.Engine, ensuring no uncles are set, nor block\n// rewards given.\nfunc (p *Parlia) Finalize(chain consensus.ChainHeaderReader, header *types.Header, state vm.StateDB, txs *[]*types.Transaction,\n\tuncles []*types.Header, _ []*types.Withdrawal, receipts *[]*types.Receipt, systemTxs *[]*types.Transaction, usedGas *uint64, tracer *tracing.Hooks) error {\n\t// warn if not in majority fork\n\tp.detectNewVersionWithFork(chain, header, state)\n\n\t// If the block is an epoch end block, verify the validator list\n\t// The verification can only be done when the state is ready, it can't be done in VerifyHeader.\n\tif err := p.verifyValidators(chain, header)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 2142,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gasUsed\n\ttracingReceipt = types.NewReceipt(root, false, *usedGas)\n\ttracingReceipt.TxHash = expectedTx.Hash()\n\ttracingReceipt.GasUsed = gasUsed\n\n\t// Set the receipt logs and create a bloom for filtering\n\ttracingReceipt.Logs = state.GetLogs(expectedTx.Hash(), header.Number.Uint64(), header.Hash(), header.Time)\n\ttracingReceipt.Bloom = types.CreateBloom(tracingReceipt)\n\ttracingReceipt.BlockHash = header.Hash()\n\ttracingReceipt.BlockNumber = header.Number\n\ttracingReceipt.TransactionIndex = uint(state.TxIndex())\n\t*receipts = append(*receipts, tracingReceipt)\n\treturn nil\n}\n\n// GetJustifiedNumberAndHash retrieves the number and hash of the highest justified block\n// within the branch including `headers` and utilizing the latest element as the head.\nfunc (p *Parlia) GetJustifiedNumberAndHash(chain consensus.ChainHeaderReader, headers []*types.Header) (uint64, common.Hash, error) {\n\tif chain == nil || len(headers) == 0 || headers[len(headers)-1] == nil {\n\t\treturn 0, common.Hash{}, errors.New(\"illegal chain or header\")\n\t}\n\thead := headers[len(headers)-1]\n\tsnap, err := p.snapshot(chain, head.Number.Uint64(), head.Hash(), headers)\n\tif err != nil {\n\t\tlog.Error(\"Unexpected error when getting snapshot\",\n\t\t\t\"error\", err, \"blockNumber\", head.Number.Uint64(), \"blockHash\", head.Hash())\n\t\treturn 0, common.Hash{}, err\n\t}\n\n\tif snap.Attestation == nil {\n\t\tif p.chainConfig.IsLuban(head.Number) {\n\t\t\tlog.Debug(\"once one attestation generated, attestation of snap would not be nil forever basically\")\n\t\t}\n\t\treturn 0, chain.GetHeaderByNumber(0).Hash(), nil\n\t}\n\treturn snap.Attestation.TargetNumber, snap.Attestation.TargetHash, nil\n}\n\n// GetFinalizedHeader returns highest finalized block header.\nfunc (p *Parlia) GetFinalizedHeader(chain consensus.ChainHeaderReader, header *types.Header) *types.Header {\n\tif chain == nil || header == nil {\n\t\treturn nil\n\t}\n\tif !chain.Config().IsPlato(header.Number) {\n\t\treturn chain.GetHeaderByNumber(0)\n\t}\n\n\tsnap, err := p.snapshot(chain, header.Number.Uint64(), header.Hash(), nil)\n\tif err != nil {\n\t\tlog.Error(\"Unexpected error when getting snapshot\",\n\t\t\t\"error\", err, \"blockNumber\", header.Number.Uint64(), \"blockHash\", header.Hash())\n\t\treturn nil\n\t}\n\n\tif snap.Attestation == nil {\n\t\treturn chain.GetHeaderByNumber(0) // keep consistent with GetJustifiedNumberAndHash\n\t}\n\n\treturn chain.GetHeader(snap.Attestation.SourceHash, snap.Attestation.SourceNumber)\n}\n\n// ===========================     utility function        ==========================\nfunc (p *Parlia) backOffTime(snap *Snapshot, parent, header *types.Header, val common.Address) uint64 {\n\tif snap.inturn(val) {\n\t\tlog.Debug(\"backOffTime\", \"blockNumber\", header.Number, \"in turn validator\", val)\n\t\treturn 0\n\t} else {\n\t\tdelay := defaultInitialBackOffTime\n\t\t// When mining blocks, `header.Time` is temporarily set to time.Now() + 1.\n\t\t// Therefore, using `header.Time` to determine whether a hard fork has occurred is incorrect.\n\t\t// As a result, during the Bohr and Lorentz hard forks, the network may experience some instability,\n\t\t// So use `parent.Time` instead.\n\t\tisParerntLorentz := p.chainConfig.IsLorentz(parent.Number, parent.Time)\n\t\tif isParerntLorentz {\n\t\t\t// If the in-turn validator has not signed recently, the expected backoff times are [2, 3, 4, ...].\n\t\t\tdelay = lorentzInitialBackOffTime\n\t\t}\n\t\tvalidators := snap.validators()\n\t\tif p.chainConfig.IsPlanck(header.Number) {\n\t\t\tcounts := snap.countRecents()\n\t\t\tfor addr, seenTimes := range counts {\n\t\t\t\tlog.Trace(\"backOffTime\", \"blockNumber\", header.Number, \"validator\", addr, \"seenTimes\", seenTimes)\n\t\t\t}\n\n\t\t\t// The backOffTime does not matter when a validator has signed recently.\n\t\t\tif snap.signRecentlyByCounts(val, counts) {\n\t\t\t\treturn 0\n\t\t\t}\n\n\t\t\tinTurnAddr := snap.inturnValidator()\n\t\t\tif snap.signRecentlyByCounts(inTurnAddr, counts) {\n\t\t\t\tlog.Debug(\"in turn validator has recently signed, skip initialBackOffTime\",\n\t\t\t\t\t\"inTurnAddr\", inTurnAddr)\n\t\t\t\tdelay = 0\n\t\t\t}\n\n\t\t\t// Exclude the recently signed validators and the in turn validator\n\t\t\ttemp := make([]common.Address, 0, len(validators))\n\t\t\tfor _, addr := range validators {\n\t\t\t\tif snap.signRecentlyByCounts(addr, counts) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif p.chainConfig.IsBohr(header.Number, header.Time) {\n\t\t\t\t\tif addr == inTurnAddr {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttemp = append(temp, addr)\n\t\t\t}\n\t\t\tvalidators = temp\n\t\t}\n\n\t\t// get the index of current validator and its shuffled backoff time.\n\t\tidx := -1\n\t\tfor index, itemAddr := range validators {\n\t\t\tif val == itemAddr {\n\t\t\t\tidx = index\n\t\t\t}\n\t\t}\n\t\tif idx < 0 {\n\t\t\tlog.Debug(\"The validator is not authorized\", \"addr\", val)\n\t\t\treturn 0\n\t\t}\n\n\t\trandSeed := snap.Number\n\t\tif p.chainConfig.IsBohr(header.Number, header.Time) {\n\t\t\trandSeed = header.Number.Uint64() / uint64(snap.TurnLength)\n\t\t}\n\t\ts := rand.NewSource(int64(randSeed))\n\t\tr := rand.New(s)\n\t\tn := len(validators)\n\t\tbackOffSteps := make([]uint64, 0, n)\n\n\t\tfor i := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 2290,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= backOffSteps[idx] * wiggleTime\n\t\treturn delay\n\t}\n}\n\n// BlockInterval returns number of blocks in one epoch for the given header\nfunc (p *Parlia) epochLength(chain consensus.ChainHeaderReader, header *types.Header, parents []*types.Header) (uint64, error) {\n\tif header == nil {\n\t\treturn defaultEpochLength, errUnknownBlock\n\t}\n\tif header.Number.Uint64() == 0 {\n\t\treturn defaultEpochLength, nil\n\t}\n\tsnap, err := p.snapshot(chain, header.Number.Uint64()-1, header.ParentHash, parents)\n\tif err != nil {\n\t\treturn defaultEpochLength, err\n\t}\n\treturn snap.EpochLength, nil\n}\n\n// BlockInterval returns the block interval in milliseconds for the given header\nfunc (p *Parlia) BlockInterval(chain consensus.ChainHeaderReader, header *types.Header) (uint64, error) {\n\tif header == nil {\n\t\treturn defaultBlockInterval, errUnknownBlock\n\t}\n\tif header.Number.Uint64() == 0 {\n\t\treturn defaultBlockInterval, nil\n\t}\n\tsnap, err := p.snapshot(chain, header.Number.Uint64()-1, header.ParentHash, nil)\n\tif err != nil {\n\t\treturn defaultBlockInterval, err\n\t}\n\treturn snap.BlockInterval, nil\n}\n\nfunc (p *Parlia) NextProposalBlock(chain consensus.ChainHeaderReader, header *types.Header, proposer common.Address) (uint64, uint64, error) {\n\tsnap, err := p.snapshot(chain, header.Number.Uint64(), header.Hash(), nil)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\treturn snap.nextProposalBlock(proposer)\n}\n\nfunc (p *Parlia) checkNanoBlackList(state vm.StateDB, header *types.Header) error {\n\tif p.chainConfig.IsNano(header.Number) {\n\t\tfor _, blackListAddr := range types.NanoBlackList {\n\t\t\tif state.IsAddressInMutations(blackListAddr) {\n\t\t\t\tlog.Error(\"blacklisted address found\", \"address\", blackListAddr)\n\t\t\t\treturn fmt.Errorf(\"block contains blacklisted address: %s\", blackListAddr.Hex())\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (p *Parlia) detectNewVersionWithFork(chain consensus.ChainHeaderReader, header *types.Header, state vm.StateDB) {\n\t// Ignore blocks that are considered too old\n\tconst maxBlockReceiveDelay = 10 * time.Second\n\tblockTime := time.UnixMilli(int64(header.MilliTimestamp()))\n\tif time.Since(blockTime) > maxBlockReceiveDelay {\n\t\treturn\n\t}\n\n\t// If the fork is not a majority, log a warning or debug message\n\tnumber := header.Number.Uint64()\n\tsnap, err := p.snapshot(chain, number-1, header.ParentHash, nil)\n\tif err != nil {\n\t\treturn\n\t}\n\tnextForkHash := forkid.NextForkHash(p.chainConfig, p.genesisHash, chain.GenesisHeader().Time, number, header.Time)\n\tforkHashHex := hex.EncodeToString(nextForkHash[:])\n\tif !snap.isMajorityFork(forkHashHex) {\n\t\tlogFn := log.Debug\n\t\tif state.NoTries() {\n\t\t\tlogFn = log.Warn\n\t\t}\n\t\tlogFn(\"possible fork detected: client is not in majority\", \"nextForkHash\", forkHashHex)\n\t}\n}\n\n// chain context\ntype chainContext struct {\n\tChain  consensus.ChainHeaderReader\n\tparlia consensus.Engine\n}\n\nfunc (c chainContext) Engine() consensus.Engine {\n\treturn c.parlia\n}\n\nfunc (c chainContext) GetHeader(hash common.Hash, number uint64) *types.Header {\n\treturn c.Chain.GetHeader(hash, number)\n}\n\nfunc (c chainContext) Config() *params.ChainConfig {\n\treturn c.Chain.Config()\n}\n\n// apply message\nfunc applyMessage(\n\tmsg *core.Message,\n\tevm *vm.EVM,\n\tstate vm.StateDB,\n\theader *types.Header,\n\tchainConfig *params.ChainConfig,\n\tchainContext core.ChainContext,\n) (uint64, error) {\n\t// Apply the transaction to the current state (included in the env)\n\tif chainConfig.IsCancun(header.Number, header.Time) {\n\t\trules := evm.ChainConfig().Rules(evm.Context.BlockNumber, evm.Context.Random != nil, evm.Context.Time)\n\t\tstate.Prepare(rules, msg.From, evm.Context.Coinbase, msg.To, vm.ActivePrecompiles(rules), msg.AccessList)\n\t} else {\n\t\tstate.ClearAccessList()\n\t}\n\t// Increment the nonce for the next transaction\n\tstate.SetNonce(msg.From, state.GetNonce(msg.From)+1, tracing.NonceChangeEoACall)\n\n\tret, returnGas, err := evm.Call(\n\t\tmsg.From,\n\t\t*msg.To,\n\t\tmsg.Data,\n\t\tmsg.GasLimit,\n\t\tuint256.MustFromBig(msg.Value),\n\t)\n\tif err != nil {\n\t\tlog.Error(\"apply message failed\", \"msg\", string(ret), \"err\", err)\n\t}\n\treturn msg.GasLimit - returnGas, err\n}\n\n// proposalKey build a key which is a combination of the block number and the proposer address.\nfunc proposalKey(header types.Header) string {\n\treturn header.ParentHash.String() + header.Coinbase.String()\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 710,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= -1\n\t}\n\tgasLimitBoundDivisor := gasLimitBoundDivisorBeforeLorentz\n\tif p.chainConfig.IsLorentz(header.Number, header.Time) {\n\t\tgasLimitBoundDivisor = params.GasLimitBoundDivisor\n\t}\n\tlimit := parent.GasLimit / gasLimitBoundDivisor\n\n\tif uint64(diff) >= limit || header.GasLimit < params.MinGasLimit {\n\t\treturn fmt.Errorf(\"invalid gas limit: have %d, want %d += %d\", header.GasLimit, parent.GasLimit, limit-1)\n\t}\n\n\t// Verify vote attestation for fast finality.\n\tif err := p.verifyVoteAttestation(chain, header, parents)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1896,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 2408,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/stakehub.go",
          "line": 39,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/consensus/parlia/stakehub.go",
          "line": 81,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 251,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\treturn counts\n}\n\nfunc (s *Snapshot) signRecentlyByCounts(validator common.Address, counts map[common.Address]uint8) bool {\n\tif seenTimes, ok := counts[validator]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 432,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(headers))\n\tsnap.Hash = headers[len(headers)-1].Hash()\n\treturn snap, nil\n}\n\n// validators retrieves the list of validators in ascending order.\nfunc (s *Snapshot) validators() []common.Address {\n\tvalidators := make([]common.Address, 0, len(s.Validators))\n\tfor v := range s.Validators {\n\t\tvalidators = append(validators, v)\n\t}\n\tsort.Sort(validatorsAscending(validators))\n\treturn validators\n}\n\n// lastBlockInOneTurn returns if the block at height `blockNumber` is the last block in current turn.\nfunc (s *Snapshot) lastBlockInOneTurn(blockNumber uint64) bool {\n\treturn (blockNumber+1)%uint64(s.TurnLength) == 0\n}\n\n// inturn returns if a validator at a given block height is in-turn or not.\nfunc (s *Snapshot) inturn(validator common.Address) bool {\n\treturn s.inturnValidator() == validator\n}\n\n// inturnValidator returns the validator for the following block height.\nfunc (s *Snapshot) inturnValidator() common.Address {\n\tvalidators := s.validators()\n\toffset := (s.Number + 1) / uint64(s.TurnLength) % uint64(len(validators))\n\treturn validators[offset]\n}\n\nfunc (s *Snapshot) nexValidatorsChangeBlock() uint64 {\n\tepochLength := s.EpochLength\n\tcurrentEpoch := s.Number - s.Number%epochLength\n\tcheckLen := s.minerHistoryCheckLen()\n\tif s.Number%epochLength < checkLen {\n\t\treturn currentEpoch + checkLen\n\t}\n\treturn currentEpoch + epochLength + checkLen\n}\n\n// nextProposalBlock returns the validator next proposal block.\nfunc (s *Snapshot) nextProposalBlock(proposer common.Address) (uint64, uint64, error) {\n\tvalidators := s.validators()\n\tcurrentIndex := int(s.Number / uint64(s.TurnLength) % uint64(len(validators)))\n\texpectIndex := s.indexOfVal(proposer)\n\tif expectIndex < 0 {\n\t\treturn 0, 0, errors.New(\"proposer not in validator set\")\n\t}\n\tstartBlock := s.Number + uint64(((expectIndex+len(validators)-currentIndex)%len(validators))*int(s.TurnLength))\n\tstartBlock = startBlock - startBlock%uint64(s.TurnLength)\n\tendBlock := startBlock + uint64(s.TurnLength) - 1\n\n\tchangeValidatorsBlock := s.nexValidatorsChangeBlock()\n\tif startBlock >= changeValidatorsBlock {\n\t\treturn 0, 0, errors.New(\"next proposal block is out of current epoch\")\n\t}\n\tif endBlock >= changeValidatorsBlock {\n\t\tendBlock = changeValidatorsBlock\n\t}\n\treturn startBlock, endBlock, nil\n}\n\nfunc (s *Snapshot) enoughDistance(validator common.Address, header *types.Header) bool {\n\tidx := s.indexOfVal(validator)\n\tif idx < 0 {\n\t\treturn true\n\t}\n\tvalidatorNum := int64(len(s.validators()))\n\tif validatorNum == 1 {\n\t\treturn true\n\t}\n\tif validator == header.Coinbase {\n\t\treturn false\n\t}\n\toffset := (int64(s.Number) + 1) % validatorNum\n\tif int64(idx) >= offset {\n\t\treturn int64(idx)-offset >= validatorNum-2\n\t} else {\n\t\treturn validatorNum+int64(idx)-offset >= validatorNum-2\n\t}\n}\n\nfunc (s *Snapshot) indexOfVal(validator common.Address) int {\n\tif validator, ok := s.Validators[validator]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/bohrFork.go",
          "line": 69,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 63,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t}\n\treturn recentSignTimes >= turnLength\n}\n\n// refer Snapshot.minerHistoryCheckLen\nfunc minerHistoryCheckLen(totalValidators int, turnLength int) uint64 {\n\treturn uint64(totalValidators/2+1)*uint64(turnLength) - 1\n}\n\n// refer Snapshot.inturnValidator\nfunc inturnValidator(totalValidators int, turnLength int, height int) int {\n\treturn height / turnLength % totalValidators\n}\n\nfunc simulateValidatorOutOfService(totalValidators int, downValidators int, turnLength int) {\n\tdownBlocks := 10000\n\trecoverBlocks := 10000\n\trecents := make(map[uint64]int)\n\n\tvalidators := make(map[int]bool, totalValidators)\n\tdown := make([]int, totalValidators)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 145,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 192,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/parlia/ramanujanfork.go",
          "line": 26,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fixedBackOffTimeBeforeFork + time.Duration(rand.Int63n(int64(wiggle)))\n\t}\n\treturn delay\n}\n\nfunc (p *Parlia) blockTimeForRamanujanFork(snap *Snapshot, header, parent *types.Header) uint64 {\n\tblockTime := parent.MilliTimestamp() + snap.BlockInterval\n\tif p.chainConfig.IsRamanujan(header.Number) {\n\t\tblockTime = blockTime + p.backOffTime(snap, parent, header, p.val)\n\t}\n\tif now := uint64(time.Now().UnixMilli())",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/clique/clique.go",
          "line": 671,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Duration(rand.Int63n(int64(wiggle)))\n\n\t\tlog.Trace(\"Out-of-turn signing requested\", \"wiggle\", common.PrettyDuration(wiggle))\n\t}\n\t// Sign all the things!\n\tsighash, err := signFn(accounts.Account{Address: signer}, accounts.MimetypeClique, CliqueRLP(header))\n\tif err != nil {\n\t\treturn err\n\t}\n\tcopy(header.Extra[len(header.Extra)-extraSeal:], sighash)\n\t// Wait until sealing is terminated or delay timeout.\n\tlog.Trace(\"Waiting for slot to sign and propagate\", \"delay\", common.PrettyDuration(delay))\n\tgopool.Submit(func() {\n\t\tselect {\n\t\tcase <-stop:\n\t\t\treturn\n\t\tcase <-time.After(delay):\n\t\t}\n\n\t\tselect {\n\t\tcase results <- block.WithSeal(header):\n\t\tdefault:\n\t\t\tlog.Warn(\"Sealing result is not read by miner\", \"sealhash\", SealHash(header))\n\t\t}\n\t})\n\n\treturn nil\n}\n\n// CalcDifficulty is the difficulty adjustment algorithm. It returns the difficulty\n// that a new block should have:\n// * DIFF_NOTURN(2) if BLOCK_NUMBER % SIGNER_COUNT != SIGNER_INDEX\n// * DIFF_INTURN(1) if BLOCK_NUMBER % SIGNER_COUNT == SIGNER_INDEX\nfunc (c *Clique) CalcDifficulty(chain consensus.ChainHeaderReader, time uint64, parent *types.Header) *big.Int {\n\tsnap, err := c.snapshot(chain, parent.Number.Uint64(), parent.Hash(), nil)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tc.lock.RLock()\n\tsigner := c.signer\n\tc.lock.RUnlock()\n\treturn calcDifficulty(snap, signer)\n}\n\nfunc calcDifficulty(snap *Snapshot, signer common.Address) *big.Int {\n\tif snap.inturn(snap.Number+1, signer) {\n\t\treturn new(big.Int).Set(diffInTurn)\n\t}\n\treturn new(big.Int).Set(diffNoTurn)\n}\n\n// SealHash returns the hash of a block prior to it being sealed.\nfunc (c *Clique) SealHash(header *types.Header) common.Hash {\n\treturn SealHash(header)\n}\n\n// Close implements consensus.Engine. It's a noop for clique as there are no background threads.\nfunc (c *Clique) Close() error {\n\treturn nil\n}\n\n// SealHash returns the hash of a block prior to it being sealed.\nfunc SealHash(header *types.Header) (hash common.Hash) {\n\thasher := sha3.NewLegacyKeccak256()\n\tencodeSigHeader(hasher, header)\n\thasher.(crypto.KeccakState).Read(hash[:])\n\treturn hash\n}\n\n// CliqueRLP returns the rlp bytes which needs to be signed for the proof-of-authority\n// sealing. The RLP to sign consists of the entire header apart from the 65 byte signature\n// contained at the end of the extra data.\n//\n// Note, the method requires the extra data to be at least 65 bytes, otherwise it\n// panics. This is done to avoid accidentally using both forms (signature present\n// or not), which could be abused to produce different hashes for the same header.\nfunc CliqueRLP(header *types.Header) []byte {\n\tb := new(bytes.Buffer)\n\tencodeSigHeader(b, header)\n\treturn b.Bytes()\n}\n\nfunc encodeSigHeader(w io.Writer, header *types.Header) {\n\tenc := []interface{}{\n\t\theader.ParentHash,\n\t\theader.UncleHash,\n\t\theader.Coinbase,\n\t\theader.Root,\n\t\theader.TxHash,\n\t\theader.ReceiptHash,\n\t\theader.Bloom,\n\t\theader.Difficulty,\n\t\theader.Number,\n\t\theader.GasLimit,\n\t\theader.GasUsed,\n\t\theader.Time,\n\t\theader.Extra[:len(header.Extra)-crypto.SignatureLength], // Yes, this will panic if extra is too short\n\t\theader.MixDigest,\n\t\theader.Nonce,\n\t}\n\tif header.BaseFee != nil {\n\t\tenc = append(enc, header.BaseFee)\n\t}\n\tif header.WithdrawalsHash != nil {\n\t\tpanic(\"unexpected withdrawal hash value in clique\")\n\t}\n\tif header.ExcessBlobGas != nil {\n\t\tpanic(\"unexpected excess blob gas value in clique\")\n\t}\n\tif header.BlobGasUsed != nil {\n\t\tpanic(\"unexpected blob gas used value in clique\")\n\t}\n\tif header.ParentBeaconRoot != nil {\n\t\tpanic(\"unexpected parent beacon root value in clique\")\n\t}\n\tif err := rlp.Encode(w, enc)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/consensus/clique/snapshot.go",
          "line": 288,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(headers))\n\tsnap.Hash = headers[len(headers)-1].Hash()\n\n\treturn snap, nil\n}\n\n// signers retrieves the list of authorized signers in ascending order.\nfunc (s *Snapshot) signers() []common.Address {\n\tsigs := make([]common.Address, 0, len(s.Signers))\n\tfor sig := range s.Signers {\n\t\tsigs = append(sigs, sig)\n\t}\n\tslices.SortFunc(sigs, common.Address.Cmp)\n\treturn sigs\n}\n\n// inturn returns if a signer at a given block height is in-turn or not.\nfunc (s *Snapshot) inturn(number uint64, signer common.Address) bool {\n\tsigners, offset := s.signers(), 0\n\tfor offset < len(signers) && signers[offset] != signer {\n\t\toffset++\n\t}\n\treturn (number % uint64(len(signers))) == uint64(offset)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/signer/core/stdioui.go",
          "line": 46,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/signer/core/signed_data.go",
          "line": 70,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 27 // Transform V from 0/1 to 27/28 according to the yellow paper\n\t}\n\treturn signature, nil\n}\n\n// SignData signs the hash of the provided data, but does so differently\n// depending on the content-type specified.\n//\n// Different types of validation occur.\nfunc (api *SignerAPI) SignData(ctx context.Context, contentType string, addr common.MixedcaseAddress, data interface{}) (hexutil.Bytes, error) {\n\tvar req, transformV, err = api.determineSignatureFormat(ctx, contentType, addr, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsignature, err := api.sign(req, transformV)\n\tif err != nil {\n\t\tapi.UI.ShowError(err.Error())\n\t\treturn nil, err\n\t}\n\treturn signature, nil\n}\n\n// determineSignatureFormat determines which signature method should be used based upon the mime type\n// In the cases where it matters ensure that the charset is handled. The charset\n// resides in the 'params' returned as the second returnvalue from mime.ParseMediaType\n// charset, ok := params[\"charset\"]\n// As it is now, we accept any charset and just treat it as 'raw'.\n// This method returns the mimetype for signing along with the request\nfunc (api *SignerAPI) determineSignatureFormat(ctx context.Context, contentType string, addr common.MixedcaseAddress, data interface{}) (*SignDataRequest, bool, error) {\n\tvar (\n\t\treq          *SignDataRequest\n\t\tuseEthereumV = true // Default to use V = 27 or 28, the legacy Ethereum format\n\t)\n\tmediaType, _, err := mime.ParseMediaType(contentType)\n\tif err != nil {\n\t\treturn nil, useEthereumV, err\n\t}\n\n\tswitch mediaType {\n\tcase apitypes.IntendedValidator.Mime:\n\t\t// Data with an intended validator\n\t\tvalidatorData, err := UnmarshalValidatorData(data)\n\t\tif err != nil {\n\t\t\treturn nil, useEthereumV, err\n\t\t}\n\t\tsighash, msg := SignTextValidator(validatorData)\n\t\tmessages := []*apitypes.NameValueType{\n\t\t\t{\n\t\t\t\tName:  \"This is a request to sign data intended for a particular validator (see EIP 191 version 0)\",\n\t\t\t\tTyp:   \"description\",\n\t\t\t\tValue: \"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"Intended validator address\",\n\t\t\t\tTyp:   \"address\",\n\t\t\t\tValue: validatorData.Address.String(),\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"Application-specific data\",\n\t\t\t\tTyp:   \"hexdata\",\n\t\t\t\tValue: validatorData.Message,\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"Full message for signing\",\n\t\t\t\tTyp:   \"hexdata\",\n\t\t\t\tValue: fmt.Sprintf(\"%#x\", msg),\n\t\t\t},\n\t\t}\n\t\treq = &SignDataRequest{ContentType: mediaType, Rawdata: []byte(msg), Messages: messages, Hash: sighash}\n\tcase apitypes.ApplicationClique.Mime:\n\t\t// Clique is the Ethereum PoA standard\n\t\tcliqueData, err := fromHex(data)\n\t\tif err != nil {\n\t\t\treturn nil, useEthereumV, err\n\t\t}\n\t\theader := &types.Header{}\n\t\tif err := rlp.DecodeBytes(cliqueData, header)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/signer/core/signed_data.go",
          "line": 359,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 27 // Transform yellow paper V from 27/28 to 0/1\n\thash := accounts.TextHash(data)\n\trpk, err := crypto.SigToPub(hash, sig)\n\tif err != nil {\n\t\treturn common.Address{}, err\n\t}\n\treturn crypto.PubkeyToAddress(*rpk), nil\n}\n\n// UnmarshalValidatorData converts the bytes input to typed data\nfunc UnmarshalValidatorData(data interface{}) (apitypes.ValidatorData, error) {\n\traw, ok := data.(map[string]interface{})\n\tif !ok {\n\t\treturn apitypes.ValidatorData{}, errors.New(\"validator input is not a map[string]interface{}\")\n\t}\n\taddrBytes, err := fromHex(raw[\"address\"])\n\tif err != nil {\n\t\treturn apitypes.ValidatorData{}, fmt.Errorf(\"validator address error: %w\", err)\n\t}\n\tif len(addrBytes) == 0 {\n\t\treturn apitypes.ValidatorData{}, errors.New(\"validator address is undefined\")\n\t}\n\tmessageBytes, err := fromHex(raw[\"message\"])\n\tif err != nil {\n\t\treturn apitypes.ValidatorData{}, fmt.Errorf(\"message error: %w\", err)\n\t}\n\tif len(messageBytes) == 0 {\n\t\treturn apitypes.ValidatorData{}, errors.New(\"message is undefined\")\n\t}\n\treturn apitypes.ValidatorData{\n\t\tAddress: common.BytesToAddress(addrBytes),\n\t\tMessage: messageBytes,\n\t}, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/signer/rules/rules_test.go",
          "line": 307,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= storage.get(\"mykey\")\n\n\n\t\tstorage.put(\"mykey\", {\"an\": \"object\"}) \t// Should result in \"[object Object]\"\n\t\ta += storage.get(\"mykey\")\n\n\n\t\tstorage.put(\"mykey\", JSON.stringify({\"an\": \"object\"})) // Should result in '{\"an\":\"object\"}'\n\t\ta += storage.get(\"mykey\")\n\n\t\ta += storage.get(\"missingkey\")\t\t//Missing keys should result in empty string\n\t\tstorage.put(\"\",\"missing key==noop\") // Can't store with 0-length key\n\t\ta += storage.get(\"\")\t\t\t\t// Should result in ''\n\n\t\tvar b = new BigNumber(2)\n\t\tvar c = new BigNumber(16)//\"0xf0\",16)\n\t\tvar d = b.plus(c)\n\t\tconsole.log(d)\n\t\treturn a\n\t}\n`\n\tr, err := initRuleEngine(js)\n\tif err != nil {\n\t\tt.Errorf(\"Couldn't create evaluator %v\", err)\n\t\treturn\n\t}\n\n\tv, err := r.execute(\"testStorage\", nil)\n\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error %v\", err)\n\t}\n\tretval := v.ToString().String()\n\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error %v\", err)\n\t}\n\texp := `myvaluea,list[object Object]{\"an\":\"object\"}`\n\tif retval != exp {\n\t\tt.Errorf(\"Unexpected data, expected '%v', got '%v'\", exp, retval)\n\t}\n\tt.Logf(\"Err %v\", err)\n}\n\nconst ExampleTxWindow = `\n\tfunction big(str){\n\t\tif(str.slice(0,2) == \"0x\"){ return new BigNumber(str.slice(2),16)}\n\t\treturn new BigNumber(str)\n\t}\n\n\t// Time window: 1 week\n\tvar window = 1000* 3600*24*7",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/signer/core/apitypes/types.go",
          "line": 893,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 8 {\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"int%d\", n)] = struct{}{}\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"int%d[]\", n)] = struct{}{}\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"uint%d\", n)] = struct{}{}\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"uint%d[]\", n)] = struct{}{}\n\t}\n}\n\n// Checks if the primitive value is valid\nfunc isPrimitiveTypeValid(primitiveType string) bool {\n\tinput := strings.Split(primitiveType, \"[\")[0]\n\t_, ok := validPrimitiveTypes[input]\n\treturn ok\n}\n\n// validate checks if the given domain is valid, i.e. contains at least\n// the minimum viable keys and values\nfunc (domain *TypedDataDomain) validate() error {\n\tif domain.ChainId == nil && len(domain.Name) == 0 && len(domain.Version) == 0 && len(domain.VerifyingContract) == 0 && len(domain.Salt) == 0 {\n\t\treturn errors.New(\"domain is undefined\")\n\t}\n\n\treturn nil\n}\n\n// Map is a helper function to generate a map version of the domain\nfunc (domain *TypedDataDomain) Map() map[string]interface{} {\n\tdataMap := map[string]interface{}{}\n\n\tif domain.ChainId != nil {\n\t\tdataMap[\"chainId\"] = domain.ChainId\n\t}\n\n\tif len(domain.Name) > 0 {\n\t\tdataMap[\"name\"] = domain.Name\n\t}\n\n\tif len(domain.Version) > 0 {\n\t\tdataMap[\"version\"] = domain.Version\n\t}\n\n\tif len(domain.VerifyingContract) > 0 {\n\t\tdataMap[\"verifyingContract\"] = domain.VerifyingContract\n\t}\n\n\tif len(domain.Salt) > 0 {\n\t\tdataMap[\"salt\"] = domain.Salt\n\t}\n\treturn dataMap\n}\n\n// NameValueType is a very simple struct with Name, Value and Type. It's meant for simple\n// json structures used to communicate signing-info about typed data with the UI\ntype NameValueType struct {\n\tName  string      `json:\"name\"`\n\tValue interface{} `json:\"value\"`\n\tTyp   string      `json:\"type\"`\n}\n\n// Pprint returns a pretty-printed version of nvt\nfunc (nvt *NameValueType) Pprint(depth int) string {\n\toutput := bytes.Buffer{}\n\toutput.WriteString(strings.Repeat(\"\\u00a0\", depth*2))\n\toutput.WriteString(fmt.Sprintf(\"%s [%s]: \", nvt.Name, nvt.Typ))\n\tif nvts, ok := nvt.Value.([]*NameValueType)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/enrcmd.go",
          "line": 105,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey := kv[i].(string)\n\t\tif len(key) > longestKey {\n\t\t\tlongestKey = len(key)\n\t\t}\n\t}\n\t// Print the keys, invoking formatters for known keys.\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/devp2p/enrcmd.go",
          "line": 112,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey := kv[i].(string)\n\t\tval := kv[i+1].(rlp.RawValue)\n\t\tpad := longestKey - len(key)\n\t\tout += strings.Repeat(\" \", indent) + strconv.Quote(key) + strings.Repeat(\" \", pad+1)\n\t\tformatter := attrFormatters[key]\n\t\tif formatter == nil {\n\t\t\tformatter = formatAttrRaw\n\t\t}\n\t\tfmtval, ok := formatter(val)\n\t\tif ok {\n\t\t\tout += fmtval + \"\\n\"\n\t\t} else {\n\t\t\tout += hex.EncodeToString(val) + \" (!)\\n\"\n\t\t}\n\t}\n\treturn out\n}\n\n// parseNode parses a node record and verifies its signature.\nfunc parseNode(source string) (*enode.Node, error) {\n\tif strings.HasPrefix(source, \"enode://\") {\n\t\treturn enode.ParseV4(source)\n\t}\n\tr, err := parseRecord(source)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn enode.New(enode.ValidSchemes, r)\n}\n\n// parseRecord parses a node record from hex, base64, or raw binary input.\nfunc parseRecord(source string) (*enr.Record, error) {\n\tbin := []byte(source)\n\tif d, ok := decodeRecordHex(bytes.TrimSpace(bin))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/nodesetcmd.go",
          "line": 79,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\t\tkey := attrlist[i].(string)\n\t\t\tattrcount[key]++\n\t\t}\n\t}\n\n\tvar keys []string\n\tvar maxlength int\n\tfor key := range attrcount {\n\t\tkeys = append(keys, key)\n\t\tif len(key) > maxlength {\n\t\t\tmaxlength = len(key)\n\t\t}\n\t}\n\tsort.Strings(keys)\n\tfmt.Println(\"ENR attribute counts:\")\n\tfor _, key := range keys {\n\t\tfmt.Printf(\"%s%s: %d\\n\", strings.Repeat(\" \", maxlength-len(key)+1), key, attrcount[key])\n\t}\n}\n\nfunc nodesetFilter(ctx *cli.Context) error {\n\tif ctx.NArg() < 1 {\n\t\treturn errors.New(\"need nodes file as argument\")\n\t}\n\t// Parse -limit.\n\tlimit, err := parseFilterLimit(ctx.Args().Tail())\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Parse the filters.\n\tfilter, err := andFilter(ctx.Args().Tail())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Load nodes and apply filters.\n\tns := loadNodesJSON(ctx.Args().First())\n\tresult := make(nodeSet)\n\tfor id, n := range ns {\n\t\tif filter(n) {\n\t\t\tresult[id] = n\n\t\t}\n\t}\n\tif limit >= 0 {\n\t\tresult = result.topN(limit)\n\t}\n\twriteNodesJSON(\"-\", result)\n\treturn nil\n}\n\ntype nodeFilter func(nodeJSON) bool\n\ntype nodeFilterC struct {\n\tnarg int\n\tfn   func([]string) (nodeFilter, error)\n}\n\nvar filterFlags = map[string]nodeFilterC{\n\t\"-limit\":       {1, trueFilter}, // needed to skip over -limit\n\t\"-ip\":          {1, ipFilter},\n\t\"-min-age\":     {1, minAgeFilter},\n\t\"-eth-network\": {1, ethFilter},\n\t\"-les-server\":  {0, lesFilter},\n\t\"-snap\":        {0, snapFilter},\n}\n\n// parseFilters parses nodeFilters from args.\nfunc parseFilters(args []string) ([]nodeFilter, error) {\n\tvar filters []nodeFilter\n\tfor len(args) > 0 {\n\t\tfc, ok := filterFlags[args[0]]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"invalid filter %q\", args[0])\n\t\t}\n\t\tif len(args)-1 < fc.narg {\n\t\t\treturn nil, fmt.Errorf(\"filter %q wants %d arguments, have %d\", args[0], fc.narg, len(args)-1)\n\t\t}\n\t\tfilter, err := fc.fn(args[1 : 1+fc.narg])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: %v\", args[0], err)\n\t\t}\n\t\tfilters = append(filters, filter)\n\t\targs = args[1+fc.narg:]\n\t}\n\treturn filters, nil\n}\n\n// parseFilterLimit parses the -limit option in args. It returns -1 if there is no limit.\nfunc parseFilterLimit(args []string) (int, error) {\n\tlimit := -1\n\tfor i, arg := range args {\n\t\tif arg == \"-limit\" {\n\t\t\tif i == len(args)-1 {\n\t\t\t\treturn -1, errors.New(\"-limit requires an argument\")\n\t\t\t}\n\t\t\tn, err := strconv.Atoi(args[i+1])\n\t\t\tif err != nil {\n\t\t\t\treturn -1, fmt.Errorf(\"invalid -limit %q\", args[i+1])\n\t\t\t}\n\t\t\tlimit = n\n\t\t}\n\t}\n\treturn limit, nil\n}\n\n// andFilter parses node filters in args and returns a single filter that requires all\n// of them to match.\nfunc andFilter(args []string) (nodeFilter, error) {\n\tchecks, err := parseFilters(args)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := func(n nodeJSON) bool {\n\t\tfor _, filter := range checks {\n\t\t\tif !filter(n) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\treturn f, nil\n}\n\nfunc trueFilter(args []string) (nodeFilter, error) {\n\treturn func(n nodeJSON) bool { return true }, nil\n}\n\nfunc ipFilter(args []string) (nodeFilter, error) {\n\tprefix, err := netip.ParsePrefix(args[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := func(n nodeJSON) bool { return prefix.Contains(n.N.IPAddr()) }\n\treturn f, nil\n}\n\nfunc minAgeFilter(args []string) (nodeFilter, error) {\n\tminage, err := time.ParseDuration(args[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := func(n nodeJSON) bool {\n\t\tage := n.LastResponse.Sub(n.FirstResponse)\n\t\treturn age >= minage\n\t}\n\treturn f, nil\n}\n\nfunc ethFilter(args []string) (nodeFilter, error) {\n\tvar filter forkid.Filter\n\tswitch args[0] {\n\tcase \"mainnet\":\n\t\tfilter = forkid.NewStaticFilter(params.MainnetChainConfig, core.DefaultGenesisBlock().ToBlock())\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown network %q\", args[0])\n\t}\n\n\tf := func(n nodeJSON) bool {\n\t\tvar eth struct {\n\t\t\tForkID forkid.ID\n\t\t\tTail   []rlp.RawValue `rlp:\"tail\"`\n\t\t}\n\t\tif n.N.Load(enr.WithEntry(\"eth\", &eth)) != nil {\n\t\t\treturn false\n\t\t}\n\t\treturn filter(eth.ForkID) == nil\n\t}\n\treturn f, nil\n}\n\nfunc lesFilter(args []string) (nodeFilter, error) {\n\tf := func(n nodeJSON) bool {\n\t\tvar les struct {\n\t\t\tTail []rlp.RawValue `rlp:\"tail\"`\n\t\t}\n\t\treturn n.N.Load(enr.WithEntry(\"les\", &les)) == nil\n\t}\n\treturn f, nil\n}\n\nfunc snapFilter(args []string) (nodeFilter, error) {\n\tf := func(n nodeJSON) bool {\n\t\tvar snap struct {\n\t\t\tTail []rlp.RawValue `rlp:\"tail\"`\n\t\t}\n\t\treturn n.N.Load(enr.WithEntry(\"snap\", &snap)) == nil\n\t}\n\treturn f, nil\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/dns_route53.go",
          "line": 320,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\t\tbatchCount += count\n\t}\n\treturn batches\n}\n\n// changeSize returns the RDATA size of a DNS change.\nfunc changeSize(ch types.Change) int {\n\tsize := 0\n\tfor _, rr := range ch.ResourceRecordSet.ResourceRecords {\n\t\tif rr.Value != nil {\n\t\t\tsize += len(*rr.Value)\n\t\t}\n\t}\n\treturn size\n}\n\nfunc changeCount(ch types.Change) int {\n\tif ch.Action == types.ChangeActionUpsert {\n\t\treturn 2\n\t}\n\treturn 1\n}\n\n// collectRecords collects all TXT records below the given name.\nfunc (c *route53Client) collectRecords(name string) (map[string]recordSet, error) {\n\tvar req route53.ListResourceRecordSetsInput\n\treq.HostedZoneId = &c.zoneID\n\texisting := make(map[string]recordSet)\n\tlog.Info(\"Loading existing TXT records\", \"name\", name, \"zone\", c.zoneID)\n\tfor page := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/geth/config.go",
          "line": 229,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(`\n\n Running in ephemeral mode.  The following account has been prefunded in the genesis:\n\n       Account\n       ------------------\n       0x%x (10^49 ETH)\n`, cfg.Eth.Miner.Etherbase)\n\t\tif cfg.Eth.Miner.Etherbase == utils.DeveloperAddr {\n\t\t\tdevModeBanner += fmt.Sprintf(` \n       Private Key\n       ------------------\n       0x%x\n`, crypto.FromECDSA(utils.DeveloperKey))\n\t\t}\n\t}\n\n\treturn devModeBanner\n}\n\n// makeFullNode loads geth configuration and creates the Ethereum backend.\nfunc makeFullNode(ctx *cli.Context) (*node.Node, ethapi.Backend) {\n\tstack, cfg := makeConfigNode(ctx)\n\tif ctx.IsSet(utils.RialtoHash.Name) {\n\t\tv := ctx.String(utils.RialtoHash.Name)\n\t\tparams.RialtoGenesisHash = common.HexToHash(v)\n\t}\n\n\tif ctx.IsSet(utils.OverridePassedForkTime.Name) {\n\t\tv := ctx.Uint64(utils.OverridePassedForkTime.Name)\n\t\tcfg.Eth.OverridePassedForkTime = &v\n\t}\n\tif ctx.IsSet(utils.OverrideLorentz.Name) {\n\t\tv := ctx.Uint64(utils.OverrideLorentz.Name)\n\t\tcfg.Eth.OverrideLorentz = &v\n\t}\n\tif ctx.IsSet(utils.OverrideMaxwell.Name) {\n\t\tv := ctx.Uint64(utils.OverrideMaxwell.Name)\n\t\tcfg.Eth.OverrideMaxwell = &v\n\t}\n\tif ctx.IsSet(utils.OverrideFermi.Name) {\n\t\tv := ctx.Uint64(utils.OverrideFermi.Name)\n\t\tcfg.Eth.OverrideFermi = &v\n\t}\n\tif ctx.IsSet(utils.OverrideVerkle.Name) {\n\t\tv := ctx.Uint64(utils.OverrideVerkle.Name)\n\t\tcfg.Eth.OverrideVerkle = &v\n\t}\n\tif ctx.IsSet(utils.OverrideFullImmutabilityThreshold.Name) {\n\t\tparams.FullImmutabilityThreshold = ctx.Uint64(utils.OverrideFullImmutabilityThreshold.Name)\n\t\tdownloader.FullMaxForkAncestry = ctx.Uint64(utils.OverrideFullImmutabilityThreshold.Name)\n\t}\n\tif ctx.IsSet(utils.OverrideMinBlocksForBlobRequests.Name) {\n\t\tparams.MinBlocksForBlobRequests = ctx.Uint64(utils.OverrideMinBlocksForBlobRequests.Name)\n\t\tparams.MinTimeDurationForBlobRequests = uint64(float64(params.MinBlocksForBlobRequests) * 0.75 /*maxwellBlockInterval*/)\n\t}\n\tif ctx.IsSet(utils.OverrideDefaultExtraReserveForBlobRequests.Name) {\n\t\tparams.DefaultExtraReserveForBlobRequests = ctx.Uint64(utils.OverrideDefaultExtraReserveForBlobRequests.Name)\n\t}\n\tif ctx.IsSet(utils.OverrideBreatheBlockInterval.Name) {\n\t\tparams.BreatheBlockInterval = ctx.Uint64(utils.OverrideBreatheBlockInterval.Name)\n\t}\n\tif ctx.IsSet(utils.OverrideFixedTurnLength.Name) {\n\t\tparams.FixedTurnLength = ctx.Uint64(utils.OverrideFixedTurnLength.Name)\n\t}\n\n\tbackend, eth := utils.RegisterEthService(stack, &cfg.Eth)\n\n\t// Create gauge with geth system and build information\n\tif eth != nil { // The 'eth' backend may be nil in light mode\n\t\tvar protos []string\n\t\tfor _, p := range eth.Protocols() {\n\t\t\tprotos = append(protos, fmt.Sprintf(\"%v/%d\", p.Name, p.Version))\n\t\t}\n\t\tmetrics.NewRegisteredGaugeInfo(\"geth/info\", nil).Update(metrics.GaugeInfoValue{\n\t\t\t\"arch\":      runtime.GOARCH,\n\t\t\t\"os\":        runtime.GOOS,\n\t\t\t\"version\":   cfg.Node.Version,\n\t\t\t\"protocols\": strings.Join(protos, \",\"),\n\t\t})\n\t}\n\n\t// Configure log filter RPC API.\n\tfilterSystem := utils.RegisterFilterAPI(stack, backend, &cfg.Eth)\n\n\t// Configure GraphQL if requested.\n\tif ctx.IsSet(utils.GraphQLEnabledFlag.Name) {\n\t\tutils.RegisterGraphQLService(stack, backend, filterSystem, &cfg.Node)\n\t}\n\t// Add the Ethereum Stats daemon if requested.\n\tif cfg.Ethstats.URL != \"\" {\n\t\tutils.RegisterEthStatsService(stack, backend, cfg.Ethstats.URL)\n\t}\n\n\tif ctx.IsSet(utils.DeveloperFlag.Name) {\n\t\t// Start dev mode.\n\t\tsimBeacon, err := catalyst.NewSimulatedBeacon(ctx.Uint64(utils.DeveloperPeriodFlag.Name), cfg.Eth.Miner.Etherbase, eth)\n\t\tif err != nil {\n\t\t\tutils.Fatalf(\"failed to register dev mode catalyst service: %v\", err)\n\t\t}\n\t\tcatalyst.RegisterSimulatedBeaconAPIs(stack, simBeacon)\n\t\tstack.RegisterLifecycle(simBeacon)\n\n\t\tbanner := constructDevModeBanner(ctx, cfg)\n\t\tfor _, line := range strings.Split(banner, \"\\n\") {\n\t\t\tlog.Warn(line)\n\t\t}\n\t}\n\n\tif ctx.IsSet(utils.FakeBeaconAddrFlag.Name) {\n\t\tcfg.FakeBeacon.Addr = ctx.String(utils.FakeBeaconAddrFlag.Name)\n\t}\n\tif ctx.IsSet(utils.FakeBeaconPortFlag.Name) {\n\t\tcfg.FakeBeacon.Port = ctx.Int(utils.FakeBeaconPortFlag.Name)\n\t}\n\tif cfg.FakeBeacon.Enable || ctx.IsSet(utils.FakeBeaconEnabledFlag.Name) {\n\t\tgo fakebeacon.NewService(&cfg.FakeBeacon, backend).Run()\n\t}\n\n\tgit, _ := version.VCS()\n\tutils.SetupMetrics(&cfg.Metrics,\n\t\tutils.EnableBuildInfo(git.Commit, git.Date),\n\t\tutils.EnableMinerInfo(ctx, &cfg.Eth.Miner),\n\t\tutils.EnableNodeInfo(&cfg.Eth.TxPool, stack.Server().NodeInfo()),\n\t\tutils.EnableNodeTrack(ctx, &cfg.Eth, stack),\n\t)\n\treturn stack, backend\n}\n\n// dumpConfig is the dumpconfig command.\nfunc dumpConfig(ctx *cli.Context) error {\n\t_, cfg := makeConfigNode(ctx)\n\tcomment := \"\"\n\n\tif cfg.Eth.Genesis != nil {\n\t\tcfg.Eth.Genesis = nil\n\t\tcomment += \"# Note: this config doesn't contain the genesis block.\\n\\n\"\n\t}\n\n\tout, err := tomlSettings.Marshal(&cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdump := os.Stdout\n\tif ctx.NArg() > 0 {\n\t\tdump, err = os.OpenFile(ctx.Args().Get(0), os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0644)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer dump.Close()\n\t}\n\tdump.WriteString(comment)\n\tdump.Write(out)\n\n\treturn nil\n}\n\nfunc applyMetricConfig(ctx *cli.Context, cfg *gethConfig) {\n\tif ctx.IsSet(utils.MetricsEnabledFlag.Name) {\n\t\tcfg.Metrics.Enabled = ctx.Bool(utils.MetricsEnabledFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsEnabledExpensiveFlag.Name) {\n\t\tlog.Warn(\"Expensive metrics will remain in BSC and may be removed in the future\", \"flag\", utils.MetricsEnabledExpensiveFlag.Name)\n\t\tcfg.Metrics.EnabledExpensive = ctx.Bool(utils.MetricsEnabledExpensiveFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsHTTPFlag.Name) {\n\t\tcfg.Metrics.HTTP = ctx.String(utils.MetricsHTTPFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsPortFlag.Name) {\n\t\tcfg.Metrics.Port = ctx.Int(utils.MetricsPortFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsEnableInfluxDBFlag.Name) {\n\t\tcfg.Metrics.EnableInfluxDB = ctx.Bool(utils.MetricsEnableInfluxDBFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBEndpointFlag.Name) {\n\t\tcfg.Metrics.InfluxDBEndpoint = ctx.String(utils.MetricsInfluxDBEndpointFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBDatabaseFlag.Name) {\n\t\tcfg.Metrics.InfluxDBDatabase = ctx.String(utils.MetricsInfluxDBDatabaseFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBUsernameFlag.Name) {\n\t\tcfg.Metrics.InfluxDBUsername = ctx.String(utils.MetricsInfluxDBUsernameFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBPasswordFlag.Name) {\n\t\tcfg.Metrics.InfluxDBPassword = ctx.String(utils.MetricsInfluxDBPasswordFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBTagsFlag.Name) {\n\t\tcfg.Metrics.InfluxDBTags = ctx.String(utils.MetricsInfluxDBTagsFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsEnableInfluxDBV2Flag.Name) {\n\t\tcfg.Metrics.EnableInfluxDBV2 = ctx.Bool(utils.MetricsEnableInfluxDBV2Flag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBTokenFlag.Name) {\n\t\tcfg.Metrics.InfluxDBToken = ctx.String(utils.MetricsInfluxDBTokenFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBBucketFlag.Name) {\n\t\tcfg.Metrics.InfluxDBBucket = ctx.String(utils.MetricsInfluxDBBucketFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBOrganizationFlag.Name) {\n\t\tcfg.Metrics.InfluxDBOrganization = ctx.String(utils.MetricsInfluxDBOrganizationFlag.Name)\n\t}\n\t// Sanity-check the commandline flags. It is fine if some unused fields is part\n\t// of the toml-config, but we expect the commandline to only contain relevant\n\t// arguments, otherwise it indicates an error.\n\tvar (\n\t\tenableExport   = ctx.Bool(utils.MetricsEnableInfluxDBFlag.Name)\n\t\tenableExportV2 = ctx.Bool(utils.MetricsEnableInfluxDBV2Flag.Name)\n\t)\n\tif enableExport || enableExportV2 {\n\t\tv1FlagIsSet := ctx.IsSet(utils.MetricsInfluxDBUsernameFlag.Name) ||\n\t\t\tctx.IsSet(utils.MetricsInfluxDBPasswordFlag.Name)\n\n\t\tv2FlagIsSet := ctx.IsSet(utils.MetricsInfluxDBTokenFlag.Name) ||\n\t\t\tctx.IsSet(utils.MetricsInfluxDBOrganizationFlag.Name) ||\n\t\t\tctx.IsSet(utils.MetricsInfluxDBBucketFlag.Name)\n\n\t\tif enableExport && v2FlagIsSet {\n\t\t\tutils.Fatalf(\"Flags --influxdb.metrics.organization, --influxdb.metrics.token, --influxdb.metrics.bucket are only available for influxdb-v2\")\n\t\t} else if enableExportV2 && v1FlagIsSet {\n\t\t\tutils.Fatalf(\"Flags --influxdb.metrics.username, --influxdb.metrics.password are only available for influxdb-v1\")\n\t\t}\n\t}\n}\n\nfunc setAccountManagerBackends(conf *node.Config, am *accounts.Manager, keydir string) error {\n\tscryptN := keystore.StandardScryptN\n\tscryptP := keystore.StandardScryptP\n\tif conf.UseLightweightKDF {\n\t\tscryptN = keystore.LightScryptN\n\t\tscryptP = keystore.LightScryptP\n\t}\n\n\t// Assemble the supported backends\n\tif len(conf.ExternalSigner) > 0 {\n\t\tlog.Info(\"Using external signer\", \"url\", conf.ExternalSigner)\n\t\tif extBackend, err := external.NewExternalBackend(conf.ExternalSigner)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/geth/snapshot.go",
          "line": 334,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tvar acc types.StateAccount\n\t\tif err := rlp.DecodeBytes(accIter.Value, &acc)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/geth/snapshot.go",
          "line": 354,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\n\t\t\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\t\t\tlog.Info(\"Traversing state\", \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\tlastReport = time.Now()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif storageIter.Err != nil {\n\t\t\t\tlog.Error(\"Failed to traverse storage trie\", \"root\", acc.Root, \"err\", storageIter.Err)\n\t\t\t\treturn storageIter.Err\n\t\t\t}\n\t\t}\n\t\tif !bytes.Equal(acc.CodeHash, types.EmptyCodeHash.Bytes()) {\n\t\t\tif !rawdb.HasCode(chaindb, common.BytesToHash(acc.CodeHash)) {\n\t\t\t\tlog.Error(\"Code is missing\", \"hash\", common.BytesToHash(acc.CodeHash))\n\t\t\t\treturn errors.New(\"missing code\")\n\t\t\t}\n\t\t\tcodes += 1\n\t\t}\n\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\tlog.Info(\"Traversing state\", \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\tlastReport = time.Now()\n\t\t}\n\t}\n\tif accIter.Err != nil {\n\t\tlog.Error(\"Failed to traverse state trie\", \"root\", root, \"err\", accIter.Err)\n\t\treturn accIter.Err\n\t}\n\tlog.Info(\"State is complete\", \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\treturn nil\n}\n\n// traverseRawState is a helper function used for pruning verification.\n// Basically it just iterates the trie, ensure all nodes and associated\n// contract codes are present. It's basically identical to traverseState\n// but it will check each trie node.\nfunc traverseRawState(ctx *cli.Context) error {\n\tstack, _ := makeConfigNode(ctx)\n\tdefer stack.Close()\n\n\tchaindb := utils.MakeChainDatabase(ctx, stack, true)\n\tdefer chaindb.Close()\n\n\ttriedb := utils.MakeTrieDatabase(ctx, stack, chaindb, false, true, false)\n\tdefer triedb.Close()\n\n\theadBlock := rawdb.ReadHeadBlock(chaindb)\n\tif headBlock == nil {\n\t\tlog.Error(\"Failed to load head block\")\n\t\treturn errors.New(\"no head block\")\n\t}\n\tif ctx.NArg() > 1 {\n\t\tlog.Error(\"Too many arguments given\")\n\t\treturn errors.New(\"too many arguments\")\n\t}\n\tvar (\n\t\troot common.Hash\n\t\terr  error\n\t)\n\tif ctx.NArg() == 1 {\n\t\troot, err = parseRoot(ctx.Args().First())\n\t\tif err != nil {\n\t\t\tlog.Error(\"Failed to resolve state root\", \"err\", err)\n\t\t\treturn err\n\t\t}\n\t\tlog.Info(\"Start traversing the state\", \"root\", root)\n\t} else {\n\t\troot = headBlock.Root()\n\t\tlog.Info(\"Start traversing the state\", \"root\", root, \"number\", headBlock.NumberU64())\n\t}\n\tt, err := trie.NewStateTrie(trie.StateTrieID(root), triedb)\n\tif err != nil {\n\t\tlog.Error(\"Failed to open trie\", \"root\", root, \"err\", err)\n\t\treturn err\n\t}\n\tvar (\n\t\tnodes      int\n\t\taccounts   int\n\t\tslots      int\n\t\tcodes      int\n\t\tlastReport time.Time\n\t\tstart      = time.Now()\n\t\thasher     = crypto.NewKeccakState()\n\t\tgot        = make([]byte, 32)\n\t)\n\taccIter, err := t.NodeIterator(nil)\n\tif err != nil {\n\t\tlog.Error(\"Failed to open iterator\", \"root\", root, \"err\", err)\n\t\treturn err\n\t}\n\treader, err := triedb.NodeReader(root)\n\tif err != nil {\n\t\tlog.Error(\"State is non-existent\", \"root\", root)\n\t\treturn nil\n\t}\n\tfor accIter.Next(true) {\n\t\tnodes += 1\n\t\tnode := accIter.Hash()\n\n\t\t// Check the present for non-empty hash node(embedded node doesn't\n\t\t// have their own hash).\n\t\tif node != (common.Hash{}) {\n\t\t\tblob, _ := reader.Node(common.Hash{}, accIter.Path(), node)\n\t\t\tif len(blob) == 0 {\n\t\t\t\tlog.Error(\"Missing trie node(account)\", \"hash\", node)\n\t\t\t\treturn errors.New(\"missing account\")\n\t\t\t}\n\t\t\thasher.Reset()\n\t\t\thasher.Write(blob)\n\t\t\thasher.Read(got)\n\t\t\tif !bytes.Equal(got, node.Bytes()) {\n\t\t\t\tlog.Error(\"Invalid trie node(account)\", \"hash\", node.Hex(), \"value\", blob)\n\t\t\t\treturn errors.New(\"invalid account node\")\n\t\t\t}\n\t\t}\n\t\t// If it's a leaf node, yes we are touching an account,\n\t\t// dig into the storage trie further.\n\t\tif accIter.Leaf() {\n\t\t\taccounts += 1\n\t\t\tvar acc types.StateAccount\n\t\t\tif err := rlp.DecodeBytes(accIter.LeafBlob(), &acc)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/cmd/geth/snapshot.go",
          "line": 491,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\t\tnode := storageIter.Hash()\n\n\t\t\t\t\t// Check the presence for non-empty hash node(embedded node doesn't\n\t\t\t\t\t// have their own hash).\n\t\t\t\t\tif node != (common.Hash{}) {\n\t\t\t\t\t\tblob, _ := reader.Node(common.BytesToHash(accIter.LeafKey()), storageIter.Path(), node)\n\t\t\t\t\t\tif len(blob) == 0 {\n\t\t\t\t\t\t\tlog.Error(\"Missing trie node(storage)\", \"hash\", node)\n\t\t\t\t\t\t\treturn errors.New(\"missing storage\")\n\t\t\t\t\t\t}\n\t\t\t\t\t\thasher.Reset()\n\t\t\t\t\t\thasher.Write(blob)\n\t\t\t\t\t\thasher.Read(got)\n\t\t\t\t\t\tif !bytes.Equal(got, node.Bytes()) {\n\t\t\t\t\t\t\tlog.Error(\"Invalid trie node(storage)\", \"hash\", node.Hex(), \"value\", blob)\n\t\t\t\t\t\t\treturn errors.New(\"invalid storage node\")\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Bump the counter if it's leaf node.\n\t\t\t\t\tif storageIter.Leaf() {\n\t\t\t\t\t\tslots += 1\n\t\t\t\t\t}\n\t\t\t\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\t\t\t\tlog.Info(\"Traversing state\", \"nodes\", nodes, \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\t\tlastReport = time.Now()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif storageIter.Error() != nil {\n\t\t\t\t\tlog.Error(\"Failed to traverse storage trie\", \"root\", acc.Root, \"err\", storageIter.Error())\n\t\t\t\t\treturn storageIter.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !bytes.Equal(acc.CodeHash, types.EmptyCodeHash.Bytes()) {\n\t\t\t\tif !rawdb.HasCode(chaindb, common.BytesToHash(acc.CodeHash)) {\n\t\t\t\t\tlog.Error(\"Code is missing\", \"account\", common.BytesToHash(accIter.LeafKey()))\n\t\t\t\t\treturn errors.New(\"missing code\")\n\t\t\t\t}\n\t\t\t\tcodes += 1\n\t\t\t}\n\t\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\t\tlog.Info(\"Traversing state\", \"nodes\", nodes, \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\tlastReport = time.Now()\n\t\t\t}\n\t\t}\n\t}\n\tif accIter.Error() != nil {\n\t\tlog.Error(\"Failed to traverse state trie\", \"root\", root, \"err\", accIter.Error())\n\t\treturn accIter.Error()\n\t}\n\tlog.Info(\"State is complete\", \"nodes\", nodes, \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\treturn nil\n}\n\nfunc parseRoot(input string) (common.Hash, error) {\n\tvar h common.Hash\n\tif err := h.UnmarshalText([]byte(input))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/geth/dbcmd.go",
          "line": 337,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"\\t- %s\\n\", path)\n\t}\n\tfmt.Println(msg)\n\tif ctx.IsSet(removeFlagName) {\n\t\tconfirm = ctx.Bool(removeFlagName)\n\t\tif confirm {\n\t\t\tfmt.Printf(\"Remove '%s'? [y/n] y\\n\", kind)\n\t\t} else {\n\t\t\tfmt.Printf(\"Remove '%s'? [y/n] n\\n\", kind)\n\t\t}\n\t} else {\n\t\tconfirm, err = prompt.Stdin.PromptConfirm(fmt.Sprintf(\"Remove '%s'?\", kind))\n\t}\n\tswitch {\n\tcase err != nil:\n\t\tutils.Fatalf(\"%v\", err)\n\tcase !confirm:\n\t\tlog.Info(\"Database deletion skipped\", \"kind\", kind, \"paths\", paths)\n\tdefault:\n\t\tvar (\n\t\t\tdeleted []string\n\t\t\tstart   = time.Now()\n\t\t)\n\t\tfor _, path := range paths {\n\t\t\tif common.FileExist(path) {\n\t\t\t\tremoveFolder(path)\n\t\t\t\tdeleted = append(deleted, path)\n\t\t\t} else {\n\t\t\t\tlog.Info(\"Folder is not existent\", \"path\", path)\n\t\t\t}\n\t\t}\n\t\tlog.Info(\"Database successfully deleted\", \"kind\", kind, \"paths\", deleted, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t}\n}\n\nfunc inspectTrie(ctx *cli.Context) error {\n\tif ctx.NArg() < 1 {\n\t\treturn fmt.Errorf(\"required arguments: %v\", ctx.Command.ArgsUsage)\n\t}\n\n\tif ctx.NArg() > 3 {\n\t\treturn fmt.Errorf(\"Max 3 arguments: %v\", ctx.Command.ArgsUsage)\n\t}\n\n\tvar (\n\t\tblockNumber  uint64\n\t\ttrieRootHash common.Hash\n\t\tjobnum       uint64\n\t\ttopN         uint64\n\t)\n\n\tstack, _ := makeConfigNode(ctx)\n\tdefer stack.Close()\n\n\tdb := utils.MakeChainDatabase(ctx, stack, true)\n\tdefer db.Close()\n\tvar headerBlockHash common.Hash\n\tif ctx.NArg() >= 1 {\n\t\tif ctx.Args().Get(0) == \"latest\" {\n\t\t\theaderHash := rawdb.ReadHeadHeaderHash(db)\n\t\t\tblockNumber = *(rawdb.ReadHeaderNumber(db, headerHash))\n\t\t} else if ctx.Args().Get(0) == \"snapshot\" {\n\t\t\ttrieRootHash = rawdb.ReadSnapshotRoot(db)\n\t\t\tblockNumber = math.MaxUint64\n\t\t} else {\n\t\t\tvar err error\n\t\t\tblockNumber, err = strconv.ParseUint(ctx.Args().Get(0), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse blocknum, Args[0]: %v, err: %v\", ctx.Args().Get(0), err)\n\t\t\t}\n\t\t}\n\n\t\tif ctx.NArg() == 1 {\n\t\t\tjobnum = 1000\n\t\t\ttopN = 10\n\t\t} else if ctx.NArg() == 2 {\n\t\t\tvar err error\n\t\t\tjobnum, err = strconv.ParseUint(ctx.Args().Get(1), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse jobnum, Args[1]: %v, err: %v\", ctx.Args().Get(1), err)\n\t\t\t}\n\t\t\ttopN = 10\n\t\t} else {\n\t\t\tvar err error\n\t\t\tjobnum, err = strconv.ParseUint(ctx.Args().Get(1), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse jobnum, Args[1]: %v, err: %v\", ctx.Args().Get(1), err)\n\t\t\t}\n\n\t\t\ttopN, err = strconv.ParseUint(ctx.Args().Get(2), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse topn, Args[1]: %v, err: %v\", ctx.Args().Get(1), err)\n\t\t\t}\n\t\t}\n\n\t\tif blockNumber != math.MaxUint64 {\n\t\t\theaderBlockHash = rawdb.ReadCanonicalHash(db, blockNumber)\n\t\t\tif headerBlockHash == (common.Hash{}) {\n\t\t\t\treturn errors.New(\"ReadHeadBlockHash empty hash\")\n\t\t\t}\n\t\t\tblockHeader := rawdb.ReadHeader(db, headerBlockHash, blockNumber)\n\t\t\ttrieRootHash = blockHeader.Root\n\t\t}\n\t\tif (trieRootHash == common.Hash{}) {\n\t\t\tlog.Error(\"Empty root hash\")\n\t\t}\n\t\tfmt.Printf(\"ReadBlockHeader, root: %v, blocknum: %v\\n\", trieRootHash, blockNumber)\n\n\t\tdbScheme := rawdb.ReadStateScheme(db)\n\t\tvar config *triedb.Config\n\t\tif dbScheme == rawdb.PathScheme {\n\t\t\tconfig = &triedb.Config{\n\t\t\t\tPathDB: utils.PathDBConfigAddJournalFilePath(stack, pathdb.ReadOnly),\n\t\t\t}\n\t\t} else if dbScheme == rawdb.HashScheme {\n\t\t\tconfig = triedb.HashDefaults\n\t\t}\n\n\t\ttriedb := triedb.NewDatabase(db, config)\n\t\ttheTrie, err := trie.New(trie.TrieID(trieRootHash), triedb)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"fail to new trie tree, err: %v, rootHash: %v\\n\", err, trieRootHash.String())\n\t\t\treturn err\n\t\t}\n\t\ttheInspect, err := trie.NewInspector(theTrie, triedb, trieRootHash, blockNumber, jobnum, int(topN))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttheInspect.Run()\n\t\ttheInspect.DisplayResult()\n\t}\n\treturn nil\n}\n\nfunc inspect(ctx *cli.Context) error {\n\tvar (\n\t\tprefix []byte\n\t\tstart  []byte\n\t)\n\tif ctx.NArg() > 2 {\n\t\treturn fmt.Errorf(\"max 2 arguments: %v\", ctx.Command.ArgsUsage)\n\t}\n\tif ctx.NArg() >= 1 {\n\t\tif d, err := hexutil.Decode(ctx.Args().Get(0))",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/rlpdump/main.go",
          "line": 237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\treturn n, err\n}\n\nfunc (rc *inStream) ReadByte() (byte, error) {\n\tb, err := rc.br.ReadByte()\n\tif err == nil {\n\t\trc.pos++\n\t}\n\treturn b, err\n}\n\nfunc (rc *inStream) posLabel() string {\n\tl := strconv.FormatInt(int64(rc.pos), 10)\n\tif len(l) < rc.columns {\n\t\tl = strings.Repeat(\" \", rc.columns-len(l)) + l\n\t}\n\treturn l\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 315,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\n\t\t\t\t// Give the user some feedback that something is happening.\n\t\t\t\tif time.Since(reported) >= 8*time.Second {\n\t\t\t\t\tlog.Info(\"Importing Era files\", \"head\", it.Number(), \"imported\", imported, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\timported = 0\n\t\t\t\t\treported = time.Now()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc missingBlocks(chain *core.BlockChain, blocks []*types.Block) []*types.Block {\n\thead := chain.CurrentBlock()\n\tfor i, block := range blocks {\n\t\t// If we're behind the chain head, only check block, state is available at head\n\t\tif head.Number.Uint64() > block.NumberU64() {\n\t\t\tif !chain.HasBlock(block.Hash(), block.NumberU64()) {\n\t\t\t\treturn blocks[i:]\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// If we're above the chain head, state availability is a must\n\t\tif !chain.HasBlockAndState(block.Hash(), block.NumberU64()) {\n\t\t\treturn blocks[i:]\n\t\t}\n\t}\n\treturn nil\n}\n\n// ExportChain exports a blockchain into the specified file, truncating any data\n// already present in the file.\nfunc ExportChain(blockchain *core.BlockChain, fn string) error {\n\tlog.Info(\"Exporting blockchain\", \"file\", fn)\n\n\t// Open the file handle and potentially wrap with a gzip stream\n\tfh, err := os.OpenFile(fn, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.ModePerm)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer fh.Close()\n\n\tvar writer io.Writer = fh\n\tif strings.HasSuffix(fn, \".gz\") {\n\t\twriter = gzip.NewWriter(writer)\n\t\tdefer writer.(*gzip.Writer).Close()\n\t}\n\t// Iterate over the blocks and export them\n\tif err := blockchain.Export(writer)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 425,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= step {\n\t\terr := func() error {\n\t\t\tfilename := filepath.Join(dir, era.Filename(network, int(i/step), common.Hash{}))\n\t\t\tf, err := os.Create(filename)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not create era file: %w\", err)\n\t\t\t}\n\t\t\tdefer f.Close()\n\n\t\t\tw := era.NewBuilder(f)\n\t\t\tfor j := uint64(0)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 616,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\thashCh <- hashAndPreimageSize{Hash: accIt.Hash(), Size: common.AddressLength}\n\n\t\t\tif acc.Root != (common.Hash{}) && acc.Root != types.EmptyRootHash {\n\t\t\t\tstIt, err := snaptree.StorageIterator(root, accIt.Hash(), common.Hash{})\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Error(\"Failed to create storage iterator\", \"error\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfor stIt.Next() {\n\t\t\t\t\tpreimages += 1\n\t\t\t\t\thashCh <- hashAndPreimageSize{Hash: stIt.Hash(), Size: common.HashLength}\n\n\t\t\t\t\tif time.Since(logged) > time.Second*8 {\n\t\t\t\t\t\tlogged = time.Now()\n\t\t\t\t\t\tlog.Info(\"Exporting preimages\", \"count\", preimages, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tstIt.Release()\n\t\t\t}\n\t\t\tif time.Since(logged) > time.Second*8 {\n\t\t\t\tlogged = time.Now()\n\t\t\t\tlog.Info(\"Exporting preimages\", \"count\", preimages, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor item := range hashCh {\n\t\tpreimage := rawdb.ReadPreimage(chaindb, item.Hash)\n\t\tif len(preimage) == 0 {\n\t\t\treturn fmt.Errorf(\"missing preimage for %v\", item.Hash)\n\t\t}\n\t\tif len(preimage) != item.Size {\n\t\t\treturn fmt.Errorf(\"invalid preimage size, have %d\", len(preimage))\n\t\t}\n\t\trlpenc, err := rlp.EncodeToBytes(preimage)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error encoding preimage: %w\", err)\n\t\t}\n\t\tif _, err := writer.Write(rlpenc)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 773,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\t// Flush the last batch snapshot data\n\tif batch.ValueSize() > 0 {\n\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/utils/flags.go",
          "line": 2144,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= cfg.SnapshotCache\n\t\t\tcfg.SnapshotCache = 0 // Disabled\n\t\t}\n\t}\n\tif ctx.IsSet(VMEnableDebugFlag.Name) {\n\t\tcfg.EnablePreimageRecording = ctx.Bool(VMEnableDebugFlag.Name)\n\t}\n\n\tif ctx.IsSet(RPCGlobalGasCapFlag.Name) {\n\t\tcfg.RPCGasCap = ctx.Uint64(RPCGlobalGasCapFlag.Name)\n\t}\n\tif cfg.RPCGasCap != 0 {\n\t\tlog.Info(\"Set global gas cap\", \"cap\", cfg.RPCGasCap)\n\t} else {\n\t\tlog.Info(\"Global gas cap disabled\")\n\t}\n\tif ctx.IsSet(RPCGlobalEVMTimeoutFlag.Name) {\n\t\tcfg.RPCEVMTimeout = ctx.Duration(RPCGlobalEVMTimeoutFlag.Name)\n\t}\n\tif ctx.IsSet(RPCGlobalTxFeeCapFlag.Name) {\n\t\tcfg.RPCTxFeeCap = ctx.Float64(RPCGlobalTxFeeCapFlag.Name)\n\t}\n\tif ctx.IsSet(NoDiscoverFlag.Name) {\n\t\tcfg.EthDiscoveryURLs, cfg.SnapDiscoveryURLs, cfg.BscDiscoveryURLs = []string{}, []string{}, []string{}\n\t} else if ctx.IsSet(DNSDiscoveryFlag.Name) {\n\t\turls := ctx.String(DNSDiscoveryFlag.Name)\n\t\tif urls == \"\" {\n\t\t\tcfg.EthDiscoveryURLs = []string{}\n\t\t} else {\n\t\t\tcfg.EthDiscoveryURLs = SplitAndTrim(urls)\n\t\t}\n\t}\n\t// Override any default configs for hard coded networks.\n\tswitch {\n\tcase ctx.Bool(BSCMainnetFlag.Name):\n\t\tif !ctx.IsSet(NetworkIdFlag.Name) {\n\t\t\tcfg.NetworkId = 56\n\t\t}\n\t\tcfg.Genesis = core.DefaultBSCGenesisBlock()\n\t\tSetDNSDiscoveryDefaults(cfg, params.BSCGenesisHash)\n\tcase ctx.Bool(ChapelFlag.Name) || cfg.NetworkId == 97:\n\t\tif !ctx.IsSet(NetworkIdFlag.Name) {\n\t\t\tcfg.NetworkId = 97\n\t\t}\n\t\tcfg.Genesis = core.DefaultChapelGenesisBlock()\n\t\tSetDNSDiscoveryDefaults(cfg, params.ChapelGenesisHash)\n\tcase ctx.Bool(DeveloperFlag.Name):\n\t\tcfg.NetworkId = 1337\n\t\tcfg.SyncMode = ethconfig.FullSync\n\t\tcfg.EnablePreimageRecording = true\n\t\t// Create new developer account or reuse existing one\n\t\tvar (\n\t\t\tdeveloper  accounts.Account\n\t\t\tpassphrase string\n\t\t\terr        error\n\t\t)\n\t\tif list := MakePasswordList(ctx)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/utils/export_test.go",
          "line": 53,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif iter.index == 42 {\n\t\titer.index += 1\n\t}\n\treturn OpBatchAdd, fmt.Appendf(nil, \"key-%04d\", iter.index),\n\t\tfmt.Appendf(nil, \"value %d\", iter.index), true\n}\n\nfunc (iter *testIterator) Release() {}\n\nfunc testExport(t *testing.T, f string) {\n\terr := ExportChaindata(f, \"testdata\", newTestIterator(), make(chan struct{}))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdb := rawdb.NewMemoryDatabase()\n\terr = ImportLDBData(db, f, 5, make(chan struct{}))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// verify\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/utils/export_test.go",
          "line": 119,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif iter.index == 42 {\n\t\titer.index += 1\n\t}\n\treturn OpBatchDel, fmt.Appendf(nil, \"key-%04d\", iter.index), nil, true\n}\n\nfunc (iter *deletionIterator) Release() {}\n\nfunc testDeletion(t *testing.T, f string) {\n\terr := ExportChaindata(f, \"testdata\", newDeletionIterator(), make(chan struct{}))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdb := rawdb.NewMemoryDatabase()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/maliciousvote-submit/slash_indicator.go",
          "line": 165,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/maliciousvote-submit/slash_indicator.go",
          "line": 184,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/evm/runner.go",
          "line": 330,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/evm/reporter.go",
          "line": 64,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"\\n\" + string(state)\n\t}\n\treturn out\n}\n\n// report prints the after-test summary.\nfunc report(ctx *cli.Context, results []testResult) {\n\tif ctx.Bool(HumanReadableFlag.Name) {\n\t\tpass := 0\n\t\tfor _, r := range results {\n\t\t\tif r.Pass {\n\t\t\t\tpass++\n\t\t\t}\n\t\t}\n\t\tfor _, r := range results {\n\t\t\tfmt.Println(r)\n\t\t}\n\t\tfmt.Println(\"--\")\n\t\tfmt.Printf(\"%d tests passed, %d tests failed.\\n\", pass, len(results)-pass)\n\t\treturn\n\t}\n\tout, _ := json.MarshalIndent(results, \"\", \"  \")\n\tfmt.Println(string(out))\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/workload/filtertestgen.go",
          "line": 172,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= d\n\t\t} else {\n\t\t\textBefore = q.FromBlock\n\t\t}\n\t}\n\treturn &filterQuery{\n\t\tFromBlock: q.FromBlock - extBefore,\n\t\tToBlock:   q.ToBlock + extAfter,\n\t\tAddress:   q.Address,\n\t\tTopics:    q.Topics,\n\t}\n}\n\n// newQuery generates a new filter query.\nfunc (s *filterTestGen) newQuery() *filterQuery {\n\tfor {\n\t\tt := rand.Intn(100)\n\t\tif t < filterSeedChance {\n\t\t\treturn s.newSeedQuery()\n\t\t}\n\t\tif t < filterSeedChance+filterMergeChance {\n\t\t\tif query := s.newMergedQuery()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/workload/filtertestgen.go",
          "line": 170,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= d\n\t\tif extBefore+d <= q.FromBlock {\n\t\t\textBefore += d\n\t\t} else {\n\t\t\textBefore = q.FromBlock\n\t\t}\n\t}\n\treturn &filterQuery{\n\t\tFromBlock: q.FromBlock - extBefore,\n\t\tToBlock:   q.ToBlock + extAfter,\n\t\tAddress:   q.Address,\n\t\tTopics:    q.Topics,\n\t}\n}\n\n// newQuery generates a new filter query.\nfunc (s *filterTestGen) newQuery() *filterQuery {\n\tfor {\n\t\tt := rand.Intn(100)\n\t\tif t < filterSeedChance {\n\t\t\treturn s.newSeedQuery()\n\t\t}\n\t\tif t < filterSeedChance+filterMergeChance {\n\t\t\tif query := s.newMergedQuery()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/workload/historytestgen.go",
          "line": 82,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= stride {\n\t\ttest.BlockNumbers = append(test.BlockNumbers, b)\n\t}\n\n\t// Get blocks and assign block info into the test\n\tfmt.Println(\"Fetching blocks\")\n\tblocks := make([]*types.Block, len(test.BlockNumbers))\n\tfor i, blocknum := range test.BlockNumbers {\n\t\tb, err := client.Eth.BlockByNumber(ctx, new(big.Int).SetUint64(blocknum))\n\t\tif err != nil {\n\t\t\texit(fmt.Errorf(\"error fetching block %d: %v\", blocknum, err))\n\t\t}\n\t\tblocks[i] = b\n\t}\n\ttest.BlockHashes = make([]common.Hash, len(blocks))\n\ttest.TxCounts = make([]int, len(blocks))\n\tfor i, block := range blocks {\n\t\ttest.BlockHashes[i] = block.Hash()\n\t\ttest.TxCounts[i] = len(block.Transactions())\n\t}\n\n\t// Fill tx index.\n\ttest.TxHashIndex = make([]int, len(blocks))\n\ttest.TxHashes = make([]*common.Hash, len(blocks))\n\tfor i, block := range blocks {\n\t\ttxs := block.Transactions()\n\t\tif len(txs) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tindex := len(txs) / 2\n\t\ttxhash := txs[index].Hash()\n\t\ttest.TxHashIndex[i] = index\n\t\ttest.TxHashes[i] = &txhash\n\t}\n\n\t// Get receipts.\n\tfmt.Println(\"Fetching receipts\")\n\ttest.ReceiptsHashes = make([]common.Hash, len(blocks))\n\tfor i, blockHash := range test.BlockHashes {\n\t\treceipts, err := client.getBlockReceipts(ctx, blockHash)\n\t\tif err != nil {\n\t\t\texit(fmt.Errorf(\"error fetching block %v receipts: %v\", blockHash, err))\n\t\t}\n\t\ttest.ReceiptsHashes[i] = calcReceiptsHash(receipts)\n\t}\n\n\t// Write output file.\n\twriteJSON(outputFile, test)\n\treturn nil\n}\n\nfunc calcReceiptsHash(rcpt []*types.Receipt) common.Hash {\n\th := crypto.NewKeccakState()\n\trlp.Encode(h, rcpt)\n\treturn common.Hash(h.Sum(nil))\n}\n\nfunc writeJSON(fileName string, value any) {\n\tfile, err := os.Create(fileName)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"error creating %s: %v\", fileName, err))\n\t\treturn\n\t}\n\tdefer file.Close()\n\tjson.NewEncoder(file).Encode(value)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/workload/tracetestgen.go",
          "line": 119,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tblob, err := json.Marshal(result)\n\t\t\tif err != nil {\n\t\t\t\tfailed += 1\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttest.TxHashes = append(test.TxHashes, tx.Hash())\n\t\t\ttest.TraceConfigs = append(test.TraceConfigs, *config)\n\t\t\ttest.ResultHashes = append(test.ResultHashes, crypto.Keccak256Hash(blob))\n\t\t\twriteTraceResult(outputDir, tx.Hash(), result, configName)\n\t\t}\n\t\tif time.Since(logged) > time.Second*8 {\n\t\t\tlogged = time.Now()\n\t\t\tlog.Info(\"Tracing transactions\", \"executed\", len(test.TxHashes), \"failed\", failed, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t}\n\t}\n\tlog.Info(\"Traced transactions\", \"executed\", len(test.TxHashes), \"failed\", failed, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\n\t// Write output file.\n\twriteJSON(outputFile, test)\n\treturn nil\n}\n\nfunc randomTraceOption() (*tracers.TraceConfig, string) {\n\tx := rand.Intn(11)\n\tif x == 0 {\n\t\t// struct-logger, with all fields enabled, very heavy\n\t\treturn &tracers.TraceConfig{\n\t\t\tConfig: &logger.Config{\n\t\t\t\tEnableMemory:     true,\n\t\t\t\tEnableReturnData: true,\n\t\t\t},\n\t\t}, \"structAll\"\n\t}\n\tif x == 1 {\n\t\t// default options for struct-logger, with stack and storage capture\n\t\t// enabled\n\t\treturn &tracers.TraceConfig{\n\t\t\tConfig: &logger.Config{},\n\t\t}, \"structDefault\"\n\t}\n\tif x == 2 || x == 3 || x == 4 {\n\t\t// struct-logger with storage capture enabled\n\t\treturn &tracers.TraceConfig{\n\t\t\tConfig: &logger.Config{\n\t\t\t\tDisableStack: true,\n\t\t\t},\n\t\t}, \"structStorage\"\n\t}\n\t// Native tracer\n\tloggers := []string{\"callTracer\", \"4byteTracer\", \"flatCallTracer\", \"muxTracer\", \"noopTracer\", \"prestateTracer\"}\n\treturn &tracers.TraceConfig{\n\t\tTracer: &loggers[x-5],\n\t}, loggers[x-5]\n}\n\nfunc writeTraceResult(dir string, hash common.Hash, result any, configName string) {\n\tif dir == \"\" {\n\t\treturn\n\t}\n\tname := filepath.Join(dir, configName+\"_\"+hash.String())\n\tfile, err := os.Create(name)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"error creating %s: %v\", name, err))\n\t\treturn\n\t}\n\tdefer file.Close()\n\n\tdata, _ := json.MarshalIndent(result, \"\", \"    \")\n\t_, err = file.Write(data)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"error writing %s: %v\", name, err))\n\t\treturn\n\t}\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/workload/filtertest.go",
          "line": 166,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(bucket)\n\t}\n\tif count == 0 {\n\t\treturn fmt.Errorf(\"filterQueryFile %s is empty\", s.cfg.filterQueryFile)\n\t}\n\ts.queries = queries\n\treturn nil\n}\n\n// filterQuery is a single query for testing.\ntype filterQuery struct {\n\tFromBlock  int64            `json:\"fromBlock\"`\n\tToBlock    int64            `json:\"toBlock\"`\n\tAddress    []common.Address `json:\"address\"`\n\tTopics     [][]common.Hash  `json:\"topics\"`\n\tResultHash *common.Hash     `json:\"resultHash,omitempty\"`\n\tresults    []types.Log\n\tErr        error `json:\"error,omitempty\"`\n}\n\nfunc (fq *filterQuery) isWildcard() bool {\n\tif len(fq.Address) != 0 {\n\t\treturn false\n\t}\n\tfor _, topics := range fq.Topics {\n\t\tif len(topics) != 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (fq *filterQuery) calculateHash() common.Hash {\n\tenc, err := rlp.EncodeToBytes(&fq.results)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"Error encoding logs: %v\", err))\n\t}\n\treturn crypto.Keccak256Hash(enc)\n}\n\nfunc (fq *filterQuery) run(client *client, historyPruneBlock *uint64) {\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second*30)\n\tdefer cancel()\n\tlogs, err := client.Eth.FilterLogs(ctx, ethereum.FilterQuery{\n\t\tFromBlock: big.NewInt(fq.FromBlock),\n\t\tToBlock:   big.NewInt(fq.ToBlock),\n\t\tAddresses: fq.Address,\n\t\tTopics:    fq.Topics,\n\t})\n\tfq.results = logs\n\tfq.Err = validateHistoryPruneErr(err, uint64(fq.FromBlock), historyPruneBlock)\n}\n\nfunc (fq *filterQuery) printError() {\n\tfmt.Printf(\"Filter query failed: fromBlock: %d toBlock: %d addresses: %v topics: %v error: %v\\n\",\n\t\tfq.FromBlock, fq.ToBlock, fq.Address, fq.Topics, fq.Err)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/workload/filtertestperf.go",
          "line": 115,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= qt.query.ToBlock + 1 - qt.query.FromBlock\n\t\tbs.count++\n\t\tbs.logs += len(qt.query.results)\n\t\tbs.runtime += qt.medianTime\n\t}\n\n\tfmt.Println()\n\tfor i := range stats {\n\t\tstats[i].print(fmt.Sprintf(\"bucket #%d\", i+1))\n\t}\n\twildcardStats.print(\"wild card queries\")\n\tfmt.Println()\n\tsort.Slice(queries, func(i, j int) bool {\n\t\treturn queries[i].medianTime > queries[j].medianTime\n\t})\n\tfor i, q := range queries {\n\t\tif i >= 10 {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"Most expensive query #%-2d   median runtime: %13v  max runtime: %13v  result count: %4d  fromBlock: %9d  toBlock: %9d  addresses: %v  topics: %v\\n\",\n\t\t\ti+1, q.medianTime, q.runtime[len(q.runtime)-1], len(q.query.results), q.query.FromBlock, q.query.ToBlock, q.query.Address, q.query.Topics)\n\t}\n\twriteErrors(ctx.String(filterErrorFileFlag.Name), errors)\n\treturn nil\n}\n\ntype bucketStats struct {\n\tblocks      int64\n\tcount, logs int\n\truntime     time.Duration\n}\n\nfunc (st *bucketStats) print(name string) {\n\tif st.count == 0 {\n\t\treturn\n\t}\n\tfmt.Printf(\"%-20s queries: %4d  average block length: %12.2f  average log count: %7.2f  average runtime: %13v\\n\",\n\t\tname, st.count, float64(st.blocks)/float64(st.count), float64(st.logs)/float64(st.count), st.runtime/time.Duration(st.count))\n}\n\n// writeQueries serializes the generated errors to the error file.\nfunc writeErrors(errorFile string, errors []*filterQuery) {\n\tfile, err := os.Create(errorFile)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"Error creating filter error file %s: %v\", errorFile, err))\n\t\treturn\n\t}\n\tdefer file.Close()\n\tjson.NewEncoder(file).Encode(errors)\n}\n",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/evm/internal/t8ntool/execution.go",
          "line": 277,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= txBlobGas\n\t\tgasUsed += msgResult.UsedGas\n\n\t\t// Receipt:\n\t\t{\n\t\t\tvar root []byte\n\t\t\tif chainConfig.IsByzantium(vmContext.BlockNumber) {\n\t\t\t\tstatedb.Finalise(true)\n\t\t\t} else {\n\t\t\t\troot = statedb.IntermediateRoot(chainConfig.IsEIP158(vmContext.BlockNumber)).Bytes()\n\t\t\t}\n\n\t\t\t// Create a new receipt for the transaction, storing the intermediate root and\n\t\t\t// gas used by the tx.\n\t\t\treceipt := &types.Receipt{Type: tx.Type(), PostState: root, CumulativeGasUsed: gasUsed}\n\t\t\tif msgResult.Failed() {\n\t\t\t\treceipt.Status = types.ReceiptStatusFailed\n\t\t\t} else {\n\t\t\t\treceipt.Status = types.ReceiptStatusSuccessful\n\t\t\t}\n\t\t\treceipt.TxHash = tx.Hash()\n\t\t\treceipt.GasUsed = msgResult.UsedGas\n\n\t\t\t// If the transaction created a contract, store the creation address in the receipt.\n\t\t\tif msg.To == nil {\n\t\t\t\treceipt.ContractAddress = crypto.CreateAddress(evm.TxContext.Origin, tx.Nonce())\n\t\t\t}\n\n\t\t\t// Set the receipt logs and create the bloom filter.\n\t\t\treceipt.Logs = statedb.GetLogs(tx.Hash(), vmContext.BlockNumber.Uint64(), blockHash, vmContext.Time)\n\t\t\treceipt.Bloom = types.CreateBloom(receipt)\n\n\t\t\t// These three are non-consensus fields:\n\t\t\t//receipt.BlockHash\n\t\t\t//receipt.BlockNumber\n\t\t\treceipt.TransactionIndex = uint(txIndex)\n\t\t\treceipts = append(receipts, receipt)\n\t\t\tif evm.Config.Tracer != nil && evm.Config.Tracer.OnTxEnd != nil {\n\t\t\t\tevm.Config.Tracer.OnTxEnd(receipt, nil)\n\t\t\t}\n\t\t}\n\n\t\ttxIndex++\n\t}\n\tstatedb.IntermediateRoot(chainConfig.IsEIP158(vmContext.BlockNumber))\n\t// Add mining reward? (-1 means rewards are disabled)\n\tif miningReward >= 0 {\n\t\t// Add mining reward. The mining reward may be `0`, which only makes a difference in the cases\n\t\t// where\n\t\t// - the coinbase self-destructed, or\n\t\t// - there are only 'bad' transactions, which aren't executed. In those cases,\n\t\t//   the coinbase gets no txfee, so isn't created, and thus needs to be touched\n\t\tvar (\n\t\t\tblockReward = big.NewInt(miningReward)\n\t\t\tminerReward = new(big.Int).Set(blockReward)\n\t\t\tperOmmer    = new(big.Int).Rsh(blockReward, 5)\n\t\t)\n\t\tfor _, ommer := range pre.Env.Ommers {\n\t\t\t// Add 1/32th for each ommer included\n\t\t\tminerReward.Add(minerReward, perOmmer)\n\t\t\t// Add (8-delta)/8\n\t\t\treward := big.NewInt(8)\n\t\t\treward.Sub(reward, new(big.Int).SetUint64(ommer.Delta))\n\t\t\treward.Mul(reward, blockReward)\n\t\t\treward.Rsh(reward, 3)\n\t\t\tstatedb.AddBalance(ommer.Address, uint256.MustFromBig(reward), tracing.BalanceIncreaseRewardMineUncle)\n\t\t}\n\t\tstatedb.AddBalance(pre.Env.Coinbase, uint256.MustFromBig(minerReward), tracing.BalanceIncreaseRewardMineBlock)\n\t}\n\t// Apply withdrawals\n\tfor _, w := range pre.Env.Withdrawals {\n\t\t// Amount is in gwei, turn into wei\n\t\tamount := new(big.Int).Mul(new(big.Int).SetUint64(w.Amount), big.NewInt(params.GWei))\n\t\tstatedb.AddBalance(w.Address, uint256.MustFromBig(amount), tracing.BalanceIncreaseWithdrawal)\n\t}\n\n\t// Gather the execution-layer triggered requests.\n\tvar requests [][]byte\n\tif chainConfig.IsPrague(vmContext.BlockNumber, vmContext.Time) {\n\t\trequests = [][]byte{}\n\t\t// EIP-6110\n\t\tvar allLogs []*types.Log\n\t\tfor _, receipt := range receipts {\n\t\t\tallLogs = append(allLogs, receipt.Logs...)\n\t\t}\n\t\tif err := core.ParseDepositLogs(&requests, allLogs, chainConfig)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/internal/ethtest/suite.go",
          "line": 896,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\t}\n\t\tinner := &types.BlobTx{\n\t\t\tChainID:    uint256.MustFromBig(s.chain.config.ChainID),\n\t\t\tNonce:      nonce + uint64(i),\n\t\t\tGasTipCap:  uint256.NewInt(1),\n\t\t\tGasFeeCap:  uint256.MustFromBig(s.chain.Head().BaseFee()),\n\t\t\tGas:        100000,\n\t\t\tBlobFeeCap: uint256.MustFromBig(eip4844.CalcBlobFee(s.chain.config, s.chain.Head().Header())),\n\t\t\tBlobHashes: makeSidecar(blobdata...).BlobHashes(),\n\t\t\tSidecar:    makeSidecar(blobdata...),\n\t\t}\n\t\ttx, err := s.chain.SignTx(from, types.NewTx(inner))\n\t\tif err != nil {\n\t\t\tpanic(\"blob tx signing failed\")\n\t\t}\n\t\ttxs = append(txs, tx)\n\t}\n\treturn txs\n}\n\nfunc (s *Suite) TestBlobViolations(t *utesting.T) {\n\tt.Log(`This test sends some invalid blob tx announcements and expects the node to disconnect.`)\n\n\tif err := s.engine.sendForkchoiceUpdated()",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/internal/ethtest/chain.go",
          "line": 180,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= amt\n}\n\n// Balance returns the balance of an account at the head of the chain.\nfunc (c *Chain) Balance(addr common.Address) *big.Int {\n\tbal := new(big.Int)\n\tif acc, ok := c.state[addr]",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/devp2p/internal/ethtest/chain.go",
          "line": 230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= (1 + req.Skip)\n\t\theaders[i] = c.blocks[blockNumber].Header()\n\t}\n\treturn headers, nil\n}\n\n// Shorten returns a copy chain of a desired height from the imported\nfunc (c *Chain) Shorten(height int) *Chain {\n\tblocks := make([]*types.Block, height)\n\tcopy(blocks, c.blocks[:height])\n\n\tconfig := *c.config\n\treturn &Chain{\n\t\tblocks: blocks,\n\t\tconfig: &config,\n\t}\n}\n\nfunc loadGenesis(genesisFile string) (core.Genesis, error) {\n\tchainConfig, err := os.ReadFile(genesisFile)\n\tif err != nil {\n\t\treturn core.Genesis{}, err\n\t}\n\tvar gen core.Genesis\n\tif err := json.Unmarshal(chainConfig, &gen)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/cmd/devp2p/internal/ethtest/chain.go",
          "line": 224,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= (1 - req.Skip)\n\t\t\theaders[i] = c.blocks[blockNumber].Header()\n\t\t}\n\t\treturn headers, nil\n\t}\n\tfor i := 1",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/internal/ethtest/conn.go",
          "line": 154,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= baseProtoLen\n\n\t\tvar msg any\n\t\tswitch int(code) {\n\t\tcase eth.StatusMsg:\n\t\t\tmsg = new(eth.StatusPacket68)\n\t\tcase eth.GetBlockHeadersMsg:\n\t\t\tmsg = new(eth.GetBlockHeadersPacket)\n\t\tcase eth.BlockHeadersMsg:\n\t\t\tmsg = new(eth.BlockHeadersPacket)\n\t\tcase eth.GetBlockBodiesMsg:\n\t\t\tmsg = new(eth.GetBlockBodiesPacket)\n\t\tcase eth.BlockBodiesMsg:\n\t\t\tmsg = new(eth.BlockBodiesPacket)\n\t\tcase eth.NewBlockMsg:\n\t\t\tmsg = new(eth.NewBlockPacket)\n\t\tcase eth.NewBlockHashesMsg:\n\t\t\tmsg = new(eth.NewBlockHashesPacket)\n\t\tcase eth.TransactionsMsg:\n\t\t\tmsg = new(eth.TransactionsPacket)\n\t\tcase eth.NewPooledTransactionHashesMsg:\n\t\t\tmsg = new(eth.NewPooledTransactionHashesPacket)\n\t\tcase eth.GetPooledTransactionsMsg:\n\t\t\tmsg = new(eth.GetPooledTransactionsPacket)\n\t\tcase eth.PooledTransactionsMsg:\n\t\t\tmsg = new(eth.PooledTransactionsPacket)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unhandled eth msg code %d\", code))\n\t\t}\n\t\tif err := rlp.DecodeBytes(data, msg)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/devp2p/internal/ethtest/conn.go",
          "line": 202,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= baseProtoLen + ethProtoLen\n\n\t\tvar msg any\n\t\tswitch int(code) {\n\t\tcase snap.GetAccountRangeMsg:\n\t\t\tmsg = new(snap.GetAccountRangePacket)\n\t\tcase snap.AccountRangeMsg:\n\t\t\tmsg = new(snap.AccountRangePacket)\n\t\tcase snap.GetStorageRangesMsg:\n\t\t\tmsg = new(snap.GetStorageRangesPacket)\n\t\tcase snap.StorageRangesMsg:\n\t\t\tmsg = new(snap.StorageRangesPacket)\n\t\tcase snap.GetByteCodesMsg:\n\t\t\tmsg = new(snap.GetByteCodesPacket)\n\t\tcase snap.ByteCodesMsg:\n\t\t\tmsg = new(snap.ByteCodesPacket)\n\t\tcase snap.GetTrieNodesMsg:\n\t\t\tmsg = new(snap.GetTrieNodesPacket)\n\t\tcase snap.TrieNodesMsg:\n\t\t\tmsg = new(snap.TrieNodesPacket)\n\t\tdefault:\n\t\t\tpanic(fmt.Errorf(\"unhandled snap code: %d\", code))\n\t\t}\n\t\tif err := rlp.DecodeBytes(data, msg)",
          "severity": "HIGH",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 77,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 147,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0002",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 164,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0003",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 183,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0004",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 212,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0005",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 224,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0006",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 242,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0007",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 264,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0008",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 300,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0009",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 324,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0010",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 355,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0011",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 358,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0012",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 385,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0013",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 399,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0014",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 414,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0015",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 441,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0016",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 457,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0017",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 472,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0018",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 495,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/influxdb/influxdbv2.go",
          "line": 67,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/influxdb/influxdb_test.go",
          "line": 62,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/metrics/influxdb/influxdb_test.go",
          "line": 99,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0000",
          "file": "bsc/metrics/influxdb/influxdbv1.go",
          "line": 82,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        },
        {
          "id": "CLASSIC_0001",
          "file": "bsc/metrics/influxdb/influxdbv1.go",
          "line": 110,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "classical",
          "confidence": 0.85
        }
      ],
      "stats": {
        "total_vulnerabilities": 865,
        "confidence": 0.85,
        "severity_distribution": {
          "HIGH": 577,
          "CRITICAL": 2,
          "MEDIUM": 286
        },
        "analysis_time": 2.4758639335632324,
        "files_processed": 1441
      }
    },
    "omega": {
      "vulnerabilities": [
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 59,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 67,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 73,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 79,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 84,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 743,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[0] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 744,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[1] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 791,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[0] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 844,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[0] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 323,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(srcNumA)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 325,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(tarNumA)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 331,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(srcNumB)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 333,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(tarNumB)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 50,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 273,
          "category": "mathematical_inconsistencies",
          "pattern": "block\\.timestamp.*[<>]=",
          "match": "block.timestamp >=",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 333,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(deadAddress, toLock)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 335,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(validator, initShares)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 343,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(account, shares)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 70,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 173,
          "category": "mathematical_inconsistencies",
          "pattern": "block\\.timestamp.*[<>]=",
          "match": "block.timestamp >=",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 130,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 144,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 157,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 190,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 171,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 175,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 179,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 51,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorExtraSet[index] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 160,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[validatorSetPkg.validatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 195,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorSet[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 422,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0007",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 716,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[newValidatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0008",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 717,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0009",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 736,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[newValidatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0010",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 771,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[startIdx + i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0011",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 772,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[offset + random] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0012",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 924,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0013",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 925,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorExtraSet[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0014",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 926,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[currentValidatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0015",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 1003,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[0] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0016",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 1012,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0017",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 1087,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorSet[j] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/GovToken.sol",
          "line": 93,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(account, _needMint)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/GovToken.sol",
          "line": 117,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address to, uint256 amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/GovToken.sol",
          "line": 118,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(to, amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 288,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 292,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 296,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 93,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 97,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 101,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 150,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i]] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 160,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[j]] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 175,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/GovHub.sol",
          "line": 24,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/GovHub.sol",
          "line": 29,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/GovHub.sol",
          "line": 34,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 46,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorSet[j] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/TokenManager.sol",
          "line": 42,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/TokenManager.sol",
          "line": 46,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/deprecated/TokenManager.sol",
          "line": 50,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/RelayerIncentivize.sol",
          "line": 63,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 3,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 10,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 10,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 49,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChainPackage",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 97,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 101,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 105,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 159,
          "category": "topological_instabilities",
          "pattern": "delegated\\w*\\[.*\\]\\s*=",
          "match": "delegated[msg.sender] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/contracts/interface/0.6.x/ICrossChain.sol",
          "line": 3,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 6,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 67,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 67,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 106,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 106,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 107,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 107,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0007",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 142,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0008",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 142,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/interface/ICrossChain.sol",
          "line": 4,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/interface/ICrossChain.sol",
          "line": 16,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChainPackage",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 190,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 175,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 176,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 194,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 212,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 190,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 175,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 176,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 194,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 212,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 184,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 169,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 170,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 188,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 206,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 201,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(uint256 amount) public onlyOwner returns (bool) {\n        _mint(_msgSender(), amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 239,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 224,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 225,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 243,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 261,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 190,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 175,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 176,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 194,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 212,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 158,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(sub(number()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/consensus.go",
          "line": 71,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/state_processor.go",
          "line": 106,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/state_processor.go",
          "line": 319,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(prevHash common.Hash, evm *vm.EVM)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/chain_makers.go",
          "line": 424,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(b.header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/chain_makers.go",
          "line": 538,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(b.header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/blockchain.go",
          "line": 702,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/blockchain.go",
          "line": 702,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/blockchain.go",
          "line": 738,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/blockchain.go",
          "line": 791,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/core/blockchain.go",
          "line": 1103,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db, header.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc/core/blockchain.go",
          "line": 1105,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db, common.Hash{})",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc/core/blockchain.go",
          "line": 1155,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, newHeadBlock.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0007",
          "file": "bsc/core/blockchain.go",
          "line": 1183,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, newHeadSnapBlock.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0008",
          "file": "bsc/core/blockchain.go",
          "line": 1403,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockBatch, block.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0009",
          "file": "bsc/core/blockchain.go",
          "line": 1404,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockBatch, block.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0010",
          "file": "bsc/core/blockchain.go",
          "line": 1503,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db, recent.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0011",
          "file": "bsc/core/blockchain.go",
          "line": 1634,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(batch, hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0012",
          "file": "bsc/core/blockchain.go",
          "line": 3308,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(batch, last.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/blockchain_test.go",
          "line": 235,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockchain.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/blockchain_test.go",
          "line": 1789,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(ancientDb, midBlock.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/headerchain.go",
          "line": 92,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(chainDb)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/txindexer.go",
          "line": 220,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(indexer.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/bench_test.go",
          "line": 298,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8899999999999999,
          "confidence": 0.9801
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/genesis.go",
          "line": 600,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, block.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/genesis.go",
          "line": 601,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, block.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 228,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(t *testing.T)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 246,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 250,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(statedb, uint64(i)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 271,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(statedb *state.StateDB, number uint64, isVerkle bool)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 748,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "Balance[15] = 42",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.9600000000000001,
          "confidence": 0.9864
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 962,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "Balance[15] = 42",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.9600000000000001,
          "confidence": 0.9864
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/tests/state_test_util.go",
          "line": 494,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(n uint64)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8899999999999999,
          "confidence": 0.9801
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/ethclient/ethclient.go",
          "line": 773,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(ctx context.Context, msg ethereum.CallMsg, blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/ethclient/ethclient_test.go",
          "line": 657,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(context.Background()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/handler.go",
          "line": 849,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/eth/handler.go",
          "line": 234,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorAddressMap[address] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/state_accessor.go",
          "line": 261,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/handler_test.go",
          "line": 379,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/miner/worker.go",
          "line": 1104,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.ParentHash, env.evm)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/beacon/types/exec_header.go",
          "line": 73,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8099999999999999,
          "confidence": 0.9729
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/filters/filter_test.go",
          "line": 106,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, block.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 96,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(b.db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.85,
          "confidence": 0.9765
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 179,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.Number.Uint64()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 227,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(payload.Number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 292,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/tracers/api.go",
          "line": 418,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(next.ParentHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/eth/tracers/api.go",
          "line": 578,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/eth/tracers/api.go",
          "line": 661,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/eth/tracers/api.go",
          "line": 886,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/eth/tracers/api.go",
          "line": 1096,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(n)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.82,
          "confidence": 0.9738
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 778,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BLOCKHASH(0)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.82,
          "confidence": 0.9738
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 780,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BLOCKHASH(0x1336)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 782,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BLOCKHASH(0x1337)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 214,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "Balance[addr] = (*hexutil.Big)(new)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 300,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block *types.Block)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/internal/ethapi/simulate.go",
          "line": 292,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/internal/ethapi/api.go",
          "line": 661,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(ctx context.Context, blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 348,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 291,
          "category": "spectral_anomalies",
          "pattern": "votes\\[.*\\]\\s*=",
          "match": "Votes[blockHash] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/vote/vote_pool_test.go",
          "line": 364,
          "category": "spectral_anomalies",
          "pattern": "votes\\[.*\\]\\s*=",
          "match": "Votes[futureBlockHash] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/monitor/malicious_vote_monitor.go",
          "line": 40,
          "category": "spectral_anomalies",
          "pattern": "votes\\[.*\\]\\s*=",
          "match": "Votes[newVote.VoteAddress] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 145,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 161,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 262,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(nfdb)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.85,
          "confidence": 0.9765
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 289,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(nfdb)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.85,
          "confidence": 0.9765
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 85,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8899999999999999,
          "confidence": 0.9801
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/rawdb/database.go",
          "line": 994,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/rawdb/database.go",
          "line": 995,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 188,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueReader)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 197,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueWriter, hash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 204,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueReader)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 213,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueWriter, hash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 220,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueReader)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 229,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueWriter, hash common.Hash)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 1129,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 326,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 329,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 334,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, blockFull.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 335,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, blockFast.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 341,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 344,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.83,
          "confidence": 0.9747
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 152,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 154,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 208,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 210,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 357,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 359,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 378,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0007",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 380,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0008",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 399,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0009",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 401,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0010",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 420,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0011",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 422,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0012",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 456,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0013",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 458,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0014",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 487,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0015",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 489,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0016",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 592,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0017",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 594,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0018",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 658,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0019",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 660,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0020",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 835,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0021",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 837,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0022",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 926,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0023",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 928,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/const.go",
          "line": 14,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/vm/instructions.go",
          "line": 426,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "Blockhash(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/vm/instructions.go",
          "line": 444,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(num64)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/stateless/witness.go",
          "line": 74,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 72,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 79,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 94,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 99,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0004",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 105,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0005",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 117,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0006",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 135,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(n)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.82,
          "confidence": 0.9738
        },
        {
          "id": "OMEGA_0007",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 135,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(n)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.82,
          "confidence": 0.9738
        },
        {
          "id": "OMEGA_0008",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 166,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0009",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 166,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001
        },
        {
          "id": "OMEGA_0010",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 198,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 322,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "Blockhash(t *testing.T)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 349,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(x)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.82,
          "confidence": 0.9738
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 350,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(x-1)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.84,
          "confidence": 0.9756
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 352,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(x - i)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.86,
          "confidence": 0.9774
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 15,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 16,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 29,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 30,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 11,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 12,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 21,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 22,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 35,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 36,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 90,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 91,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/niels/types.go",
          "line": 25,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/niels/types.go",
          "line": 26,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 11,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 12,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 21,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 22,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 15,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 16,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 43,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0003",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 44,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/core/systemcontracts/ramanujan/types.go",
          "line": 25,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/core/systemcontracts/ramanujan/types.go",
          "line": 26,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1043,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(parent.Hash()",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 109,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1999,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/parlia/abi.go",
          "line": 1067,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/parlia/snapshot_test.go",
          "line": 17,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 88,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[v] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 92,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[v] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 166,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[v] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 87,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 130,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[down[i]] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0002",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 429,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 167,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorItems[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0001",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 228,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[i] =",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 1.0,
          "confidence": 0.99
        },
        {
          "id": "OMEGA_0000",
          "file": "bsc/cmd/evm/internal/t8ntool/execution.go",
          "line": 221,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(prevHash, evm)",
          "severity": "CRITICAL",
          "model": "omega",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981
        }
      ],
      "stats": {
        "total_vulnerabilities": 276,
        "confidence": 0.9860119565217391,
        "severity_distribution": {
          "CRITICAL": 276
        },
        "analysis_time": 2.4758639335632324,
        "files_processed": 1441
      }
    },
    "ensemble": {
      "vulnerabilities": [
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/ValidatorSet.t.sol",
          "line": 305,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= systemRewardAntiMEVRatio * (block.number % turnLength) / (turnLength - 1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 59,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 67,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 73,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 79,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/GovHub.t.sol",
          "line": 84,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 273,
          "category": "reentrancy",
          "pattern": "address\\(.*\\)\\.call",
          "match": "address(stakeHub).call",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 275,
          "category": "reentrancy",
          "pattern": "address\\(.*\\)\\.call",
          "match": "address(stakeHub).call",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 743,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[0] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 744,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[1] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 791,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[0] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/test/StakeHub.t.sol",
          "line": 844,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorsToQuery[0] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 353,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= systemRewardAntiMEVRatio * (block.number % turnLength) / (turnLength - 1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 323,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(srcNumA)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 325,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(tarNumA)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 331,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(srcNumB)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/SlashIndicator.t.sol",
          "line": 333,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(tarNumB)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 81,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 40,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCoinbase",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 45,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyZeroGasPrice",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 50,
          "category": "ensemble_access_control",
          "pattern": "crossChain\\w* | modifier\\s+only\\w+",
          "match": "modifier onlyCrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": true,
          "omega_detected": true,
          "ensemble_confidence": 0.948,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 55,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyValidatorContract",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 60,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlySlash",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 65,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGov",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 70,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGovernor",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 75,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyStakeHub",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc-genesis-contract/contracts/SystemV2.sol",
          "line": 80,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenRecoverPortal",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 76,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 77,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 170,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= request.bnbAmount",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 195,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= _reward",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 196,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= _reward",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 299,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= request.bnbAmount",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 344,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= bnbAmount",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 350,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= bnbAmount",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 273,
          "category": "mathematical_inconsistencies",
          "pattern": "block\\.timestamp.*[<>]=",
          "match": "block.timestamp >=",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 333,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(deadAddress, toLock)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 335,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(validator, initShares)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc-genesis-contract/contracts/StakeCredit.sol",
          "line": 343,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(account, shares)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 31,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 51,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 56,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 61,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 66,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 71,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 76,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 86,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 91,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 96,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 101,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 30,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCoinbase",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 35,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyZeroGasPrice",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 40,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyNotInit",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 45,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyInit",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 50,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlySlash",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0016",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 55,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenHub",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0017",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 60,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGov",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0018",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 65,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyValidatorContract",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0019",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 70,
          "category": "ensemble_access_control",
          "pattern": "crossChain\\w* | modifier\\s+only\\w+",
          "match": "modifier onlyCrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": true,
          "omega_detected": true,
          "ensemble_confidence": 0.948,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0020",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 75,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyRelayerIncentivize",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0021",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 80,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyRelayer",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0022",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 85,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenManager",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0023",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 90,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyStakeHub",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0024",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 95,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyGovernorTimelock",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0025",
          "file": "bsc-genesis-contract/contracts/System.sol",
          "line": 100,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenRecoverPortal",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 77,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 76,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyTokenOwner",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 173,
          "category": "mathematical_inconsistencies",
          "pattern": "block\\.timestamp.*[<>]=",
          "match": "block.timestamp >=",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 130,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 144,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 157,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/contracts/TokenHub.sol",
          "line": 190,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 255,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= systemRewardAntiMEVRatio * (block.number % turnLength) / (turnLength - 1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 314,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= weights[i]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 529,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 171,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 175,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 179,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 51,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorExtraSet[index] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 160,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[validatorSetPkg.validatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 195,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorSet[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 422,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 716,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[newValidatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 717,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 736,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[newValidatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 771,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[startIdx + i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 772,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[offset + random] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 924,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0016",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 925,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorExtraSet[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0017",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 926,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSetMap[currentValidatorSet[i].consensusAddress] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0018",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 1003,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[0] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0019",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 1012,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorSet[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0020",
          "file": "bsc-genesis-contract/contracts/BSCValidatorSet.sol",
          "line": 1087,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorSet[j] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/GovToken.sol",
          "line": 93,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(account, _needMint)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/GovToken.sol",
          "line": 117,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address to, uint256 amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/GovToken.sol",
          "line": 118,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(to, amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/TokenRecoverPortal.sol",
          "line": 172,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 27",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/SystemReward.sol",
          "line": 24,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyOperator",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 700,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 732,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 1265,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 495,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 588,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= feeCharge",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 288,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 292,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc-genesis-contract/contracts/StakeHub.sol",
          "line": 296,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 93,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 97,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 101,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 150,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i]] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 160,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[j]] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/SlashIndicator.sol",
          "line": 175,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/GovHub.sol",
          "line": 24,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/GovHub.sol",
          "line": 29,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/GovHub.sol",
          "line": 34,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 20,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 56,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= wad",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 25,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wad",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 52,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wad",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/wbnb/WBNB.sol",
          "line": 55,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wad",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/extension/Protectable.sol",
          "line": 39,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyProtector",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 93,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 99,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 104,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/extension/BSCValidatorSetTool.sol",
          "line": 46,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorSet[j] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/RelayerHub.sol",
          "line": 35,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyManager",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/RelayerHub.sol",
          "line": 45,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyProvisionalRelayer",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/TokenManager.sol",
          "line": 42,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/TokenManager.sol",
          "line": 46,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/deprecated/TokenManager.sol",
          "line": 50,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/RelayerIncentivize.sol",
          "line": 63,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 95,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyRegisteredContractChannel",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 111,
          "category": "access_control",
          "pattern": "modifier\\s+only\\w+",
          "match": "modifier onlyCabinet",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 3,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 10,
          "category": "ensemble_quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.693,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/deprecated/CrossChain.sol",
          "line": 49,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChainPackage",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 97,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 101,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 105,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/deprecated/Staking.sol",
          "line": 159,
          "category": "topological_instabilities",
          "pattern": "delegated\\w*\\[.*\\]\\s*=",
          "match": "delegated[msg.sender] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.8.x/RLPDecode.sol",
          "line": 271,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.8.x/RLPDecode.sol",
          "line": 272,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.8.x/RLPDecode.sol",
          "line": 266,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n\n            src += WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/RLPDecode.sol",
          "line": 263,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/RLPDecode.sol",
          "line": 264,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/RLPDecode.sol",
          "line": 258,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n\n            src += WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/SizeOf.sol",
          "line": 17,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 33,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 39,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 43,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 49,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 53,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/MerkleProof.sol",
          "line": 60,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/BytesToTypes.sol",
          "line": 50,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n            }\n        }\n    }\n\n    function bytesToBytes32(uint256 _offst, bytes memory _input, bytes32 _output) internal pure {\n        assembly {\n            mstore(_output, add(_input, _offst))\n            mstore(add(_output, 32), add(add(_input, _offst), 32))\n        }\n    }\n\n    function bytesToInt8(uint256 _offst, bytes memory _input) internal pure returns (int8 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt16(uint256 _offst, bytes memory _input) internal pure returns (int16 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt24(uint256 _offst, bytes memory _input) internal pure returns (int24 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt32(uint256 _offst, bytes memory _input) internal pure returns (int32 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt40(uint256 _offst, bytes memory _input) internal pure returns (int40 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt48(uint256 _offst, bytes memory _input) internal pure returns (int48 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt56(uint256 _offst, bytes memory _input) internal pure returns (int56 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt64(uint256 _offst, bytes memory _input) internal pure returns (int64 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt72(uint256 _offst, bytes memory _input) internal pure returns (int72 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt80(uint256 _offst, bytes memory _input) internal pure returns (int80 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt88(uint256 _offst, bytes memory _input) internal pure returns (int88 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt96(uint256 _offst, bytes memory _input) internal pure returns (int96 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt104(uint256 _offst, bytes memory _input) internal pure returns (int104 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt112(uint256 _offst, bytes memory _input) internal pure returns (int112 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt120(uint256 _offst, bytes memory _input) internal pure returns (int120 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt128(uint256 _offst, bytes memory _input) internal pure returns (int128 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt136(uint256 _offst, bytes memory _input) internal pure returns (int136 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt144(uint256 _offst, bytes memory _input) internal pure returns (int144 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt152(uint256 _offst, bytes memory _input) internal pure returns (int152 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt160(uint256 _offst, bytes memory _input) internal pure returns (int160 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt168(uint256 _offst, bytes memory _input) internal pure returns (int168 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt176(uint256 _offst, bytes memory _input) internal pure returns (int176 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt184(uint256 _offst, bytes memory _input) internal pure returns (int184 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt192(uint256 _offst, bytes memory _input) internal pure returns (int192 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt200(uint256 _offst, bytes memory _input) internal pure returns (int200 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt208(uint256 _offst, bytes memory _input) internal pure returns (int208 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt216(uint256 _offst, bytes memory _input) internal pure returns (int216 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt224(uint256 _offst, bytes memory _input) internal pure returns (int224 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt232(uint256 _offst, bytes memory _input) internal pure returns (int232 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt240(uint256 _offst, bytes memory _input) internal pure returns (int240 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt248(uint256 _offst, bytes memory _input) internal pure returns (int248 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToInt256(uint256 _offst, bytes memory _input) internal pure returns (int256 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint8(uint256 _offst, bytes memory _input) internal pure returns (uint8 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint16(uint256 _offst, bytes memory _input) internal pure returns (uint16 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint24(uint256 _offst, bytes memory _input) internal pure returns (uint24 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint32(uint256 _offst, bytes memory _input) internal pure returns (uint32 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint40(uint256 _offst, bytes memory _input) internal pure returns (uint40 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint48(uint256 _offst, bytes memory _input) internal pure returns (uint48 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint56(uint256 _offst, bytes memory _input) internal pure returns (uint56 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint64(uint256 _offst, bytes memory _input) internal pure returns (uint64 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint72(uint256 _offst, bytes memory _input) internal pure returns (uint72 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint80(uint256 _offst, bytes memory _input) internal pure returns (uint80 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint88(uint256 _offst, bytes memory _input) internal pure returns (uint88 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint96(uint256 _offst, bytes memory _input) internal pure returns (uint96 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint104(uint256 _offst, bytes memory _input) internal pure returns (uint104 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint112(uint256 _offst, bytes memory _input) internal pure returns (uint112 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint120(uint256 _offst, bytes memory _input) internal pure returns (uint120 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint128(uint256 _offst, bytes memory _input) internal pure returns (uint128 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint136(uint256 _offst, bytes memory _input) internal pure returns (uint136 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint144(uint256 _offst, bytes memory _input) internal pure returns (uint144 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint152(uint256 _offst, bytes memory _input) internal pure returns (uint152 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint160(uint256 _offst, bytes memory _input) internal pure returns (uint160 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint168(uint256 _offst, bytes memory _input) internal pure returns (uint168 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint176(uint256 _offst, bytes memory _input) internal pure returns (uint176 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint184(uint256 _offst, bytes memory _input) internal pure returns (uint184 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint192(uint256 _offst, bytes memory _input) internal pure returns (uint192 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint200(uint256 _offst, bytes memory _input) internal pure returns (uint200 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint208(uint256 _offst, bytes memory _input) internal pure returns (uint208 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint216(uint256 _offst, bytes memory _input) internal pure returns (uint216 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint224(uint256 _offst, bytes memory _input) internal pure returns (uint224 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint232(uint256 _offst, bytes memory _input) internal pure returns (uint232 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint240(uint256 _offst, bytes memory _input) internal pure returns (uint240 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint248(uint256 _offst, bytes memory _input) internal pure returns (uint248 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n\n    function bytesToUint256(uint256 _offst, bytes memory _input) internal pure returns (uint256 _output) {\n        assembly {\n            _output := mload(add(_input, _offst))\n        }\n    }\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/Memory.sol",
          "line": 46,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/Memory.sol",
          "line": 47,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/contracts/lib/0.6.x/Memory.sol",
          "line": 42,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n            dest += WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/contracts/interface/0.6.x/ICrossChain.sol",
          "line": 3,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/RLPDecode.sol",
          "line": 267,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/RLPDecode.sol",
          "line": 268,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/RLPDecode.sol",
          "line": 262,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= WORD_SIZE) {\n            assembly {\n                mstore(dest, mload(src))\n            }\n\n            src += WORD_SIZE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 6,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 67,
          "category": "ensemble_quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.693,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 106,
          "category": "ensemble_quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.693,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 107,
          "category": "ensemble_quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.693,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/utils/Deployer.sol",
          "line": 142,
          "category": "ensemble_quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.693,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/interface/ICrossChain.sol",
          "line": 4,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/interface/ICrossChain.sol",
          "line": 16,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChainPackage",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 190,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 175,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 176,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 194,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/ABCToken.sol",
          "line": 212,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 190,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 175,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 176,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 194,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/MiniToken.sol",
          "line": 212,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 184,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 169,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 170,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 188,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/MaliciousToken.sol",
          "line": 206,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 201,
          "category": "ensemble_access_control",
          "pattern": "onlyOwner|onlyAdmin | mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "onlyOwner",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": true,
          "omega_detected": true,
          "ensemble_confidence": 0.948,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 239,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 224,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 225,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 243,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc-genesis-contract/test/utils/test_token/XYZToken.sol",
          "line": 261,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 190,
          "category": "spectral_anomalies",
          "pattern": "mint\\s*\\(\\s*[^,]+\\s*,\\s*[^)]+\\s*\\)",
          "match": "mint(address account, uint256 amount)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 23,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[msg.sender] = _totalSupply;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 175,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[sender] = _balances[sender] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 176,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[recipient] = _balances[recipient] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 194,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] + amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc-genesis-contract/test/utils/test_token/DEFToken.sol",
          "line": 212,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "balances[account] = _balances[account] - amount;",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/bind/v2/internal/contracts/db/contract.sol",
          "line": 60,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= msg.value",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 18,
          "category": "ensemble_access_control",
          "pattern": "onlyOwner|onlyAdmin | modifier\\s+only\\w+",
          "match": "onlyOwner",
          "severity": "MEDIUM",
          "model": "ensemble_fusion",
          "classical_detected": true,
          "omega_detected": false,
          "ensemble_confidence": 0.255,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 19,
          "category": "access_control",
          "pattern": "require\\s*\\(\\s*msg\\.sender\\s*==",
          "match": "require(msg.sender ==",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/tests/solidity/contracts/OpCodes.sol",
          "line": 158,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(sub(number()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/sample_test.go",
          "line": 21,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(i)\n\t}\n\tmean := float64(sum) / float64(len(s))\n\tb.ResetTimer()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/metrics/sample_test.go",
          "line": 35,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(i)\n\t}\n\tmean := float64(sum) / float64(len(s))\n\tb.ResetTimer()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/metrics/sample_test.go",
          "line": 117,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= float64(v[i])\n\t}\n\tavg /= float64(len(v))\n\tif avg > 16 || avg < 14 {\n\t\tt.Errorf(\"out of range [14, 16]: %v\\n\", avg)\n\t}\n}\n\nfunc TestExpDecaySampleRescale(t *testing.T) {\n\ts := NewExpDecaySample(2, 0.001).(*ExpDecaySample)\n\ts.update(time.Now(), 1)\n\ts.update(time.Now().Add(time.Hour+time.Microsecond), 1)\n\tfor _, v := range s.values.Values() {\n\t\tif v.k == 0.0 {\n\t\t\tt.Fatal(\"v.k == 0.0\")\n\t\t}\n\t}\n}\n\nfunc TestExpDecaySampleSnapshot(t *testing.T) {\n\tnow := time.Now()\n\ts := NewExpDecaySample(100, 0.99).(*ExpDecaySample).SetRand(rand.New(rand.NewSource(1)))\n\tfor i := 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/metrics/sample_test.go",
          "line": 190,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(v[i])\n\t}\n\tif exp != sum {\n\t\tt.Errorf(\"sum: %v != %v\\n\", exp, sum)\n\t}\n}\n\nfunc TestUniformSampleSnapshot(t *testing.T) {\n\ts := NewUniformSample(100).(*UniformSample).SetRand(rand.New(rand.NewSource(1)))\n\tfor i := 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/resetting_timer.go",
          "line": 68,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(d)\n}\n\n// UpdateSince records the duration of an event that started at a time and ends now.\nfunc (t *ResettingTimer) UpdateSince(ts time.Time) {\n\tt.Update(time.Since(ts))\n}\n\n// ResettingTimerSnapshot is a point-in-time copy of another ResettingTimer.\ntype ResettingTimerSnapshot struct {\n\tvalues              []int64\n\tmean                float64\n\tmax                 int64\n\tmin                 int64\n\tthresholdBoundaries []float64\n\tcalculated          bool\n}\n\n// Count return the length of the values from snapshot.\nfunc (t *ResettingTimerSnapshot) Count() int {\n\treturn len(t.values)\n}\n\n// Percentiles returns the boundaries for the input percentiles.\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Percentiles(percentiles []float64) []float64 {\n\tt.calc(percentiles)\n\treturn t.thresholdBoundaries\n}\n\n// Mean returns the mean of the snapshotted values\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Mean() float64 {\n\tif !t.calculated {\n\t\tt.calc(nil)\n\t}\n\n\treturn t.mean\n}\n\n// Max returns the max of the snapshotted values\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Max() int64 {\n\tif !t.calculated {\n\t\tt.calc(nil)\n\t}\n\treturn t.max\n}\n\n// Min returns the min of the snapshotted values\n// note: this method is not thread safe\nfunc (t *ResettingTimerSnapshot) Min() int64 {\n\tif !t.calculated {\n\t\tt.calc(nil)\n\t}\n\treturn t.min\n}\n\nfunc (t *ResettingTimerSnapshot) calc(percentiles []float64) {\n\tscores := CalculatePercentiles(t.values, percentiles)\n\tt.thresholdBoundaries = scores\n\tif len(t.values) == 0 {\n\t\treturn\n\t}\n\tt.min = t.values[0]\n\tt.max = t.values[len(t.values)-1]\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/sample.go",
          "line": 67,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= v\n\t\tif v > max {\n\t\t\tmax = v\n\t\t}\n\t\tif v < min {\n\t\t\tmin = v\n\t\t}\n\t}\n\treturn newSampleSnapshotPrecalculated(count, values, min, max, sum)\n}\n\n// Count returns the count of inputs at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Count() int64 { return s.count }\n\n// Max returns the maximal value at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Max() int64 { return s.max }\n\n// Mean returns the mean value at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Mean() float64 { return s.mean }\n\n// Min returns the minimal value at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Min() int64 { return s.min }\n\n// Percentile returns an arbitrary percentile of values at the time the\n// snapshot was taken.\nfunc (s *sampleSnapshot) Percentile(p float64) float64 {\n\treturn SamplePercentile(s.values, p)\n}\n\n// Percentiles returns a slice of arbitrary percentiles of values at the time\n// the snapshot was taken.\nfunc (s *sampleSnapshot) Percentiles(ps []float64) []float64 {\n\treturn CalculatePercentiles(s.values, ps)\n}\n\n// Size returns the size of the sample at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Size() int { return len(s.values) }\n\n// StdDev returns the standard deviation of values at the time the snapshot was\n// taken.\nfunc (s *sampleSnapshot) StdDev() float64 {\n\tif s.variance == 0.0 {\n\t\ts.variance = SampleVariance(s.mean, s.values)\n\t}\n\treturn math.Sqrt(s.variance)\n}\n\n// Sum returns the sum of values at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Sum() int64 { return s.sum }\n\n// Values returns a copy of the values in the sample.\nfunc (s *sampleSnapshot) Values() []int64 {\n\treturn slices.Clone(s.values)\n}\n\n// Variance returns the variance of values at the time the snapshot was taken.\nfunc (s *sampleSnapshot) Variance() float64 {\n\tif s.variance == 0.0 {\n\t\ts.variance = SampleVariance(s.mean, s.values)\n\t}\n\treturn s.variance\n}\n\n// ExpDecaySample is an exponentially-decaying sample using a forward-decaying\n// priority reservoir.  See Cormode et al's \"Forward Decay: A Practical Time\n// Decay Model for Streaming Systems\".\n//\n// <http://dimacs.rutgers.edu/~graham/pubs/papers/fwddecay.pdf>\ntype ExpDecaySample struct {\n\talpha         float64\n\tcount         int64\n\tmutex         sync.Mutex\n\treservoirSize int\n\tt0, t1        time.Time\n\tvalues        *expDecaySampleHeap\n\trand          *rand.Rand\n}\n\n// NewExpDecaySample constructs a new exponentially-decaying sample with the\n// given reservoir size and alpha.\nfunc NewExpDecaySample(reservoirSize int, alpha float64) Sample {\n\ts := &ExpDecaySample{\n\t\talpha:         alpha,\n\t\treservoirSize: reservoirSize,\n\t\tt0:            time.Now(),\n\t\tvalues:        newExpDecaySampleHeap(reservoirSize),\n\t}\n\ts.t1 = s.t0.Add(rescaleThreshold)\n\treturn s\n}\n\n// SetRand sets the random source (useful in tests)\nfunc (s *ExpDecaySample) SetRand(prng *rand.Rand) Sample {\n\ts.rand = prng\n\treturn s\n}\n\n// Clear clears all samples.\nfunc (s *ExpDecaySample) Clear() {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count = 0\n\ts.t0 = time.Now()\n\ts.t1 = s.t0.Add(rescaleThreshold)\n\ts.values.Clear()\n}\n\n// Snapshot returns a read-only copy of the sample.\nfunc (s *ExpDecaySample) Snapshot() *sampleSnapshot {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\tvar (\n\t\tsamples       = s.values.Values()\n\t\tvalues        = make([]int64, len(samples))\n\t\tmax     int64 = math.MinInt64\n\t\tmin     int64 = math.MaxInt64\n\t\tsum     int64\n\t)\n\tfor i, item := range samples {\n\t\tv := item.v\n\t\tvalues[i] = v\n\t\tsum += v\n\t\tif v > max {\n\t\t\tmax = v\n\t\t}\n\t\tif v < min {\n\t\t\tmin = v\n\t\t}\n\t}\n\treturn newSampleSnapshotPrecalculated(s.count, values, min, max, sum)\n}\n\n// Update samples a new value.\nfunc (s *ExpDecaySample) Update(v int64) {\n\tif !metricsEnabled {\n\t\treturn\n\t}\n\ts.update(time.Now(), v)\n}\n\n// update samples a new value at a particular timestamp.  This is a method all\n// its own to facilitate testing.\nfunc (s *ExpDecaySample) update(t time.Time, v int64) {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count++\n\tif s.values.Size() == s.reservoirSize {\n\t\ts.values.Pop()\n\t}\n\tvar f64 float64\n\tif s.rand != nil {\n\t\tf64 = s.rand.Float64()\n\t} else {\n\t\tf64 = rand.Float64()\n\t}\n\ts.values.Push(expDecaySample{\n\t\tk: math.Exp(t.Sub(s.t0).Seconds()*s.alpha) / f64,\n\t\tv: v,\n\t})\n\tif t.After(s.t1) {\n\t\tvalues := s.values.Values()\n\t\tt0 := s.t0\n\t\ts.values.Clear()\n\t\ts.t0 = t\n\t\ts.t1 = s.t0.Add(rescaleThreshold)\n\t\tfor _, v := range values {\n\t\t\tv.k = v.k * math.Exp(-s.alpha*s.t0.Sub(t0).Seconds())\n\t\t\ts.values.Push(v)\n\t\t}\n\t}\n}\n\n// SamplePercentile returns an arbitrary percentile of the slice of int64.\nfunc SamplePercentile(values []int64, p float64) float64 {\n\treturn CalculatePercentiles(values, []float64{p})[0]\n}\n\n// CalculatePercentiles returns a slice of arbitrary percentiles of the slice of\n// int64. This method returns interpolated results, so e.g. if there are only two\n// values, [0, 10], a 50% percentile will land between them.\n//\n// Note: As a side-effect, this method will also sort the slice of values.\n// Note2: The input format for percentiles is NOT percent! To express 50%, use 0.5, not 50.\nfunc CalculatePercentiles(values []int64, ps []float64) []float64 {\n\tscores := make([]float64, len(ps))\n\tsize := len(values)\n\tif size == 0 {\n\t\treturn scores\n\t}\n\tslices.Sort(values)\n\tfor i, p := range ps {\n\t\tpos := p * float64(size+1)\n\n\t\tif pos < 1.0 {\n\t\t\tscores[i] = float64(values[0])\n\t\t} else if pos >= float64(size) {\n\t\t\tscores[i] = float64(values[size-1])\n\t\t} else {\n\t\t\tlower := float64(values[int(pos)-1])\n\t\t\tupper := float64(values[int(pos)])\n\t\t\tscores[i] = lower + (pos-math.Floor(pos))*(upper-lower)\n\t\t}\n\t}\n\treturn scores\n}\n\n// SampleVariance returns the variance of the slice of int64.\nfunc SampleVariance(mean float64, values []int64) float64 {\n\tif len(values) == 0 {\n\t\treturn 0.0\n\t}\n\tvar sum float64\n\tfor _, v := range values {\n\t\td := float64(v) - mean\n\t\tsum += d * d\n\t}\n\treturn sum / float64(len(values))\n}\n\n// UniformSample implements a uniform sample using Vitter's Algorithm R.\n//\n// <http://www.cs.umd.edu/~samir/498/vitter.pdf>\ntype UniformSample struct {\n\tcount         int64\n\tmutex         sync.Mutex\n\treservoirSize int\n\tvalues        []int64\n\trand          *rand.Rand\n}\n\n// NewUniformSample constructs a new uniform sample with the given reservoir\n// size.\nfunc NewUniformSample(reservoirSize int) Sample {\n\treturn &UniformSample{\n\t\treservoirSize: reservoirSize,\n\t\tvalues:        make([]int64, 0, reservoirSize),\n\t}\n}\n\n// SetRand sets the random source (useful in tests)\nfunc (s *UniformSample) SetRand(prng *rand.Rand) Sample {\n\ts.rand = prng\n\treturn s\n}\n\n// Clear clears all samples.\nfunc (s *UniformSample) Clear() {\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count = 0\n\tclear(s.values)\n}\n\n// Snapshot returns a read-only copy of the sample.\nfunc (s *UniformSample) Snapshot() *sampleSnapshot {\n\ts.mutex.Lock()\n\tvalues := slices.Clone(s.values)\n\tcount := s.count\n\ts.mutex.Unlock()\n\treturn newSampleSnapshot(count, values)\n}\n\n// Update samples a new value.\nfunc (s *UniformSample) Update(v int64) {\n\tif !metricsEnabled {\n\t\treturn\n\t}\n\ts.mutex.Lock()\n\tdefer s.mutex.Unlock()\n\ts.count++\n\tif len(s.values) < s.reservoirSize {\n\t\ts.values = append(s.values, v)\n\t\treturn\n\t}\n\tvar r int64\n\tif s.rand != nil {\n\t\tr = s.rand.Int63n(s.count)\n\t} else {\n\t\tr = rand.Int63n(s.count)\n\t}\n\tif r < int64(len(s.values)) {\n\t\ts.values[int(r)] = v\n\t}\n}\n\n// expDecaySample represents an individual sample in a heap.\ntype expDecaySample struct {\n\tk float64\n\tv int64\n}\n\nfunc newExpDecaySampleHeap(reservoirSize int) *expDecaySampleHeap {\n\treturn &expDecaySampleHeap{make([]expDecaySample, 0, reservoirSize)}\n}\n\n// expDecaySampleHeap is a min-heap of expDecaySamples.\n// The internal implementation is copied from the standard library's container/heap\ntype expDecaySampleHeap struct {\n\ts []expDecaySample\n}\n\nfunc (h *expDecaySampleHeap) Clear() {\n\th.s = h.s[:0]\n}\n\nfunc (h *expDecaySampleHeap) Push(s expDecaySample) {\n\tn := len(h.s)\n\th.s = h.s[0 : n+1]\n\th.s[n] = s\n\th.up(n)\n}\n\nfunc (h *expDecaySampleHeap) Pop() expDecaySample {\n\tn := len(h.s) - 1\n\th.s[0], h.s[n] = h.s[n], h.s[0]\n\th.down(0, n)\n\n\tn = len(h.s)\n\ts := h.s[n-1]\n\th.s = h.s[0 : n-1]\n\treturn s\n}\n\nfunc (h *expDecaySampleHeap) Size() int {\n\treturn len(h.s)\n}\n\nfunc (h *expDecaySampleHeap) Values() []expDecaySample {\n\treturn h.s\n}\n\nfunc (h *expDecaySampleHeap) up(j int) {\n\tfor {\n\t\ti := (j - 1) / 2 // parent\n\t\tif i == j || !(h.s[j].k < h.s[i].k) {\n\t\t\tbreak\n\t\t}\n\t\th.s[i], h.s[j] = h.s[j], h.s[i]\n\t\tj = i\n\t}\n}\n\nfunc (h *expDecaySampleHeap) down(i, n int) {\n\tfor {\n\t\tj1 := 2*i + 1\n\t\tif j1 >= n || j1 < 0 { // j1 < 0 after int overflow\n\t\t\tbreak\n\t\t}\n\t\tj := j1 // left child\n\t\tif j2 := j1 + 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/ewma.go",
          "line": 84,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a.alpha * (instantRate - currentRate)\n\ta.rate.Store(math.Float64bits(currentRate))\n}\n\n// Update adds n uncounted events.\nfunc (a *EWMA) Update(n int64) {\n\ta.uncounted.Add(n)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/runtimehistogram.go",
          "line": 101,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(c)\n\t\tsum += h.midpoint(i) * float64(c)\n\t\t// Set max on every iteration\n\t\tedge := h.internal.Buckets[i+1]\n\t\tif math.IsInf(edge, 1) {\n\t\t\tedge = h.internal.Buckets[i]\n\t\t}\n\t\tif edge > max {\n\t\t\tmax = edge\n\t\t}\n\t}\n\th.min = min\n\th.max = int64(max)\n\th.mean = sum / float64(count)\n\th.count = count\n}\n\n// Count returns the sample count.\nfunc (h *runtimeHistogramSnapshot) Count() int64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.count\n}\n\n// Size returns the size of the sample at the time the snapshot was taken.\nfunc (h *runtimeHistogramSnapshot) Size() int {\n\treturn len(h.internal.Counts)\n}\n\n// Mean returns an approximation of the mean.\nfunc (h *runtimeHistogramSnapshot) Mean() float64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.mean\n}\n\nfunc (h *runtimeHistogramSnapshot) midpoint(bucket int) float64 {\n\thigh := h.internal.Buckets[bucket+1]\n\tlow := h.internal.Buckets[bucket]\n\tif math.IsInf(high, 1) {\n\t\t// The edge of the highest bucket can be +Inf, and it's supposed to mean that this\n\t\t// bucket contains all remaining samples > low. We can't get the middle of an\n\t\t// infinite range, so just return the lower bound of this bucket instead.\n\t\treturn low\n\t}\n\tif math.IsInf(low, -1) {\n\t\t// Similarly, we can get -Inf in the left edge of the lowest bucket,\n\t\t// and it means the bucket contains all remaining values < high.\n\t\treturn high\n\t}\n\treturn (low + high) / 2\n}\n\n// StdDev approximates the standard deviation of the histogram.\nfunc (h *runtimeHistogramSnapshot) StdDev() float64 {\n\treturn math.Sqrt(h.Variance())\n}\n\n// Variance approximates the variance of the histogram.\nfunc (h *runtimeHistogramSnapshot) Variance() float64 {\n\tif len(h.internal.Counts) == 0 {\n\t\treturn 0\n\t}\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\tif h.count <= 1 {\n\t\t// There is no variance when there are zero or one items.\n\t\treturn 0\n\t}\n\t// Variance is not calculated in 'calc', because it requires a second iteration.\n\t// Therefore we calculate it lazily in this method, triggered either by\n\t// a direct call to Variance or via StdDev.\n\tif h.variance != 0.0 {\n\t\treturn h.variance\n\t}\n\tvar sum float64\n\n\tfor i, c := range h.internal.Counts {\n\t\tmidpoint := h.midpoint(i)\n\t\td := midpoint - h.mean\n\t\tsum += float64(c) * (d * d)\n\t}\n\th.variance = sum / float64(h.count-1)\n\treturn h.variance\n}\n\n// Percentile computes the p'th percentile value.\nfunc (h *runtimeHistogramSnapshot) Percentile(p float64) float64 {\n\tthreshold := float64(h.Count()) * p\n\tvalues := [1]float64{threshold}\n\th.computePercentiles(values[:])\n\treturn values[0]\n}\n\n// Percentiles computes all requested percentile values.\nfunc (h *runtimeHistogramSnapshot) Percentiles(ps []float64) []float64 {\n\t// Compute threshold values. We need these to be sorted\n\t// for the percentile computation, but restore the original\n\t// order later, so keep the indexes as well.\n\tcount := float64(h.Count())\n\tthresholds := make([]float64, len(ps))\n\tindexes := make([]int, len(ps))\n\tfor i, percentile := range ps {\n\t\tthresholds[i] = count * math.Max(0, math.Min(1.0, percentile))\n\t\tindexes[i] = i\n\t}\n\tsort.Sort(floatsAscendingKeepingIndex{thresholds, indexes})\n\n\t// Now compute. The result is stored back into the thresholds slice.\n\th.computePercentiles(thresholds)\n\n\t// Put the result back into the requested order.\n\tsort.Sort(floatsByIndex{thresholds, indexes})\n\treturn thresholds\n}\n\nfunc (h *runtimeHistogramSnapshot) computePercentiles(thresh []float64) {\n\tvar totalCount float64\n\tfor i, count := range h.internal.Counts {\n\t\ttotalCount += float64(count)\n\n\t\tfor len(thresh) > 0 && thresh[0] < totalCount {\n\t\t\tthresh[0] = h.internal.Buckets[i]\n\t\t\tthresh = thresh[1:]\n\t\t}\n\t\tif len(thresh) == 0 {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// Note: runtime/metrics.Float64Histogram is a collection of float64s, but the methods\n// below need to return int64 to satisfy the interface. The histogram provided by runtime\n// also doesn't keep track of individual samples, so results are approximated.\n\n// Max returns the highest sample value.\nfunc (h *runtimeHistogramSnapshot) Max() int64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.max\n}\n\n// Min returns the lowest sample value.\nfunc (h *runtimeHistogramSnapshot) Min() int64 {\n\tif !h.calculated {\n\t\th.calc()\n\t}\n\treturn h.min\n}\n\n// Sum returns the sum of all sample values.\nfunc (h *runtimeHistogramSnapshot) Sum() int64 {\n\tvar sum float64\n\tfor i := range h.internal.Counts {\n\t\tsum += h.internal.Buckets[i] * float64(h.internal.Counts[i])\n\t}\n\treturn int64(math.Ceil(sum))\n}\n\ntype floatsAscendingKeepingIndex struct {\n\tvalues  []float64\n\tindexes []int\n}\n\nfunc (s floatsAscendingKeepingIndex) Len() int {\n\treturn len(s.values)\n}\n\nfunc (s floatsAscendingKeepingIndex) Less(i, j int) bool {\n\treturn s.values[i] < s.values[j]\n}\n\nfunc (s floatsAscendingKeepingIndex) Swap(i, j int) {\n\ts.values[i], s.values[j] = s.values[j], s.values[i]\n\ts.indexes[i], s.indexes[j] = s.indexes[j], s.indexes[i]\n}\n\ntype floatsByIndex struct {\n\tvalues  []float64\n\tindexes []int\n}\n\nfunc (s floatsByIndex) Len() int {\n\treturn len(s.values)\n}\n\nfunc (s floatsByIndex) Less(i, j int) bool {\n\treturn s.indexes[i] < s.indexes[j]\n}\n\nfunc (s floatsByIndex) Swap(i, j int) {\n\ts.values[i], s.values[j] = s.values[j], s.values[i]\n\ts.indexes[i], s.indexes[j] = s.indexes[j], s.indexes[i]\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/consensus.go",
          "line": 71,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/sender_cacher.go",
          "line": 73,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= task.inc {\n\t\t\ttypes.Sender(task.signer, task.txs[i])\n\t\t}\n\t}\n}\n\n// Recover recovers the senders from a batch of transactions and caches them\n// back into the same data structures. There is no validation being done, nor\n// any reaction to invalid signatures. That is up to calling code later.\nfunc (cacher *txSenderCacher) Recover(signer types.Signer, txs []*types.Transaction) {\n\t// If there's nothing to recover, abort\n\tif len(txs) == 0 {\n\t\treturn\n\t}\n\t// Ensure we have meaningful task sizes and schedule the recoveries\n\ttasks := cacher.threads\n\tif len(txs) < tasks*4 {\n\t\ttasks = (len(txs) + 3) / 4\n\t}\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/sender_cacher.go",
          "line": 107,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(block.Transactions())\n\t}\n\ttxs := make([]*types.Transaction, 0, count)\n\tfor _, block := range blocks {\n\t\ttxs = append(txs, block.Transactions()...)\n\t}\n\tcacher.Recover(signer, txs)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state_processor.go",
          "line": 229,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= result.UsedGas\n\n\t// Merge the tx-local access event into the \"block-local\" one, in order to collect\n\t// all values, so that the witness can be built.\n\tif statedb.Database().TrieDB().IsVerkle() {\n\t\tstatedb.AccessEvents().Merge(evm.AccessEvents)\n\t}\n\n\treturn MakeReceipt(evm, result, statedb, blockNumber, blockHash, blockTime, tx, *usedGas, root, receiptProcessors...), nil\n}\n\n// MakeReceipt generates the receipt object for a transaction given its execution result.\nfunc MakeReceipt(evm *vm.EVM, result *ExecutionResult, statedb *state.StateDB, blockNumber *big.Int, blockHash common.Hash, blockTime uint64, tx *types.Transaction, usedGas uint64, root []byte, receiptProcessors ...ReceiptProcessor) *types.Receipt {\n\t// Create a new receipt for the transaction, storing the intermediate root and gas used\n\t// by the tx.\n\treceipt := &types.Receipt{Type: tx.Type(), PostState: root, CumulativeGasUsed: usedGas}\n\tif result.Failed() {\n\t\treceipt.Status = types.ReceiptStatusFailed\n\t} else {\n\t\treceipt.Status = types.ReceiptStatusSuccessful\n\t}\n\treceipt.TxHash = tx.Hash()\n\treceipt.GasUsed = result.UsedGas\n\n\tif tx.Type() == types.BlobTxType {\n\t\treceipt.BlobGasUsed = uint64(len(tx.BlobHashes()) * params.BlobTxBlobGasPerBlob)\n\t\treceipt.BlobGasPrice = evm.Context.BlobBaseFee\n\t}\n\n\t// If the transaction created a contract, store the creation address in the receipt.\n\tif tx.To() == nil {\n\t\treceipt.ContractAddress = crypto.CreateAddress(evm.TxContext.Origin, tx.Nonce())\n\t}\n\n\t// Set the receipt logs and create the bloom filter.\n\treceipt.Logs = statedb.GetLogs(tx.Hash(), blockNumber.Uint64(), blockHash, blockTime)\n\treceipt.BlockHash = blockHash\n\treceipt.BlockNumber = blockNumber\n\treceipt.TransactionIndex = uint(statedb.TxIndex())\n\tfor _, receiptProcessor := range receiptProcessors {\n\t\treceiptProcessor.Apply(receipt)\n\t}\n\treturn receipt\n}\n\n// ApplyTransaction attempts to apply a transaction to the given state database\n// and uses the input parameters for its environment. It returns the receipt\n// for the transaction, gas used and an error if the transaction failed,\n// indicating the block was invalid.\nfunc ApplyTransaction(evm *vm.EVM, gp *GasPool, statedb *state.StateDB, header *types.Header, tx *types.Transaction, usedGas *uint64, receiptProcessors ...ReceiptProcessor) (*types.Receipt, error) {\n\tmsg, err := TransactionToMessage(tx, types.MakeSigner(evm.ChainConfig(), header.Number, header.Time), header.BaseFee)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\t// Create a new context to be used in the EVM environment\n\treturn ApplyTransactionWithEVM(msg, gp, statedb, header.Number, header.Hash(), header.Time, tx, usedGas, evm, receiptProcessors...)\n}\n\n// ProcessBeaconBlockRoot applies the EIP-4788 system call to the beacon block root\n// contract. This method is exported to be used in tests.\nfunc ProcessBeaconBlockRoot(beaconRoot common.Hash, evm *vm.EVM) {\n\t// Return immediately if beaconRoot equals the zero hash when using the Parlia engine.\n\tif beaconRoot == (common.Hash{}) {\n\t\tif chainConfig := evm.ChainConfig()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state_processor.go",
          "line": 313,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/state_processor.go",
          "line": 337,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/state_processor.go",
          "line": 376,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/state_processor.go",
          "line": 106,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/state_processor.go",
          "line": 319,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(prevHash common.Hash, evm *vm.EVM)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state_transition.go",
          "line": 97,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nz * nonZeroGas\n\n\t\tif (math.MaxUint64-gas)/params.TxDataZeroGas < z {\n\t\t\treturn 0, ErrGasUintOverflow\n\t\t}\n\t\tgas += z * params.TxDataZeroGas\n\n\t\tif isContractCreation && isEIP3860 {\n\t\t\tlenWords := toWordSize(dataLen)\n\t\t\tif (math.MaxUint64-gas)/params.InitCodeWordGas < lenWords {\n\t\t\t\treturn 0, ErrGasUintOverflow\n\t\t\t}\n\t\t\tgas += lenWords * params.InitCodeWordGas\n\t\t}\n\t}\n\tif accessList != nil {\n\t\tgas += uint64(len(accessList)) * params.TxAccessListAddressGas\n\t\tgas += uint64(accessList.StorageKeys()) * params.TxAccessListStorageKeyGas\n\t}\n\tif authList != nil {\n\t\tgas += uint64(len(authList)) * params.CallNewAccountGas\n\t}\n\treturn gas, nil\n}\n\n// FloorDataGas computes the minimum gas required for a transaction based on its data tokens (EIP-7623).\nfunc FloorDataGas(data []byte) (uint64, error) {\n\tvar (\n\t\tz      = uint64(bytes.Count(data, []byte{0}))\n\t\tnz     = uint64(len(data)) - z\n\t\ttokens = nz*params.TxTokenPerNonZeroByte + z\n\t)\n\t// Check for overflow\n\tif (math.MaxUint64-params.TxGas)/params.TxCostFloorPerToken < tokens {\n\t\treturn 0, ErrGasUintOverflow\n\t}\n\t// Minimum gas required for a transaction based on its data tokens (EIP-7623).\n\treturn params.TxGas + tokens*params.TxCostFloorPerToken, nil\n}\n\n// toWordSize returns the ceiled word size required for init code payment calculation.\nfunc toWordSize(size uint64) uint64 {\n\tif size > math.MaxUint64-31 {\n\t\treturn math.MaxUint64/32 + 1\n\t}\n\n\treturn (size + 31) / 32\n}\n\n// A Message contains the data derived from a single transaction that is relevant to state\n// processing.\ntype Message struct {\n\tTo                    *common.Address\n\tFrom                  common.Address\n\tNonce                 uint64\n\tValue                 *big.Int\n\tGasLimit              uint64\n\tGasPrice              *big.Int\n\tGasFeeCap             *big.Int\n\tGasTipCap             *big.Int\n\tData                  []byte\n\tAccessList            types.AccessList\n\tBlobGasFeeCap         *big.Int\n\tBlobHashes            []common.Hash\n\tSetCodeAuthorizations []types.SetCodeAuthorization\n\n\t// When SkipNonceChecks is true, the message nonce is not checked against the\n\t// account nonce in state.\n\t//\n\t// This field will be set to true for operations like RPC eth_call\n\t// or the state prefetching.\n\tSkipNonceChecks bool\n\n\t// When SkipFromEOACheck is true, the message sender is not checked to be an EOA.\n\tSkipFromEOACheck bool\n}\n\n// TransactionToMessage converts a transaction into a Message.\nfunc TransactionToMessage(tx *types.Transaction, s types.Signer, baseFee *big.Int) (*Message, error) {\n\tmsg := &Message{\n\t\tNonce:                 tx.Nonce(),\n\t\tGasLimit:              tx.Gas(),\n\t\tGasPrice:              new(big.Int).Set(tx.GasPrice()),\n\t\tGasFeeCap:             new(big.Int).Set(tx.GasFeeCap()),\n\t\tGasTipCap:             new(big.Int).Set(tx.GasTipCap()),\n\t\tTo:                    tx.To(),\n\t\tValue:                 tx.Value(),\n\t\tData:                  tx.Data(),\n\t\tAccessList:            tx.AccessList(),\n\t\tSetCodeAuthorizations: tx.SetCodeAuthorizations(),\n\t\tSkipNonceChecks:       false,\n\t\tSkipFromEOACheck:      false,\n\t\tBlobHashes:            tx.BlobHashes(),\n\t\tBlobGasFeeCap:         tx.BlobGasFeeCap(),\n\t}\n\t// If baseFee provided, set gasPrice to effectiveGasPrice.\n\tif baseFee != nil {\n\t\tmsg.GasPrice = msg.GasPrice.Add(msg.GasTipCap, baseFee)\n\t\tif msg.GasPrice.Cmp(msg.GasFeeCap) > 0 {\n\t\t\tmsg.GasPrice = msg.GasFeeCap\n\t\t}\n\t}\n\tvar err error\n\tmsg.From, err = types.Sender(s, tx)\n\treturn msg, err\n}\n\n// ApplyMessage computes the new state by applying the given message\n// against the old state within the environment.\n//\n// ApplyMessage returns the bytes returned by any EVM execution (if it took place),\n// the gas used (which includes gas refunds) and an error if it failed. An error always\n// indicates a core error meaning that the message would always fail for that particular\n// state and would never be accepted within a block.\nfunc ApplyMessage(evm *vm.EVM, msg *Message, gp *GasPool) (*ExecutionResult, error) {\n\tevm.SetTxContext(NewEVMTxContext(msg))\n\treturn newStateTransition(evm, msg, gp).execute()\n}\n\n// stateTransition represents a state transition.\n//\n// == The State Transitioning Model\n//\n// A state transition is a change made when a transaction is applied to the current world\n// state. The state transitioning model does all the necessary work to work out a valid new\n// state root.\n//\n//  1. Nonce handling\n//  2. Pre pay gas\n//  3. Create a new state object if the recipient is nil\n//  4. Value transfer\n//\n// == If contract creation ==\n//\n//\t4a. Attempt to run transaction data\n//\t4b. If valid, use result as code for the new state object\n//\n// == end ==\n//\n//  5. Run Script section\n//  6. Derive new state root\ntype stateTransition struct {\n\tgp           *GasPool\n\tmsg          *Message\n\tgasRemaining uint64\n\tinitialGas   uint64\n\tstate        vm.StateDB\n\tevm          *vm.EVM\n}\n\n// newStateTransition initialises and returns a new state transition object.\nfunc newStateTransition(evm *vm.EVM, msg *Message, gp *GasPool) *stateTransition {\n\treturn &stateTransition{\n\t\tgp:    gp,\n\t\tevm:   evm,\n\t\tmsg:   msg,\n\t\tstate: evm.StateDB,\n\t}\n}\n\n// to returns the recipient of the message.\nfunc (st *stateTransition) to() common.Address {\n\tif st.msg == nil || st.msg.To == nil /* contract creation */ {\n\t\treturn common.Address{}\n\t}\n\treturn *st.msg.To\n}\n\nfunc (st *stateTransition) buyGas() error {\n\tmgval := new(big.Int).SetUint64(st.msg.GasLimit)\n\tmgval.Mul(mgval, st.msg.GasPrice)\n\tbalanceCheck := new(big.Int).Set(mgval)\n\tif st.msg.GasFeeCap != nil {\n\t\tbalanceCheck.SetUint64(st.msg.GasLimit)\n\t\tbalanceCheck = balanceCheck.Mul(balanceCheck, st.msg.GasFeeCap)\n\t}\n\tbalanceCheck.Add(balanceCheck, st.msg.Value)\n\n\tif st.evm.ChainConfig().IsCancun(st.evm.Context.BlockNumber, st.evm.Context.Time) {\n\t\tif blobGas := st.blobGasUsed()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state_transition.go",
          "line": 531,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= st.calcRefund()\n\tif rules.IsPrague {\n\t\t// After EIP-7623: Data-heavy transactions pay the floor gas.\n\t\tif st.gasUsed() < floorDataGas {\n\t\t\tprev := st.gasRemaining\n\t\t\tst.gasRemaining = st.initialGas - floorDataGas\n\t\t\tif t := st.evm.Config.Tracer",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/state_transition.go",
          "line": 466,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas\n\n\tif rules.IsEIP4762 {\n\t\tst.evm.AccessEvents.AddTxOrigin(msg.From)\n\n\t\tif targetAddr := msg.To",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/state_transition.go",
          "line": 523,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/chain_makers.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= receipt.BlobGasUsed\n\t}\n}\n\n// AddTx adds a transaction to the generated block. If no coinbase has\n// been set, the block's coinbase is set to the zero address.\n//\n// AddTx panics if the transaction cannot be executed. In addition to the protocol-imposed\n// limitations (gas limit, etc.), there are some further limitations on the content of\n// transactions that can be added. Notably, contract code relying on the BLOCKHASH\n// instruction will panic during execution if it attempts to access a block number outside\n// of the range created by GenerateChain.\nfunc (b *BlockGen) AddTx(tx *types.Transaction) {\n\t// Wrap the chain config in an empty BlockChain object to satisfy ChainContext.\n\tbc := &BlockChain{chainConfig: b.cm.config}\n\tb.addTx(bc, vm.Config{}, tx)\n}\n\n// AddTxWithChain adds a transaction to the generated block. If no coinbase has\n// been set, the block's coinbase is set to the zero address.\n//\n// AddTxWithChain panics if the transaction cannot be executed. In addition to the\n// protocol-imposed limitations (gas limit, etc.), there are some further limitations on\n// the content of transactions that can be added. If contract code relies on the BLOCKHASH\n// instruction, the block in chain will be returned.\nfunc (b *BlockGen) AddTxWithChain(bc *BlockChain, tx *types.Transaction) {\n\tb.addTx(bc, vm.Config{}, tx)\n}\n\n// AddTxWithVMConfig adds a transaction to the generated block. If no coinbase has\n// been set, the block's coinbase is set to the zero address.\n// The evm interpreter can be customized with the provided vm config.\nfunc (b *BlockGen) AddTxWithVMConfig(tx *types.Transaction, config vm.Config) {\n\tb.addTx(nil, config, tx)\n}\n\n// GetBalance returns the balance of the given address at the generated block.\nfunc (b *BlockGen) GetBalance(addr common.Address) *uint256.Int {\n\treturn b.statedb.GetBalance(addr)\n}\n\n// AddUncheckedTx forcefully adds a transaction to the block without any validation.\n//\n// AddUncheckedTx will cause consensus failures when used during real\n// chain processing. This is best used in conjunction with raw block insertion.\nfunc (b *BlockGen) AddUncheckedTx(tx *types.Transaction) {\n\tb.txs = append(b.txs, tx)\n}\n\n// AddBlobSidecar add block's blob sidecar for DA checking.\nfunc (b *BlockGen) AddBlobSidecar(sidecar *types.BlobSidecar) {\n\tb.sidecars = append(b.sidecars, sidecar)\n}\n\nfunc (b *BlockGen) HeadBlock() *types.Header {\n\treturn b.header\n}\n\n// Number returns the block number of the block being generated.\nfunc (b *BlockGen) Number() *big.Int {\n\treturn new(big.Int).Set(b.header.Number)\n}\n\n// Timestamp returns the timestamp of the block being generated.\nfunc (b *BlockGen) Timestamp() uint64 {\n\treturn b.header.Time\n}\n\n// BaseFee returns the EIP-1559 base fee of the block being generated.\nfunc (b *BlockGen) BaseFee() *big.Int {\n\treturn new(big.Int).Set(b.header.BaseFee)\n}\n\n// ExcessBlobGas returns the EIP-4844 ExcessBlobGas of the block.\nfunc (b *BlockGen) ExcessBlobGas() uint64 {\n\texcessBlobGas := b.header.ExcessBlobGas\n\tif excessBlobGas == nil {\n\t\treturn 0\n\t}\n\treturn *excessBlobGas\n}\n\n// Gas returns the amount of gas left in the current block.\nfunc (b *BlockGen) Gas() uint64 {\n\treturn b.header.GasLimit - b.header.GasUsed\n}\n\n// Signer returns a valid signer instance for the current block.\nfunc (b *BlockGen) Signer() types.Signer {\n\treturn types.MakeSigner(b.cm.config, b.header.Number, b.header.Time)\n}\n\n// AddUncheckedReceipt forcefully adds a receipts to the block without a\n// backing transaction.\n//\n// AddUncheckedReceipt will cause consensus failures when used during real\n// chain processing. This is best used in conjunction with raw block insertion.\nfunc (b *BlockGen) AddUncheckedReceipt(receipt *types.Receipt) {\n\tb.receipts = append(b.receipts, receipt)\n}\n\n// TxNonce returns the next valid transaction nonce for the\n// account at addr. It panics if the account does not exist.\nfunc (b *BlockGen) TxNonce(addr common.Address) uint64 {\n\tif !b.statedb.Exist(addr) {\n\t\tpanic(\"account does not exist\")\n\t}\n\treturn b.statedb.GetNonce(addr)\n}\n\n// AddUncle adds an uncle header to the generated block.\nfunc (b *BlockGen) AddUncle(h *types.Header) {\n\t// The uncle will have the same timestamp and auto-generated difficulty\n\th.Time = b.header.Time\n\n\tvar parent *types.Header\n\tfor i := b.i - 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/chain_makers.go",
          "line": 318,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(seconds)\n\tif b.header.Time <= b.cm.bottom.Header().Time {\n\t\tpanic(\"block time out of range\")\n\t}\n\tb.header.Difficulty = b.engine.CalcDifficulty(b.cm, b.header.Time, b.parent.Header())\n}\n\n// ConsensusLayerRequests returns the EIP-7685 requests which have accumulated so far.\nfunc (b *BlockGen) ConsensusLayerRequests() [][]byte {\n\treturn b.collectRequests(true)\n}\n\nfunc (b *BlockGen) collectRequests(readonly bool) (requests [][]byte) {\n\tstatedb := b.statedb\n\tif readonly {\n\t\t// The system contracts clear themselves on a system-initiated read.\n\t\t// When reading the requests mid-block, we don't want this behavior, so fork\n\t\t// off the statedb before executing the system calls.\n\t\tstatedb = statedb.Copy()\n\t}\n\n\tif b.cm.config.IsPrague(b.header.Number, b.header.Time) && b.cm.config.Parlia == nil {\n\t\trequests = [][]byte{}\n\t\t// EIP-6110 deposits\n\t\tvar blockLogs []*types.Log\n\t\tfor _, r := range b.receipts {\n\t\t\tblockLogs = append(blockLogs, r.Logs...)\n\t\t}\n\t\tif err := ParseDepositLogs(&requests, blockLogs, b.cm.config)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/chain_makers.go",
          "line": 424,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(b.header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/chain_makers.go",
          "line": 538,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(b.header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/block_validator_test.go",
          "line": 138,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(block.Difficulty().Uint64())\n\t\t}\n\t\tpreBlocks = blocks\n\t\tgspec.Config.TerminalTotalDifficulty = big.NewInt(int64(td))\n\t\tpostBlocks, _ = GenerateChain(gspec.Config, preBlocks[len(preBlocks)-1], engine, genDb, 8, nil)\n\t} else {\n\t\tconfig := *params.TestChainConfig\n\t\tgspec = &Genesis{Config: &config}\n\t\tengine = beacon.New(ethash.NewFaker())\n\t\ttd := int(params.GenesisDifficulty.Uint64())\n\t\tgenDb, blocks, _ := GenerateChainWithGenesis(gspec, engine, 8, nil)\n\t\tfor _, block := range blocks {\n\t\t\t// calculate td\n\t\t\ttd += int(block.Difficulty().Uint64())\n\t\t}\n\t\tpreBlocks = blocks\n\t\tgspec.Config.TerminalTotalDifficulty = big.NewInt(int64(td))\n\t\tpostBlocks, _ = GenerateChain(gspec.Config, preBlocks[len(preBlocks)-1], engine, genDb, 8, func(i int, gen *BlockGen) {\n\t\t\tgen.SetPoS()\n\t\t})\n\t}\n\t// Assemble header batch\n\tpreHeaders := make([]*types.Header, len(preBlocks))\n\tfor i, block := range preBlocks {\n\t\tpreHeaders[i] = block.Header()\n\t}\n\tpostHeaders := make([]*types.Header, len(postBlocks))\n\tfor i, block := range postBlocks {\n\t\tpostHeaders[i] = block.Header()\n\t}\n\t// Run the header checker for blocks one-by-one, checking for both valid and invalid nonces\n\tchain, err := NewBlockChain(rawdb.NewMemoryDatabase(), gspec, engine, nil)\n\tdefer chain.Stop()\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\n\t// Verify the blocks before the merging\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/genesis_alloc.go",
          "line": 24,
          "category": "ensemble_overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath) | \\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=\\x19f<\\xdbL0\\x9d[O/\\xc2\\u06cf\\x89\\x05k\\xc7^-c\\x10\\x00\\x00\\u07d4W\\xbd\\xdf\\a\\x884\\x00\\x9c\\x89\\u060eb\\x82u\\x9d\\xc4S5\\xb4p\\x89tq|\\xfbh\\x83\\x10\\x00\\x00\\u07d4W\\xbe\\xeaql\\xbd\\x81p\\ns\\xd6\\u007f\\x9f\\xf09R\\x9c-\\x90%\\x89\\n\\u05ce\\xbcZ\\xc6 \\x00\\x00\\u07d4W\\xd02\\xa4=\\x16Nq\\xaa.\\xf3\\xff\\xd8I\\x1b\\nN\\xf1\\xea[\\x89lk\\x93[\\x8b\\xbd@\\x00\\x00\\u07d4W\\xd3\\u07c0O+\\xee\\xe6\\xefS\\xab\\x94\\xcb>\\xe9\\xcfRJ\\x18\\u04c9\\x15Vak\\x96\\x06g\\x00\\x00\\u07d4W\\xd5\\xfd\\x0e=0I3\\x0f\\xfc\\xdc\\xd0 Ei\\x17e{\\xa2\\u0689k\\xf2\\x01\\x95\\xf5T\\xd4\\x00\\x00\\u07d4W\\u0754q\\xcb\\xfa&'\\t\\xf5\\U00106f37t\\xc5\\xf5'\\xb8\\xf8\\x89\\n\\xad\\xec\\x98?\\xcf\\xf4\\x00\\x00\\u07d4W\\xdf#\\xbe\\xbd\\xc6^\\xb7_\\ub732\\xfa\\xd1\\xc0si++\\xaf\\x89\\xd8\\xd7&\\xb7\\x17z\\x80\\x00\\x00\\u07d4X\\x00\\u03410\\x83\\x9e\\x94I]-\\x84\\x15\\xa8\\xea,\\x90\\xe0\\xc5\\u02c9\\n\\u05ce\\xbcZ\\xc6 \\x00\\x00\\xe0\\x94X\\x03\\xe6\\x8b4\\xda\\x12\\x1a\\xef\\b\\xb6\\x02\\xba\\u06ef\\xb4\\xd1$\\x81\\u028a\\x03\\xcf\\xc8.7\\xe9\\xa7@\\x00\\x00\\xe0\\x94X\\x16\\xc2hww\\xb6\\xd7\\u04a2C-Y\\xa4\\x1f\\xa0Y\\xe3\\xa4\\x06\\x8a\\x1cO\\xe4:\\xdb\\n^\\x90\\x00\\x00\\u07d4X\\x1a:\\xf2\\x97\\xef\\xa4Cj)\\xaf\\x00r\\x92\\x9a\\xbf\\x98&\\xf5\\x8b\\x89lk\\x93[\\x8b\\xbd@\\x00\\x00\\xe0\\x94X\\x1b\\x9f\\xd6\\xea\\xe3r\\xf3P\\x1fB\\xeb\\x96\\x19\\xee\\xc8 \\xb7\\x8a\\x84\\x8a\\x04+\\xe2\\xc0\\f\\xa5",
          "severity": "HIGH",
          "model": "ensemble_fusion",
          "classical_detected": true,
          "omega_detected": false,
          "ensemble_confidence": 0.255,
          "fusion_score": 3.5
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/gaspool.go",
          "line": 33,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= amount\n\treturn gp\n}\n\n// SubGas deducts the given amount from the pool if enough gas is\n// available and returns an error otherwise.\nfunc (gp *GasPool) SubGas(amount uint64) error {\n\tif uint64(*gp) < amount {\n\t\treturn ErrGasLimitReached\n\t}\n\t*(*uint64)(gp) -= amount\n\treturn nil\n}\n\n// Gas returns the amount of gas remaining in the pool.\nfunc (gp *GasPool) Gas() uint64 {\n\treturn uint64(*gp)\n}\n\n// SetGas sets the amount of gas with the provided number.\nfunc (gp *GasPool) SetGas(gas uint64) {\n\t*(*uint64)(gp) = gas\n}\n\nfunc (gp *GasPool) String() string {\n\treturn fmt.Sprintf(\"%d\", *gp)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/gaspool.go",
          "line": 43,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= amount\n\treturn nil\n}\n\n// Gas returns the amount of gas remaining in the pool.\nfunc (gp *GasPool) Gas() uint64 {\n\treturn uint64(*gp)\n}\n\n// SetGas sets the amount of gas with the provided number.\nfunc (gp *GasPool) SetGas(gas uint64) {\n\t*(*uint64)(gp) = gas\n}\n\nfunc (gp *GasPool) String() string {\n\treturn fmt.Sprintf(\"%d\", *gp)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/data_availability.go",
          "line": 108,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(s.Blobs)\n\t}\n\tmaxBlobPerBlock := eip4844.MaxBlobsPerBlock(chain.Config(), block.Time())\n\tif blobCnt > maxBlobPerBlock {\n\t\treturn fmt.Errorf(\"too many blobs in block: have %d, permitted %d\", blobCnt, maxBlobPerBlock)\n\t}\n\n\t// check blob and versioned hash\n\tfor i, tx := range blobTxs {\n\t\t// check sidecar tx related\n\t\tif sidecars[i].TxHash != tx.Hash() {\n\t\t\treturn fmt.Errorf(\"sidecar's TxHash mismatch with expected transaction, want: %v, have: %v\", sidecars[i].TxHash, tx.Hash())\n\t\t}\n\t\tif sidecars[i].TxIndex != blobTxIndexes[i] {\n\t\t\treturn fmt.Errorf(\"sidecar's TxIndex mismatch with expected transaction, want: %v, have: %v\", sidecars[i].TxIndex, blobTxIndexes[i])\n\t\t}\n\t\tif err := validateBlobSidecar(tx.BlobHashes(), sidecars[i])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/blockchain.go",
          "line": 1658,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeSize\n\t\t\t\tlog.Info(\"Wrote genesis to ancients\")\n\t\t\t}\n\t\t}\n\t\t// Write all chain data to ancients.\n\t\tfirst := blockChain[0]\n\t\tptd := bc.GetTd(first.ParentHash(), first.NumberU64()-1)\n\t\tif ptd == nil {\n\t\t\treturn 0, consensus.ErrUnknownAncestor\n\t\t}\n\t\ttd := new(big.Int).Add(ptd, first.Difficulty())\n\t\twriteSize, err := rawdb.WriteAncientBlocksWithBlobs(bc.db, blockChain, receiptChain, td)\n\t\tif err != nil {\n\t\t\tlog.Error(\"Error importing chain data to ancients\", \"err\", err)\n\t\t\treturn 0, err\n\t\t}\n\t\tsize += writeSize\n\n\t\t// Sync the ancient store explicitly to ensure all data has been flushed to disk.\n\t\tif err := bc.db.SyncAncient()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/blockchain.go",
          "line": 1692,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int32(len(blockChain))\n\t\treturn 0, nil\n\t}\n\n\t// writeLive writes the blockchain and corresponding receipt chain to the active store.\n\t//\n\t// Notably, in different snap sync cycles, the supplied chain may partially reorganize\n\t// existing local chain segments (reorg around the chain tip). The reorganized part\n\t// will be included in the provided chain segment, and stale canonical markers will be\n\t// silently rewritten. Therefore, no explicit reorg logic is needed.\n\twriteLive := func(blockChain types.Blocks, receiptChain []rlp.RawValue) (int, error) {\n\t\tvar (\n\t\t\tskipPresenceCheck = false\n\t\t\tbatch             = bc.db.NewBatch()\n\t\t)\n\t\tfirst := blockChain[0]\n\t\tptd := bc.GetTd(first.ParentHash(), first.NumberU64()-1)\n\t\tif ptd == nil {\n\t\t\treturn 0, consensus.ErrUnknownAncestor\n\t\t}\n\t\ttdSum := new(big.Int).Set(ptd)\n\t\tfor i, block := range blockChain {\n\t\t\ttdSum.Add(tdSum, block.Difficulty())\n\t\t\t// Short circuit insertion if shutting down or processing failed\n\t\t\tif bc.insertStopped() {\n\t\t\t\treturn 0, errInsertionInterrupted\n\t\t\t}\n\t\t\tif !skipPresenceCheck {\n\t\t\t\t// Ignore if the entire data is already known\n\t\t\t\tif bc.HasBlock(block.Hash(), block.NumberU64()) {\n\t\t\t\t\tstats.ignored++\n\t\t\t\t\tcontinue\n\t\t\t\t} else {\n\t\t\t\t\t// If block N is not present, neither are the later blocks.\n\t\t\t\t\t// This should be true, but if we are mistaken, the shortcut\n\t\t\t\t\t// here will only cause overwriting of some existing data\n\t\t\t\t\tskipPresenceCheck = true\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Write all the data out into the database\n\t\t\trawdb.WriteTd(batch, block.Hash(), block.NumberU64(), tdSum)\n\t\t\trawdb.WriteCanonicalHash(batch, block.Hash(), block.NumberU64())\n\t\t\trawdb.WriteBlock(batch, block)\n\t\t\trawdb.WriteRawReceipts(batch, block.Hash(), block.NumberU64(), receiptChain[i])\n\t\t\tif bc.chainConfig.IsCancun(block.Number(), block.Time()) {\n\t\t\t\trawdb.WriteBlobSidecars(batch, block.Hash(), block.NumberU64(), block.Sidecars())\n\t\t\t}\n\n\t\t\t// Write everything belongs to the blocks into the database. So that\n\t\t\t// we can ensure all components of body is completed(body, receipts)\n\t\t\t// except transaction indexes(will be created once sync is finished).\n\t\t\tif batch.ValueSize() >= ethdb.IdealBatchSize {\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/blockchain.go",
          "line": 1747,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(batch.ValueSize())\n\t\t\t\tbatch.Reset()\n\t\t\t}\n\t\t\tstats.processed++\n\t\t}\n\t\t// Write everything belongs to the blocks into the database. So that\n\t\t// we can ensure all components of body is completed(body, receipts,\n\t\t// tx indexes)\n\t\tif batch.ValueSize() > 0 {\n\t\t\tsize += int64(batch.ValueSize())\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/blockchain.go",
          "line": 2230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= it.remaining()\n\n\t\t// If there are any still remaining, mark as ignored\n\t\treturn nil, it.index, err\n\n\t// Some other error(except ErrKnownBlock) occurred, abort.\n\t// ErrKnownBlock is allowed here since some known blocks\n\t// still need re-execution to generate snapshots that are missing\n\tcase err != nil && !errors.Is(err, ErrKnownBlock):\n\t\tbc.futureBlocks.Remove(block.Hash())\n\t\tstats.ignored += len(it.chain)\n\t\tbc.reportBlock(block, nil, err)\n\t\treturn nil, it.index, err\n\t}\n\t// Track the singleton witness from this chain insertion (if any)\n\tvar witness *stateless.Witness\n\n\tfor ",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/blockchain.go",
          "line": 2315,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= res.usedGas\n\t\twitness = res.witness\n\n\t\tvar snapDiffItems, snapBufItems common.StorageSize\n\t\tif bc.snaps != nil {\n\t\t\tsnapDiffItems, snapBufItems, _ = bc.snaps.Size()\n\t\t}\n\t\ttrieDiffNodes, trieBufNodes, trieImmutableBufNodes, _ := bc.triedb.Size()\n\t\tstats.report(chain, it.index, snapDiffItems, snapBufItems, trieDiffNodes, trieBufNodes, trieImmutableBufNodes, res.status == CanonStatTy)\n\n\t\t// Print confirmation that a future fork is scheduled, but not yet active.\n\t\tbc.logForkReadiness(block)\n\n\t\tif !setHead {\n\t\t\t// After merge we expect few side chains. Simply count\n\t\t\t// all blocks the CL gives us for GC processing time\n\t\t\tbc.gcproc += res.procTime\n\t\t\treturn witness, it.index, nil // Direct block insertion of a single block\n\t\t}\n\t\tswitch res.status {\n\t\tcase CanonStatTy:\n\t\t\tlog.Debug(\"Inserted new block\", \"number\", block.Number(), \"hash\", block.Hash(),\n\t\t\t\t\"uncles\", len(block.Uncles()), \"txs\", len(block.Transactions()), \"gas\", block.GasUsed(),\n\t\t\t\t\"elapsed\", common.PrettyDuration(time.Since(start)),\n\t\t\t\t\"root\", block.Root())\n\n\t\t\tlastCanon = block\n\n\t\t\t// Only count canonical blocks for GC processing time\n\t\t\tbc.gcproc += res.procTime\n\n\t\tcase SideStatTy:\n\t\t\tlog.Debug(\"Inserted forked block\", \"number\", block.Number(), \"hash\", block.Hash(),\n\t\t\t\t\"diff\", block.Difficulty(), \"elapsed\", common.PrettyDuration(time.Since(start)),\n\t\t\t\t\"txs\", len(block.Transactions()), \"gas\", block.GasUsed(), \"uncles\", len(block.Uncles()),\n\t\t\t\t\"root\", block.Root())\n\n\t\tdefault:\n\t\t\t// This in theory is impossible, but lets be nice to our future selves and leave\n\t\t\t// a log, instead of trying to track down blocks imports that don't emit logs.\n\t\t\tlog.Warn(\"Inserted block with unknown status\", \"number\", block.Number(), \"hash\", block.Hash(),\n\t\t\t\t\"diff\", block.Difficulty(), \"elapsed\", common.PrettyDuration(time.Since(start)),\n\t\t\t\t\"txs\", len(block.Transactions()), \"gas\", block.GasUsed(), \"uncles\", len(block.Uncles()),\n\t\t\t\t\"root\", block.Root())\n\t\t}\n\t\tbc.chainBlockFeed.Send(ChainHeadEvent{block.Header()})\n\t}\n\n\t// Any blocks remaining here? The only ones we care about are the future ones\n\tif block != nil && errors.Is(err, consensus.ErrFutureBlock) {\n\t\tif err := bc.addFutureBlock(block)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/blockchain.go",
          "line": 2377,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= it.remaining()\n\treturn witness, it.index, err\n}\n\nfunc (bc *BlockChain) updateHighestVerifiedHeader(header *types.Header) {\n\tif header == nil || header.Number == nil {\n\t\treturn\n\t}\n\tcurrentBlock := bc.CurrentBlock()\n\treorg, err := bc.forker.ReorgNeededWithFastFinality(currentBlock, header)\n\tif err == nil && reorg {\n\t\tbc.highestVerifiedHeader.Store(types.CopyHeader(header))\n\t\tlog.Trace(\"updateHighestVerifiedHeader\", \"number\", header.Number.Uint64(), \"hash\", header.Hash())\n\t}\n}\n\nfunc (bc *BlockChain) GetHighestVerifiedHeader() *types.Header {\n\treturn bc.highestVerifiedHeader.Load()\n}\n\n// blockProcessingResult is a summary of block processing\n// used for updating the stats.\ntype blockProcessingResult struct {\n\tusedGas  uint64\n\tprocTime time.Duration\n\tstatus   WriteStatus\n\twitness  *stateless.Witness\n}\n\n// processBlock executes and validates the given block. If there was no error\n// it writes the block and associated state to database.\nfunc (bc *BlockChain) processBlock(parentRoot common.Hash, block *types.Block, setHead bool, makeWitness bool) (_ *blockProcessingResult, blockEndErr error) {\n\tvar (\n\t\terr       error\n\t\tstartTime = time.Now()\n\t\tstatedb   *state.StateDB\n\t\tinterrupt atomic.Bool\n\t)\n\tdefer interrupt.Store(true) // terminate the prefetch at the end\n\n\tneedBadSharedStorage := bc.chainConfig.NeedBadSharedStorage(block.Number())\n\tneedPrefetch := needBadSharedStorage || (!bc.cfg.NoPrefetch && len(block.Transactions()) >= prefetchTxNumber)\n\tif !needPrefetch {\n\t\tstatedb, err = state.New(parentRoot, bc.statedb)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\t// If prefetching is enabled, run that against the current state to pre-cache\n\t\t// transactions and probabilistically some of the account/storage trie nodes.\n\t\t//\n\t\t// Note: the main processor and prefetcher share the same reader with a local\n\t\t// cache for mitigating the overhead of state access.\n\t\tprefetch, process, err := bc.statedb.ReadersWithCacheStats(parentRoot)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tthrowaway, err := state.NewWithReader(parentRoot, bc.statedb, prefetch)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tstatedb, err = state.NewWithReader(parentRoot, bc.statedb, process)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Upload the statistics of reader at the end\n\t\tdefer func() {\n\t\t\tstats := prefetch.GetStats()\n\t\t\taccountCacheHitPrefetchMeter.Mark(stats.AccountHit)\n\t\t\taccountCacheMissPrefetchMeter.Mark(stats.AccountMiss)\n\t\t\tstorageCacheHitPrefetchMeter.Mark(stats.StorageHit)\n\t\t\tstorageCacheMissPrefetchMeter.Mark(stats.StorageMiss)\n\t\t\tstats = process.GetStats()\n\t\t\taccountCacheHitMeter.Mark(stats.AccountHit)\n\t\t\taccountCacheMissMeter.Mark(stats.AccountMiss)\n\t\t\tstorageCacheHitMeter.Mark(stats.StorageHit)\n\t\t\tstorageCacheMissMeter.Mark(stats.StorageMiss)\n\t\t}()\n\n\t\tgo func(start time.Time, throwaway *state.StateDB, block *types.Block) {\n\t\t\t// Disable tracing for prefetcher executions.\n\t\t\tvmCfg := bc.cfg.VmConfig\n\t\t\tvmCfg.Tracer = nil\n\t\t\tbc.prefetcher.Prefetch(block.Transactions(), block.Header(), block.GasLimit(), throwaway, vmCfg, &interrupt)\n\n\t\t\tblockPrefetchExecuteTimer.Update(time.Since(start))\n\t\t\tif interrupt.Load() {\n\t\t\t\tblockPrefetchInterruptMeter.Mark(1)\n\t\t\t}\n\t\t}(time.Now(), throwaway, block)\n\t}\n\n\t// If we are past Byzantium, enable prefetching to pull in trie node paths\n\t// while processing transactions. Before Byzantium the prefetcher is mostly\n\t// useless due to the intermediate root hashing after each transaction.\n\tvar witness *stateless.Witness\n\tif bc.chainConfig.IsByzantium(block.Number()) {\n\t\t// Generate witnesses either if we're self-testing, or if it's the\n\t\t// only block being inserted. A bit crude, but witnesses are huge,\n\t\t// so we refuse to make an entire chain of them.\n\t\tif bc.cfg.VmConfig.StatelessSelfValidation || makeWitness {\n\t\t\twitness, err = stateless.NewWitness(block.Header(), bc)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\tstatedb.StartPrefetcher(\"chain\", witness)\n\t\tdefer statedb.StopPrefetcher()\n\t}\n\n\tif bc.logger != nil && bc.logger.OnBlockStart != nil {\n\t\ttd := bc.GetTd(block.ParentHash(), block.NumberU64()-1)\n\t\tbc.logger.OnBlockStart(tracing.BlockEvent{\n\t\t\tBlock:     block,\n\t\t\tTD:        td,\n\t\t\tFinalized: bc.CurrentFinalBlock(),\n\t\t\tSafe:      bc.CurrentSafeBlock(),\n\t\t})\n\t}\n\tif bc.logger != nil && bc.logger.OnBlockEnd != nil {\n\t\tdefer func() {\n\t\t\tbc.logger.OnBlockEnd(blockEndErr)\n\t\t}()\n\t}\n\n\t// Process block using the parent state as reference point\n\tpstart := time.Now()\n\tstatedb.SetExpectedStateRoot(block.Root())\n\tstatedb.SetNeedBadSharedStorage(needBadSharedStorage)\n\tres, err := bc.processor.Process(block, statedb, bc.cfg.VmConfig)\n\tif err != nil {\n\t\tbc.reportBlock(block, res, err)\n\t\treturn nil, err\n\t}\n\tptime := time.Since(pstart)\n\n\t// Validate the state using the default validator\n\tvstart := time.Now()\n\tif err := bc.validator.ValidateState(block, statedb, res, false)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/blockchain.go",
          "line": 2723,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= block.Size()\n\n\t\t// If memory use grew too large, import and continue. Sadly we need to discard\n\t\t// all raised events and logs from notifications since we're too heavy on the\n\t\t// memory here.\n\t\tif len(blocks) >= 2048 || memory > 64*1024*1024 {\n\t\t\tlog.Info(\"Importing heavy sidechain segment\", \"blocks\", len(blocks), \"start\", blocks[0].NumberU64(), \"end\", block.NumberU64())\n\t\t\tif _, _, err := bc.insertChain(blocks, true, false)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/blockchain.go",
          "line": 3192,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"\\n  %d: cumulative: %v gas: %v contract: %v status: %v tx: %v logs: %v bloom: %x state: %x\",\n\t\t\ti, receipt.CumulativeGasUsed, receipt.GasUsed, receipt.ContractAddress.Hex(),\n\t\t\treceipt.Status, receipt.TxHash.Hex(), receipt.Logs, receipt.Bloom, receipt.PostState)\n\t}\n\tversion, vcs := version.Info()\n\tplatform := fmt.Sprintf(\"%s %s %s %s\", version, runtime.Version(), runtime.GOARCH, runtime.GOOS)\n\tif vcs != \"\" {\n\t\tvcs = fmt.Sprintf(\"\\nVCS: %s\", vcs)\n\t}\n\n\tif badBlockRecords.Cardinality() < badBlockRecordslimit {\n\t\tbadBlockRecords.Add(block.Hash())\n\t\tbadBlockGauge.Update(int64(badBlockRecords.Cardinality()))\n\t}\n\n\treturn fmt.Sprintf(`\n########## BAD BLOCK #########\nBlock: %v (%#x)\nMiner: %v\nError: %v\nPlatform: %v%v\nChain config: %#v\nReceipts: %v\n##############################\n`, block.Number(), block.Hash(), block.Coinbase(), err, platform, vcs, config, receiptString)\n}\n\n// InsertHeaderChain attempts to insert the given header chain in to the local\n// chain, possibly creating a reorg. If an error is returned, it will return the\n// index number of the failing header as well an error describing what went wrong.\nfunc (bc *BlockChain) InsertHeaderChain(chain []*types.Header) (int, error) {\n\tif len(chain) == 0 {\n\t\treturn 0, nil\n\t}\n\tstart := time.Now()\n\tif i, err := bc.hc.ValidateHeaderChain(chain)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/core/blockchain.go",
          "line": 903,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/core/blockchain.go",
          "line": 928,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/core/blockchain.go",
          "line": 1980,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc/core/blockchain.go",
          "line": 2012,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc/core/blockchain.go",
          "line": 2014,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc/core/blockchain.go",
          "line": 2028,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc/core/blockchain.go",
          "line": 2030,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc/core/blockchain.go",
          "line": 2110,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0016",
          "file": "bsc/core/blockchain.go",
          "line": 2114,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0017",
          "file": "bsc/core/blockchain.go",
          "line": 2129,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0018",
          "file": "bsc/core/blockchain.go",
          "line": 2132,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0019",
          "file": "bsc/core/blockchain.go",
          "line": 2360,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0020",
          "file": "bsc/core/blockchain.go",
          "line": 2926,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0021",
          "file": "bsc/core/blockchain.go",
          "line": 2931,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0022",
          "file": "bsc/core/blockchain.go",
          "line": 2967,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0023",
          "file": "bsc/core/blockchain.go",
          "line": 2974,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0024",
          "file": "bsc/core/blockchain.go",
          "line": 3056,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0025",
          "file": "bsc/core/blockchain.go",
          "line": 3058,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0026",
          "file": "bsc/core/blockchain.go",
          "line": 3060,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0027",
          "file": "bsc/core/blockchain.go",
          "line": 702,
          "category": "ensemble_mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.68418,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0028",
          "file": "bsc/core/blockchain.go",
          "line": 738,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.86,
          "confidence": 0.9774,
          "ensemble_confidence": 0.8796600000000001
        },
        {
          "id": "ENSEMBLE_0029",
          "file": "bsc/core/blockchain.go",
          "line": 791,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.86,
          "confidence": 0.9774,
          "ensemble_confidence": 0.8796600000000001
        },
        {
          "id": "ENSEMBLE_0030",
          "file": "bsc/core/blockchain.go",
          "line": 1103,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db, header.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0031",
          "file": "bsc/core/blockchain.go",
          "line": 1105,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db, common.Hash{})",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0032",
          "file": "bsc/core/blockchain.go",
          "line": 1155,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, newHeadBlock.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0033",
          "file": "bsc/core/blockchain.go",
          "line": 1183,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, newHeadSnapBlock.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0034",
          "file": "bsc/core/blockchain.go",
          "line": 1403,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockBatch, block.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0035",
          "file": "bsc/core/blockchain.go",
          "line": 1404,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockBatch, block.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0036",
          "file": "bsc/core/blockchain.go",
          "line": 1503,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(bc.db, recent.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0037",
          "file": "bsc/core/blockchain.go",
          "line": 1634,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(batch, hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0038",
          "file": "bsc/core/blockchain.go",
          "line": 3308,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(batch, last.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/blockchain_test.go",
          "line": 1320,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(x)\n\t\tfor _, log := range x {\n\t\t\t// We expect added logs to be in ascending order: 0:0, 0:1, 1:0 ...\n\t\t\thave := 100*int(log.BlockNumber) + int(log.TxIndex)\n\t\t\tif have < prev {\n\t\t\t\tt.Fatalf(\"Expected new logs to arrive in ascending order (%d < %d)\", have, prev)\n\t\t\t}\n\t\t\tprev = have\n\t\t}\n\t}\n\tprev = 0\n\tfor len(rmLogsCh) > 0 {\n\t\tx := <-rmLogsCh\n\t\tcountRm += len(x.Logs)\n\t\tfor _, log := range x.Logs {\n\t\t\t// We expect removed logs to be in ascending order: 0:0, 0:1, 1:0 ...\n\t\t\thave := 100*int(log.BlockNumber) + int(log.TxIndex)\n\t\t\tif have < prev {\n\t\t\t\tt.Fatalf(\"Expected removed logs to arrive in ascending order (%d < %d)\", have, prev)\n\t\t\t}\n\t\t\tprev = have\n\t\t}\n\t}\n\n\tif countNew != wantNew {\n\t\tt.Fatalf(\"wrong number of log events: got %d, want %d\", countNew, wantNew)\n\t}\n\tif countRm != wantRemoved {\n\t\tt.Fatalf(\"wrong number of removed log events: got %d, want %d\", countRm, wantRemoved)\n\t}\n}\n\n// Tests if the canonical block can be fetched from the database during chain insertion.\nfunc TestCanonicalBlockRetrieval(t *testing.T) {\n\ttestCanonicalBlockRetrieval(t, rawdb.HashScheme)\n\ttestCanonicalBlockRetrieval(t, rawdb.PathScheme)\n}\n\nfunc testCanonicalBlockRetrieval(t *testing.T, scheme string) {\n\t_, gspec, blockchain, err := newCanonical(ethash.NewFaker(), 0, true, scheme)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create pristine chain: %v\", err)\n\t}\n\tdefer blockchain.Stop()\n\n\t_, chain, _ := GenerateChainWithGenesis(gspec, ethash.NewFaker(), 10, func(i int, gen *BlockGen) {})\n\n\tvar pend sync.WaitGroup\n\tpend.Add(len(chain))\n\n\tfor i := range chain {\n\t\tgo func(block *types.Block) {\n\t\t\tdefer pend.Done()\n\n\t\t\t// try to retrieve a block by its canonical hash and see if the block data can be retrieved.\n\t\t\tfor {\n\t\t\t\tch := rawdb.ReadCanonicalHash(blockchain.db, block.NumberU64())\n\t\t\t\tif ch == (common.Hash{}) {\n\t\t\t\t\tcontinue // busy wait for canonical hash to be written\n\t\t\t\t}\n\t\t\t\tif ch != block.Hash() {\n\t\t\t\t\tt.Errorf(\"unknown canonical hash, want %s, got %s\", block.Hash().Hex(), ch.Hex())\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfb := rawdb.ReadBlock(blockchain.db, ch, block.NumberU64())\n\t\t\t\tif fb == nil {\n\t\t\t\t\tt.Errorf(\"unable to retrieve block %d for canonical hash: %s\", block.NumberU64(), ch.Hex())\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tif fb.Hash() != block.Hash() {\n\t\t\t\t\tt.Errorf(\"invalid block hash for block %d, want %s, got %s\", block.NumberU64(), block.Hash().Hex(), fb.Hash().Hex())\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\treturn\n\t\t\t}\n\t\t}(chain[i])\n\n\t\tif _, err := blockchain.InsertChain(types.Blocks{chain[i]})",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/blockchain_test.go",
          "line": 4342,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/blockchain_test.go",
          "line": 235,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockchain.db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/blockchain_test.go",
          "line": 1789,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(ancientDb, midBlock.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/headerchain.go",
          "line": 453,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= ancestor\n\t\t\t\treturn ancestorHash, number\n\t\t\t}\n\t\t}\n\t\tif *maxNonCanonical == 0 {\n\t\t\treturn common.Hash{}, 0\n\t\t}\n\t\t*maxNonCanonical--\n\t\tancestor--\n\t\theader := hc.GetHeader(hash, number)\n\t\tif header == nil {\n\t\t\treturn common.Hash{}, 0\n\t\t}\n\t\thash = header.ParentHash\n\t\tnumber--\n\t}\n\treturn hash, number\n}\n\n// GetTd retrieves a block's total difficulty in the canonical chain from the\n// database by hash and number, caching it if found.\nfunc (hc *HeaderChain) GetTd(hash common.Hash, number uint64) *big.Int {\n\t// Short circuit if the td's already in the cache, retrieve otherwise\n\tif cached, ok := hc.tdCache.Get(hash)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/headerchain.go",
          "line": 543,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= number - current\n\t\t\tnumber = current\n\t\t} else {\n\t\t\treturn nil\n\t\t}\n\t}\n\tvar headers []rlp.RawValue\n\t// If we have some of the headers in cache already, use that before going to db.\n\thash := rawdb.ReadCanonicalHash(hc.chainDb, number)\n\tif hash == (common.Hash{}) {\n\t\treturn nil\n\t}\n\tfor count > 0 {\n\t\theader, ok := hc.headerCache.Get(hash)\n\t\tif !ok {\n\t\t\tbreak\n\t\t}\n\t\trlpData, _ := rlp.EncodeToBytes(header)\n\t\theaders = append(headers, rlpData)\n\t\thash = header.ParentHash\n\t\tcount--\n\t\tnumber--\n\t}\n\t// Read remaining from db\n\tif count > 0 {\n\t\theaders = append(headers, rawdb.ReadHeaderRange(hc.chainDb, number, count)...)\n\t}\n\treturn headers\n}\n\nfunc (hc *HeaderChain) GetCanonicalHash(number uint64) common.Hash {\n\treturn rawdb.ReadCanonicalHash(hc.chainDb, number)\n}\n\n// CurrentHeader retrieves the current head header of the canonical chain. The\n// header is retrieved from the HeaderChain's internal cache.\nfunc (hc *HeaderChain) CurrentHeader() *types.Header {\n\treturn hc.currentHeader.Load()\n}\n\n// SetCurrentHeader sets the in-memory head header marker of the canonical chan\n// as the given header.\nfunc (hc *HeaderChain) SetCurrentHeader(head *types.Header) {\n\thc.currentHeader.Store(head)\n\thc.currentHeaderHash = head.Hash()\n\theadHeaderGauge.Update(head.Number.Int64())\n\tjustifiedBlockGauge.Update(int64(hc.GetJustifiedNumber(head)))\n\tfinalizedBlockGauge.Update(int64(hc.GetFinalizedNumber(head)))\n}\n\ntype (\n\t// UpdateHeadBlocksCallback is a callback function that is called by SetHead\n\t// before head header is updated. The method will return the actual block it\n\t// updated the head to (missing state) and a flag if setHead should continue\n\t// rewinding till that forcefully (exceeded ancient limits)\n\tUpdateHeadBlocksCallback func(ethdb.KeyValueWriter, *types.Header) (*types.Header, bool)\n\n\t// DeleteBlockContentCallback is a callback function that is called by SetHead\n\t// before each header is deleted.\n\tDeleteBlockContentCallback func(ethdb.KeyValueWriter, common.Hash, uint64)\n)\n\n// SetHead rewinds the local chain to a new head. Everything above the new head\n// will be deleted and the new one set.\nfunc (hc *HeaderChain) SetHead(head uint64, updateFn UpdateHeadBlocksCallback, delFn DeleteBlockContentCallback) {\n\thc.setHead(head, 0, updateFn, delFn)\n}\n\n// SetHeadWithTimestamp rewinds the local chain to a new head timestamp. Everything\n// above the new head will be deleted and the new one set.\nfunc (hc *HeaderChain) SetHeadWithTimestamp(time uint64, updateFn UpdateHeadBlocksCallback, delFn DeleteBlockContentCallback) {\n\thc.setHead(0, time, updateFn, delFn)\n}\n\n// setHead rewinds the local chain to a new head block or a head timestamp.\n// Everything above the new head will be deleted and the new one set.\nfunc (hc *HeaderChain) setHead(headBlock uint64, headTime uint64, updateFn UpdateHeadBlocksCallback, delFn DeleteBlockContentCallback) {\n\t// Sanity check that there's no attempt to undo the genesis block. This is\n\t// a fairly synthetic case where someone enables a timestamp based fork\n\t// below the genesis timestamp. It's nice to not allow that instead of the\n\t// entire chain getting deleted.\n\tif headTime > 0 && hc.genesisHeader.Time > headTime {\n\t\t// Note, a critical error is quite brutal, but we should really not reach\n\t\t// this point. Since pre-timestamp based forks it was impossible to have\n\t\t// a fork before block 0, the setHead would always work. With timestamp\n\t\t// forks it becomes possible to specify below the genesis. That said, the\n\t\t// only time we setHead via timestamp is with chain config changes on the\n\t\t// startup, so failing hard there is ok.\n\t\tlog.Crit(\"Rejecting genesis rewind via timestamp\", \"target\", headTime, \"genesis\", hc.genesisHeader.Time)\n\t}\n\tvar (\n\t\tparentHash common.Hash\n\t\tbatch      = hc.chainDb.NewBatch()\n\t\torigin     = true\n\t)\n\tdone := func(header *types.Header) bool {\n\t\tif headTime > 0 {\n\t\t\treturn header.Time <= headTime\n\t\t}\n\t\treturn header.Number.Uint64() <= headBlock\n\t}\n\tfor hdr := hc.CurrentHeader()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/headerchain.go",
          "line": 92,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(chainDb)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8799999999999999,
          "confidence": 0.9792000000000001,
          "ensemble_confidence": 0.8812800000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txindexer.go",
          "line": 220,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(indexer.db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/data_availability_test.go",
          "line": 426,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gokzg4844.SerializedScalarSize {\n\t\tfieldElementBytes := randFieldElement()\n\t\tcopy(blob[i:i+gokzg4844.SerializedScalarSize], fieldElementBytes[:])\n\t}\n\treturn blob\n}\n\nfunc randomSidecar() *types.BlobTxSidecar {\n\tblob := randBlob()\n\tcommitment, _ := kzg4844.BlobToCommitment(&blob)\n\tproof, _ := kzg4844.ComputeBlobProof(&blob, commitment)\n\treturn &types.BlobTxSidecar{\n\t\tBlobs:       []kzg4844.Blob{blob},\n\t\tCommitments: []kzg4844.Commitment{commitment},\n\t\tProofs:      []kzg4844.Proof{proof},\n\t}\n}\n\nfunc emptySidecar() *types.BlobTxSidecar {\n\treturn &types.BlobTxSidecar{\n\t\tBlobs:       []kzg4844.Blob{emptyBlob},\n\t\tCommitments: []kzg4844.Commitment{emptyBlobCommit},\n\t\tProofs:      []kzg4844.Proof{emptyBlobProof},\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/blockchain_insert.go",
          "line": 54,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(block.Transactions())\n\t\t\tfor _, sidecar := range block.Sidecars() {\n\t\t\t\tblobs += len(sidecar.Blobs)\n\t\t\t}\n\t\t}\n\t\tend := chain[index]\n\n\t\t// Assemble the log context and send it to the logger\n\t\tcontext := []interface{}{\n\t\t\t\"number\", end.Number(), \"hash\", end.Hash(), \"miner\", end.Coinbase(),\n\t\t\t\"blocks\", st.processed, \"txs\", txs, \"blobs\", blobs, \"mgas\", float64(st.usedGas) / 1000000,\n\t\t\t\"elapsed\", common.PrettyDuration(elapsed), \"mgasps\", mgasps,\n\t\t}\n\t\tblockInsertMgaspsGauge.Update(int64(mgasps))\n\t\tif timestamp := time.Unix(int64(end.Time()), 0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state_processor_test.go",
          "line": 406,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= tx.Gas()\n\t\tnBlobs += len(tx.BlobHashes())\n\t}\n\theader.Root = common.BytesToHash(hasher.Sum(nil))\n\tif config.IsCancun(header.Number, header.Time) {\n\t\texcess := eip4844.CalcExcessBlobGas(config, parent.Header(), header.Time)\n\t\tused := uint64(nBlobs * params.BlobTxBlobGasPerBlob)\n\t\theader.ExcessBlobGas = &excess\n\t\theader.BlobGasUsed = &used\n\n\t\tbeaconRoot := common.HexToHash(\"0xbeac00\")\n\t\tif config.Parlia == nil {\n\t\t\theader.ParentBeaconRoot = &beaconRoot\n\t\t}\n\t}\n\t// Assemble and return the final block for sealing\n\tbody := &types.Body{Transactions: txs}\n\tif config.IsShanghai(header.Number, header.Time) {\n\t\tbody.Withdrawals = []*types.Withdrawal{}\n\t}\n\treturn types.NewBlock(header, body, receipts, trie.NewStackTrie(nil))\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/block_validator.go",
          "line": 92,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(tx.BlobHashes())\n\n\t\t\t\t// If the tx is a blob tx, it must NOT have a sidecar attached to be valid in a block.\n\t\t\t\tif tx.BlobTxSidecar() != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unexpected blob sidecar in transaction at index %d\", i)\n\t\t\t\t}\n\n\t\t\t\t// The individual checks for blob validity (version-check + not empty)\n\t\t\t\t// happens in state transition.\n\t\t\t}\n\n\t\t\t// Check blob gas usage.\n\t\t\tif header.BlobGasUsed != nil {\n\t\t\t\tif want := *header.BlobGasUsed / params.BlobTxBlobGasPerBlob",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/bench_test.go",
          "line": 139,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= params.TxGas\n\t\t\tif gas < params.TxGas {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tto := (from + 1) % naccounts\n\t\t\tburn := new(big.Int).SetUint64(params.TxGas)\n\t\t\tburn.Mul(burn, gen.header.BaseFee)\n\t\t\tavailableFunds.Sub(availableFunds, burn)\n\t\t\tif availableFunds.Cmp(big.NewInt(1)) < 0 {\n\t\t\t\tpanic(\"not enough funds\")\n\t\t\t}\n\t\t\ttx, err := types.SignNewTx(ringKeys[from], signer,\n\t\t\t\t&types.LegacyTx{\n\t\t\t\t\tNonce:    gen.TxNonce(ringAddrs[from]),\n\t\t\t\t\tTo:       &ringAddrs[to],\n\t\t\t\t\tValue:    availableFunds,\n\t\t\t\t\tGas:      params.TxGas,\n\t\t\t\t\tGasPrice: gasPrice,\n\t\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\t\t\tgen.AddTx(tx)\n\t\t\tfrom = to\n\t\t}\n\t}\n}\n\n// genUncles generates blocks with two uncle headers.\nfunc genUncles(i int, gen *BlockGen) {\n\tif i >= 7 {\n\t\tb2 := gen.PrevBlock(i - 6).Header()\n\t\tb2.Extra = []byte(\"foo\")\n\t\tgen.AddUncle(b2)\n\t\tb3 := gen.PrevBlock(i - 6).Header()\n\t\tb3.Extra = []byte(\"bar\")\n\t\tgen.AddUncle(b3)\n\t}\n}\n\nfunc benchInsertChain(b *testing.B, disk bool, gen func(int, *BlockGen)) {\n\t// Create the database in memory or in a temporary directory.\n\tvar db ethdb.Database\n\tif !disk {\n\t\tdb = rawdb.NewMemoryDatabase()\n\t} else {\n\t\tpdb, err := pebble.New(b.TempDir(), 128, 128, \"\", false)\n\t\tif err != nil {\n\t\t\tb.Fatalf(\"cannot create temporary database: %v\", err)\n\t\t}\n\t\tdb = rawdb.NewDatabase(pdb)\n\t\tdefer db.Close()\n\t}\n\t// Generate a chain of b.N blocks using the supplied block\n\t// generator function.\n\tgspec := &Genesis{\n\t\tConfig: params.TestChainConfig,\n\t\tAlloc:  types.GenesisAlloc{benchRootAddr: {Balance: benchRootFunds}},\n\t}\n\t_, chain, _ := GenerateChainWithGenesis(gspec, ethash.NewFaker(), b.N, gen)\n\n\t// Time the insertion of the new chain.\n\t// State and blocks are stored in the same DB.\n\tchainman, _ := NewBlockChain(db, gspec, ethash.NewFaker(), nil)\n\tdefer chainman.Stop()\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\tif i, err := chainman.InsertChain(chain)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/bench_test.go",
          "line": 298,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8899999999999999,
          "confidence": 0.9801,
          "ensemble_confidence": 0.88209
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/genesis.go",
          "line": 600,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, block.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/genesis.go",
          "line": 601,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, block.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txindexer_test.go",
          "line": 91,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t})\n\tvar cases = []struct {\n\t\tlimits []uint64\n\t\ttails  []uint64\n\t}{\n\t\t{\n\t\t\tlimits: []uint64{0, 1, 64, 129, 0},\n\t\t\ttails:  []uint64{0, 128, 65, 0, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{64, 1, 64, 0},\n\t\t\ttails:  []uint64{65, 128, 65, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{127, 1, 64, 0},\n\t\t\ttails:  []uint64{2, 128, 65, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{128, 1, 64, 0},\n\t\t\ttails:  []uint64{1, 128, 65, 0},\n\t\t},\n\t\t{\n\t\t\tlimits: []uint64{129, 1, 64, 0},\n\t\t\ttails:  []uint64{0, 128, 65, 0},\n\t\t},\n\t}\n\tfor _, c := range cases {\n\t\tdb, _ := rawdb.Open(rawdb.NewMemoryDatabase(), rawdb.OpenOptions{})\n\t\trawdb.WriteAncientBlocks(db, append([]*types.Block{gspec.ToBlock()}, blocks...), types.EncodeBlockReceiptLists(append([]types.Receipts{{}}, receipts...)), big.NewInt(0))\n\n\t\t// Index the initial blocks from ancient store\n\t\tindexer := &txIndexer{\n\t\t\tlimit: 0,\n\t\t\tdb:    db,\n\t\t}\n\t\tfor i, limit := range c.limits {\n\t\t\tindexer.limit = limit\n\t\t\tindexer.run(chainHead, make(chan struct{}), make(chan struct{}))\n\t\t\tverify(t, db, blocks, c.tails[i])\n\t\t}\n\t\tdb.Close()\n\t}\n}\n\nfunc TestTxIndexerRepair(t *testing.T) {\n\tvar (\n\t\ttestBankKey, _  = crypto.GenerateKey()\n\t\ttestBankAddress = crypto.PubkeyToAddress(testBankKey.PublicKey)\n\t\ttestBankFunds   = big.NewInt(1000000000000000000)\n\n\t\tgspec = &Genesis{\n\t\t\tConfig:  params.TestChainConfig,\n\t\t\tAlloc:   types.GenesisAlloc{testBankAddress: {Balance: testBankFunds}},\n\t\t\tBaseFee: big.NewInt(params.InitialBaseFee),\n\t\t}\n\t\tengine    = ethash.NewFaker()\n\t\tnonce     = uint64(0)\n\t\tchainHead = uint64(128)\n\t)\n\t_, blocks, receipts := GenerateChainWithGenesis(gspec, engine, int(chainHead), func(i int, gen *BlockGen) {\n\t\ttx, _ := types.SignTx(types.NewTransaction(nonce, common.HexToAddress(\"0xdeadbeef\"), big.NewInt(1000), params.TxGas, big.NewInt(10*params.InitialBaseFee), nil), types.HomesteadSigner{}, testBankKey)\n\t\tgen.AddTx(tx)\n\t\tnonce += 1\n\t})\n\ttailPointer := func(n uint64) *uint64 {\n\t\treturn &n\n\t}\n\tvar cases = []struct {\n\t\tlimit   uint64\n\t\thead    uint64\n\t\tcutoff  uint64\n\t\texpTail *uint64\n\t}{\n\t\t// if *tail > head => purge indexes\n\t\t{\n\t\t\tlimit:   0,\n\t\t\thead:    chainHead / 2,\n\t\t\tcutoff:  0,\n\t\t\texpTail: tailPointer(0),\n\t\t},\n\t\t{\n\t\t\tlimit:   1,             // tail = 128\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: nil,\n\t\t},\n\t\t{\n\t\t\tlimit:   64,            // tail = 65\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: nil,\n\t\t},\n\t\t{\n\t\t\tlimit:   65,            // tail = 64\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: tailPointer(64),\n\t\t},\n\t\t{\n\t\t\tlimit:   66,            // tail = 63\n\t\t\thead:    chainHead / 2, // newhead = 64\n\t\t\tcutoff:  0,\n\t\t\texpTail: tailPointer(63),\n\t\t},\n\n\t\t// if tail < cutoff => remove indexes below cutoff\n\t\t{\n\t\t\tlimit:   0,         // tail = 0\n\t\t\thead:    chainHead, // head = 128\n\t\t\tcutoff:  chainHead, // cutoff = 128\n\t\t\texpTail: tailPointer(chainHead),\n\t\t},\n\t\t{\n\t\t\tlimit:   1,         // tail = 128\n\t\t\thead:    chainHead, // head = 128\n\t\t\tcutoff:  chainHead, // cutoff = 128\n\t\t\texpTail: tailPointer(128),\n\t\t},\n\t\t{\n\t\t\tlimit:   2,         // tail = 127\n\t\t\thead:    chainHead, // head = 128\n\t\t\tcutoff:  chainHead, // cutoff = 128\n\t\t\texpTail: tailPointer(chainHead),\n\t\t},\n\t\t{\n\t\t\tlimit:   2,             // tail = 127\n\t\t\thead:    chainHead,     // head = 128\n\t\t\tcutoff:  chainHead / 2, // cutoff = 64\n\t\t\texpTail: tailPointer(127),\n\t\t},\n\n\t\t// if head < cutoff => purge indexes\n\t\t{\n\t\t\tlimit:   0,             // tail = 0\n\t\t\thead:    chainHead,     // head = 128\n\t\t\tcutoff:  2 * chainHead, // cutoff = 256\n\t\t\texpTail: nil,\n\t\t},\n\t\t{\n\t\t\tlimit:   64,            // tail = 65\n\t\t\thead:    chainHead,     // head = 128\n\t\t\tcutoff:  chainHead / 2, // cutoff = 64\n\t\t\texpTail: tailPointer(65),\n\t\t},\n\t}\n\tfor _, c := range cases {\n\t\tdb, _ := rawdb.Open(rawdb.NewMemoryDatabase(), rawdb.OpenOptions{})\n\t\tencReceipts := types.EncodeBlockReceiptLists(append([]types.Receipts{{}}, receipts...))\n\t\trawdb.WriteAncientBlocks(db, append([]*types.Block{gspec.ToBlock()}, blocks...), encReceipts, big.NewInt(0))\n\n\t\t// Index the initial blocks from ancient store\n\t\tindexer := &txIndexer{\n\t\t\tlimit: c.limit,\n\t\t\tdb:    db,\n\t\t}\n\t\tindexer.run(chainHead, make(chan struct{}), make(chan struct{}))\n\n\t\tindexer.cutoff = c.cutoff\n\t\tindexer.repair(c.head)\n\n\t\tif c.expTail == nil {\n\t\t\tverifyNoIndex(t, db, blocks)\n\t\t} else {\n\t\t\tverify(t, db, blocks, *c.expTail)\n\t\t}\n\t\tdb.Close()\n\t}\n}\n\nfunc TestTxIndexerReport(t *testing.T) {\n\tvar (\n\t\ttestBankKey, _  = crypto.GenerateKey()\n\t\ttestBankAddress = crypto.PubkeyToAddress(testBankKey.PublicKey)\n\t\ttestBankFunds   = big.NewInt(1000000000000000000)\n\n\t\tgspec = &Genesis{\n\t\t\tConfig:  params.TestChainConfig,\n\t\t\tAlloc:   types.GenesisAlloc{testBankAddress: {Balance: testBankFunds}},\n\t\t\tBaseFee: big.NewInt(params.InitialBaseFee),\n\t\t}\n\t\tengine    = ethash.NewFaker()\n\t\tnonce     = uint64(0)\n\t\tchainHead = uint64(128)\n\t)\n\t_, blocks, receipts := GenerateChainWithGenesis(gspec, engine, int(chainHead), func(i int, gen *BlockGen) {\n\t\ttx, _ := types.SignTx(types.NewTransaction(nonce, common.HexToAddress(\"0xdeadbeef\"), big.NewInt(1000), params.TxGas, big.NewInt(10*params.InitialBaseFee), nil), types.HomesteadSigner{}, testBankKey)\n\t\tgen.AddTx(tx)\n\t\tnonce += 1\n\t})\n\ttailPointer := func(n uint64) *uint64 {\n\t\treturn &n\n\t}\n\tvar cases = []struct {\n\t\thead         uint64\n\t\tlimit        uint64\n\t\tcutoff       uint64\n\t\ttail         *uint64\n\t\texpIndexed   uint64\n\t\texpRemaining uint64\n\t}{\n\t\t// The entire chain is supposed to be indexed\n\t\t{\n\t\t\t// head = 128, limit = 0, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  0,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 0\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 0, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  0,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 1\n\t\t\ttail:         tailPointer(1),\n\t\t\texpIndexed:   128,\n\t\t\texpRemaining: 1,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 0, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  0,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 128\n\t\t\ttail:         tailPointer(chainHead),\n\t\t\texpIndexed:   1,\n\t\t\texpRemaining: 128,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 256, cutoff = 0 => all: 129\n\t\t\thead:   chainHead,\n\t\t\tlimit:  256,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 0\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\n\t\t// The chain with specific range is supposed to be indexed\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 0, part of them need to be unindexed\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 64, one of them needs to be unindexed\n\t\t\ttail:         tailPointer(64),\n\t\t\texpIndexed:   65,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 65, all of them have been indexed\n\t\t\ttail:         tailPointer(65),\n\t\t\texpIndexed:   64,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 0 => index: [65, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 0,\n\n\t\t\t// tail = 66, one of them has to be indexed\n\t\t\ttail:         tailPointer(66),\n\t\t\texpIndexed:   63,\n\t\t\texpRemaining: 1,\n\t\t},\n\n\t\t// The chain with configured cutoff, the chain range could be capped\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 66 => index: [66, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 66,\n\n\t\t\t// tail = 0, part of them need to be unindexed\n\t\t\ttail:         tailPointer(0),\n\t\t\texpIndexed:   129,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 66 => index: [66, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 66,\n\n\t\t\t// tail = 66, all of them have been indexed\n\t\t\ttail:         tailPointer(66),\n\t\t\texpIndexed:   63,\n\t\t\texpRemaining: 0,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 66 => index: [66, 128]\n\t\t\thead:   chainHead,\n\t\t\tlimit:  64,\n\t\t\tcutoff: 66,\n\n\t\t\t// tail = 67, one of them has to be indexed\n\t\t\ttail:         tailPointer(67),\n\t\t\texpIndexed:   62,\n\t\t\texpRemaining: 1,\n\t\t},\n\t\t{\n\t\t\t// head = 128, limit = 64, cutoff = 256 => index: [66, 128]\n\t\t\thead:         chainHead,\n\t\t\tlimit:        0,\n\t\t\tcutoff:       256,\n\t\t\ttail:         nil,\n\t\t\texpIndexed:   0,\n\t\t\texpRemaining: 0,\n\t\t},\n\t}\n\tfor _, c := range cases {\n\t\tdb, _ := rawdb.Open(rawdb.NewMemoryDatabase(), rawdb.OpenOptions{})\n\t\tencReceipts := types.EncodeBlockReceiptLists(append([]types.Receipts{{}}, receipts...))\n\t\trawdb.WriteAncientBlocks(db, append([]*types.Block{gspec.ToBlock()}, blocks...), encReceipts, big.NewInt(0))\n\n\t\t// Index the initial blocks from ancient store\n\t\tindexer := &txIndexer{\n\t\t\tlimit:  c.limit,\n\t\t\tcutoff: c.cutoff,\n\t\t\tdb:     db,\n\t\t}\n\t\tp := indexer.report(c.head, c.tail)\n\t\tif p.Indexed != c.expIndexed {\n\t\t\tt.Fatalf(\"Unexpected indexed: %d, expected: %d\", p.Indexed, c.expIndexed)\n\t\t}\n\t\tif p.Remaining != c.expRemaining {\n\t\t\tt.Fatalf(\"Unexpected remaining: %d, expected: %d\", p.Remaining, c.expRemaining)\n\t\t}\n\t\tdb.Close()\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 228,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(t *testing.T)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 246,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 250,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(statedb, uint64(i)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 271,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(statedb *state.StateDB, number uint64, isVerkle bool)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 748,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "Balance[15] = 42",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.9600000000000001,
          "confidence": 0.9864,
          "ensemble_confidence": 0.8877600000000001
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/verkle_witness_test.go",
          "line": 962,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "Balance[15] = 42",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.9600000000000001,
          "confidence": 0.9864,
          "ensemble_confidence": 0.8877600000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/chain_makers_test.go",
          "line": 166,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\n\t\t// Verify parent beacon root.\n\t\twant := common.Hash{byte(blocknum)}\n\t\tif got := block.BeaconRoot()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethstats/ethstats.go",
          "line": 794,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(basefee.Uint64())\n\t\t}\n\t} else {\n\t\tsync := s.backend.SyncProgress(context.Background())\n\t\tsyncing = !sync.Done()\n\t}\n\t// Assemble the node stats and send it to the server\n\tlog.Trace(\"Sending node details to ethstats\")\n\n\tstats := map[string]interface{}{\n\t\t\"id\": s.node,\n\t\t\"stats\": &nodeStats{\n\t\t\tActive:   true,\n\t\t\tMining:   mining,\n\t\t\tPeers:    s.server.PeerCount(),\n\t\t\tGasPrice: gasprice,\n\t\t\tSyncing:  syncing,\n\t\t\tUptime:   100,\n\t\t},\n\t}\n\treport := map[string][]interface{}{\n\t\t\"emit\": {\"stats\", stats},\n\t}\n\treturn conn.WriteJSON(report)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/tests/state_test_util.go",
          "line": 494,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(n uint64)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8899999999999999,
          "confidence": 0.9801,
          "ensemble_confidence": 0.88209
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/tests/init_test.go",
          "line": 284,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/tests/state_test.go",
          "line": 324,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(time.Since(start))\n\t\t\t\trefund += state.StateDB.GetRefund()\n\t\t\t\tgasUsed += msg.GasLimit - leftOverGas\n\n\t\t\t\tstate.StateDB.RevertToSnapshot(snapshot)\n\t\t\t}\n\t\t\tif elapsed < 1 {\n\t\t\t\telapsed = 1\n\t\t\t}\n\t\t\t// Keep it as uint64, multiply 100 to get two digit float later\n\t\t\tmgasps := (100 * 1000 * (gasUsed - refund)) / elapsed\n\t\t\tb.ReportMetric(float64(mgasps)/100, \"mgas/s\")\n\t\t})\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/tests/state_test.go",
          "line": 317,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/preimages.go",
          "line": 55,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(common.HashLength + len(preimage))\n\t}\n}\n\n// preimage retrieves a cached trie node pre-image from memory. If it cannot be\n// found cached, the method queries the persistent database for the content.\nfunc (store *preimageStore) preimage(hash common.Hash) []byte {\n\tstore.lock.RLock()\n\tpreimage := store.preimages[hash]\n\tstore.lock.RUnlock()\n\n\tif preimage != nil {\n\t\treturn preimage\n\t}\n\treturn rawdb.ReadPreimage(store.disk, hash)\n}\n\n// commit flushes the cached preimages into the disk.\nfunc (store *preimageStore) commit(force bool) error {\n\tstore.lock.Lock()\n\tdefer store.lock.Unlock()\n\n\tif store.preimagesSize <= 4*1024*1024 && !force {\n\t\treturn nil\n\t}\n\tbatch := store.disk.NewBatch()\n\trawdb.WritePreimages(batch, store.preimages)\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rlp/decode_test.go",
          "line": 209,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rlp/encbuffer.go",
          "line": 71,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\t\tstrpos += n\n\t\t// write the header\n\t\tenc := head.encode(dst[pos:])\n\t\tpos += len(enc)\n\t}\n\t// copy string data after the last list header\n\tcopy(dst[pos:], buf.str[strpos:])\n}\n\n// writeTo writes the encoder output to w.\nfunc (buf *encBuffer) writeTo(w io.Writer) (err error) {\n\tstrpos := 0\n\tfor _, head := range buf.lheads {\n\t\t// write string data before header\n\t\tif head.offset-strpos > 0 {\n\t\t\tn, err := w.Write(buf.str[strpos:head.offset])\n\t\t\tstrpos += n\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t}\n\t\t// write the header\n\t\tenc := head.encode(buf.sizebuf[:])\n\t\tif _, err = w.Write(enc)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rlp/encbuffer.go",
          "line": 205,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 + intsize(uint64(lh.size))\n\t}\n}\n\nfunc (buf *encBuffer) encode(val interface{}) error {\n\trval := reflect.ValueOf(val)\n\twriter, err := cachedWriter(rval.Type())\n\tif err != nil {\n\t\treturn err\n\t}\n\treturn writer(rval, buf)\n}\n\nfunc (buf *encBuffer) encodeStringHeader(size int) {\n\tif size < 56 {\n\t\tbuf.str = append(buf.str, 0x80+byte(size))\n\t} else {\n\t\tsizesize := putint(buf.sizebuf[1:], uint64(size))\n\t\tbuf.sizebuf[0] = 0xB7 + byte(sizesize)\n\t\tbuf.str = append(buf.str, buf.sizebuf[:sizesize+1]...)\n\t}\n}\n\n// encReader is the io.Reader returned by EncodeToReader.\n// It releases its encbuf at EOF.\ntype encReader struct {\n\tbuf    *encBuffer // the buffer we're reading from. this is nil when we're at EOF.\n\tlhpos  int        // index of list header that we're reading\n\tstrpos int        // current position in string buffer\n\tpiece  []byte     // next piece to be read\n}\n\nfunc (r *encReader) Read(b []byte) (n int, err error) {\n\tfor {\n\t\tif r.piece = r.next()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/rlp/encbuffer.go",
          "line": 250,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nn\n\t\tif nn < len(r.piece) {\n\t\t\t// piece didn't fit, see you next time.\n\t\t\tr.piece = r.piece[nn:]\n\t\t\treturn n, nil\n\t\t}\n\t\tr.piece = nil\n\t}\n}\n\n// next returns the next piece of data to be read.\n// it returns nil at EOF.\nfunc (r *encReader) next() []byte {\n\tswitch {\n\tcase r.buf == nil:\n\t\treturn nil\n\n\tcase r.piece != nil:\n\t\t// There is still data available for reading.\n\t\treturn r.piece\n\n\tcase r.lhpos < len(r.buf.lheads):\n\t\t// We're before the last list header.\n\t\thead := r.buf.lheads[r.lhpos]\n\t\tsizebefore := head.offset - r.strpos\n\t\tif sizebefore > 0 {\n\t\t\t// String data before header.\n\t\t\tp := r.buf.str[r.strpos:head.offset]\n\t\t\tr.strpos += sizebefore\n\t\t\treturn p\n\t\t}\n\t\tr.lhpos++\n\t\treturn head.encode(r.buf.sizebuf[:])\n\n\tcase r.strpos < len(r.buf.str):\n\t\t// String data at the end, after all list headers.\n\t\tp := r.buf.str[r.strpos:]\n\t\tr.strpos = len(r.buf.str)\n\t\treturn p\n\n\tdefault:\n\t\treturn nil\n\t}\n}\n\nfunc encBufferFromWriter(w io.Writer) *encBuffer {\n\tswitch w := w.(type) {\n\tcase EncoderBuffer:\n\t\treturn w.buf\n\tcase *EncoderBuffer:\n\t\treturn w.buf\n\tcase *encBuffer:\n\t\treturn w\n\tdefault:\n\t\treturn nil\n\t}\n}\n\n// EncoderBuffer is a buffer for incremental encoding.\n//\n// The zero value is NOT ready for use. To get a usable buffer,\n// create it using NewEncoderBuffer or call Reset.\ntype EncoderBuffer struct {\n\tbuf *encBuffer\n\tdst io.Writer\n\n\townBuffer bool\n}\n\n// NewEncoderBuffer creates an encoder buffer.\nfunc NewEncoderBuffer(dst io.Writer) EncoderBuffer {\n\tvar w EncoderBuffer\n\tw.Reset(dst)\n\treturn w\n}\n\n// Reset truncates the buffer and sets the output destination.\nfunc (w *EncoderBuffer) Reset(dst io.Writer) {\n\tif w.buf != nil && !w.ownBuffer {\n\t\tpanic(\"can't Reset derived EncoderBuffer\")\n\t}\n\n\t// If the destination writer has an *encBuffer, use it.\n\t// Note that w.ownBuffer is left false here.\n\tif dst != nil {\n\t\tif outer := encBufferFromWriter(dst)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rlp/decode.go",
          "line": 119,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= err.ctx[i]\n\t\t}\n\t}\n\treturn fmt.Sprintf(\"rlp: %s for %v%s\", err.msg, err.typ, ctx)\n}\n\nfunc wrapStreamError(err error, typ reflect.Type) error {\n\tswitch err {\n\tcase ErrCanonInt:\n\t\treturn &decodeError{msg: \"non-canonical integer (leading zero bytes)\", typ: typ}\n\tcase ErrCanonSize:\n\t\treturn &decodeError{msg: \"non-canonical size information\", typ: typ}\n\tcase ErrExpectedList:\n\t\treturn &decodeError{msg: \"expected input list\", typ: typ}\n\tcase ErrExpectedString:\n\t\treturn &decodeError{msg: \"expected input string or byte\", typ: typ}\n\tcase errUintOverflow:\n\t\treturn &decodeError{msg: \"input string too long\", typ: typ}\n\tcase errNotAtEOL:\n\t\treturn &decodeError{msg: \"input list has too many elements\", typ: typ}\n\t}\n\treturn err\n}\n\nfunc addErrorContext(err error, ctx string) error {\n\tif decErr, ok := err.(*decodeError)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rlp/decode.go",
          "line": 1127,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nn\n\t}\n\tif err == io.EOF {\n\t\tif n < len(buf) {\n\t\t\terr = io.ErrUnexpectedEOF\n\t\t} else {\n\t\t\t// Readers are allowed to give EOF even though the read succeeded.\n\t\t\t// In such cases, we discard the EOF, like io.ReadFull() does.\n\t\t\terr = nil\n\t\t}\n\t}\n\treturn err\n}\n\n// readByte reads a single byte from the underlying stream.\nfunc (s *Stream) readByte() (byte, error) {\n\tif err := s.willRead(1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/rlp/decode.go",
          "line": 1168,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= n\n\t}\n\treturn nil\n}\n\n// listLimit returns the amount of data remaining in the innermost list.\nfunc (s *Stream) listLimit() (inList bool, limit uint64) {\n\tif len(s.stack) == 0 {\n\t\treturn false, 0\n\t}\n\treturn true, s.stack[len(s.stack)-1]\n}\n\ntype sliceReader []byte\n\nfunc (sr *sliceReader) Read(b []byte) (int, error) {\n\tif len(*sr) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tn := copy(b, *sr)\n\t*sr = (*sr)[n:]\n\treturn n, nil\n}\n\nfunc (sr *sliceReader) ReadByte() (byte, error) {\n\tif len(*sr) == 0 {\n\t\treturn 0, io.EOF\n\t}\n\tb := (*sr)[0]\n\t*sr = (*sr)[1:]\n\treturn b, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rlp/encode_test.go",
          "line": 473,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= remaining\n\t\t\t} else {\n\t\t\t\tend = start + 3\n\t\t\t}\n\t\t\tn, err := r.Read(output[start:end])\n\t\t\tend = start + n\n\t\t\tif err == io.EOF {\n\t\t\t\tbreak\n\t\t\t} else if err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t}\n\t\treturn output, nil\n\t})\n}\n\n// This is a regression test verifying that encReader\n// returns its encbuf to the pool only once.\nfunc TestEncodeToReaderReturnToPool(t *testing.T) {\n\tbuf := make([]byte, 50)\n\twg := new(sync.WaitGroup)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/types.go",
          "line": 276,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n\t\t}\n\t}\n\treturn buf[:]\n}\n\nfunc (a Address) hex() []byte {\n\tvar buf [len(a)*2 + 2]byte\n\tcopy(buf[:2], \"0x\")\n\thex.Encode(buf[2:], a[:])\n\treturn buf[:]\n}\n\n// Format implements fmt.Formatter.\n// Address supports the %v, %s, %q, %x, %X and %d format verbs.\nfunc (a Address) Format(s fmt.State, c rune) {\n\tswitch c {\n\tcase 'v', 's':\n\t\ts.Write(a.checksumHex())\n\tcase 'q':\n\t\tq := []byte{'\"'}\n\t\ts.Write(q)\n\t\ts.Write(a.checksumHex())\n\t\ts.Write(q)\n\tcase 'x', 'X':\n\t\t// %x disables the checksum.\n\t\thex := a.hex()\n\t\tif !s.Flag('#') {\n\t\t\thex = hex[2:]\n\t\t}\n\t\tif c == 'X' {\n\t\t\thex = bytes.ToUpper(hex)\n\t\t}\n\t\ts.Write(hex)\n\tcase 'd':\n\t\tfmt.Fprint(s, ([len(a)]byte)(a))\n\tdefault:\n\t\tfmt.Fprintf(s, \"%%!%c(address=%x)\", c, a)\n\t}\n}\n\n// SetBytes sets the address to the value of b.\n// If b is larger than len(a), b will be cropped from the left.\nfunc (a *Address) SetBytes(b []byte) {\n\tif len(b) > len(a) {\n\t\tb = b[len(b)-AddressLength:]\n\t}\n\tcopy(a[AddressLength-len(b):], b)\n}\n\n// MarshalText returns the hex representation of a.\nfunc (a Address) MarshalText() ([]byte, error) {\n\treturn hexutil.Bytes(a[:]).MarshalText()\n}\n\n// UnmarshalText parses a hash in hex syntax.\nfunc (a *Address) UnmarshalText(input []byte) error {\n\treturn hexutil.UnmarshalFixedText(\"Address\", input, a[:])\n}\n\n// UnmarshalJSON parses a hash in hex syntax.\nfunc (a *Address) UnmarshalJSON(input []byte) error {\n\treturn hexutil.UnmarshalFixedJSON(addressT, input, a[:])\n}\n\n// Scan implements Scanner for database/sql.\nfunc (a *Address) Scan(src interface{}) error {\n\tsrcB, ok := src.([]byte)\n\tif !ok {\n\t\treturn fmt.Errorf(\"can't scan %T into Address\", src)\n\t}\n\tif len(srcB) != AddressLength {\n\t\treturn fmt.Errorf(\"can't scan []byte of len %d into Address, want %d\", len(srcB), AddressLength)\n\t}\n\tcopy(a[:], srcB)\n\treturn nil\n}\n\n// Value implements valuer for database/sql.\nfunc (a Address) Value() (driver.Value, error) {\n\treturn a[:], nil\n}\n\n// ImplementsGraphQLType returns true if Hash implements the specified GraphQL type.\nfunc (a Address) ImplementsGraphQLType(name string) bool { return name == \"Address\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (a *Address) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\terr = a.UnmarshalText([]byte(input))\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for Address\", input)\n\t}\n\treturn err\n}\n\n// UnprefixedAddress allows marshaling an Address without 0x prefix.\ntype UnprefixedAddress Address\n\n// UnmarshalText decodes the address from hex. The 0x prefix is optional.\nfunc (a *UnprefixedAddress) UnmarshalText(input []byte) error {\n\treturn hexutil.UnmarshalFixedUnprefixedText(\"UnprefixedAddress\", input, a[:])\n}\n\n// MarshalText encodes the address as hex.\nfunc (a UnprefixedAddress) MarshalText() ([]byte, error) {\n\treturn []byte(hex.EncodeToString(a[:])), nil\n}\n\n// MixedcaseAddress retains the original string, which may or may not be\n// correctly checksummed\ntype MixedcaseAddress struct {\n\taddr     Address\n\toriginal string\n}\n\n// NewMixedcaseAddress constructor (mainly for testing)\nfunc NewMixedcaseAddress(addr Address) MixedcaseAddress {\n\treturn MixedcaseAddress{addr: addr, original: addr.Hex()}\n}\n\n// NewMixedcaseAddressFromString is mainly meant for unit-testing\nfunc NewMixedcaseAddressFromString(hexaddr string) (*MixedcaseAddress, error) {\n\tif !IsHexAddress(hexaddr) {\n\t\treturn nil, errors.New(\"invalid address\")\n\t}\n\ta := FromHex(hexaddr)\n\treturn &MixedcaseAddress{addr: BytesToAddress(a), original: hexaddr}, nil\n}\n\n// UnmarshalJSON parses MixedcaseAddress\nfunc (ma *MixedcaseAddress) UnmarshalJSON(input []byte) error {\n\tif err := hexutil.UnmarshalFixedJSON(addressT, input, ma.addr[:])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/format.go",
          "line": 76,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/stacktrie_test.go",
          "line": 417,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1024\n\t\t}\n\t\tif hash == (common.Hash{}) {\n\t\t\thash = s.Hash()\n\t\t} else {\n\t\t\tif hash != s.Hash() && false {\n\t\t\t\tb.Fatalf(\"hash wrong, have %x want %x\", s.Hash(), hash)\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc TestInsert100K(t *testing.T) {\n\tvar num = 100_000\n\tvar key = make([]byte, 8)\n\tvar val = make([]byte, 20)\n\ts := NewStackTrie(nil)\n\tvar k uint64\n\tfor j := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/trie/stacktrie_test.go",
          "line": 440,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1024\n\t}\n\twant := common.HexToHash(\"0xb0071bd257342925d9d8a9f002b9d2b646a35437aa8b089628ab56e428d29a1a\")\n\tif have := s.Hash()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/verkle_test.go",
          "line": 101,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\t\tcode[i] = 0x60\n\t\t\tcode[i+1] = byte(i % 256)\n\t\t}\n\t\tif err := tr.UpdateAccount(addr, acct, len(code))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/stacktrie_fuzzer_test.go",
          "line": 139,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tif checked != len(nodeset) {\n\t\tpanic(\"node number is not matched\")\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/sync.go",
          "line": 217,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength + uint64(len(code))\n}\n\n// addNode caches a node database write operation.\nfunc (batch *syncMemBatch) addNode(owner common.Hash, path []byte, blob []byte, hash common.Hash) {\n\tif batch.scheme == rawdb.PathScheme {\n\t\tif owner == (common.Hash{}) {\n\t\t\tbatch.size += uint64(len(path) + len(blob))\n\t\t} else {\n\t\t\tbatch.size += common.HashLength + uint64(len(path)+len(blob))\n\t\t}\n\t} else {\n\t\tbatch.size += common.HashLength + uint64(len(blob))\n\t}\n\tbatch.nodes = append(batch.nodes, nodeOp{\n\t\towner: owner,\n\t\tpath:  path,\n\t\tblob:  blob,\n\t\thash:  hash,\n\t})\n}\n\n// delNode caches a node database delete operation.\nfunc (batch *syncMemBatch) delNode(owner common.Hash, path []byte) {\n\tif batch.scheme != rawdb.PathScheme {\n\t\tlog.Error(\"Unexpected node deletion\", \"owner\", owner, \"path\", path, \"scheme\", batch.scheme)\n\t\treturn // deletion is not supported in hash mode.\n\t}\n\tif owner == (common.Hash{}) {\n\t\tbatch.size += uint64(len(path))\n\t} else {\n\t\tbatch.size += common.HashLength + uint64(len(path))\n\t}\n\tbatch.nodes = append(batch.nodes, nodeOp{\n\t\tdel:   true,\n\t\towner: owner,\n\t\tpath:  path,\n\t})\n}\n\n// Sync is the main state trie synchronisation scheduler, which provides yet\n// unknown trie hashes to retrieve, accepts node data associated with said hashes\n// and reconstructs the trie step by step until all is done.\ntype Sync struct {\n\tscheme   string                       // Node scheme descriptor used in database.\n\tdatabase ethdb.Database               // Persistent database to check for existing entries\n\tmembatch *syncMemBatch                // Memory buffer to avoid frequent database writes\n\tnodeReqs map[string]*nodeRequest      // Pending requests pertaining to a trie node path\n\tcodeReqs map[common.Hash]*codeRequest // Pending requests pertaining to a code hash\n\tqueue    *prque.Prque[int64, any]     // Priority queue with the pending requests\n\tfetches  map[int]int                  // Number of active fetches per trie node depth\n}\n\n// NewSync creates a new trie data download scheduler.\nfunc NewSync(root common.Hash, database ethdb.Database, callback LeafCallback, scheme string) *Sync {\n\tts := &Sync{\n\t\tscheme:   scheme,\n\t\tdatabase: database,\n\t\tmembatch: newSyncMemBatch(scheme),\n\t\tnodeReqs: make(map[string]*nodeRequest),\n\t\tcodeReqs: make(map[common.Hash]*codeRequest),\n\t\tqueue:    prque.New[int64, any](nil), // Ugh, can contain both string and hash, whyyy\n\t\tfetches:  make(map[int]int),\n\t}\n\tts.AddSubTrie(root, nil, common.Hash{}, nil, callback)\n\treturn ts\n}\n\n// AddSubTrie registers a new trie to the sync code, rooted at the designated\n// parent for completion tracking. The given path is a unique node path in\n// hex format and contain all the parent path if it's layered trie node.\nfunc (s *Sync) AddSubTrie(root common.Hash, path []byte, parent common.Hash, parentPath []byte, callback LeafCallback) {\n\tif root == types.EmptyRootHash {\n\t\treturn\n\t}\n\towner, inner := ResolvePath(path)\n\texist, inconsistent := s.hasNode(owner, inner, root)\n\tif exist {\n\t\t// The entire subtrie is already present in the database.\n\t\treturn\n\t} else if inconsistent {\n\t\t// There is a pre-existing node with the wrong hash in DB, remove it.\n\t\ts.membatch.delNode(owner, inner)\n\t}\n\t// Assemble the new sub-trie sync request\n\treq := &nodeRequest{\n\t\thash:     root,\n\t\tpath:     path,\n\t\tcallback: callback,\n\t}\n\t// If this sub-trie has a designated parent, link them together\n\tif parent != (common.Hash{}) {\n\t\tancestor := s.nodeReqs[string(parentPath)]\n\t\tif ancestor == nil {\n\t\t\tpanic(fmt.Sprintf(\"sub-trie ancestor not found: %x\", parent))\n\t\t}\n\t\tancestor.deps++\n\t\treq.parent = ancestor\n\t}\n\ts.scheduleNodeRequest(req)\n}\n\n// AddCodeEntry schedules the direct retrieval of a contract code that should not\n// be interpreted as a trie node, but rather accepted and stored into the database\n// as is.\nfunc (s *Sync) AddCodeEntry(hash common.Hash, path []byte, parent common.Hash, parentPath []byte) {\n\t// Short circuit if the entry is empty or already known\n\tif hash == types.EmptyCodeHash {\n\t\treturn\n\t}\n\tif s.membatch.hasCode(hash) {\n\t\treturn\n\t}\n\t// If database says duplicate, the blob is present for sure.\n\t// Note we only check the existence with new code scheme, snap\n\t// sync is expected to run with a fresh new node. Even there\n\t// exists the code with legacy format, fetch and store with\n\t// new scheme anyway.\n\tif rawdb.HasCodeWithPrefix(s.database, hash) {\n\t\treturn\n\t}\n\t// Assemble the new sub-trie sync request\n\treq := &codeRequest{\n\t\tpath: path,\n\t\thash: hash,\n\t}\n\t// If this sub-trie has a designated parent, link them together\n\tif parent != (common.Hash{}) {\n\t\tancestor := s.nodeReqs[string(parentPath)] // the parent of codereq can ONLY be nodereq\n\t\tif ancestor == nil {\n\t\t\tpanic(fmt.Sprintf(\"raw-entry ancestor not found: %x\", parent))\n\t\t}\n\t\tancestor.deps++\n\t\treq.parents = append(req.parents, ancestor)\n\t}\n\ts.scheduleCodeRequest(req)\n}\n\n// Missing retrieves the known missing nodes from the trie for retrieval. To aid\n// both eth/6x style fast sync and snap/1x style state sync, the paths of trie\n// nodes are returned too, as well as separate hash list for codes.\nfunc (s *Sync) Missing(max int) ([]string, []common.Hash, []common.Hash) {\n\tvar (\n\t\tnodePaths  []string\n\t\tnodeHashes []common.Hash\n\t\tcodeHashes []common.Hash\n\t)\n\tfor !s.queue.Empty() && (max == 0 || len(nodeHashes)+len(codeHashes) < max) {\n\t\t// Retrieve the next item in line\n\t\titem, prio := s.queue.Peek()\n\n\t\t// If we have too many already-pending tasks for this depth, throttle\n\t\tdepth := int(prio >> 56)\n\t\tif s.fetches[depth] > maxFetchesPerDepth {\n\t\t\tbreak\n\t\t}\n\t\t// Item is allowed to be scheduled, add it to the task list\n\t\ts.queue.Pop()\n\t\ts.fetches[depth]++\n\n\t\tswitch item := item.(type) {\n\t\tcase common.Hash:\n\t\t\tcodeHashes = append(codeHashes, item)\n\t\tcase string:\n\t\t\treq, ok := s.nodeReqs[item]\n\t\t\tif !ok {\n\t\t\t\tlog.Error(\"Missing node request\", \"path\", item)\n\t\t\t\tcontinue // System very wrong, shouldn't happen\n\t\t\t}\n\t\t\tnodePaths = append(nodePaths, item)\n\t\t\tnodeHashes = append(nodeHashes, req.hash)\n\t\t}\n\t}\n\treturn nodePaths, nodeHashes, codeHashes\n}\n\n// ProcessCode injects the received data for requested item. Note it can\n// happen that the single response commits two pending requests(e.g.\n// there are two requests one for code and one for node but the hash\n// is same). In this case the second response for the same hash will\n// be treated as \"non-requested\" item or \"already-processed\" item but\n// there is no downside.\nfunc (s *Sync) ProcessCode(result CodeSyncResult) error {\n\t// If the code was not requested or it's already processed, bail out\n\treq := s.codeReqs[result.Hash]\n\tif req == nil {\n\t\treturn ErrNotRequested\n\t}\n\tif req.data != nil {\n\t\treturn ErrAlreadyProcessed\n\t}\n\treq.data = result.Data\n\treturn s.commitCodeRequest(req)\n}\n\n// ProcessNode injects the received data for requested item. Note it can\n// happen that the single response commits two pending requests(e.g.\n// there are two requests one for code and one for node but the hash\n// is same). In this case the second response for the same hash will\n// be treated as \"non-requested\" item or \"already-processed\" item but\n// there is no downside.\nfunc (s *Sync) ProcessNode(result NodeSyncResult) error {\n\t// If the trie node was not requested or it's already processed, bail out\n\treq := s.nodeReqs[result.Path]\n\tif req == nil {\n\t\treturn ErrNotRequested\n\t}\n\tif req.data != nil {\n\t\treturn ErrAlreadyProcessed\n\t}\n\t// Decode the node data content and update the request\n\tnode, err := decodeNode(req.hash.Bytes(), result.Data)\n\tif err != nil {\n\t\treturn err\n\t}\n\treq.data = result.Data\n\n\t// Create and schedule a request for all the children nodes\n\trequests, err := s.children(req, node)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif len(requests) == 0 && req.deps == 0 {\n\t\ts.commitNodeRequest(req)\n\t} else {\n\t\treq.deps += len(requests)\n\t\tfor _, child := range requests {\n\t\t\ts.scheduleNodeRequest(child)\n\t\t}\n\t}\n\treturn nil\n}\n\n// Commit flushes the data stored in the internal membatch out to persistent\n// storage, returning any occurred error. The whole data set will be flushed\n// in an atomic database batch.\nfunc (s *Sync) Commit(dbw ethdb.Batch, stateBatch ethdb.Batch) error {\n\t// Flush the pending node writes into database batch.\n\tvar (\n\t\taccount int\n\t\tstorage int\n\t)\n\tfor _, op := range s.membatch.nodes {\n\t\tif !op.valid() {\n\t\t\treturn fmt.Errorf(\"invalid op, %s\", op.string())\n\t\t}\n\t\tif op.del {\n\t\t\t// node deletion is only supported in path mode.\n\t\t\tif op.owner == (common.Hash{}) {\n\t\t\t\tif stateBatch != nil {\n\t\t\t\t\trawdb.DeleteAccountTrieNode(stateBatch, op.path)\n\t\t\t\t} else {\n\t\t\t\t\trawdb.DeleteAccountTrieNode(dbw, op.path)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\tif stateBatch != nil {\n\t\t\t\t\trawdb.DeleteStorageTrieNode(stateBatch, op.owner, op.path)\n\t\t\t\t} else {\n\t\t\t\t\trawdb.DeleteStorageTrieNode(dbw, op.owner, op.path)\n\t\t\t\t}\n\t\t\t}\n\t\t\tdeletionGauge.Inc(1)\n\t\t} else {\n\t\t\tif op.owner == (common.Hash{}) {\n\t\t\t\taccount += 1\n\t\t\t} else {\n\t\t\t\tstorage += 1\n\t\t\t}\n\t\t\tif stateBatch != nil {\n\t\t\t\trawdb.WriteTrieNode(stateBatch, op.owner, op.path, op.hash, op.blob, s.scheme)\n\t\t\t} else {\n\t\t\t\trawdb.WriteTrieNode(dbw, op.owner, op.path, op.hash, op.blob, s.scheme)\n\t\t\t}\n\t\t}\n\t}\n\taccountNodeSyncedGauge.Inc(int64(account))\n\tstorageNodeSyncedGauge.Inc(int64(storage))\n\n\t// Flush the pending code writes into database batch.\n\tfor hash, value := range s.membatch.codes {\n\t\trawdb.WriteCode(dbw, hash, value)\n\t}\n\tcodeSyncedGauge.Inc(int64(len(s.membatch.codes)))\n\n\ts.membatch = newSyncMemBatch(s.scheme) // reset the batch\n\treturn nil\n}\n\n// MemSize returns an estimated size (in bytes) of the data held in the membatch.\nfunc (s *Sync) MemSize() uint64 {\n\treturn s.membatch.size\n}\n\n// Pending returns the number of state entries currently pending for download.\nfunc (s *Sync) Pending() int {\n\treturn len(s.nodeReqs) + len(s.codeReqs)\n}\n\n// scheduleNodeRequest inserts a new state retrieval request into the fetch queue. If there\n// is already a pending request for this node, the new request will be discarded\n// and only a parent reference added to the old one.\nfunc (s *Sync) scheduleNodeRequest(req *nodeRequest) {\n\ts.nodeReqs[string(req.path)] = req\n\n\t// Schedule the request for future retrieval. This queue is shared\n\t// by both node requests and code requests.\n\tprio := int64(len(req.path)) << 56 // depth >= 128 will never happen, storage leaves will be included in their parents\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/iterator_test.go",
          "line": 169,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tif elem, ok := elements[crypto.Keccak256Hash(it.Value())]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/trie/iterator_test.go",
          "line": 594,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tif count != len(found) {\n\t\tt.Fatal(\"Find extra trie node via iterator\")\n\t}\n}\n\n// isTrieNode is a helper function which reports if the provided\n// database entry belongs to a trie node or not. Note in tests\n// only single layer trie is used, namely storage trie is not\n// considered at all.\nfunc isTrieNode(scheme string, key, val []byte) (bool, []byte, common.Hash) {\n\tvar (\n\t\tpath []byte\n\t\thash common.Hash\n\t)\n\tif scheme == rawdb.HashScheme {\n\t\tok := rawdb.IsLegacyTrieNode(key, val)\n\t\tif !ok {\n\t\t\treturn false, nil, common.Hash{}\n\t\t}\n\t\thash = common.BytesToHash(key)\n\t} else {\n\t\tok, remain := rawdb.ResolveAccountTrieNodeKey(key)\n\t\tif !ok {\n\t\t\treturn false, nil, common.Hash{}\n\t\t}\n\t\tpath = common.CopyBytes(remain)\n\t\thash = crypto.Keccak256Hash(val)\n\t}\n\treturn true, path, hash\n}\n\nfunc BenchmarkIterator(b *testing.B) {\n\tdiskDb, srcDb, tr, _ := makeTestTrie(rawdb.HashScheme)\n\troot := tr.Hash()\n\tb.ReportAllocs()\n\tb.ResetTimer()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/trie_test.go",
          "line": 1301,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 100\n\t\t}\n\t\tt.Fatalf(\"have != want\\nhave %q\\nwant %q\", have[i:], want[i:])\n\t}\n}\nfunc printSet(set *trienode.NodeSet) string {\n\tvar out = new(strings.Builder)\n\tfmt.Fprintf(out, \"nodeset owner: %v\\n\", set.Owner)\n\tvar paths []string\n\tfor k := range set.Nodes {\n\t\tpaths = append(paths, k)\n\t}\n\tsort.Strings(paths)\n\n\tfor _, path := range paths {\n\t\tn := set.Nodes[path]\n\t\t// Deletion\n\t\tif n.IsDeleted() {\n\t\t\tfmt.Fprintf(out, \"  [-]: %x\\n\", path)\n\t\t\tcontinue\n\t\t}\n\t\t// Insertion or update\n\t\tfmt.Fprintf(out, \"  [+/*]: %x -> %v \\n\", path, n.Hash)\n\t}\n\tsort.Slice(set.Leaves, func(i, j int) bool {\n\t\ta := set.Leaves[i]\n\t\tb := set.Leaves[j]\n\t\treturn bytes.Compare(a.Parent[:], b.Parent[:]) < 0\n\t})\n\tfor _, n := range set.Leaves {\n\t\tfmt.Fprintf(out, \"[leaf]: %v\\n\", n)\n\t}\n\treturn out.String()\n}\n\nfunc TestTrieCopy(t *testing.T) {\n\ttestTrieCopy(t, []kv{\n\t\t{k: []byte(\"do\"), v: []byte(\"verb\")},\n\t\t{k: []byte(\"ether\"), v: []byte(\"wookiedoo\")},\n\t\t{k: []byte(\"horse\"), v: []byte(\"stallion\")},\n\t\t{k: []byte(\"shaman\"), v: []byte(\"horse\")},\n\t\t{k: []byte(\"doge\"), v: []byte(\"coin\")},\n\t\t{k: []byte(\"dog\"), v: []byte(\"puppy\")},\n\t})\n\n\tvar entries []kv\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/node.go",
          "line": 110,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"%s: <nil> \", indices[i])\n\t\t} else {\n\t\t\tresp += fmt.Sprintf(\"%s: %v\", indices[i], node.fstring(ind+\"  \"))\n\t\t}\n\t}\n\treturn resp + fmt.Sprintf(\"\\n%s] \", ind)\n}\n\nfunc (n *shortNode) fstring(ind string) string {\n\treturn fmt.Sprintf(\"{%x: %v} \", n.Key, n.Val.fstring(ind+\"  \"))\n}\nfunc (n hashNode) fstring(ind string) string {\n\treturn fmt.Sprintf(\"<%x> \", []byte(n))\n}\nfunc (n valueNode) fstring(ind string) string {\n\treturn fmt.Sprintf(\"%x \", []byte(n))\n}\n\nfunc NodeString(hash, buf []byte) string {\n\tnode := mustDecodeNode(hash, buf)\n\treturn node.fstring(\"NodeString: \")\n}\n\n// mustDecodeNode is a wrapper of decodeNode and panic if any error is encountered.\nfunc mustDecodeNode(hash, buf []byte) node {\n\tn, err := decodeNode(hash, buf)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"node %x: %v\", hash, err))\n\t}\n\treturn n\n}\n\n// mustDecodeNodeUnsafe is a wrapper of decodeNodeUnsafe and panic if any error is\n// encountered.\nfunc mustDecodeNodeUnsafe(hash, buf []byte) node {\n\tn, err := decodeNodeUnsafe(hash, buf)\n\tif err != nil {\n\t\tpanic(fmt.Sprintf(\"node %x: %v\", hash, err))\n\t}\n\treturn n\n}\n\n// decodeNode parses the RLP encoding of a trie node. It will deep-copy the passed\n// byte slice for decoding, so it's safe to modify the byte slice afterwards. The-\n// decode performance of this function is not optimal, but it is suitable for most\n// scenarios with low performance requirements and hard to determine whether the\n// byte slice be modified or not.\nfunc decodeNode(hash, buf []byte) (node, error) {\n\treturn decodeNodeUnsafe(hash, common.CopyBytes(buf))\n}\n\n// decodeNodeUnsafe parses the RLP encoding of a trie node. The passed byte slice\n// will be directly referenced by node without bytes deep copy, so the input MUST\n// not be changed after.\nfunc decodeNodeUnsafe(hash, buf []byte) (node, error) {\n\tif len(buf) == 0 {\n\t\treturn nil, io.ErrUnexpectedEOF\n\t}\n\telems, _, err := rlp.SplitList(buf)\n\tif err != nil {\n\t\treturn nil, fmt.Errorf(\"decode error: %v\", err)\n\t}\n\tswitch c, _ := rlp.CountValues(elems)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/verkle.go",
          "line": 374,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int(code[codeOffset] - PUSH1 + 1)\n\t\t\t\tif codeOffset+1 >= 31*(i+1) {\n\t\t\t\t\tcodeOffset++\n\t\t\t\t\tchunkOffset = codeOffset - 31*(i+1)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn chunks\n}\n\n// UpdateContractCode implements state.Trie, writing the provided contract code\n// into the trie.\n// Note that the code-size *must* be already saved by a previous UpdateAccount call.\nfunc (t *VerkleTrie) UpdateContractCode(addr common.Address, codeHash common.Hash, code []byte) error {\n\tvar (\n\t\tchunks = ChunkifyCode(code)\n\t\tvalues [][]byte\n\t\tkey    []byte\n\t\terr    error\n\t)\n\tfor i, chunknr := 0, uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/hd.go",
          "line": 112,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint32(bigval.Uint64())\n\n\t\t// Append and repeat\n\t\tresult = append(result, value)\n\t}\n\treturn result, nil\n}\n\n// String implements the stringer interface, converting a binary derivation path\n// to its canonical representation.\nfunc (path DerivationPath) String() string {\n\tresult := \"m\"\n\tfor _, component := range path {\n\t\tvar hardened bool\n\t\tif component >= 0x80000000 {\n\t\t\tcomponent -= 0x80000000\n\t\t\thardened = true\n\t\t}\n\t\tresult = fmt.Sprintf(\"%s/%d\", result, component)\n\t\tif hardened {\n\t\t\tresult += \"'\"\n\t\t}\n\t}\n\treturn result\n}\n\n// MarshalJSON turns a derivation path into its json-serialized string\nfunc (path DerivationPath) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(path.String())\n}\n\n// UnmarshalJSON a json-serialized string back into a derivation path\nfunc (path *DerivationPath) UnmarshalJSON(b []byte) error {\n\tvar dp string\n\tvar err error\n\tif err = json.Unmarshal(b, &dp)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/hd.go",
          "line": 127,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 0x80000000\n\t\t\thardened = true\n\t\t}\n\t\tresult = fmt.Sprintf(\"%s/%d\", result, component)\n\t\tif hardened {\n\t\t\tresult += \"'\"\n\t\t}\n\t}\n\treturn result\n}\n\n// MarshalJSON turns a derivation path into its json-serialized string\nfunc (path DerivationPath) MarshalJSON() ([]byte, error) {\n\treturn json.Marshal(path.String())\n}\n\n// UnmarshalJSON a json-serialized string back into a derivation path\nfunc (path *DerivationPath) UnmarshalJSON(b []byte) error {\n\tvar dp string\n\tvar err error\n\tif err = json.Unmarshal(b, &dp)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/manager.go",
          "line": 147,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/params/config.go",
          "line": 733,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"Chain ID:  %v (%s)\\n\", c.ChainID, network)\n\tswitch {\n\tcase c.Parlia != nil:\n\t\tbanner += \"Consensus: Parlia (proof-of-staked--authority)\\n\"\n\tcase c.Ethash != nil:\n\t\tbanner += \"Consensus: Beacon (proof-of-stake), merged from Ethash (proof-of-work)\\n\"\n\tcase c.Clique != nil:\n\t\tbanner += \"Consensus: Beacon (proof-of-stake), merged from Clique (proof-of-authority)\\n\"\n\tdefault:\n\t\tbanner += \"Consensus: unknown\\n\"\n\t}\n\n\treturn banner\n}\n\n// String implements the fmt.Stringer interface.\nfunc (c *ChainConfig) String() string {\n\tvar engine interface{}\n\n\tswitch {\n\tcase c.Ethash != nil:\n\t\tengine = c.Ethash\n\tcase c.Clique != nil:\n\t\tengine = c.Clique\n\tcase c.Parlia != nil:\n\t\tengine = c.Parlia\n\tdefault:\n\t\tengine = \"unknown\"\n\t}\n\n\tvar ShanghaiTime *big.Int\n\tif c.ShanghaiTime != nil {\n\t\tShanghaiTime = big.NewInt(0).SetUint64(*c.ShanghaiTime)\n\t}\n\n\tvar KeplerTime *big.Int\n\tif c.KeplerTime != nil {\n\t\tKeplerTime = big.NewInt(0).SetUint64(*c.KeplerTime)\n\t}\n\n\tvar FeynmanTime *big.Int\n\tif c.FeynmanTime != nil {\n\t\tFeynmanTime = big.NewInt(0).SetUint64(*c.FeynmanTime)\n\t}\n\n\tvar FeynmanFixTime *big.Int\n\tif c.FeynmanFixTime != nil {\n\t\tFeynmanFixTime = big.NewInt(0).SetUint64(*c.FeynmanFixTime)\n\t}\n\n\tvar CancunTime *big.Int\n\tif c.CancunTime != nil {\n\t\tCancunTime = big.NewInt(0).SetUint64(*c.CancunTime)\n\t}\n\n\tvar HaberTime *big.Int\n\tif c.HaberTime != nil {\n\t\tHaberTime = big.NewInt(0).SetUint64(*c.HaberTime)\n\t}\n\n\tvar HaberFixTime *big.Int\n\tif c.HaberFixTime != nil {\n\t\tHaberFixTime = big.NewInt(0).SetUint64(*c.HaberFixTime)\n\t}\n\n\tvar BohrTime *big.Int\n\tif c.BohrTime != nil {\n\t\tBohrTime = big.NewInt(0).SetUint64(*c.BohrTime)\n\t}\n\n\tvar PascalTime *big.Int\n\tif c.PascalTime != nil {\n\t\tPascalTime = big.NewInt(0).SetUint64(*c.PascalTime)\n\t}\n\n\tvar PragueTime *big.Int\n\tif c.PragueTime != nil {\n\t\tPragueTime = big.NewInt(0).SetUint64(*c.PragueTime)\n\t}\n\n\tvar LorentzTime *big.Int\n\tif c.LorentzTime != nil {\n\t\tLorentzTime = big.NewInt(0).SetUint64(*c.LorentzTime)\n\t}\n\n\tvar MaxwellTime *big.Int\n\tif c.MaxwellTime != nil {\n\t\tMaxwellTime = big.NewInt(0).SetUint64(*c.MaxwellTime)\n\t}\n\n\tvar FermiTime *big.Int\n\tif c.FermiTime != nil {\n\t\tFermiTime = big.NewInt(0).SetUint64(*c.FermiTime)\n\t}\n\n\treturn fmt.Sprintf(\"{ChainID: %v, Engine: %v, Homestead: %v DAO: %v DAOSupport: %v EIP150: %v EIP155: %v EIP158: %v Byzantium: %v Constantinople: %v Petersburg: %v Istanbul: %v, Muir Glacier: %v, Ramanujan: %v, Niels: %v, \"+\n\t\t\"MirrorSync: %v, Bruno: %v, Berlin: %v, YOLO v3: %v, CatalystBlock: %v, London: %v, ArrowGlacier: %v, MergeFork:%v, Euler: %v, Gibbs: %v, Nano: %v, Moran: %v, Planck: %v,Luban: %v, Plato: %v, Hertz: %v, Hertzfix: %v, \"+\n\t\t\"ShanghaiTime: %v, KeplerTime: %v, FeynmanTime: %v, FeynmanFixTime: %v, CancunTime: %v, HaberTime: %v, HaberFixTime: %v, BohrTime: %v, PascalTime: %v, PragueTime: %v, LorentzTime: %v, MaxwellTime: %v, FermiTime: %v}\",\n\t\tc.ChainID,\n\t\tengine,\n\t\tc.HomesteadBlock,\n\t\tc.DAOForkBlock,\n\t\tc.DAOForkSupport,\n\t\tc.EIP150Block,\n\t\tc.EIP155Block,\n\t\tc.EIP158Block,\n\t\tc.ByzantiumBlock,\n\t\tc.ConstantinopleBlock,\n\t\tc.PetersburgBlock,\n\t\tc.IstanbulBlock,\n\t\tc.MuirGlacierBlock,\n\t\tc.RamanujanBlock,\n\t\tc.NielsBlock,\n\t\tc.MirrorSyncBlock,\n\t\tc.BrunoBlock,\n\t\tc.BerlinBlock,\n\t\tc.YoloV3Block,\n\t\tc.CatalystBlock,\n\t\tc.LondonBlock,\n\t\tc.ArrowGlacierBlock,\n\t\tc.MergeNetsplitBlock,\n\t\tc.EulerBlock,\n\t\tc.GibbsBlock,\n\t\tc.NanoBlock,\n\t\tc.MoranBlock,\n\t\tc.PlanckBlock,\n\t\tc.LubanBlock,\n\t\tc.PlatoBlock,\n\t\tc.HertzBlock,\n\t\tc.HertzfixBlock,\n\t\tShanghaiTime,\n\t\tKeplerTime,\n\t\tFeynmanTime,\n\t\tFeynmanFixTime,\n\t\tCancunTime,\n\t\tHaberTime,\n\t\tHaberFixTime,\n\t\tBohrTime,\n\t\tPascalTime,\n\t\tPragueTime,\n\t\tLorentzTime,\n\t\tMaxwellTime,\n\t\tFermiTime,\n\t)\n}\n\n// BlobConfig specifies the target and max blobs per block for the associated fork.\ntype BlobConfig struct {\n\tTarget         int    `json:\"target\"`\n\tMax            int    `json:\"max\"`\n\tUpdateFraction uint64 `json:\"baseFeeUpdateFraction\"`\n}\n\n// BlobScheduleConfig determines target and max number of blobs allow per fork.\ntype BlobScheduleConfig struct {\n\tCancun *BlobConfig `json:\"cancun,omitempty\"`\n\tPrague *BlobConfig `json:\"prague,omitempty\"`\n\tOsaka  *BlobConfig `json:\"osaka,omitempty\"`\n\tVerkle *BlobConfig `json:\"verkle,omitempty\"`\n}\n\n// IsHomestead returns whether num is either equal to the homestead block or greater.\nfunc (c *ChainConfig) IsHomestead(num *big.Int) bool {\n\treturn isBlockForked(c.HomesteadBlock, num)\n}\n\n// IsDAOFork returns whether num is either equal to the DAO fork block or greater.\nfunc (c *ChainConfig) IsDAOFork(num *big.Int) bool {\n\treturn isBlockForked(c.DAOForkBlock, num)\n}\n\n// IsEIP150 returns whether num is either equal to the EIP150 fork block or greater.\nfunc (c *ChainConfig) IsEIP150(num *big.Int) bool {\n\treturn isBlockForked(c.EIP150Block, num)\n}\n\n// IsEIP155 returns whether num is either equal to the EIP155 fork block or greater.\nfunc (c *ChainConfig) IsEIP155(num *big.Int) bool {\n\treturn isBlockForked(c.EIP155Block, num)\n}\n\n// IsEIP158 returns whether num is either equal to the EIP158 fork block or greater.\nfunc (c *ChainConfig) IsEIP158(num *big.Int) bool {\n\treturn isBlockForked(c.EIP158Block, num)\n}\n\n// IsByzantium returns whether num is either equal to the Byzantium fork block or greater.\nfunc (c *ChainConfig) IsByzantium(num *big.Int) bool {\n\treturn isBlockForked(c.ByzantiumBlock, num)\n}\n\n// IsConstantinople returns whether num is either equal to the Constantinople fork block or greater.\nfunc (c *ChainConfig) IsConstantinople(num *big.Int) bool {\n\treturn isBlockForked(c.ConstantinopleBlock, num)\n}\n\n// IsRamanujan returns whether num is either equal to the IsRamanujan fork block or greater.\nfunc (c *ChainConfig) IsRamanujan(num *big.Int) bool {\n\treturn isBlockForked(c.RamanujanBlock, num)\n}\n\n// IsOnRamanujan returns whether num is equal to the Ramanujan fork block\nfunc (c *ChainConfig) IsOnRamanujan(num *big.Int) bool {\n\treturn configBlockEqual(c.RamanujanBlock, num)\n}\n\n// IsNiels returns whether num is either equal to the Niels fork block or greater.\nfunc (c *ChainConfig) IsNiels(num *big.Int) bool {\n\treturn isBlockForked(c.NielsBlock, num)\n}\n\n// IsOnNiels returns whether num is equal to the IsNiels fork block\nfunc (c *ChainConfig) IsOnNiels(num *big.Int) bool {\n\treturn configBlockEqual(c.NielsBlock, num)\n}\n\n// IsMirrorSync returns whether num is either equal to the MirrorSync fork block or greater.\nfunc (c *ChainConfig) IsMirrorSync(num *big.Int) bool {\n\treturn isBlockForked(c.MirrorSyncBlock, num)\n}\n\n// IsOnMirrorSync returns whether num is equal to the MirrorSync fork block\nfunc (c *ChainConfig) IsOnMirrorSync(num *big.Int) bool {\n\treturn configBlockEqual(c.MirrorSyncBlock, num)\n}\n\n// IsBruno returns whether num is either equal to the Burn fork block or greater.\nfunc (c *ChainConfig) IsBruno(num *big.Int) bool {\n\treturn isBlockForked(c.BrunoBlock, num)\n}\n\n// IsOnBruno returns whether num is equal to the Burn fork block\nfunc (c *ChainConfig) IsOnBruno(num *big.Int) bool {\n\treturn configBlockEqual(c.BrunoBlock, num)\n}\n\n// IsEuler returns whether num is either equal to the euler fork block or greater.\nfunc (c *ChainConfig) IsEuler(num *big.Int) bool {\n\treturn isBlockForked(c.EulerBlock, num)\n}\n\n// IsOnEuler returns whether num is equal to the euler fork block\nfunc (c *ChainConfig) IsOnEuler(num *big.Int) bool {\n\treturn configBlockEqual(c.EulerBlock, num)\n}\n\n// IsLuban returns whether num is either equal to the first fast finality fork block or greater.\nfunc (c *ChainConfig) IsLuban(num *big.Int) bool {\n\treturn isBlockForked(c.LubanBlock, num)\n}\n\n// IsOnLuban returns whether num is equal to the first fast finality fork block.\nfunc (c *ChainConfig) IsOnLuban(num *big.Int) bool {\n\treturn configBlockEqual(c.LubanBlock, num)\n}\n\n// IsPlato returns whether num is either equal to the second fast finality fork block or greater.\nfunc (c *ChainConfig) IsPlato(num *big.Int) bool {\n\treturn isBlockForked(c.PlatoBlock, num)\n}\n\n// IsOnPlato returns whether num is equal to the second fast finality fork block.\nfunc (c *ChainConfig) IsOnPlato(num *big.Int) bool {\n\treturn configBlockEqual(c.PlatoBlock, num)\n}\n\n// IsHertz returns whether num is either equal to the block of enabling Berlin EIPs or greater.\nfunc (c *ChainConfig) IsHertz(num *big.Int) bool {\n\treturn isBlockForked(c.HertzBlock, num)\n}\n\n// IsOnHertz returns whether num is equal to the fork block of enabling Berlin EIPs.\nfunc (c *ChainConfig) IsOnHertz(num *big.Int) bool {\n\treturn configBlockEqual(c.HertzBlock, num)\n}\n\nfunc (c *ChainConfig) IsHertzfix(num *big.Int) bool {\n\treturn isBlockForked(c.HertzfixBlock, num)\n}\n\nfunc (c *ChainConfig) NeedBadSharedStorage(num *big.Int) bool {\n\tif c.IsHertzfix(num) || c.ChainID == nil {\n\t\treturn false\n\t}\n\n\tif c.ChainID.Cmp(big.NewInt(56)) == 0 && num.Cmp(big.NewInt(33851236)) == 0 {\n\t\treturn true\n\t}\n\n\tif c.ChainID.Cmp(big.NewInt(97)) == 0 && (num.Cmp(big.NewInt(35547779)) == 0 || num.Cmp(big.NewInt(35548081)) == 0) {\n\t\treturn true\n\t}\n\n\treturn false\n}\n\nfunc (c *ChainConfig) IsOnHertzfix(num *big.Int) bool {\n\treturn configBlockEqual(c.HertzfixBlock, num)\n}\n\n// IsMuirGlacier returns whether num is either equal to the Muir Glacier (EIP-2384) fork block or greater.\nfunc (c *ChainConfig) IsMuirGlacier(num *big.Int) bool {\n\treturn isBlockForked(c.MuirGlacierBlock, num)\n}\n\n// IsPetersburg returns whether num is either\n// - equal to or greater than the PetersburgBlock fork block,\n// - OR is nil, and Constantinople is active\nfunc (c *ChainConfig) IsPetersburg(num *big.Int) bool {\n\treturn isBlockForked(c.PetersburgBlock, num) || c.PetersburgBlock == nil && isBlockForked(c.ConstantinopleBlock, num)\n}\n\n// IsIstanbul returns whether num is either equal to the Istanbul fork block or greater.\nfunc (c *ChainConfig) IsIstanbul(num *big.Int) bool {\n\treturn isBlockForked(c.IstanbulBlock, num)\n}\n\n// IsBerlin returns whether num is either equal to the Berlin fork block or greater.\nfunc (c *ChainConfig) IsBerlin(num *big.Int) bool {\n\treturn isBlockForked(c.BerlinBlock, num)\n}\n\n// IsLondon returns whether num is either equal to the London fork block or greater.\nfunc (c *ChainConfig) IsLondon(num *big.Int) bool {\n\treturn isBlockForked(c.LondonBlock, num)\n}\n\n// IsArrowGlacier returns whether num is either equal to the Arrow Glacier (EIP-4345) fork block or greater.\nfunc (c *ChainConfig) IsArrowGlacier(num *big.Int) bool {\n\treturn isBlockForked(c.ArrowGlacierBlock, num)\n}\n\n// IsGrayGlacier returns whether num is either equal to the Gray Glacier (EIP-5133) fork block or greater.\nfunc (c *ChainConfig) IsGrayGlacier(num *big.Int) bool {\n\treturn isBlockForked(c.GrayGlacierBlock, num)\n}\n\n// IsTerminalPoWBlock returns whether the given block is the last block of PoW stage.\nfunc (c *ChainConfig) IsTerminalPoWBlock(parentTotalDiff *big.Int, totalDiff *big.Int) bool {\n\tif c.TerminalTotalDifficulty == nil {\n\t\treturn false\n\t}\n\treturn parentTotalDiff.Cmp(c.TerminalTotalDifficulty) < 0 && totalDiff.Cmp(c.TerminalTotalDifficulty) >= 0\n}\n\n// IsGibbs returns whether num is either equal to the gibbs fork block or greater.\nfunc (c *ChainConfig) IsGibbs(num *big.Int) bool {\n\treturn isBlockForked(c.GibbsBlock, num)\n}\n\n// IsOnGibbs returns whether num is equal to the gibbs fork block\nfunc (c *ChainConfig) IsOnGibbs(num *big.Int) bool {\n\treturn configBlockEqual(c.GibbsBlock, num)\n}\n\nfunc (c *ChainConfig) IsNano(num *big.Int) bool {\n\treturn isBlockForked(c.NanoBlock, num)\n}\n\nfunc (c *ChainConfig) IsOnNano(num *big.Int) bool {\n\treturn configBlockEqual(c.NanoBlock, num)\n}\n\nfunc (c *ChainConfig) IsMoran(num *big.Int) bool {\n\treturn isBlockForked(c.MoranBlock, num)\n}\n\nfunc (c *ChainConfig) IsOnMoran(num *big.Int) bool {\n\treturn configBlockEqual(c.MoranBlock, num)\n}\n\nfunc (c *ChainConfig) IsPlanck(num *big.Int) bool {\n\treturn isBlockForked(c.PlanckBlock, num)\n}\n\nfunc (c *ChainConfig) IsOnPlanck(num *big.Int) bool {\n\treturn configBlockEqual(c.PlanckBlock, num)\n}\n\n// IsShanghai returns whether time is either equal to the Shanghai fork time or greater.\nfunc (c *ChainConfig) IsShanghai(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.ShanghaiTime, time)\n}\n\n// IsOnShanghai returns whether currentBlockTime is either equal to the shanghai fork time or greater firstly.\nfunc (c *ChainConfig) IsOnShanghai(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsShanghai(lastBlockNumber, lastBlockTime) && c.IsShanghai(currentBlockNumber, currentBlockTime)\n}\n\n// IsKepler returns whether time is either equal to the kepler fork time or greater.\nfunc (c *ChainConfig) IsKepler(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.KeplerTime, time)\n}\n\n// IsOnKepler returns whether currentBlockTime is either equal to the kepler fork time or greater firstly.\nfunc (c *ChainConfig) IsOnKepler(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsKepler(lastBlockNumber, lastBlockTime) && c.IsKepler(currentBlockNumber, currentBlockTime)\n}\n\n// IsFeynman returns whether time is either equal to the Feynman fork time or greater.\nfunc (c *ChainConfig) IsFeynman(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.FeynmanTime, time)\n}\n\n// IsOnFeynman returns whether currentBlockTime is either equal to the Feynman fork time or greater firstly.\nfunc (c *ChainConfig) IsOnFeynman(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsFeynman(lastBlockNumber, lastBlockTime) && c.IsFeynman(currentBlockNumber, currentBlockTime)\n}\n\n// IsFeynmanFix returns whether time is either equal to the FeynmanFix fork time or greater.\nfunc (c *ChainConfig) IsFeynmanFix(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.FeynmanFixTime, time)\n}\n\n// IsOnFeynmanFix returns whether currentBlockTime is either equal to the FeynmanFix fork time or greater firstly.\nfunc (c *ChainConfig) IsOnFeynmanFix(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsFeynmanFix(lastBlockNumber, lastBlockTime) && c.IsFeynmanFix(currentBlockNumber, currentBlockTime)\n}\n\n// IsCancun returns whether time is either equal to the Cancun fork time or greater.\nfunc (c *ChainConfig) IsCancun(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.CancunTime, time)\n}\n\n// IsHaber returns whether time is either equal to the Haber fork time or greater.\nfunc (c *ChainConfig) IsHaber(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.HaberTime, time)\n}\n\n// IsHaberFix returns whether time is either equal to the HaberFix fork time or greater.\nfunc (c *ChainConfig) IsHaberFix(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.HaberFixTime, time)\n}\n\n// IsOnHaberFix returns whether currentBlockTime is either equal to the HaberFix fork time or greater firstly.\nfunc (c *ChainConfig) IsOnHaberFix(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsHaberFix(lastBlockNumber, lastBlockTime) && c.IsHaberFix(currentBlockNumber, currentBlockTime)\n}\n\n// IsBohr returns whether time is either equal to the Bohr fork time or greater.\nfunc (c *ChainConfig) IsBohr(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.BohrTime, time)\n}\n\n// IsOnBohr returns whether currentBlockTime is either equal to the Bohr fork time or greater firstly.\nfunc (c *ChainConfig) IsOnBohr(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsBohr(lastBlockNumber, lastBlockTime) && c.IsBohr(currentBlockNumber, currentBlockTime)\n}\n\n// IsPascal returns whether time is either equal to the Pascal fork time or greater.\nfunc (c *ChainConfig) IsPascal(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.PascalTime, time)\n}\n\n// IsOnPascal returns whether currentBlockTime is either equal to the Pascal fork time or greater firstly.\nfunc (c *ChainConfig) IsOnPascal(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsPascal(lastBlockNumber, lastBlockTime) && c.IsPascal(currentBlockNumber, currentBlockTime)\n}\n\n// IsPrague returns whether time is either equal to the Prague fork time or greater.\nfunc (c *ChainConfig) IsPrague(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.PragueTime, time)\n}\n\n// IsOnPrague returns whether currentBlockTime is either equal to the Prague fork time or greater firstly.\nfunc (c *ChainConfig) IsOnPrague(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsPrague(lastBlockNumber, lastBlockTime) && c.IsPrague(currentBlockNumber, currentBlockTime)\n}\n\n// IsLorentz returns whether time is either equal to the Lorentz fork time or greater.\nfunc (c *ChainConfig) IsLorentz(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.LorentzTime, time)\n}\n\n// IsOnLorentz returns whether currentBlockTime is either equal to the Lorentz fork time or greater firstly.\nfunc (c *ChainConfig) IsOnLorentz(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsLorentz(lastBlockNumber, lastBlockTime) && c.IsLorentz(currentBlockNumber, currentBlockTime)\n}\n\n// IsMaxwell returns whether time is either equal to the Maxwell fork time or greater.\nfunc (c *ChainConfig) IsMaxwell(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.MaxwellTime, time)\n}\n\n// IsOnMaxwell returns whether currentBlockTime is either equal to the Maxwell fork time or greater firstly.\nfunc (c *ChainConfig) IsOnMaxwell(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsMaxwell(lastBlockNumber, lastBlockTime) && c.IsMaxwell(currentBlockNumber, currentBlockTime)\n}\n\n// IsFermi returns whether time is either equal to the Fermi fork time or greater.\nfunc (c *ChainConfig) IsFermi(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.FermiTime, time)\n}\n\n// IsOnFermi returns whether currentBlockTime is either equal to the Fermi fork time or greater firstly.\nfunc (c *ChainConfig) IsOnFermi(currentBlockNumber *big.Int, lastBlockTime uint64, currentBlockTime uint64) bool {\n\tlastBlockNumber := new(big.Int)\n\tif currentBlockNumber.Cmp(big.NewInt(1)) >= 0 {\n\t\tlastBlockNumber.Sub(currentBlockNumber, big.NewInt(1))\n\t}\n\treturn !c.IsFermi(lastBlockNumber, lastBlockTime) && c.IsFermi(currentBlockNumber, currentBlockTime)\n}\n\n// IsOsaka returns whether time is either equal to the Osaka fork time or greater.\nfunc (c *ChainConfig) IsOsaka(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.OsakaTime, time)\n}\n\n// IsVerkle returns whether time is either equal to the Verkle fork time or greater.\nfunc (c *ChainConfig) IsVerkle(num *big.Int, time uint64) bool {\n\treturn c.IsLondon(num) && isTimestampForked(c.VerkleTime, time)\n}\n\n// IsVerkleGenesis checks whether the verkle fork is activated at the genesis block.\n//\n// Verkle mode is considered enabled if the verkle fork time is configured,\n// regardless of whether the local time has surpassed the fork activation time.\n// This is a temporary workaround for verkle devnet testing, where verkle is\n// activated at genesis, and the configured activation date has already passed.\n//\n// In production networks (mainnet and public testnets), verkle activation\n// always occurs after the genesis block, making this function irrelevant in\n// those cases.\nfunc (c *ChainConfig) IsVerkleGenesis() bool {\n\treturn c.EnableVerkleAtGenesis\n}\n\n// IsEIP4762 returns whether eip 4762 has been activated at given block.\nfunc (c *ChainConfig) IsEIP4762(num *big.Int, time uint64) bool {\n\treturn c.IsVerkle(num, time)\n}\n\n// CheckCompatible checks whether scheduled fork transitions have been imported\n// with a mismatching chain configuration.\nfunc (c *ChainConfig) CheckCompatible(newcfg *ChainConfig, height uint64, time uint64) *ConfigCompatError {\n\tvar (\n\t\tbhead = new(big.Int).SetUint64(height)\n\t\tbtime = time\n\t)\n\t// Iterate checkCompatible to find the lowest conflict.\n\tvar lasterr *ConfigCompatError\n\tfor {\n\t\terr := c.checkCompatible(newcfg, bhead, btime)\n\t\tif err == nil || (lasterr != nil && err.RewindToBlock == lasterr.RewindToBlock && err.RewindToTime == lasterr.RewindToTime) {\n\t\t\tbreak\n\t\t}\n\t\tlasterr = err\n\n\t\tif err.RewindToTime > 0 {\n\t\t\tbtime = err.RewindToTime\n\t\t} else {\n\t\t\tbhead.SetUint64(err.RewindToBlock)\n\t\t}\n\t}\n\treturn lasterr\n}\n\n// CheckConfigForkOrder checks that we don't \"skip\" any forks, geth isn't pluggable enough\n// to guarantee that forks can be implemented in a different order than on official networks\nfunc (c *ChainConfig) CheckConfigForkOrder() error {\n\t// skip checking for non-Parlia egine\n\tif c.Parlia == nil {\n\t\treturn nil\n\t}\n\ttype fork struct {\n\t\tname      string\n\t\tblock     *big.Int // forks up to - and including the merge - were defined with block numbers\n\t\ttimestamp *uint64  // forks after the merge are scheduled using timestamps\n\t\toptional  bool     // if true, the fork may be nil and next fork is still allowed\n\t}\n\tvar lastFork fork\n\tfor _, cur := range []fork{\n\t\t{name: \"mirrorSyncBlock\", block: c.MirrorSyncBlock},\n\t\t{name: \"brunoBlock\", block: c.BrunoBlock},\n\t\t{name: \"eulerBlock\", block: c.EulerBlock},\n\t\t{name: \"gibbsBlock\", block: c.GibbsBlock},\n\t\t{name: \"planckBlock\", block: c.PlanckBlock},\n\t\t{name: \"lubanBlock\", block: c.LubanBlock},\n\t\t{name: \"platoBlock\", block: c.PlatoBlock},\n\t\t{name: \"hertzBlock\", block: c.HertzBlock},\n\t\t{name: \"hertzfixBlock\", block: c.HertzfixBlock},\n\t\t{name: \"keplerTime\", timestamp: c.KeplerTime},\n\t\t{name: \"feynmanTime\", timestamp: c.FeynmanTime},\n\t\t{name: \"feynmanFixTime\", timestamp: c.FeynmanFixTime},\n\t\t{name: \"cancunTime\", timestamp: c.CancunTime},\n\t\t{name: \"haberTime\", timestamp: c.HaberTime},\n\t\t{name: \"haberFixTime\", timestamp: c.HaberFixTime},\n\t\t{name: \"bohrTime\", timestamp: c.BohrTime},\n\t\t{name: \"pascalTime\", timestamp: c.PascalTime},\n\t\t{name: \"pragueTime\", timestamp: c.PragueTime},\n\t\t{name: \"osakaTime\", timestamp: c.OsakaTime, optional: true},\n\t\t{name: \"lorentzTime\", timestamp: c.LorentzTime},\n\t\t{name: \"maxwellTime\", timestamp: c.MaxwellTime},\n\t\t{name: \"fermiTime\", timestamp: c.FermiTime},\n\t\t{name: \"verkleTime\", timestamp: c.VerkleTime, optional: true},\n\t} {\n\t\tif lastFork.name != \"\" {\n\t\t\tswitch {\n\t\t\t// Non-optional forks must all be present in the chain config up to the last defined fork\n\t\t\tcase lastFork.block == nil && lastFork.timestamp == nil && (cur.block != nil || cur.timestamp != nil):\n\t\t\t\tif cur.block != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v not enabled, but %v enabled at block %v\",\n\t\t\t\t\t\tlastFork.name, cur.name, cur.block)\n\t\t\t\t} else {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v not enabled, but %v enabled at timestamp %v\",\n\t\t\t\t\t\tlastFork.name, cur.name, *cur.timestamp)\n\t\t\t\t}\n\n\t\t\t// Fork (whether defined by block or timestamp) must follow the fork definition sequence\n\t\t\tcase (lastFork.block != nil && cur.block != nil) || (lastFork.timestamp != nil && cur.timestamp != nil):\n\t\t\t\tif lastFork.block != nil && lastFork.block.Cmp(cur.block) > 0 {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v enabled at block %v, but %v enabled at block %v\",\n\t\t\t\t\t\tlastFork.name, lastFork.block, cur.name, cur.block)\n\t\t\t\t} else if lastFork.timestamp != nil && *lastFork.timestamp > *cur.timestamp {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v enabled at timestamp %v, but %v enabled at timestamp %v\",\n\t\t\t\t\t\tlastFork.name, *lastFork.timestamp, cur.name, *cur.timestamp)\n\t\t\t\t}\n\n\t\t\t\t// Timestamp based forks can follow block based ones, but not the other way around\n\t\t\t\tif lastFork.timestamp != nil && cur.block != nil {\n\t\t\t\t\treturn fmt.Errorf(\"unsupported fork ordering: %v used timestamp ordering, but %v reverted to block ordering\",\n\t\t\t\t\t\tlastFork.name, cur.name)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// If it was optional and not set, then ignore it\n\t\tif !cur.optional || (cur.block != nil || cur.timestamp != nil) {\n\t\t\tlastFork = cur\n\t\t}\n\t}\n\n\t// Check that all forks with blobs explicitly define the blob schedule configuration.\n\tbsc := c.BlobScheduleConfig\n\tif bsc == nil {\n\t\tbsc = new(BlobScheduleConfig)\n\t}\n\tfor _, cur := range []struct {\n\t\tname      string\n\t\ttimestamp *uint64\n\t\tconfig    *BlobConfig\n\t}{\n\t\t{name: \"cancun\", timestamp: c.CancunTime, config: bsc.Cancun},\n\t\t{name: \"prague\", timestamp: c.PragueTime, config: bsc.Prague},\n\t\t{name: \"osaka\", timestamp: c.OsakaTime, config: bsc.Osaka},\n\t} {\n\t\tif cur.config != nil {\n\t\t\tif err := cur.config.validate()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/server.go",
          "line": 175,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" \" + c.node.ID().String()\n\t}\n\ts += \" \" + c.fd.RemoteAddr().String()\n\treturn s\n}\n\nfunc (f connFlag) String() string {\n\ts := \"\"\n\tif f&trustedConn != 0 {\n\t\ts += \"-trusted\"\n\t}\n\tif f&dynDialedConn != 0 {\n\t\ts += \"-dyndial\"\n\t}\n\tif f&staticDialedConn != 0 {\n\t\ts += \"-staticdial\"\n\t}\n\tif f&inboundConn != 0 {\n\t\ts += \"-inbound\"\n\t}\n\tif s != \"\" {\n\t\ts = s[1:]\n\t}\n\treturn s\n}\n\nfunc (c *conn) is(f connFlag) bool {\n\tflags := connFlag(atomic.LoadInt32((*int32)(&c.flags)))\n\treturn flags&f != 0\n}\n\nfunc (c *conn) set(f connFlag, val bool) {\n\tfor {\n\t\toldFlags := connFlag(atomic.LoadInt32((*int32)(&c.flags)))\n\t\tflags := oldFlags\n\t\tif val {\n\t\t\tflags |= f\n\t\t} else {\n\t\t\tflags &= ^f\n\t\t}\n\t\tif atomic.CompareAndSwapInt32((*int32)(&c.flags), int32(oldFlags), int32(flags)) {\n\t\t\treturn\n\t\t}\n\t}\n}\n\n// LocalNode returns the local node record.\nfunc (srv *Server) LocalNode() *enode.LocalNode {\n\treturn srv.localnode\n}\n\n// Peers returns all connected peers.\nfunc (srv *Server) Peers() []*Peer {\n\tvar ps []*Peer\n\tsrv.doPeerOp(func(peers map[enode.ID]*Peer) {\n\t\tfor _, p := range peers {\n\t\t\tps = append(ps, p)\n\t\t}\n\t})\n\treturn ps\n}\n\n// PeerCount returns the number of connected peers.\nfunc (srv *Server) PeerCount() int {\n\tvar count int\n\tsrv.doPeerOp(func(ps map[enode.ID]*Peer) {\n\t\tcount = len(ps)\n\t})\n\treturn count\n}\n\n// AddPeer adds the given node to the static node set. When there is room in the peer set,\n// the server will connect to the node. If the connection fails for any reason, the server\n// will attempt to reconnect the peer.\nfunc (srv *Server) AddPeer(node *enode.Node) {\n\tsrv.dialsched.addStatic(node)\n}\n\n// RemovePeer removes a node from the static node set. It also disconnects from the given\n// node if it is currently connected as a peer.\n//\n// This method blocks until all protocols have exited and the peer is removed. Do not use\n// RemovePeer in protocol implementations, call Disconnect on the Peer instead.\nfunc (srv *Server) RemovePeer(node *enode.Node) {\n\tvar (\n\t\tch  chan *PeerEvent\n\t\tsub event.Subscription\n\t)\n\n\t// Disconnect the peer on the main loop.\n\tsrv.doPeerOp(func(peers map[enode.ID]*Peer) {\n\t\t// Special case: sending a disconnect request with a hardcoded enode ID will reset the disconnect enode set\n\t\tif node.ID() == magicEnodeID {\n\t\t\tsrv.disconnectEnodeSet = make(map[enode.ID]struct{})\n\t\t\tsrv.log.Debug(\"Reset disconnect enode set\")\n\t\t\treturn\n\t\t}\n\n\t\tsrv.dialsched.removeStatic(node)\n\t\tif peer := peers[node.ID()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/p2p/server.go",
          "line": 1065,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/p2p/server.go",
          "line": 1084,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/message.go",
          "line": 144,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint32(n)\n\tif (err != nil || r.count == 0) && r.eof != nil {\n\t\tr.eof <- struct{}{} // tell Peer that msg has been consumed\n\t\tr.eof = nil\n\t}\n\treturn n, err\n}\n\n// MsgPipe creates a message pipe. Reads on one end are matched\n// with writes on the other. The pipe is full-duplex, both ends\n// implement MsgReadWriter.\nfunc MsgPipe(args ...any) (*MsgPipeRW, *MsgPipeRW) {\n\tnoBlock := false\n\tif len(args) > 0 {\n\t\tnoBlock = args[0].(bool)\n\t}\n\tc1, c2 := make(chan Msg), make(chan Msg)\n\tif noBlock {\n\t\tc1 = make(chan Msg, 1)\n\t\tc2 = make(chan Msg, 1)\n\t}\n\tvar (\n\t\tclosing = make(chan struct{})\n\t\tclosed  = new(atomic.Bool)\n\t\trw1     = &MsgPipeRW{c1, c2, closing, closed, noBlock}\n\t\trw2     = &MsgPipeRW{c2, c1, closing, closed, noBlock}\n\t)\n\treturn rw1, rw2\n}\n\n// ErrPipeClosed is returned from pipe operations after the\n// pipe has been closed.\nvar ErrPipeClosed = errors.New(\"p2p: read or write on closed message pipe\")\n\n// MsgPipeRW is an endpoint of a MsgReadWriter pipe.\ntype MsgPipeRW struct {\n\tw       chan<- Msg\n\tr       <-chan Msg\n\tclosing chan struct{}\n\tclosed  *atomic.Bool\n\tnoBlock bool\n}\n\n// WriteMsg sends a message on the pipe.\n// It blocks until the receiver has consumed the message payload.\nfunc (p *MsgPipeRW) WriteMsg(msg Msg) error {\n\tif !p.closed.Load() {\n\t\tconsumed := make(chan struct{}, 1)\n\t\tmsg.Payload = &eofSignal{msg.Payload, msg.Size, consumed}\n\t\tselect {\n\t\tcase p.w <- msg:\n\t\t\tif p.noBlock {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\tif msg.Size > 0 {\n\t\t\t\t// wait for payload read or discard\n\t\t\t\tselect {\n\t\t\t\tcase <-consumed:\n\t\t\t\tcase <-p.closing:\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\tcase <-p.closing:\n\t\t}\n\t}\n\treturn ErrPipeClosed\n}\n\n// ReadMsg returns a message sent on the other end of the pipe.\nfunc (p *MsgPipeRW) ReadMsg() (Msg, error) {\n\tif !p.closed.Load() {\n\t\tselect {\n\t\tcase msg := <-p.r:\n\t\t\treturn msg, nil\n\t\tcase <-p.closing:\n\t\t}\n\t}\n\treturn Msg{}, ErrPipeClosed\n}\n\n// Close unblocks any pending ReadMsg and WriteMsg calls on both ends\n// of the pipe. They will return ErrPipeClosed. Close also\n// interrupts any reads from a message payload.\nfunc (p *MsgPipeRW) Close() error {\n\tif p.closed.Swap(true) {\n\t\t// someone else is already closing\n\t\treturn nil\n\t}\n\tclose(p.closing)\n\treturn nil\n}\n\n// ExpectMsg reads a message from r and verifies that its\n// code and encoded RLP content match the provided values.\n// If content is nil, the payload is discarded and not verified.\nfunc ExpectMsg(r MsgReader, code uint64, content interface{}) error {\n\tmsg, err := r.ReadMsg()\n\tif err != nil {\n\t\treturn err\n\t}\n\tif msg.Code != code {\n\t\treturn fmt.Errorf(\"message code mismatch: got %d, expected %d\", msg.Code, code)\n\t}\n\tif content == nil {\n\t\treturn msg.Discard()\n\t}\n\tcontentEnc, err := rlp.EncodeToBytes(content)\n\tif err != nil {\n\t\tpanic(\"content encode error: \" + err.Error())\n\t}\n\tif int(msg.Size) != len(contentEnc) {\n\t\treturn fmt.Errorf(\"message size mismatch: got %d, want %d\", msg.Size, len(contentEnc))\n\t}\n\tactualContent, err := io.ReadAll(msg.Payload)\n\tif err != nil {\n\t\treturn err\n\t}\n\tif !bytes.Equal(actualContent, contentEnc) {\n\t\treturn fmt.Errorf(\"message payload mismatch:\\ngot:  %x\\nwant: %x\", actualContent, contentEnc)\n\t}\n\treturn nil\n}\n\n// msgEventer wraps a MsgReadWriter and sends events whenever a message is sent\n// or received\ntype msgEventer struct {\n\tMsgReadWriter\n\n\tfeed          *event.Feed\n\tpeerID        enode.ID\n\tProtocol      string\n\tlocalAddress  string\n\tremoteAddress string\n}\n\n// newMsgEventer returns a msgEventer which sends message events to the given\n// feed\nfunc newMsgEventer(rw MsgReadWriter, feed *event.Feed, peerID enode.ID, proto, remote, local string) *msgEventer {\n\treturn &msgEventer{\n\t\tMsgReadWriter: rw,\n\t\tfeed:          feed,\n\t\tpeerID:        peerID,\n\t\tProtocol:      proto,\n\t\tremoteAddress: remote,\n\t\tlocalAddress:  local,\n\t}\n}\n\n// ReadMsg reads a message from the underlying MsgReadWriter and emits a\n// \"message received\" event\nfunc (ev *msgEventer) ReadMsg() (Msg, error) {\n\tmsg, err := ev.MsgReadWriter.ReadMsg()\n\tif err != nil {\n\t\treturn msg, err\n\t}\n\tev.feed.Send(&PeerEvent{\n\t\tType:          PeerEventTypeMsgRecv,\n\t\tPeer:          ev.peerID,\n\t\tProtocol:      ev.Protocol,\n\t\tMsgCode:       &msg.Code,\n\t\tMsgSize:       &msg.Size,\n\t\tLocalAddress:  ev.localAddress,\n\t\tRemoteAddress: ev.remoteAddress,\n\t})\n\treturn msg, nil\n}\n\n// WriteMsg writes a message to the underlying MsgReadWriter and emits a\n// \"message sent\" event\nfunc (ev *msgEventer) WriteMsg(msg Msg) error {\n\terr := ev.MsgReadWriter.WriteMsg(msg)\n\tif err != nil {\n\t\treturn err\n\t}\n\tev.feed.Send(&PeerEvent{\n\t\tType:          PeerEventTypeMsgSend,\n\t\tPeer:          ev.peerID,\n\t\tProtocol:      ev.Protocol,\n\t\tMsgCode:       &msg.Code,\n\t\tMsgSize:       &msg.Size,\n\t\tLocalAddress:  ev.localAddress,\n\t\tRemoteAddress: ev.remoteAddress,\n\t})\n\treturn nil\n}\n\n// Close closes the underlying MsgReadWriter if it implements the io.Closer\n// interface\nfunc (ev *msgEventer) Close() error {\n\tif v, ok := ev.MsgReadWriter.(io.Closer)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/p2p/message.go",
          "line": 299,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/p2p/message.go",
          "line": 318,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/peer_error.go",
          "line": 46,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \": \" + fmt.Sprintf(format, v...)\n\t}\n\treturn err\n}\n\nfunc (pe *peerError) Error() string {\n\treturn pe.message\n}\n\nvar errProtocolReturned = errors.New(\"protocol returned\")\n\ntype DiscReason uint8\n\nconst (\n\tDiscRequested DiscReason = iota\n\tDiscNetworkError\n\tDiscProtocolError\n\tDiscUselessPeer\n\tDiscTooManyPeers\n\tDiscAlreadyConnected\n\tDiscIncompatibleVersion\n\tDiscInvalidIdentity\n\tDiscQuitting\n\tDiscUnexpectedIdentity\n\tDiscSelf\n\tDiscReadTimeout\n\tDiscSubprotocolError = DiscReason(0x10)\n\n\tDiscInvalid = 0xff\n)\n\nvar discReasonToString = [...]string{\n\tDiscRequested:           \"disconnect requested\",\n\tDiscNetworkError:        \"network error\",\n\tDiscProtocolError:       \"breach of protocol\",\n\tDiscUselessPeer:         \"useless peer\",\n\tDiscTooManyPeers:        \"too many peers\",\n\tDiscAlreadyConnected:    \"already connected\",\n\tDiscIncompatibleVersion: \"incompatible p2p protocol version\",\n\tDiscInvalidIdentity:     \"invalid node identity\",\n\tDiscQuitting:            \"client quitting\",\n\tDiscUnexpectedIdentity:  \"unexpected identity\",\n\tDiscSelf:                \"connected to self\",\n\tDiscReadTimeout:         \"read timeout\",\n\tDiscSubprotocolError:    \"subprotocol error\",\n\tDiscInvalid:             \"invalid disconnect reason\",\n}\n\nfunc (d DiscReason) String() string {\n\tif len(discReasonToString) <= int(d) || discReasonToString[d] == \"\" {\n\t\treturn fmt.Sprintf(\"unknown disconnect reason %d\", d)\n\t}\n\treturn discReasonToString[d]\n}\n\nfunc (d DiscReason) Error() string {\n\treturn d.String()\n}\n\nfunc discReasonForError(err error) DiscReason {\n\tif reason, ok := err.(DiscReason)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/peer.go",
          "line": 515,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= proto.Length\n\n\t\t\t\tcontinue outer\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\nfunc (p *Peer) startProtocols(writeStart <-chan struct{}, writeErr chan<- error) {\n\tp.wg.Add(len(p.running))\n\tfor _, proto := range p.running {\n\t\tproto.closed = p.closed\n\t\tproto.wstart = writeStart\n\t\tproto.werr = writeErr\n\t\tvar rw MsgReadWriter = proto\n\t\tif p.events != nil {\n\t\t\trw = newMsgEventer(rw, p.events, p.ID(), proto.Name, p.Info().Network.RemoteAddress, p.Info().Network.LocalAddress)\n\t\t}\n\t\tp.log.Trace(fmt.Sprintf(\"Starting protocol %s/%d\", proto.Name, proto.Version))\n\t\tgo func() {\n\t\t\tdefer p.wg.Done()\n\t\t\terr := proto.Run(p, rw)\n\t\t\tif err == nil {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d returned\", proto.Name, proto.Version))\n\t\t\t\terr = errProtocolReturned\n\t\t\t} else if !errors.Is(err, io.EOF) {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d failed\", proto.Name, proto.Version), \"err\", err)\n\t\t\t}\n\t\t\tp.protoErr <- err\n\t\t}()\n\t}\n}\n\n// getProto finds the protocol responsible for handling\n// the given message code.\nfunc (p *Peer) getProto(code uint64) (*protoRW, error) {\n\tfor _, proto := range p.running {\n\t\tif code >= proto.offset && code < proto.offset+proto.Length {\n\t\t\treturn proto, nil\n\t\t}\n\t}\n\treturn nil, newPeerError(errInvalidMsgCode, \"%d\", code)\n}\n\ntype protoRW struct {\n\tProtocol\n\tin     chan Msg        // receives read messages\n\tclosed <-chan struct{} // receives when peer is shutting down\n\twstart <-chan struct{} // receives when write may start\n\twerr   chan<- error    // for write results\n\toffset uint64\n\tw      MsgWriter\n}\n\nfunc (rw *protoRW) WriteMsg(msg Msg) (err error) {\n\tif msg.Code >= rw.Length {\n\t\treturn newPeerError(errInvalidMsgCode, \"not handled\")\n\t}\n\tmsg.meterCap = rw.cap()\n\tmsg.meterCode = msg.Code\n\n\tmsg.Code += rw.offset\n\n\tselect {\n\tcase <-rw.wstart:\n\t\terr = rw.w.WriteMsg(msg)\n\t\t// Report write status back to Peer.run. It will initiate\n\t\t// shutdown if the error is non-nil and unblock the next write\n\t\t// otherwise. The calling protocol code should exit for errors\n\t\t// as well but we don't want to rely on that.\n\t\trw.werr <- err\n\tcase <-rw.closed:\n\t\terr = ErrShuttingDown\n\t}\n\treturn err\n}\n\nfunc (rw *protoRW) ReadMsg() (Msg, error) {\n\tselect {\n\tcase msg := <-rw.in:\n\t\tmsg.Code -= rw.offset\n\t\treturn msg, nil\n\tcase <-rw.closed:\n\t\treturn Msg{}, io.EOF\n\t}\n}\n\n// PeerInfo represents a short summary of the information known about a connected\n// peer. Sub-protocol independent fields are contained and initialized here, with\n// protocol specifics delegated to all connected sub-protocols.\ntype PeerInfo struct {\n\tENR     string   `json:\"enr,omitempty\"` // Ethereum Node Record\n\tEnode   string   `json:\"enode\"`         // Node URL\n\tID      string   `json:\"id\"`            // Unique node identifier\n\tName    string   `json:\"name\"`          // Name of the node, including client type, version, OS, custom data\n\tCaps    []string `json:\"caps\"`          // Protocols advertised by this peer\n\tNetwork struct {\n\t\tLocalAddress  string `json:\"localAddress\"`  // Local endpoint of the TCP data connection\n\t\tRemoteAddress string `json:\"remoteAddress\"` // Remote endpoint of the TCP data connection\n\t\tInbound       bool   `json:\"inbound\"`\n\t\tTrusted       bool   `json:\"trusted\"`\n\t\tStatic        bool   `json:\"static\"`\n\t} `json:\"network\"`\n\tProtocols   map[string]interface{} `json:\"protocols\"` // Sub-protocol specific metadata fields\n\tLatency     int64                  `json:\"latency\"`   // the estimate latency from ping msg\n\tEVNPeerFlag bool                   `json:\"evnPeerFlag\"`\n}\n\n// Info gathers and returns a collection of metadata known about a peer.\nfunc (p *Peer) Info() *PeerInfo {\n\t// Gather the protocol capabilities\n\tvar caps []string\n\tfor _, cap := range p.Caps() {\n\t\tcaps = append(caps, cap.String())\n\t}\n\t// Assemble the generic peer metadata\n\tinfo := &PeerInfo{\n\t\tEnode:       p.Node().URLv4(),\n\t\tID:          p.ID().String(),\n\t\tName:        p.Fullname(),\n\t\tCaps:        caps,\n\t\tProtocols:   make(map[string]interface{}, len(p.running)),\n\t\tLatency:     p.latency.Load(),\n\t\tEVNPeerFlag: p.EVNPeerFlag.Load(),\n\t}\n\tif p.Node().Seq() > 0 {\n\t\tinfo.ENR = p.Node().String()\n\t}\n\tinfo.Network.LocalAddress = p.LocalAddr().String()\n\tinfo.Network.RemoteAddress = p.RemoteAddr().String()\n\tinfo.Network.Inbound = p.rw.is(inboundConn)\n\t// After Maxwell, we treat all EVN peers as trusted\n\tinfo.Network.Trusted = p.rw.is(trustedConn) || p.EVNPeerFlag.Load()\n\tinfo.Network.Static = p.rw.is(staticDialedConn)\n\n\t// Gather all the running protocol infos\n\tfor _, proto := range p.running {\n\t\tprotoInfo := interface{}(\"unknown\")\n\t\tif query := proto.Protocol.PeerInfo",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/p2p/peer.go",
          "line": 511,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= old.Length\n\t\t\t\t}\n\t\t\t\t// Assign the new match\n\t\t\t\tresult[cap.Name] = &protoRW{Protocol: proto, offset: offset, in: make(chan Msg), w: rw}\n\t\t\t\toffset += proto.Length\n\n\t\t\t\tcontinue outer\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\nfunc (p *Peer) startProtocols(writeStart <-chan struct{}, writeErr chan<- error) {\n\tp.wg.Add(len(p.running))\n\tfor _, proto := range p.running {\n\t\tproto.closed = p.closed\n\t\tproto.wstart = writeStart\n\t\tproto.werr = writeErr\n\t\tvar rw MsgReadWriter = proto\n\t\tif p.events != nil {\n\t\t\trw = newMsgEventer(rw, p.events, p.ID(), proto.Name, p.Info().Network.RemoteAddress, p.Info().Network.LocalAddress)\n\t\t}\n\t\tp.log.Trace(fmt.Sprintf(\"Starting protocol %s/%d\", proto.Name, proto.Version))\n\t\tgo func() {\n\t\t\tdefer p.wg.Done()\n\t\t\terr := proto.Run(p, rw)\n\t\t\tif err == nil {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d returned\", proto.Name, proto.Version))\n\t\t\t\terr = errProtocolReturned\n\t\t\t} else if !errors.Is(err, io.EOF) {\n\t\t\t\tp.log.Trace(fmt.Sprintf(\"Protocol %s/%d failed\", proto.Name, proto.Version), \"err\", err)\n\t\t\t}\n\t\t\tp.protoErr <- err\n\t\t}()\n\t}\n}\n\n// getProto finds the protocol responsible for handling\n// the given message code.\nfunc (p *Peer) getProto(code uint64) (*protoRW, error) {\n\tfor _, proto := range p.running {\n\t\tif code >= proto.offset && code < proto.offset+proto.Length {\n\t\t\treturn proto, nil\n\t\t}\n\t}\n\treturn nil, newPeerError(errInvalidMsgCode, \"%d\", code)\n}\n\ntype protoRW struct {\n\tProtocol\n\tin     chan Msg        // receives read messages\n\tclosed <-chan struct{} // receives when peer is shutting down\n\twstart <-chan struct{} // receives when write may start\n\twerr   chan<- error    // for write results\n\toffset uint64\n\tw      MsgWriter\n}\n\nfunc (rw *protoRW) WriteMsg(msg Msg) (err error) {\n\tif msg.Code >= rw.Length {\n\t\treturn newPeerError(errInvalidMsgCode, \"not handled\")\n\t}\n\tmsg.meterCap = rw.cap()\n\tmsg.meterCode = msg.Code\n\n\tmsg.Code += rw.offset\n\n\tselect {\n\tcase <-rw.wstart:\n\t\terr = rw.w.WriteMsg(msg)\n\t\t// Report write status back to Peer.run. It will initiate\n\t\t// shutdown if the error is non-nil and unblock the next write\n\t\t// otherwise. The calling protocol code should exit for errors\n\t\t// as well but we don't want to rely on that.\n\t\trw.werr <- err\n\tcase <-rw.closed:\n\t\terr = ErrShuttingDown\n\t}\n\treturn err\n}\n\nfunc (rw *protoRW) ReadMsg() (Msg, error) {\n\tselect {\n\tcase msg := <-rw.in:\n\t\tmsg.Code -= rw.offset\n\t\treturn msg, nil\n\tcase <-rw.closed:\n\t\treturn Msg{}, io.EOF\n\t}\n}\n\n// PeerInfo represents a short summary of the information known about a connected\n// peer. Sub-protocol independent fields are contained and initialized here, with\n// protocol specifics delegated to all connected sub-protocols.\ntype PeerInfo struct {\n\tENR     string   `json:\"enr,omitempty\"` // Ethereum Node Record\n\tEnode   string   `json:\"enode\"`         // Node URL\n\tID      string   `json:\"id\"`            // Unique node identifier\n\tName    string   `json:\"name\"`          // Name of the node, including client type, version, OS, custom data\n\tCaps    []string `json:\"caps\"`          // Protocols advertised by this peer\n\tNetwork struct {\n\t\tLocalAddress  string `json:\"localAddress\"`  // Local endpoint of the TCP data connection\n\t\tRemoteAddress string `json:\"remoteAddress\"` // Remote endpoint of the TCP data connection\n\t\tInbound       bool   `json:\"inbound\"`\n\t\tTrusted       bool   `json:\"trusted\"`\n\t\tStatic        bool   `json:\"static\"`\n\t} `json:\"network\"`\n\tProtocols   map[string]interface{} `json:\"protocols\"` // Sub-protocol specific metadata fields\n\tLatency     int64                  `json:\"latency\"`   // the estimate latency from ping msg\n\tEVNPeerFlag bool                   `json:\"evnPeerFlag\"`\n}\n\n// Info gathers and returns a collection of metadata known about a peer.\nfunc (p *Peer) Info() *PeerInfo {\n\t// Gather the protocol capabilities\n\tvar caps []string\n\tfor _, cap := range p.Caps() {\n\t\tcaps = append(caps, cap.String())\n\t}\n\t// Assemble the generic peer metadata\n\tinfo := &PeerInfo{\n\t\tEnode:       p.Node().URLv4(),\n\t\tID:          p.ID().String(),\n\t\tName:        p.Fullname(),\n\t\tCaps:        caps,\n\t\tProtocols:   make(map[string]interface{}, len(p.running)),\n\t\tLatency:     p.latency.Load(),\n\t\tEVNPeerFlag: p.EVNPeerFlag.Load(),\n\t}\n\tif p.Node().Seq() > 0 {\n\t\tinfo.ENR = p.Node().String()\n\t}\n\tinfo.Network.LocalAddress = p.LocalAddr().String()\n\tinfo.Network.RemoteAddress = p.RemoteAddr().String()\n\tinfo.Network.Inbound = p.rw.is(inboundConn)\n\t// After Maxwell, we treat all EVN peers as trusted\n\tinfo.Network.Trusted = p.rw.is(trustedConn) || p.EVNPeerFlag.Load()\n\tinfo.Network.Static = p.rw.is(staticDialedConn)\n\n\t// Gather all the running protocol infos\n\tfor _, proto := range p.running {\n\t\tprotoInfo := interface{}(\"unknown\")\n\t\tif query := proto.Protocol.PeerInfo",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/dial.go",
          "line": 242,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= d.startStaticDials(slots)\n\t\tif slots > 0 {\n\t\t\tnodesCh = d.nodesIn\n\t\t} else {\n\t\t\tnodesCh = nil\n\t\t}\n\t\td.rearmHistoryTimer()\n\t\td.logStats()\n\n\t\tselect {\n\t\tcase node := <-nodesCh:\n\t\t\tif err := d.checkDial(node)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/p2p/dial.go",
          "line": 600,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 2\n\t\tif t.resolveDelay > maxResolveDelay {\n\t\t\tt.resolveDelay = maxResolveDelay\n\t\t}\n\t\td.log.Debug(\"Resolving node failed\", \"id\", node.ID(), \"newdelay\", t.resolveDelay)\n\t\treturn false\n\t}\n\t// The node was found.\n\tt.resolveDelay = initialResolveDelay\n\tt.destPtr.Store(resolved)\n\tresAddr, _ := resolved.TCPEndpoint()\n\td.log.Debug(\"Resolved node\", \"id\", resolved.ID(), \"addr\", resAddr)\n\treturn true\n}\n\n// dial performs the actual connection attempt.\nfunc (t *dialTask) dial(d *dialScheduler, dest *enode.Node) error {\n\tdialMeter.Mark(1)\n\tfd, err := d.dialer.Dial(d.ctx, dest)\n\tif err != nil {\n\t\taddr, _ := dest.TCPEndpoint()\n\t\td.log.Trace(\"Dial error\", \"id\", dest.ID(), \"addr\", addr, \"conn\", t.flags, \"err\", cleanupDialErr(err))\n\t\tdialConnectionError.Mark(1)\n\t\treturn &dialError{err}\n\t}\n\treturn d.setupFunc(newMeteredConn(fd), t.flags, dest)\n}\n\nfunc (t *dialTask) String() string {\n\tnode := t.dest()\n\tid := node.ID()\n\treturn fmt.Sprintf(\"%v %x %v:%d\", t.flags, id[:8], node.IPAddr(), node.TCP())\n}\n\nfunc cleanupDialErr(err error) error {\n\tif netErr, ok := err.(*net.OpError)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/server_test.go",
          "line": 500,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"doEncHandshake,\"\n\treturn c.pubkey, c.encHandshakeErr\n}\n\nfunc (c *setupTransport) doProtoHandshake(our *protoHandshake) (*protoHandshake, error) {\n\tc.calls += \"doProtoHandshake,\"\n\tif c.protoHandshakeErr != nil {\n\t\treturn nil, c.protoHandshakeErr\n\t}\n\treturn &c.phs, nil\n}\nfunc (c *setupTransport) close(err error) {\n\tc.calls += \"close,\"\n\tc.closeErr = err\n}\n\n// setupConn shouldn't write to/read from the connection.\nfunc (c *setupTransport) WriteMsg(Msg) error {\n\tpanic(\"WriteMsg called on setupTransport\")\n}\nfunc (c *setupTransport) ReadMsg() (Msg, error) {\n\tpanic(\"ReadMsg called on setupTransport\")\n}\n\nfunc newkey() *ecdsa.PrivateKey {\n\tkey, err := crypto.GenerateKey()\n\tif err != nil {\n\t\tpanic(\"couldn't generate key: \" + err.Error())\n\t}\n\treturn key\n}\n\nfunc randomID() (id enode.ID) {\n\tfor i := range id {\n\t\tid[i] = byte(rand.Intn(255))\n\t}\n\treturn id\n}\n\n// This test checks that inbound connections are throttled by IP.\nfunc TestServerInboundThrottle(t *testing.T) {\n\tconst timeout = 5 * time.Second\n\tnewTransportCalled := make(chan struct{})\n\tsrv := &Server{\n\t\tConfig: Config{\n\t\t\tPrivateKey:  newkey(),\n\t\t\tListenAddr:  \"127.0.0.1:0\",\n\t\t\tMaxPeers:    10,\n\t\t\tNoDial:      true,\n\t\t\tNoDiscovery: true,\n\t\t\tProtocols:   []Protocol{discard},\n\t\t\tLogger:      testlog.Logger(t, log.LvlTrace),\n\t\t},\n\t\tnewTransport: func(fd net.Conn, dialDest *ecdsa.PublicKey) transport {\n\t\t\tnewTransportCalled <- struct{}{}\n\t\t\treturn newRLPX(fd, dialDest)\n\t\t},\n\t\tlistenFunc: func(network, laddr string) (net.Listener, error) {\n\t\t\tfakeAddr := &net.TCPAddr{IP: net.IP{95, 33, 21, 2}, Port: 4444}\n\t\t\treturn listenFakeAddr(network, laddr, fakeAddr)\n\t\t},\n\t}\n\tif err := srv.Start()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethclient/ethclient.go",
          "line": 773,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(ctx context.Context, msg ethereum.CallMsg, blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethclient/ethclient_test.go",
          "line": 657,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(context.Background()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/log/handler_glog.go",
          "line": 118,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"(/.*)?\"\n\t\t\t} else if comp != \"\" {\n\t\t\t\tmatcher += \"/\" + regexp.QuoteMeta(comp)\n\t\t\t}\n\t\t}\n\t\tif !strings.HasSuffix(parts[0], \".go\") {\n\t\t\tmatcher += \"/[^/]+\\\\.go\"\n\t\t}\n\t\tmatcher = matcher + \"$\"\n\n\t\tre, _ := regexp.Compile(matcher)\n\t\tfilter = append(filter, pattern{re, level})\n\t}\n\t// Swap out the vmodule pattern for the new filter system\n\th.lock.Lock()\n\tdefer h.lock.Unlock()\n\n\th.patterns = filter\n\th.siteCache = make(map[uintptr]slog.Level)\n\th.override.Store(len(filter) != 0)\n\n\treturn nil\n}\n\n// Enabled implements slog.Handler, reporting whether the handler handles records\n// at the given level.\nfunc (h *GlogHandler) Enabled(ctx context.Context, lvl slog.Level) bool {\n\t// fast-track skipping logging if override not enabled and the provided verbosity is above configured\n\treturn h.override.Load() || slog.Level(h.level.Load()) <= lvl\n}\n\n// WithAttrs implements slog.Handler, returning a new Handler whose attributes\n// consist of both the receiver's attributes and the arguments.\nfunc (h *GlogHandler) WithAttrs(attrs []slog.Attr) slog.Handler {\n\th.lock.RLock()\n\tsiteCache := maps.Clone(h.siteCache)\n\th.lock.RUnlock()\n\n\tpatterns := []pattern{}\n\tpatterns = append(patterns, h.patterns...)\n\n\tres := GlogHandler{\n\t\torigin:    h.origin.WithAttrs(attrs),\n\t\tpatterns:  patterns,\n\t\tsiteCache: siteCache,\n\t\tlocation:  h.location,\n\t}\n\n\tres.level.Store(h.level.Load())\n\tres.override.Store(h.override.Load())\n\treturn &res\n}\n\n// WithGroup implements slog.Handler, returning a new Handler with the given\n// group appended to the receiver's existing groups.\n//\n// Note, this function is not implemented.\nfunc (h *GlogHandler) WithGroup(name string) slog.Handler {\n\tpanic(\"not implemented\")\n}\n\n// Handle implements slog.Handler, filtering a log record through the global,\n// local and backtrace filters, finally emitting it if either allow it through.\nfunc (h *GlogHandler) Handle(_ context.Context, r slog.Record) error {\n\t// If the global log level allows, fast track logging\n\tif slog.Level(h.level.Load()) <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\n\t// Check callsite cache for previously calculated log levels\n\th.lock.RLock()\n\tlvl, ok := h.siteCache[r.PC]\n\th.lock.RUnlock()\n\n\t// If we didn't cache the callsite yet, calculate it\n\tif !ok {\n\t\th.lock.Lock()\n\n\t\tfs := runtime.CallersFrames([]uintptr{r.PC})\n\t\tframe, _ := fs.Next()\n\n\t\tfor _, rule := range h.patterns {\n\t\t\tif rule.pattern.MatchString(fmt.Sprintf(\"+%s\", frame.File)) {\n\t\t\t\th.siteCache[r.PC], lvl, ok = rule.level, rule.level, true\n\t\t\t}\n\t\t}\n\t\t// If no rule matched, remember to drop log the next time\n\t\tif !ok {\n\t\t\th.siteCache[r.PC] = 0\n\t\t}\n\t\th.lock.Unlock()\n\t}\n\tif lvl <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/log/handler_glog.go",
          "line": 85,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=3\"\n//\t sets V to 3 in all files of any packages whose import path contains \"foo\"\nfunc (h *GlogHandler) Vmodule(ruleset string) error {\n\tvar filter []pattern\n\tfor _, rule := range strings.Split(ruleset, \",\") {\n\t\t// Empty strings such as from a trailing comma can be ignored\n\t\tif len(rule) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\t// Ensure we have a pattern = level filter rule\n\t\tparts := strings.Split(rule, \"=\")\n\t\tif len(parts) != 2 {\n\t\t\treturn errVmoduleSyntax\n\t\t}\n\t\tparts[0] = strings.TrimSpace(parts[0])\n\t\tparts[1] = strings.TrimSpace(parts[1])\n\t\tif len(parts[0]) == 0 || len(parts[1]) == 0 {\n\t\t\treturn errVmoduleSyntax\n\t\t}\n\t\t// Parse the level and if correct, assemble the filter rule\n\t\tl, err := strconv.Atoi(parts[1])\n\t\tif err != nil {\n\t\t\treturn errVmoduleSyntax\n\t\t}\n\t\tlevel := FromLegacyLevel(l)\n\n\t\tif level == LevelCrit {\n\t\t\tcontinue // Ignore. It's harmless but no point in paying the overhead.\n\t\t}\n\t\t// Compile the rule pattern into a regular expression\n\t\tmatcher := \".*\"\n\t\tfor _, comp := range strings.Split(parts[0], \"/\") {\n\t\t\tif comp == \"*\" {\n\t\t\t\tmatcher += \"(/.*)?\"\n\t\t\t} else if comp != \"\" {\n\t\t\t\tmatcher += \"/\" + regexp.QuoteMeta(comp)\n\t\t\t}\n\t\t}\n\t\tif !strings.HasSuffix(parts[0], \".go\") {\n\t\t\tmatcher += \"/[^/]+\\\\.go\"\n\t\t}\n\t\tmatcher = matcher + \"$\"\n\n\t\tre, _ := regexp.Compile(matcher)\n\t\tfilter = append(filter, pattern{re, level})\n\t}\n\t// Swap out the vmodule pattern for the new filter system\n\th.lock.Lock()\n\tdefer h.lock.Unlock()\n\n\th.patterns = filter\n\th.siteCache = make(map[uintptr]slog.Level)\n\th.override.Store(len(filter) != 0)\n\n\treturn nil\n}\n\n// Enabled implements slog.Handler, reporting whether the handler handles records\n// at the given level.\nfunc (h *GlogHandler) Enabled(ctx context.Context, lvl slog.Level) bool {\n\t// fast-track skipping logging if override not enabled and the provided verbosity is above configured\n\treturn h.override.Load() || slog.Level(h.level.Load()) <= lvl\n}\n\n// WithAttrs implements slog.Handler, returning a new Handler whose attributes\n// consist of both the receiver's attributes and the arguments.\nfunc (h *GlogHandler) WithAttrs(attrs []slog.Attr) slog.Handler {\n\th.lock.RLock()\n\tsiteCache := maps.Clone(h.siteCache)\n\th.lock.RUnlock()\n\n\tpatterns := []pattern{}\n\tpatterns = append(patterns, h.patterns...)\n\n\tres := GlogHandler{\n\t\torigin:    h.origin.WithAttrs(attrs),\n\t\tpatterns:  patterns,\n\t\tsiteCache: siteCache,\n\t\tlocation:  h.location,\n\t}\n\n\tres.level.Store(h.level.Load())\n\tres.override.Store(h.override.Load())\n\treturn &res\n}\n\n// WithGroup implements slog.Handler, returning a new Handler with the given\n// group appended to the receiver's existing groups.\n//\n// Note, this function is not implemented.\nfunc (h *GlogHandler) WithGroup(name string) slog.Handler {\n\tpanic(\"not implemented\")\n}\n\n// Handle implements slog.Handler, filtering a log record through the global,\n// local and backtrace filters, finally emitting it if either allow it through.\nfunc (h *GlogHandler) Handle(_ context.Context, r slog.Record) error {\n\t// If the global log level allows, fast track logging\n\tif slog.Level(h.level.Load()) <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\n\t// Check callsite cache for previously calculated log levels\n\th.lock.RLock()\n\tlvl, ok := h.siteCache[r.PC]\n\th.lock.RUnlock()\n\n\t// If we didn't cache the callsite yet, calculate it\n\tif !ok {\n\t\th.lock.Lock()\n\n\t\tfs := runtime.CallersFrames([]uintptr{r.PC})\n\t\tframe, _ := fs.Next()\n\n\t\tfor _, rule := range h.patterns {\n\t\t\tif rule.pattern.MatchString(fmt.Sprintf(\"+%s\", frame.File)) {\n\t\t\t\th.siteCache[r.PC], lvl, ok = rule.level, rule.level, true\n\t\t\t}\n\t\t}\n\t\t// If no rule matched, remember to drop log the next time\n\t\tif !ok {\n\t\t\th.siteCache[r.PC] = 0\n\t\t}\n\t\th.lock.Unlock()\n\t}\n\tif lvl <= r.Level {\n\t\treturn h.origin.Handle(context.Background(), r)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/handler.go",
          "line": 963,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(hashes)\n\t\tpeer.AsyncSendTransactions(hashes)\n\t}\n\tfor peer, hashes := range annos {\n\t\tannCount += len(hashes)\n\t\tpeer.AsyncSendPooledTransactionHashes(hashes)\n\t}\n\tlog.Debug(\"Distributed transactions\", \"plaintxs\", len(txs)-blobTxs-largeTxs, \"blobtxs\", blobTxs, \"largetxs\", largeTxs,\n\t\t\"bcastpeers\", len(txset), \"bcastcount\", directCount, \"annpeers\", len(annos), \"anncount\", annCount)\n}\n\n// ReannounceTransactions will announce a batch of local pending transactions\n// to a square root of all peers.\nfunc (h *handler) ReannounceTransactions(txs types.Transactions) {\n\thashes := make([]common.Hash, 0, txs.Len())\n\tfor _, tx := range txs {\n\t\thashes = append(hashes, tx.Hash())\n\t}\n\n\t// Announce transactions hash to a batch of peers\n\tpeersCount := uint(math.Sqrt(float64(h.peers.len())))\n\tpeers := h.peers.headPeers(peersCount)\n\tfor _, peer := range peers {\n\t\tpeer.AsyncSendPooledTransactionHashes(hashes)\n\t}\n\tlog.Debug(\"Transaction reannounce\", \"txs\", len(txs),\n\t\t\"announce packs\", peersCount, \"announced hashes\", peersCount*uint(len(hashes)))\n}\n\n// BroadcastVote will propagate a batch of votes to all peers\n// which are not known to already have the given vote.\nfunc (h *handler) BroadcastVote(vote *types.VoteEnvelope) {\n\tvar (\n\t\tdirectCount int // Count of announcements made\n\t\tdirectPeers int\n\n\t\tvoteMap = make(map[*ethPeer]*types.VoteEnvelope) // Set peer->hash to transfer directly\n\t)\n\n\t// Broadcast vote to a batch of peers not knowing about it\n\tpeers := h.peers.peersWithoutVote(vote.Hash())\n\theadBlock := h.chain.CurrentBlock()\n\tcurrentTD := h.chain.GetTd(headBlock.Hash(), headBlock.Number.Uint64())\n\tfor _, peer := range peers {\n\t\t_, peerTD := peer.Head()\n\t\tdeltaTD := new(big.Int).Abs(new(big.Int).Sub(currentTD, peerTD))\n\t\tif deltaTD.Cmp(big.NewInt(deltaTdThreshold)) < 1 && peer.bscExt != nil {\n\t\t\tvoteMap[peer] = vote\n\t\t}\n\t}\n\n\tfor peer, _vote := range voteMap {\n\t\tdirectPeers++\n\t\tdirectCount += 1\n\t\tvotes := []*types.VoteEnvelope{_vote}\n\t\tpeer.bscExt.AsyncSendVotes(votes)\n\t}\n\tlog.Debug(\"Vote broadcast\", \"vote packs\", directPeers, \"broadcast vote\", directCount)\n}\n\n// minedBroadcastLoop sends mined blocks to connected peers.\nfunc (h *handler) minedBroadcastLoop() {\n\tdefer h.wg.Done()\n\n\tfor {\n\t\tselect {\n\t\tcase obj := <-h.minedBlockSub.Chan():\n\t\t\tif obj == nil {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif ev, ok := obj.Data.(core.NewSealedBlockEvent)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/handler.go",
          "line": 849,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.86,
          "confidence": 0.9774,
          "ensemble_confidence": 0.8796600000000001
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/handler.go",
          "line": 234,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "ValidatorAddressMap[address] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/backend.go",
          "line": 195,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= config.TrieDirtyCache * 3 / 5\n\t\t\tconfig.SnapshotCache += config.TrieDirtyCache * 2 / 5\n\t\t} else {\n\t\t\tconfig.TrieCleanCache += config.TrieDirtyCache\n\t\t}\n\t\tconfig.TrieDirtyCache = 0\n\t}\n\tif config.StateScheme == rawdb.PathScheme && config.TrieDirtyCache > pathdb.MaxDirtyBufferSize()/1024/1024 {\n\t\tlog.Info(\"Capped dirty cache size\", \"provided\", common.StorageSize(config.TrieDirtyCache)*1024*1024,\n\t\t\t\"adjusted\", common.StorageSize(pathdb.MaxDirtyBufferSize()))\n\t\tlog.Info(\"Clean cache size\", \"provided\", common.StorageSize(config.TrieCleanCache)*1024*1024,\n\t\t\t\"adjusted\", common.StorageSize(config.TrieCleanCache+config.TrieDirtyCache-pathdb.MaxDirtyBufferSize()/1024/1024)*1024*1024)\n\t\tconfig.TrieCleanCache += config.TrieDirtyCache - pathdb.MaxDirtyBufferSize()/1024/1024\n\t\tconfig.TrieDirtyCache = pathdb.MaxDirtyBufferSize() / 1024 / 1024\n\t}\n\tlog.Info(\"Allocated memory caches\",\n\t\t\"state_scheme\", config.StateScheme,\n\t\t\"trie_clean_cache\", common.StorageSize(config.TrieCleanCache)*1024*1024,\n\t\t\"trie_dirty_cache\", common.StorageSize(config.TrieDirtyCache)*1024*1024,\n\t\t\"snapshot_cache\", common.StorageSize(config.SnapshotCache)*1024*1024)\n\t// Try to recover offline state pruning only in hash-based.\n\tif config.StateScheme == rawdb.HashScheme {\n\t\tif err := pruner.RecoverPruning(stack.ResolvePath(\"\"), chainDb, config.TriesInMemory)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/handler_bsc_test.go",
          "line": 30,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 521,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(event.Txs)\n\t\t\tcase <-time.After(2 * time.Second):\n\t\t\t\tt.Errorf(\"sink %d: transaction propagation timed out: have %d, want %d\", i, arrived, len(txs))\n\t\t\t\ttimeout = true\n\t\t\t}\n\t\t}\n\t}\n}\n\n// Tests that local pending transactions get propagated to peers.\nfunc TestTransactionPendingReannounce(t *testing.T) {\n\tt.Parallel()\n\n\t// Create a source handler to announce transactions from and a sink handler\n\t// to receive them.\n\tsource := newTestHandler()\n\tdefer source.close()\n\n\tsink := newTestHandler()\n\tdefer sink.close()\n\tsink.handler.acceptTxs.Store(true) // mark synced to accept transactions\n\n\tsourcePipe, sinkPipe := p2p.MsgPipe()\n\tdefer sourcePipe.Close()\n\tdefer sinkPipe.Close()\n\n\tsourcePeer := eth.NewPeer(eth.ETH68, p2p.NewPeer(enode.ID{0}, \"\", nil), sourcePipe, source.txpool)\n\tsinkPeer := eth.NewPeer(eth.ETH68, p2p.NewPeer(enode.ID{0}, \"\", nil), sinkPipe, sink.txpool)\n\tdefer sourcePeer.Close()\n\tdefer sinkPeer.Close()\n\n\tgo source.handler.runEthPeer(sourcePeer, func(peer *eth.Peer) error {\n\t\treturn eth.Handle((*ethHandler)(source.handler), peer)\n\t})\n\tgo sink.handler.runEthPeer(sinkPeer, func(peer *eth.Peer) error {\n\t\treturn eth.Handle((*ethHandler)(sink.handler), peer)\n\t})\n\n\t// Subscribe transaction pools\n\ttxCh := make(chan core.NewTxsEvent, 1024)\n\tsub := sink.txpool.SubscribeTransactions(txCh, false)\n\tdefer sub.Unsubscribe()\n\n\ttxs := make([]*types.Transaction, 64)\n\tfor nonce := range txs {\n\t\ttx := types.NewTransaction(uint64(nonce), common.Address{}, big.NewInt(0), 100000, big.NewInt(0), nil)\n\t\ttx, _ = types.SignTx(tx, types.HomesteadSigner{}, testKey)\n\n\t\ttxs[nonce] = tx\n\t}\n\tsource.txpool.ReannouceTransactions(txs)\n\n\tfor arrived := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 576,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(event.Txs)\n\t\tcase <-time.NewTimer(time.Second).C:\n\t\t\tt.Errorf(\"sink: transaction propagation timed out: have %d, want %d\", arrived, len(txs))\n\t\t}\n\t}\n}\n\n// Tests that blocks are broadcast to a sqrt number of peers only.\nfunc TestBroadcastBlock1Peer(t *testing.T)    { testBroadcastBlock(t, 1, 1) }\nfunc TestBroadcastBlock2Peers(t *testing.T)   { testBroadcastBlock(t, 2, 1) }\nfunc TestBroadcastBlock3Peers(t *testing.T)   { testBroadcastBlock(t, 3, 1) }\nfunc TestBroadcastBlock4Peers(t *testing.T)   { testBroadcastBlock(t, 4, 2) }\nfunc TestBroadcastBlock5Peers(t *testing.T)   { testBroadcastBlock(t, 5, 2) }\nfunc TestBroadcastBlock8Peers(t *testing.T)   { testBroadcastBlock(t, 9, 3) }\nfunc TestBroadcastBlock12Peers(t *testing.T)  { testBroadcastBlock(t, 12, 3) }\nfunc TestBroadcastBlock16Peers(t *testing.T)  { testBroadcastBlock(t, 16, 4) }\nfunc TestBroadcastBloc26Peers(t *testing.T)   { testBroadcastBlock(t, 26, 5) }\nfunc TestBroadcastBlock100Peers(t *testing.T) { testBroadcastBlock(t, 100, 10) }\n\nfunc testBroadcastBlock(t *testing.T, peers, bcasts int) {\n\tt.Parallel()\n\n\t// Create a source handler to broadcast blocks from and a number of sinks\n\t// to receive them.\n\tsource := newTestHandlerWithBlocks(1)\n\tdefer source.close()\n\n\tsinks := make([]*testEthHandler, peers)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 58,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 62,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 66,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/handler_eth_test.go",
          "line": 70,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/state_accessor.go",
          "line": 261,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/handler_test.go",
          "line": 130,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/handler_test.go",
          "line": 142,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/handler_test.go",
          "line": 376,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/handler_test.go",
          "line": 379,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/api_debug.go",
          "line": 407,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= delta {\n\t\tif time.Since(lastLog) > 8*time.Second {\n\t\t\tlog.Info(\"Finding roots\", \"from\", start, \"to\", end, \"at\", i)\n\t\t\tlastLog = time.Now()\n\t\t}\n\t\tif i < int64(pivot) {\n\t\t\tcontinue\n\t\t}\n\t\th := api.eth.BlockChain().GetHeaderByNumber(uint64(i))\n\t\tif h == nil {\n\t\t\treturn 0, fmt.Errorf(\"missing header %d\", i)\n\t\t}\n\t\tif ok, _ := api.eth.ChainDb().Has(h.Root[:])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/build/ci.go",
          "line": 142,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \".exe\"\n\t}\n\treturn filepath.Join(GOBIN, name)\n}\n\nfunc main() {\n\tlog.SetFlags(log.Lshortfile)\n\n\tif !build.FileExist(filepath.Join(\"build\", \"ci.go\")) {\n\t\tlog.Fatal(\"this script must be run from the root of the repository\")\n\t}\n\tif len(os.Args) < 2 {\n\t\tlog.Fatal(\"need subcommand as first argument\")\n\t}\n\tswitch os.Args[1] {\n\tcase \"install\":\n\t\tdoInstall(os.Args[2:])\n\tcase \"test\":\n\t\tdoTest(os.Args[2:])\n\tcase \"lint\":\n\t\tdoLint(os.Args[2:])\n\tcase \"check_generate\":\n\t\tdoCheckGenerate()\n\tcase \"check_baddeps\":\n\t\tdoCheckBadDeps()\n\tcase \"archive\":\n\t\tdoArchive(os.Args[2:])\n\tcase \"dockerx\":\n\t\tdoDockerBuildx(os.Args[2:])\n\tcase \"debsrc\":\n\t\tdoDebianSource(os.Args[2:])\n\tcase \"nsis\":\n\t\tdoWindowsInstaller(os.Args[2:])\n\tcase \"purge\":\n\t\tdoPurge(os.Args[2:])\n\tcase \"sanitycheck\":\n\t\tdoSanityCheck()\n\tdefault:\n\t\tlog.Fatal(\"unknown command \", os.Args[1])\n\t}\n}\n\n// Compiling\n\nfunc doInstall(cmdline []string) {\n\tvar (\n\t\tdlgo       = flag.Bool(\"dlgo\", false, \"Download Go and build with it\")\n\t\tarch       = flag.String(\"arch\", \"\", \"Architecture to cross build for\")\n\t\tcc         = flag.String(\"cc\", \"\", \"C compiler to cross build with\")\n\t\tstaticlink = flag.Bool(\"static\", false, \"Create statically-linked executable\")\n\t\toutput     = flag.String(\"o\", \"\", \"Output directory for build artifacts\")\n\t)\n\tflag.CommandLine.Parse(cmdline)\n\tenv := build.Env()\n\n\t// Configure the toolchain.\n\ttc := build.GoToolchain{GOARCH: *arch, CC: *cc}\n\tif *dlgo {\n\t\tcsdb := download.MustLoadChecksums(\"build/checksums.txt\")\n\t\ttc.Root = build.DownloadGo(csdb)\n\t}\n\t// Disable CLI markdown doc generation in release builds.\n\tbuildTags := []string{\"urfave_cli_no_docs\"}\n\n\t// Enable linking the CKZG library since we can make it work with additional flags.\n\tif env.UbuntuVersion != \"trusty\" {\n\t\tbuildTags = append(buildTags, \"ckzg\")\n\t}\n\n\t// Configure the build.\n\tgobuild := tc.Go(\"build\", buildFlags(env, *staticlink, buildTags)...)\n\n\t// We use -trimpath to avoid leaking local paths into the built executables.\n\tgobuild.Args = append(gobuild.Args, \"-trimpath\")\n\n\t// Show packages during build.\n\tgobuild.Args = append(gobuild.Args, \"-v\")\n\n\t// Now we choose what we're even building.\n\t// Default: collect all 'main' packages in cmd/ and build those.\n\tpackages := flag.Args()\n\tif len(packages) == 0 {\n\t\tpackages = build.FindMainPackages(\"./cmd\")\n\t}\n\n\t// Do the build!\n\tfor _, pkg := range packages {\n\t\targs := slices.Clone(gobuild.Args)\n\t\toutputPath := executablePath(path.Base(pkg))\n\t\tif output != nil && *output != \"\" {\n\t\t\toutputPath = *output\n\t\t}\n\t\targs = append(args, \"-o\", outputPath)\n\t\targs = append(args, pkg)\n\t\tbuild.MustRun(&exec.Cmd{Path: gobuild.Path, Args: args, Env: gobuild.Env})\n\t}\n}\n\n// buildFlags returns the go tool flags for building.\nfunc buildFlags(env build.Environment, staticLinking bool, buildTags []string) (flags []string) {\n\tvar ld []string\n\t// See https://github.com/golang/go/issues/33772#issuecomment-528176001\n\t// We need to set --buildid to the linker here, and also pass --build-id to the\n\t// cgo-linker further down.\n\tld = append(ld, \"--buildid=none\")\n\tif env.Commit != \"\" {\n\t\tld = append(ld, \"-X\", \"github.com/ethereum/go-ethereum/internal/version.gitCommit=\"+env.Commit)\n\t\tld = append(ld, \"-X\", \"github.com/ethereum/go-ethereum/internal/version.gitDate=\"+env.Date)\n\t}\n\t// Strip DWARF on darwin. This used to be required for certain things,\n\t// and there is no downside to this, so we just keep doing it.\n\tif runtime.GOOS == \"darwin\" {\n\t\tld = append(ld, \"-s\")\n\t}\n\tif runtime.GOOS == \"linux\" {\n\t\t// Enforce the stacksize to 8M, which is the case on most platforms apart from\n\t\t// alpine Linux.\n\t\t// See https://sourceware.org/binutils/docs-2.23.1/ld/Options.html#Options\n\t\t// regarding the options --build-id=none and --strip-all. It is needed for\n\t\t// reproducible builds",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/build/ci.go",
          "line": 468,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"v\" + os.Getenv(\"GOARM\")\n\t}\n\tbase := fmt.Sprintf(\"golangci-lint-%s-%s-%s\", version, runtime.GOOS, arch)\n\tarchivePath := filepath.Join(cachedir, base+ext)\n\tif err := csdb.DownloadFileFromKnownURL(archivePath)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/build/ci.go",
          "line": 516,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \".zip\"\n\t} else {\n\t\tarchiveName += \".tar.gz\"\n\t}\n\n\tarchivePath := path.Join(cachedir, archiveName)\n\tif err := csdb.DownloadFileFromKnownURL(archivePath)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/build/ci.go",
          "line": 610,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= os.Getenv(\"GOARM\")\n\t}\n\tif arch == \"android\" {\n\t\tplatform = \"android-all\"\n\t}\n\tif arch == \"ios\" {\n\t\tplatform = \"ios-all\"\n\t}\n\treturn platform + \"-\" + archiveVersion\n}\n\nfunc archiveUpload(archive string, blobstore string, signer string, signifyVar string) error {\n\t// If signing was requested, generate the signature files\n\tif signer != \"\" {\n\t\tkey := getenvBase64(signer)\n\t\tif err := build.PGPSignFile(archive, archive+\".asc\", string(key))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/build/ci.go",
          "line": 988,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"+build\" + meta.Env.Buildnum\n\t}\n\tif meta.Distro != \"\" {\n\t\tvsn += \"+\" + meta.Distro\n\t}\n\treturn vsn\n}\n\n// ExeList returns the list of all executable packages.\nfunc (meta debMetadata) ExeList() string {\n\tnames := make([]string, len(meta.Executables))\n\tfor i, e := range meta.Executables {\n\t\tnames[i] = meta.ExeName(e)\n\t}\n\treturn strings.Join(names, \", \")\n}\n\n// ExeName returns the package name of an executable package.\nfunc (meta debMetadata) ExeName(exe debExecutable) string {\n\tif isUnstableBuild(meta.Env) {\n\t\treturn exe.Package() + \"-unstable\"\n\t}\n\treturn exe.Package()\n}\n\n// ExeConflicts returns the content of the Conflicts field\n// for executable packages.\nfunc (meta debMetadata) ExeConflicts(exe debExecutable) string {\n\tif isUnstableBuild(meta.Env) {\n\t\t// Set up the conflicts list so that the *-unstable packages\n\t\t// cannot be installed alongside the regular version.\n\t\t//\n\t\t// https://www.debian.org/doc/debian-policy/ch-relationships.html\n\t\t// is very explicit about Conflicts: and says that Breaks: should\n\t\t// be preferred and the conflicting files should be handled via\n\t\t// alternates. We might do this eventually but using a conflict is\n\t\t// easier now.\n\t\treturn \"ethereum, \" + exe.Package()\n\t}\n\treturn \"\"\n}\n\nfunc stageDebianSource(tmpdir string, meta debMetadata) (pkgdir string) {\n\tpkg := meta.Name() + \"-\" + meta.VersionString()\n\tpkgdir = filepath.Join(tmpdir, pkg)\n\tif err := os.Mkdir(pkgdir, 0755)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/build/ci.go",
          "line": 1112,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"-\" + env.Commit[:8]\n\t}\n\tinstaller, err := filepath.Abs(\"geth-\" + archiveBasename(*arch, version.Archive(env.Commit)) + \".exe\")\n\tif err != nil {\n\t\tlog.Fatalf(\"Failed to convert installer file path: %v\", err)\n\t}\n\tbuild.MustRunCommand(\"makensis.exe\",\n\t\t\"/DOUTPUTFILE=\"+installer,\n\t\t\"/DMAJORVERSION=\"+ver[0],\n\t\t\"/DMINORVERSION=\"+ver[1],\n\t\t\"/DBUILDVERSION=\"+ver[2],\n\t\t\"/DARCH=\"+*arch,\n\t\tfilepath.Join(*workdir, \"geth.nsi\"),\n\t)\n\t// Sign and publish installer.\n\tif err := archiveUpload(installer, *upload, *signer, *signify)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/node/config.go",
          "line": 347,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"/\" + c.UserIdent\n\t}\n\tif c.Version != \"\" {\n\t\tname += \"/v\" + c.Version\n\t}\n\tname += \"/\" + runtime.GOOS + \"-\" + runtime.GOARCH\n\tname += \"/\" + runtime.Version()\n\treturn name\n}\n\nfunc (c *Config) name() string {\n\tif c.Name == \"\" {\n\t\tprogname := strings.TrimSuffix(filepath.Base(os.Args[0]), \".exe\")\n\t\tif progname == \"\" {\n\t\t\tpanic(\"empty executable name, set Config.Name\")\n\t\t}\n\t\treturn progname\n\t}\n\treturn c.Name\n}\n\n// These resources are resolved differently for \"geth\" instances.\nvar isOldGethResource = map[string]bool{\n\t\"chaindata\":          true,\n\t\"nodes\":              true,\n\t\"nodekey\":            true,\n\t\"static-nodes.json\":  false, // no warning for these because they have their\n\t\"trusted-nodes.json\": false, // own separate warning.\n}\n\n// ResolvePath resolves path in the instance directory.\nfunc (c *Config) ResolvePath(path string) string {\n\tif filepath.IsAbs(path) {\n\t\treturn path\n\t}\n\tif c.DataDir == \"\" {\n\t\treturn \"\"\n\t}\n\t// Backwards-compatibility: ensure that data directory files created\n\t// by geth 1.4 are used if they exist.\n\tif warn, isOld := isOldGethResource[path]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/node/rpcstack_test.go",
          "line": 264,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey, value := extraHeaders[i], extraHeaders[i+1]\n\t\theaders.Set(key, value)\n\t}\n\tconn, _, err := websocket.DefaultDialer.Dial(url, headers)\n\tif conn != nil {\n\t\tconn.Close()\n\t}\n\treturn err\n}\n\n// rpcRequest performs a JSON-RPC request to the given URL.\nfunc rpcRequest(t *testing.T, url, method string, extraHeaders ...string) *http.Response {\n\tt.Helper()\n\n\tbody := fmt.Sprintf(`{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"%s\",\"params\":[]}`, method)\n\treturn baseRpcRequest(t, url, body, extraHeaders...)\n}\n\nfunc batchRpcRequest(t *testing.T, url string, methods []string, extraHeaders ...string) *http.Response {\n\treqs := make([]string, len(methods))\n\tfor i, m := range methods {\n\t\treqs[i] = fmt.Sprintf(`{\"jsonrpc\":\"2.0\",\"id\":1,\"method\":\"%s\",\"params\":[]}`, m)\n\t}\n\tbody := fmt.Sprintf(`[%s]`, strings.Join(reqs, \",\"))\n\treturn baseRpcRequest(t, url, body, extraHeaders...)\n}\n\nfunc baseRpcRequest(t *testing.T, url, bodyStr string, extraHeaders ...string) *http.Response {\n\tt.Helper()\n\n\t// Create the request.\n\tbody := bytes.NewReader([]byte(bodyStr))\n\treq, err := http.NewRequest(http.MethodPost, url, body)\n\tif err != nil {\n\t\tt.Fatal(\"could not create http request:\", err)\n\t}\n\treq.Header.Set(\"content-type\", \"application/json\")\n\treq.Header.Set(\"accept-encoding\", \"identity\")\n\n\t// Apply extra headers.\n\tif len(extraHeaders)%2 != 0 {\n\t\tpanic(\"odd extraHeaders length\")\n\t}\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/node/rpcstack_test.go",
          "line": 308,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey, value := extraHeaders[i], extraHeaders[i+1]\n\t\tif strings.EqualFold(key, \"host\") {\n\t\t\treq.Host = value\n\t\t} else {\n\t\t\treq.Header.Set(key, value)\n\t\t}\n\t}\n\n\t// Perform the request.\n\tt.Logf(\"checking RPC/HTTP on %s %v\", url, extraHeaders)\n\tresp, err := http.DefaultClient.Do(req)\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tt.Cleanup(func() { resp.Body.Close() })\n\treturn resp\n}\n\ntype testClaim map[string]interface{}\n\nfunc (testClaim) Valid() error {\n\treturn nil\n}\n\nfunc TestJWT(t *testing.T) {\n\tvar secret = []byte(\"secret\")\n\tissueToken := func(secret []byte, method jwt.SigningMethod, input map[string]interface{}) string {\n\t\tif method == nil {\n\t\t\tmethod = jwt.SigningMethodHS256\n\t\t}\n\t\tss, _ := jwt.NewWithClaims(method, testClaim(input)).SignedString(secret)\n\t\treturn ss\n\t}\n\tcfg := rpcEndpointConfig{jwtSecret: []byte(\"secret\")}\n\thttpcfg := &httpConfig{rpcEndpointConfig: cfg}\n\twscfg := &wsConfig{Origins: []string{\"*\"}, rpcEndpointConfig: cfg}\n\tsrv := createAndStartServer(t, httpcfg, true, wscfg, nil)\n\twsUrl := fmt.Sprintf(\"ws://%v\", srv.listenAddr())\n\thtUrl := fmt.Sprintf(\"http://%v\", srv.listenAddr())\n\n\texpOk := []func() string{\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\"iat\": time.Now().Unix()}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\"iat\": time.Now().Unix() + 4}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\"iat\": time.Now().Unix() - 4}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\n\t\t\t\t\"iat\": time.Now().Unix(),\n\t\t\t\t\"exp\": time.Now().Unix() + 2,\n\t\t\t}))\n\t\t},\n\t\tfunc() string {\n\t\t\treturn fmt.Sprintf(\"Bearer %v\", issueToken(secret, nil, testClaim{\n\t\t\t\t\"iat\": time.Now().Unix(),\n\t\t\t\t\"bar\": \"baz\",\n\t\t\t}))\n\t\t},\n\t}\n\tfor i, tokenFn := range expOk {\n\t\ttoken := tokenFn()\n\t\tif err := wsRequest(t, wsUrl, \"Authorization\", token)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/node/node_auth_test.go",
          "line": 217,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tif i > len(provs) {\n\t\t\ti = len(provs)\n\t\t}\n\t\treturn provs[i-1](header)\n\t}\n}\n\nfunc offsetTimeAuth(secret [32]byte, offset time.Duration) rpc.HTTPAuth {\n\treturn func(header http.Header) error {\n\t\ttoken := jwt.NewWithClaims(jwt.SigningMethodHS256, jwt.MapClaims{\n\t\t\t\"iat\": &jwt.NumericDate{Time: time.Now().Add(offset)},\n\t\t})\n\t\ts, err := token.SignedString(secret[:])\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"failed to create JWT token: %w\", err)\n\t\t}\n\t\theader.Set(\"Authorization\", \"Bearer \"+s)\n\t\treturn nil\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/node/rpcstack.go",
          "line": 165,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= h.wsConfig.prefix\n\t\t}\n\t\th.log.Info(\"WebSocket enabled\", \"url\", url)\n\t}\n\t// if server is websocket only, return after logging\n\tif !h.rpcAllowed() {\n\t\treturn nil\n\t}\n\t// Log http endpoint.\n\th.log.Info(\"HTTP server started\",\n\t\t\"endpoint\", listener.Addr(), \"auth\", (h.httpConfig.jwtSecret != nil),\n\t\t\"prefix\", h.httpConfig.prefix,\n\t\t\"cors\", strings.Join(h.httpConfig.CorsAllowedOrigins, \",\"),\n\t\t\"vhosts\", strings.Join(h.httpConfig.Vhosts, \",\"),\n\t)\n\n\t// Log all handlers mounted on server.\n\tvar paths []string\n\tfor path := range h.handlerNames {\n\t\tpaths = append(paths, path)\n\t}\n\tsort.Strings(paths)\n\tlogged := make(map[string]bool, len(paths))\n\tfor _, path := range paths {\n\t\tname := h.handlerNames[path]\n\t\tif !logged[name] {\n\t\t\tlog.Info(name+\" enabled\", \"url\", \"http://\"+listener.Addr().String()+path)\n\t\t\tlogged[name] = true\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (h *httpServer) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n\t// check if ws request and serve if ws enabled\n\tws := h.wsHandler.Load()\n\tif ws != nil && isWebsocket(r) {\n\t\tif checkPath(r, ws.prefix) {\n\t\t\tws.ServeHTTP(w, r)\n\t\t}\n\t\treturn\n\t}\n\n\t// if http-rpc is enabled, try to serve request\n\trpc := h.httpHandler.Load()\n\tif rpc != nil {\n\t\t// First try to route in the mux.\n\t\t// Requests to a path below root are handled by the mux,\n\t\t// which has all the handlers registered via Node.RegisterHandler.\n\t\t// These are made available when RPC is enabled.\n\t\tmuxHandler, pattern := h.mux.Handler(r)\n\t\tif pattern != \"\" {\n\t\t\tmuxHandler.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\n\t\tif checkPath(r, rpc.prefix) {\n\t\t\trpc.ServeHTTP(w, r)\n\t\t\treturn\n\t\t}\n\t}\n\tw.WriteHeader(http.StatusNotFound)\n}\n\n// checkPath checks whether a given request URL matches a given path prefix.\nfunc checkPath(r *http.Request, path string) bool {\n\t// if no prefix has been specified, request URL must be on root\n\tif path == \"\" {\n\t\treturn r.URL.Path == \"/\"\n\t}\n\t// otherwise, check to make sure prefix matches\n\treturn len(r.URL.Path) >= len(path) && r.URL.Path[:len(path)] == path\n}\n\n// validatePrefix checks if 'path' is a valid configuration value for the RPC prefix option.\nfunc validatePrefix(what, path string) error {\n\tif path == \"\" {\n\t\treturn nil\n\t}\n\tif path[0] != '/' {\n\t\treturn fmt.Errorf(`%s RPC path prefix %q does not contain leading \"/\"`, what, path)\n\t}\n\tif strings.ContainsAny(path, \"?#\") {\n\t\t// This is just to avoid confusion. While these would match correctly (i.e. they'd\n\t\t// match if URL-escaped into path), it's not easy to understand for users when\n\t\t// setting that on the command line.\n\t\treturn fmt.Errorf(\"%s RPC path prefix %q contains URL meta-characters\", what, path)\n\t}\n\treturn nil\n}\n\n// stop shuts down the HTTP server.\nfunc (h *httpServer) stop() {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\th.doStop()\n}\n\nfunc (h *httpServer) doStop() {\n\tif h.listener == nil {\n\t\treturn // not running\n\t}\n\n\t// Shut down the server.\n\thttpHandler := h.httpHandler.Load()\n\twsHandler := h.wsHandler.Load()\n\tif httpHandler != nil {\n\t\th.httpHandler.Store(nil)\n\t\thttpHandler.server.Stop()\n\t}\n\tif wsHandler != nil {\n\t\th.wsHandler.Store(nil)\n\t\twsHandler.server.Stop()\n\t}\n\n\tctx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)\n\tdefer cancel()\n\terr := h.server.Shutdown(ctx)\n\tif err != nil && err == ctx.Err() {\n\t\th.log.Warn(\"HTTP server graceful shutdown timed out\")\n\t\th.server.Close()\n\t}\n\n\th.listener.Close()\n\th.log.Info(\"HTTP server stopped\", \"endpoint\", h.listener.Addr())\n\n\t// Clear out everything to allow re-configuring it later.\n\th.host, h.port, h.endpoint = \"\", 0, \"\"\n\th.server, h.listener = nil, nil\n}\n\n// enableRPC turns on JSON-RPC over HTTP on the server.\nfunc (h *httpServer) enableRPC(apis []rpc.API, config httpConfig) error {\n\th.mu.Lock()\n\tdefer h.mu.Unlock()\n\n\tif h.rpcAllowed() {\n\t\treturn errors.New(\"JSON-RPC over HTTP is already enabled\")\n\t}\n\n\t// Create RPC server and handler.\n\tsrv := rpc.NewServer()\n\tsrv.SetBatchLimits(config.batchItemLimit, config.batchResponseSizeLimit)\n\tif config.httpBodyLimit > 0 {\n\t\tsrv.SetHTTPBodyLimit(config.httpBodyLimit)\n\t}\n\tif err := RegisterApis(apis, config.Modules, srv)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/node/rpcstack.go",
          "line": 546,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(n)\n\tif w.hasLength && w.written >= w.contentLength {\n\t\t// The HTTP handler has finished writing the entire uncompressed response. Close\n\t\t// the gzip stream to ensure the footer will be seen by the client in case the\n\t\t// response is flushed after this call to write.\n\t\terr = w.gz.Close()\n\t}\n\treturn n, err\n}\n\nfunc (w *gzipResponseWriter) Flush() {\n\tif w.gz != nil {\n\t\tw.gz.Flush()\n\t}\n\tif f, ok := w.resp.(http.Flusher)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/subscription.go",
          "line": 142,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/subscription.go",
          "line": 165,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/handler.go",
          "line": 240,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(resp.Result)\n\t\t\t\tif responseBytes > h.batchResponseMaxSize {\n\t\t\t\t\terr := &internalServerError{errcodeResponseTooLarge, errMsgResponseTooLarge}\n\t\t\t\t\tcallBuffer.respondWithError(cp.ctx, h.conn, err)\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif timer != nil {\n\t\t\ttimer.Stop()\n\t\t}\n\n\t\th.addSubscriptions(cp.notifiers)\n\t\tcallBuffer.write(cp.ctx, h.conn)\n\t\tfor _, n := range cp.notifiers {\n\t\t\tn.activate()\n\t\t}\n\t})\n}\n\nfunc (h *handler) respondWithBatchTooLarge(cp *callProc, batch []*jsonrpcMessage) {\n\tresp := errorMessage(&invalidRequestError{errMsgBatchTooLarge})\n\t// Find the first call and add its \"id\" field to the error.\n\t// This is the best we can do, given that the protocol doesn't have a way\n\t// of reporting an error for the entire batch.\n\tfor _, msg := range batch {\n\t\tif msg.isCall() {\n\t\t\tresp.ID = msg.ID\n\t\t\tbreak\n\t\t}\n\t}\n\th.conn.writeJSON(cp.ctx, []*jsonrpcMessage{resp}, true)\n}\n\n// handleMsg handles a single non-batch message.\nfunc (h *handler) handleMsg(ctx context.Context, msg *jsonrpcMessage) {\n\tmsgs := []*jsonrpcMessage{msg}\n\th.handleResponses(msgs, func(msg *jsonrpcMessage) {\n\t\th.startCallProc(func(cp *callProc) {\n\t\t\th.handleNonBatchCall(cp, ctx, msg)\n\t\t})\n\t})\n}\n\nfunc (h *handler) handleNonBatchCall(cp *callProc, reqCtx context.Context, msg *jsonrpcMessage) {\n\tvar (\n\t\tresponded sync.Once\n\t\ttimer     *time.Timer\n\t\tcancel    context.CancelFunc\n\t)\n\tcp.ctx, cancel = context.WithCancel(cp.ctx)\n\tdefer cancel()\n\n\t// Cancel the request context after timeout and send an error response. Since the\n\t// running method might not return immediately on timeout, we must wait for the\n\t// timeout concurrently with processing the request.\n\tif timeout, ok := ContextRequestTimeout(cp.ctx)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/handler.go",
          "line": 593,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/testservice_test.go",
          "line": 145,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/testservice_test.go",
          "line": 157,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/service.go",
          "line": 205,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/http_test.go",
          "line": 138,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/http_test.go",
          "line": 162,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/rpc/http_test.go",
          "line": 204,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/websocket_test.go",
          "line": 101,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/websocket_test.go",
          "line": 110,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/rpc/websocket_test.go",
          "line": 153,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/rpc/websocket_test.go",
          "line": 161,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/rpc/websocket_test.go",
          "line": 196,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/rpc/websocket_test.go",
          "line": 284,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/rpc/websocket_test.go",
          "line": 453,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/client.go",
          "line": 354,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/client.go",
          "line": 423,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/rpc/client.go",
          "line": 482,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/rpc/client.go",
          "line": 527,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/http.go",
          "line": 401,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 100 * time.Millisecond\n\t\tsetTimeout(wt)\n\t}\n\n\treturn timeout, hasTimeout\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/websocket.go",
          "line": 125,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" (HTTP status \" + e.status + \")\"\n\t}\n\treturn s\n}\n\nfunc (e wsHandshakeError) Unwrap() error {\n\treturn e.err\n}\n\nfunc originIsAllowed(allowedOrigins mapset.Set[string], browserOrigin string) bool {\n\tit := allowedOrigins.Iterator()\n\tfor origin := range it.C {\n\t\tif ruleAllowsOrigin(origin, browserOrigin) {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc ruleAllowsOrigin(allowedOrigin string, browserOrigin string) bool {\n\tvar (\n\t\tallowedScheme, allowedHostname, allowedPort string\n\t\tbrowserScheme, browserHostname, browserPort string\n\t\terr                                         error\n\t)\n\tallowedScheme, allowedHostname, allowedPort, err = parseOriginURL(allowedOrigin)\n\tif err != nil {\n\t\tlog.Warn(\"Error parsing allowed origin specification\", \"spec\", allowedOrigin, \"error\", err)\n\t\treturn false\n\t}\n\tbrowserScheme, browserHostname, browserPort, err = parseOriginURL(browserOrigin)\n\tif err != nil {\n\t\tlog.Warn(\"Error parsing browser 'Origin' field\", \"Origin\", browserOrigin, \"error\", err)\n\t\treturn false\n\t}\n\tif allowedScheme != \"\" && allowedScheme != browserScheme {\n\t\treturn false\n\t}\n\tif allowedHostname != \"\" && allowedHostname != browserHostname {\n\t\treturn false\n\t}\n\tif allowedPort != \"\" && allowedPort != browserPort {\n\t\treturn false\n\t}\n\treturn true\n}\n\nfunc parseOriginURL(origin string) (string, string, string, error) {\n\tparsedURL, err := url.Parse(strings.ToLower(origin))\n\tif err != nil {\n\t\treturn \"\", \"\", \"\", err\n\t}\n\tvar scheme, hostname, port string\n\tif strings.Contains(origin, \"://\") {\n\t\tscheme = parsedURL.Scheme\n\t\thostname = parsedURL.Hostname()\n\t\tport = parsedURL.Port()\n\t} else {\n\t\tscheme = \"\"\n\t\thostname = parsedURL.Scheme\n\t\tport = parsedURL.Opaque\n\t\tif hostname == \"\" {\n\t\t\thostname = origin\n\t\t}\n\t}\n\treturn scheme, hostname, port, nil\n}\n\n// DialWebsocketWithDialer creates a new RPC client using WebSocket.\n//\n// The context is used for the initial connection establishment. It does not\n// affect subsequent interactions with the client.\n//\n// Deprecated: use DialOptions and the WithWebsocketDialer option.\nfunc DialWebsocketWithDialer(ctx context.Context, endpoint, origin string, dialer websocket.Dialer) (*Client, error) {\n\tcfg := new(clientConfig)\n\tcfg.wsDialer = &dialer\n\tif origin != \"\" {\n\t\tcfg.setHeader(\"origin\", origin)\n\t}\n\tconnect, err := newClientTransportWS(endpoint, cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newClient(ctx, cfg, connect)\n}\n\n// DialWebsocket creates a new RPC client that communicates with a JSON-RPC server\n// that is listening on the given endpoint.\n//\n// The context is used for the initial connection establishment. It does not\n// affect subsequent interactions with the client.\nfunc DialWebsocket(ctx context.Context, endpoint, origin string) (*Client, error) {\n\tcfg := new(clientConfig)\n\tif origin != \"\" {\n\t\tcfg.setHeader(\"origin\", origin)\n\t}\n\tconnect, err := newClientTransportWS(endpoint, cfg)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn newClient(ctx, cfg, connect)\n}\n\nfunc newClientTransportWS(endpoint string, cfg *clientConfig) (reconnectFunc, error) {\n\tdialer := cfg.wsDialer\n\tif dialer == nil {\n\t\tdialer = &websocket.Dialer{\n\t\t\tReadBufferSize:  wsReadBuffer,\n\t\t\tWriteBufferSize: wsWriteBuffer,\n\t\t\tWriteBufferPool: wsBufferPool,\n\t\t\tProxy:           http.ProxyFromEnvironment,\n\t\t}\n\t}\n\n\tdialURL, header, err := wsClientHeaders(endpoint, \"\")\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tmaps.Copy(header, cfg.httpHeaders)\n\n\tconnect := func(ctx context.Context) (ServerCodec, error) {\n\t\theader := header.Clone()\n\t\tif cfg.httpAuth != nil {\n\t\t\tif err := cfg.httpAuth(header)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rpc/client_test.go",
          "line": 49,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/rpc/client_test.go",
          "line": 65,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/rpc/client_test.go",
          "line": 70,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/rpc/client_test.go",
          "line": 87,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/rpc/client_test.go",
          "line": 108,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/rpc/client_test.go",
          "line": 816,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/subscription.go",
          "line": 216,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 2\n\t\tif s.waitTime > s.backoffMax {\n\t\t\ts.waitTime = s.backoffMax\n\t\t}\n\t}\n\n\tt := time.NewTimer(s.waitTime)\n\tdefer t.Stop()\n\tselect {\n\tcase <-t.C:\n\t\treturn false\n\tcase <-s.unsub:\n\t\treturn true\n\t}\n}\n\n// SubscriptionScope provides a facility to unsubscribe multiple subscriptions at once.\n//\n// For code that handle more than one subscription, a scope can be used to conveniently\n// unsubscribe all of them with a single call. The example demonstrates a typical use in a\n// larger program.\n//\n// The zero value is ready to use.\ntype SubscriptionScope struct {\n\tmu     sync.Mutex\n\tsubs   map[*scopeSub]struct{}\n\tclosed bool\n}\n\ntype scopeSub struct {\n\tsc *SubscriptionScope\n\ts  Subscription\n}\n\n// Track starts tracking a subscription. If the scope is closed, Track returns nil. The\n// returned subscription is a wrapper. Unsubscribing the wrapper removes it from the\n// scope.\nfunc (sc *SubscriptionScope) Track(s Subscription) Subscription {\n\tsc.mu.Lock()\n\tdefer sc.mu.Unlock()\n\tif sc.closed {\n\t\treturn nil\n\t}\n\tif sc.subs == nil {\n\t\tsc.subs = make(map[*scopeSub]struct{})\n\t}\n\tss := &scopeSub{sc, s}\n\tsc.subs[ss] = struct{}{}\n\treturn ss\n}\n\n// Close calls Unsubscribe on all tracked subscriptions and prevents further additions to\n// the tracked set. Calls to Track after Close return nil.\nfunc (sc *SubscriptionScope) Close() {\n\tsc.mu.Lock()\n\tdefer sc.mu.Unlock()\n\tif sc.closed {\n\t\treturn\n\t}\n\tsc.closed = true\n\tfor s := range sc.subs {\n\t\ts.s.Unsubscribe()\n\t}\n\tsc.subs = nil\n}\n\n// Count returns the number of tracked subscriptions.\n// It is meant to be used for debugging.\nfunc (sc *SubscriptionScope) Count() int {\n\tsc.mu.Lock()\n\tdefer sc.mu.Unlock()\n\treturn len(sc.subs)\n}\n\nfunc (s *scopeSub) Unsubscribe() {\n\ts.s.Unsubscribe()\n\ts.sc.mu.Lock()\n\tdefer s.sc.mu.Unlock()\n\tdelete(s.sc.subs, s)\n}\n\nfunc (s *scopeSub) Err() <-chan error {\n\treturn s.s.Err()\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/feed.go",
          "line": 228,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \", \"\n//             }\n//             switch cas.Dir {\n//             case reflect.SelectSend:\n//                     s += fmt.Sprintf(\"%v<-\", cas.Chan.Interface())\n//             case reflect.SelectRecv:\n//                     s += fmt.Sprintf(\"<-%v\", cas.Chan.Interface())\n//             }\n//     }\n//     return s + \"]\"\n// }\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/feedof_test.go",
          "line": 64,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/event/feedof_test.go",
          "line": 67,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/event/feedof_test.go",
          "line": 83,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/event/feedof_test.go",
          "line": 130,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/event/feedof_test.go",
          "line": 167,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/event/feedof_test.go",
          "line": 199,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/event/feedof_test.go",
          "line": 216,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/event/feedof_test.go",
          "line": 272,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/multisub_test.go",
          "line": 38,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/event/multisub_test.go",
          "line": 48,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/event/multisub_test.go",
          "line": 65,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/event/multisub_test.go",
          "line": 72,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/event/multisub_test.go",
          "line": 102,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/event/multisub_test.go",
          "line": 109,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/event/multisub_test.go",
          "line": 126,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/event/multisub_test.go",
          "line": 133,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/event/multisub_test.go",
          "line": 162,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/event/multisub_test.go",
          "line": 169,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/example_feed_test.go",
          "line": 58,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/example_scope_test.go",
          "line": 36,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/event/example_scope_test.go",
          "line": 42,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/event/feed_test.go",
          "line": 31,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/event/feed_test.go",
          "line": 33,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/event/feed_test.go",
          "line": 42,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/event/feed_test.go",
          "line": 48,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/event/feed_test.go",
          "line": 120,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/event/feed_test.go",
          "line": 123,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/event/feed_test.go",
          "line": 139,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/event/feed_test.go",
          "line": 186,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/event/feed_test.go",
          "line": 223,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/event/feed_test.go",
          "line": 255,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/event/feed_test.go",
          "line": 272,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc/event/feed_test.go",
          "line": 328,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/console/console.go",
          "line": 304,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"coinbase: \" + eth.coinbase + \"\\n\"",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/console/console.go",
          "line": 306,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"at block: \" + eth.blockNumber + \" (\" + new Date(1000 * eth.getBlock(eth.blockNumber).timestamp) + \")\\n\"",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/console/console.go",
          "line": 308,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" datadir: \" + admin.datadir + \"\\n\"",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/console/console.go",
          "line": 312,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= res.String()\n\t}\n\t// List all the supported modules for the user to call\n\tif apis, err := c.client.SupportedModules()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/console/console.go",
          "line": 321,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" modules: \" + strings.Join(modules, \" \") + \"\\n\"\n\t}\n\tmessage += \"\\nTo exit, press ctrl-d or type exit\"\n\tfmt.Fprintln(c.printer, message)\n}\n\n// Evaluate executes code and pretty prints the result to the specified output\n// stream.\nfunc (c *Console) Evaluate(statement string) {\n\tdefer func() {\n\t\tif r := recover()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/console/console.go",
          "line": 446,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= line + \"\\n\"\n\t\t\tindents = countIndents(input)\n\t\t\tif indents <= 0 {\n\t\t\t\tprompt = c.prompt\n\t\t\t} else {\n\t\t\t\tprompt = strings.Repeat(\".\", indents*3) + \" \"\n\t\t\t}\n\t\t\t// If all the needed lines are present, save the command and run it.\n\t\t\tif indents <= 0 {\n\t\t\t\tif len(input) > 0 && input[0] != ' ' && !passwordRegexp.MatchString(input) {\n\t\t\t\t\tif command := strings.TrimSpace(input)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/console/bridge.go",
          "line": 93,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/console/bridge.go",
          "line": 98,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/console/bridge.go",
          "line": 151,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/miner/bid_simulator.go",
          "line": 833,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= receipt.GasUsed\n\t\t\t\teffectiveTip, er := tx.EffectiveGasTip(bidRuntime.env.header.BaseFee)\n\t\t\t\tif er != nil {\n\t\t\t\t\terr = errors.New(\"failed to calculate effective tip\")\n\t\t\t\t\treturn\n\t\t\t\t}\n\n\t\t\t\tif bidRuntime.env.header.BaseFee != nil {\n\t\t\t\t\teffectiveTip.Add(effectiveTip, bidRuntime.env.header.BaseFee)\n\t\t\t\t}\n\n\t\t\t\tgasFee := new(big.Int).Mul(effectiveTip, new(big.Int).SetUint64(receipt.GasUsed))\n\t\t\t\tbidGasFee.Add(bidGasFee, gasFee)\n\n\t\t\t\tif tx.Type() == types.BlobTxType {\n\t\t\t\t\tblobFee := new(big.Int).Mul(receipt.BlobGasPrice, new(big.Int).SetUint64(receipt.BlobGasUsed))\n\t\t\t\t\tbidGasFee.Add(bidGasFee, blobFee)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\n\t\t// if bid txs are all from mempool, do not check gas price\n\t\tif bidGasUsed != 0 {\n\t\t\tbidGasPrice := new(big.Int).Div(bidGasFee, new(big.Int).SetUint64(bidGasUsed))\n\t\t\tif bidGasPrice.Cmp(b.minGasPrice) < 0 {\n\t\t\t\terr = fmt.Errorf(\"bid gas price is lower than min gas price, bid:%v, min:%v\", bidGasPrice, b.minGasPrice)\n\t\t\t\treturn\n\t\t\t}\n\t\t}\n\t}\n\n\t// if enable greedy merge, fill bid env with transactions from mempool\n\tgreedyMergeElapsed := time.Duration(0)\n\tif *b.config.GreedyMergeTx {\n\t\tendingBidsExtra := 20 * time.Millisecond // Add a buffer to ensure ending bids before `delayLeftOver`\n\t\tminTimeLeftForEndingBids := b.delayLeftOver + endingBidsExtra\n\t\tdelay := b.engine.Delay(b.chain, bidRuntime.env.header, &minTimeLeftForEndingBids)\n\t\tif delay != nil && *delay > 0 {\n\t\t\tgreedyMergeStartTs := time.Now()\n\t\t\tbidTxsSet := mapset.NewThreadUnsafeSetWithSize[common.Hash](len(bidRuntime.bid.Txs))\n\t\t\tfor _, tx := range bidRuntime.bid.Txs {\n\t\t\t\tbidTxsSet.Add(tx.Hash())\n\t\t\t}\n\t\t\tstopTimer := time.NewTimer(*delay)\n\t\t\tdefer stopTimer.Stop()\n\t\t\tfillErr := b.bidWorker.fillTransactions(interruptCh, bidRuntime.env, stopTimer, bidTxsSet)\n\n\t\t\t// recalculate the packed reward\n\t\t\tbidRuntime.packReward(*b.config.ValidatorCommission)\n\t\t\tgreedyMergeElapsed = time.Since(greedyMergeStartTs)\n\n\t\t\tlog.Debug(\"BidSimulator: greedy merge stopped\", \"block\", bidRuntime.env.header.Number,\n\t\t\t\t\"builder\", bidRuntime.bid.Builder, \"tx count\", bidRuntime.env.tcount-bidTxLen+1, \"err\", fillErr, \"greedyMergeElapsed\", greedyMergeElapsed)\n\t\t}\n\t}\n\n\t// commit payBidTx at the end of the block\n\tbidRuntime.env.gasPool.AddGas(params.PayBidTxGasLimit)\n\terr = bidRuntime.commitTransaction(b.chain, b.chainConfig, payBidTx, true)\n\tif err != nil {\n\t\tlog.Error(\"BidSimulator: failed to commit tx\", \"builder\", bidRuntime.bid.Builder,\n\t\t\t\"bidHash\", bidRuntime.bid.Hash(), \"tx\", payBidTx.Hash(), \"err\", err)\n\t\terr = fmt.Errorf(\"invalid tx in bid, %v\", err)\n\t\treturn\n\t}\n\n\tbestBid := b.GetBestBid(parentHash)\n\tsimElapsed := time.Since(startTS)\n\tif bestBid == nil {\n\t\twinResult := \"true[first]\"\n\t\tlog.Info(\"[BID RESULT]\", \"win\", winResult, \"builder\", bidRuntime.bid.Builder, \"hash\", bidRuntime.bid.Hash().TerminalString(), \"simElapsed\", simElapsed)\n\t} else if bidRuntime.bid.Hash() != bestBid.bid.Hash() { // skip log flushing when only one bid is present\n\t\tlog.Info(\"[BID RESULT]\",\n\t\t\t\"win\", bidRuntime.packedBlockReward.Cmp(bestBid.packedBlockReward) > 0,\n\n\t\t\t\"bidHash\", bidRuntime.bid.Hash().TerminalString(),\n\t\t\t\"bestHash\", bestBid.bid.Hash().TerminalString(),\n\n\t\t\t\"bidGasFee\", weiToEtherStringF6(bidRuntime.packedBlockReward),\n\t\t\t\"bestGasFee\", weiToEtherStringF6(bestBid.packedBlockReward),\n\n\t\t\t\"bidBlockTx\", bidRuntime.env.tcount,\n\t\t\t\"bestBlockTx\", bestBid.env.tcount,\n\n\t\t\t\"simElapsed\", simElapsed,\n\t\t)\n\t}\n\tconst minGasForSpeedMetric = 30_000_000\n\tif bidRuntime.bid.GasUsed > minGasForSpeedMetric {\n\t\ttimeCostMs := (simElapsed - greedyMergeElapsed).Milliseconds()\n\t\tif timeCostMs > 0 {\n\t\t\tsimulateSpeedGauge.Update(int64(float64(bidRuntime.bid.GasUsed) / float64(timeCostMs) / 1000))\n\t\t}\n\t}\n\n\t// this is the simplest strategy: best for all the delegators.\n\tif bestBid == nil || bidRuntime.packedBlockReward.Cmp(bestBid.packedBlockReward) > 0 {\n\t\tb.SetBestBid(bidRuntime.bid.ParentHash, bidRuntime)\n\t\tbidRuntime.duration = time.Since(startTS)\n\t\tbidSimTimer.UpdateSince(startTS)\n\t\tsuccess = true\n\t}\n}\n\n// reportIssue reports the issue to the mev-sentry\nfunc (b *bidSimulator) reportIssue(bidRuntime *BidRuntime, err error) {\n\tmetrics.GetOrRegisterCounter(fmt.Sprintf(\"bid/err/%v\", bidRuntime.bid.Builder), nil).Inc(1)\n\n\tcli := b.builders[bidRuntime.bid.Builder]\n\tif cli != nil {\n\t\terr = cli.ReportIssue(context.Background(), &types.BidIssue{\n\t\t\tValidator: bidRuntime.env.header.Coinbase,\n\t\t\tBuilder:   bidRuntime.bid.Builder,\n\t\t\tBidHash:   bidRuntime.bid.Hash(),\n\t\t\tMessage:   err.Error(),\n\t\t})\n\n\t\tif err != nil {\n\t\t\tlog.Warn(\"BidSimulator: failed to report issue\", \"builder\", bidRuntime.bid.Builder, \"err\", err)\n\t\t}\n\t}\n}\n\ntype BidRuntime struct {\n\tbid *types.Bid\n\n\tenv *environment\n\n\texpectedBlockReward     *big.Int\n\texpectedValidatorReward *big.Int\n\n\tpackedBlockReward     *big.Int\n\tpackedValidatorReward *big.Int\n\n\tfinished chan struct{}\n\tduration time.Duration\n}\n\nfunc newBidRuntime(newBid *types.Bid, validatorCommission uint64) (*BidRuntime, error) {\n\t// check the block reward and validator reward of the newBid\n\texpectedBlockReward := newBid.GasFee\n\texpectedValidatorReward := new(big.Int).Mul(expectedBlockReward, big.NewInt(int64(validatorCommission)))\n\texpectedValidatorReward.Div(expectedValidatorReward, big.NewInt(10000))\n\texpectedValidatorReward.Sub(expectedValidatorReward, newBid.BuilderFee)\n\n\tif expectedValidatorReward.Cmp(big.NewInt(0)) < 0 {\n\t\t// damage self profit, ignore\n\t\tlog.Debug(\"BidSimulator: invalid bid, validator reward is less than 0, ignore\",\n\t\t\t\"builder\", newBid.Builder, \"bidHash\", newBid.Hash().Hex())\n\t\treturn nil, fmt.Errorf(\"validator reward is less than 0, value: %s, commissionConfig: %d\", expectedValidatorReward, validatorCommission)\n\t}\n\n\tbidRuntime := &BidRuntime{\n\t\tbid:                     newBid,\n\t\texpectedBlockReward:     expectedBlockReward,\n\t\texpectedValidatorReward: expectedValidatorReward,\n\t\tpackedBlockReward:       big.NewInt(0),\n\t\tpackedValidatorReward:   big.NewInt(0),\n\t\tfinished:                make(chan struct{}),\n\t}\n\n\treturn bidRuntime, nil\n}\n\nfunc (r *BidRuntime) validReward() bool {\n\treturn r.packedBlockReward.Cmp(r.expectedBlockReward) >= 0 &&\n\t\tr.packedValidatorReward.Cmp(r.expectedValidatorReward) >= 0\n}\n\nfunc (r *BidRuntime) isExpectedBetterThan(other *BidRuntime) bool {\n\treturn r.expectedBlockReward.Cmp(other.expectedBlockReward) >= 0 &&\n\t\tr.expectedValidatorReward.Cmp(other.expectedValidatorReward) >= 0\n}\n\n// packReward calculates packedBlockReward and packedValidatorReward\nfunc (r *BidRuntime) packReward(validatorCommission uint64) {\n\tr.packedBlockReward = r.env.state.GetBalance(consensus.SystemAddress).ToBig()\n\tr.packedValidatorReward = new(big.Int).Mul(r.packedBlockReward, big.NewInt(int64(validatorCommission)))\n\tr.packedValidatorReward.Div(r.packedValidatorReward, big.NewInt(10000))\n\tr.packedValidatorReward.Sub(r.packedValidatorReward, r.bid.BuilderFee)\n}\n\nfunc (r *BidRuntime) commitTransaction(chain *core.BlockChain, chainConfig *params.ChainConfig, tx *types.Transaction, unRevertible bool) error {\n\tvar (\n\t\tenv = r.env\n\t\tsc  *types.BlobSidecar\n\t)\n\n\t// Start executing the transaction\n\tr.env.state.SetTxContext(tx.Hash(), r.env.tcount)\n\n\tif tx.Type() == types.BlobTxType {\n\t\tsc = types.NewBlobSidecarFromTx(tx)\n\t\tif sc == nil {\n\t\t\treturn errors.New(\"blob transaction without blobs in miner\")\n\t\t}\n\t\t// Checking against blob gas limit: It's kind of ugly to perform this check here, but there\n\t\t// isn't really a better place right now. The blob gas limit is checked at block validation time\n\t\t// and not during execution. This means core.ApplyTransaction will not return an error if the\n\t\t// tx has too many blobs. So we have to explicitly check it here.\n\t\tif (env.blobs + len(sc.Blobs)) > eip4844.MaxBlobsPerBlock(chainConfig, r.env.header.Time) {\n\t\t\treturn errors.New(\"max data blobs reached\")\n\t\t}\n\t}\n\n\treceipt, err := core.ApplyTransaction(env.evm, env.gasPool, env.state, env.header, tx,\n\t\t&env.header.GasUsed, core.NewReceiptBloomGenerator())\n\tif err != nil {\n\t\treturn err\n\t} else if unRevertible && receipt.Status == types.ReceiptStatusFailed {\n\t\treturn errors.New(\"no revertible transaction failed\")\n\t}\n\n\tif tx.Type() == types.BlobTxType {\n\t\tsc.TxIndex = uint64(len(env.txs))\n\t\tenv.txs = append(env.txs, tx.WithoutBlobTxSidecar())\n\t\tenv.receipts = append(env.receipts, receipt)\n\t\tenv.sidecars = append(env.sidecars, sc)\n\t\tenv.blobs += len(sc.Blobs)\n\t\t*env.header.BlobGasUsed += receipt.BlobGasUsed\n\t} else {\n\t\tenv.txs = append(env.txs, tx)\n\t\tenv.receipts = append(env.receipts, receipt)\n\t}\n\n\tr.env.tcount++\n\n\treturn nil\n}\n\nfunc weiToEtherStringF6(wei *big.Int) string {\n\tf, _ := new(big.Float).Quo(new(big.Float).SetInt(wei), big.NewFloat(params.Ether)).Float64()\n\treturn strconv.FormatFloat(f, 'f', 6, 64)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/miner/worker.go",
          "line": 784,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(sc.Blobs)\n\t*env.header.BlobGasUsed += receipt.BlobGasUsed\n\treturn receipt.Logs, nil\n}\n\n// applyTransaction runs the transaction. If execution fails, state and gas pool are reverted.\nfunc (w *worker) applyTransaction(env *environment, tx *types.Transaction, receiptProcessors ...core.ReceiptProcessor) (*types.Receipt, error) {\n\tvar (\n\t\tsnap = env.state.Snapshot()\n\t\tgp   = env.gasPool.Gas()\n\t)\n\n\treceipt, err := core.ApplyTransaction(env.evm, env.gasPool, env.state, env.header, tx, &env.header.GasUsed, receiptProcessors...)\n\tif err != nil {\n\t\tenv.state.RevertToSnapshot(snap)\n\t\tenv.gasPool.SetGas(gp)\n\t}\n\treturn receipt, err\n}\n\nfunc (w *worker) commitTransactions(env *environment, plainTxs, blobTxs *transactionsByPriceAndNonce,\n\tinterruptCh chan int32, stopTimer *time.Timer) error {\n\tgasLimit := env.header.GasLimit\n\tif env.gasPool == nil {\n\t\tenv.gasPool = new(core.GasPool).AddGas(gasLimit)\n\t\tif p, ok := w.engine.(*parlia.Parlia)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/miner/worker.go",
          "line": 994,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/miner/worker.go",
          "line": 1104,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.ParentHash, env.evm)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/miner/ordering_test.go",
          "line": 102,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= count\n\t}\n\t// Sort the transactions and cross check the nonce ordering\n\ttxset := newTransactionsByPriceAndNonce(signer, groups, baseFee)\n\n\ttxs := types.Transactions{}\n\tfor tx, _ := txset.Peek()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/beacon/types/committee.go",
          "line": 188,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= bits.OnesCount8(v)\n\t}\n\treturn count\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/beacon/types/exec_header.go",
          "line": 73,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8099999999999999,
          "confidence": 0.9729,
          "ensemble_confidence": 0.87561
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/beacon/fakebeacon/utils.go",
          "line": 45,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= (ts-headerTime)/3 + 1\n\t\t} else {\n\t\t\t// search one by one\n\t\t\tfor headerTime >= ts {\n\t\t\t\theader, err = backend.HeaderByNumber(ctx, rpc.BlockNumber(estimateEndNumber-1))\n\t\t\t\tif err != nil {\n\t\t\t\t\ttime.Sleep(time.Duration(rand.Int()%180) * time.Millisecond)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\theaderTime = int64(header.Time)\n\t\t\t\tif headerTime == ts {\n\t\t\t\t\treturn header, nil\n\t\t\t\t}\n\t\t\t\testimateEndNumber -= 1\n\t\t\t\tif headerTime < ts { //found the real endNumber\n\t\t\t\t\treturn nil, fmt.Errorf(\"block not found by time %d\", ts)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/beacon/fakebeacon/utils.go",
          "line": 32,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\t\ttime.Sleep(time.Duration(rand.Int()%180) * time.Millisecond)\n\t\t\tcontinue\n\t\t}\n\t\theaderTime := int64(header.Time)\n\t\tif headerTime == ts {\n\t\t\treturn header, nil\n\t\t}\n\n\t\t// let the estimateEndNumber a little bigger than real value\n\t\tif headerTime > ts+8 {\n\t\t\testimateEndNumber -= (headerTime - ts) / 3\n\t\t} else if headerTime < ts {\n\t\t\testimateEndNumber += (ts-headerTime)/3 + 1\n\t\t} else {\n\t\t\t// search one by one\n\t\t\tfor headerTime >= ts {\n\t\t\t\theader, err = backend.HeaderByNumber(ctx, rpc.BlockNumber(estimateEndNumber-1))\n\t\t\t\tif err != nil {\n\t\t\t\t\ttime.Sleep(time.Duration(rand.Int()%180) * time.Millisecond)\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\theaderTime = int64(header.Time)\n\t\t\t\tif headerTime == ts {\n\t\t\t\t\treturn header, nil\n\t\t\t\t}\n\t\t\t\testimateEndNumber -= 1\n\t\t\t\tif headerTime < ts { //found the real endNumber\n\t\t\t\t\treturn nil, fmt.Errorf(\"block not found by time %d\", ts)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethdb/remotedb/remotedb.go",
          "line": 44,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/ethdb/remotedb/remotedb.go",
          "line": 53,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/ethdb/remotedb/remotedb.go",
          "line": 66,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethdb/leveldb/leveldb.go",
          "line": 277,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" Level |   Tables   |    Size(MB)   |    Time(sec)  |    Read(MB)   |   Write(MB)\\n\" +\n\t\t\t\"-------+------------+---------------+---------------+---------------+---------------\\n\"\n\t\tfor level, size := range stats.LevelSizes {\n\t\t\tread := stats.LevelRead[level]\n\t\t\twrite := stats.LevelWrite[level]\n\t\t\tduration := stats.LevelDurations[level]\n\t\t\ttables := stats.LevelTablesCounts[level]\n\n\t\t\tif tables == 0 && duration == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\ttotalTables += tables\n\t\t\ttotalSize += size\n\t\t\ttotalRead += read\n\t\t\ttotalWrite += write\n\t\t\ttotalDuration += duration\n\t\t\tmessage += fmt.Sprintf(\" %3d   | %10d | %13.5f | %13.5f | %13.5f | %13.5f\\n\",\n\t\t\t\tlevel, tables, float64(size)/1048576.0, duration.Seconds(),\n\t\t\t\tfloat64(read)/1048576.0, float64(write)/1048576.0)\n\t\t}\n\t\tmessage += \"-------+------------+---------------+---------------+---------------+---------------\\n\"\n\t\tmessage += fmt.Sprintf(\" Total | %10d | %13.5f | %13.5f | %13.5f | %13.5f\\n\",\n\t\t\ttotalTables, float64(totalSize)/1048576.0, totalDuration.Seconds(),\n\t\t\tfloat64(totalRead)/1048576.0, float64(totalWrite)/1048576.0)\n\t\tmessage += \"-------+------------+---------------+---------------+---------------+---------------\\n\\n\"\n\t}\n\tmessage += fmt.Sprintf(\"Read(MB):%.5f Write(MB):%.5f\\n\", float64(stats.IORead)/1048576.0, float64(stats.IOWrite)/1048576.0)\n\tmessage += fmt.Sprintf(\"BlockCache(MB):%.5f FileCache:%d\\n\", float64(stats.BlockCacheSize)/1048576.0, stats.OpenedTablesCount)\n\tmessage += fmt.Sprintf(\"MemoryCompaction:%d Level0Compaction:%d NonLevel0Compaction:%d SeekCompaction:%d\\n\", stats.MemComp, stats.Level0Comp, stats.NonLevel0Comp, stats.SeekComp)\n\tmessage += fmt.Sprintf(\"WriteDelayCount:%d WriteDelayDuration:%s Paused:%t\\n\", stats.WriteDelayCount, common.PrettyDuration(stats.WriteDelayDuration), stats.WritePaused)\n\tmessage += fmt.Sprintf(\"Snapshots:%d Iterators:%d\\n\", stats.AliveSnapshots, stats.AliveIterators)\n\treturn message, nil\n}\n\n// Compact flattens the underlying data store for the given key range. In essence,\n// deleted and overwritten versions are discarded, and the data is rearranged to\n// reduce the cost of operations needed to access them.\n//\n// A nil start is treated as a key before all keys in the data store",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/ethdb/leveldb/leveldb.go",
          "line": 380,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= t.Nanoseconds()\n\t\t}\n\t\tcompactions[i%2][2] = stats.LevelRead.Sum()\n\t\tcompactions[i%2][3] = stats.LevelWrite.Sum()\n\t\t// Update all the requested meters\n\t\tdb.diskSizeGauge.Update(compactions[i%2][0])\n\t\tdb.compTimeMeter.Mark(compactions[i%2][1] - compactions[(i-1)%2][1])\n\t\tdb.compReadMeter.Mark(compactions[i%2][2] - compactions[(i-1)%2][2])\n\t\tdb.compWriteMeter.Mark(compactions[i%2][3] - compactions[(i-1)%2][3])\n\t\tvar (\n\t\t\tdelayN   = int64(stats.WriteDelayCount)\n\t\t\tduration = stats.WriteDelayDuration\n\t\t\tpaused   = stats.WritePaused\n\t\t)\n\t\tdb.writeDelayNMeter.Mark(delayN - delaystats[0])\n\t\tdb.writeDelayMeter.Mark(duration.Nanoseconds() - delaystats[1])\n\t\t// If a warning that db is performing compaction has been displayed, any subsequent\n\t\t// warnings will be withheld for one minute not to overwhelm the user.\n\t\tif paused && delayN-delaystats[0] == 0 && duration.Nanoseconds()-delaystats[1] == 0 &&\n\t\t\ttime.Now().After(lastWritePaused.Add(degradationWarnInterval)) {\n\t\t\tdb.log.Warn(\"Database compacting, degraded performance\")\n\t\t\tlastWritePaused = time.Now()\n\t\t}\n\t\tdelaystats[0], delaystats[1] = delayN, duration.Nanoseconds()\n\n\t\tvar (\n\t\t\tnRead  = int64(stats.IORead)\n\t\t\tnWrite = int64(stats.IOWrite)\n\t\t)\n\t\tdb.diskReadMeter.Mark(nRead - iostats[0])\n\t\tdb.diskWriteMeter.Mark(nWrite - iostats[1])\n\t\tiostats[0], iostats[1] = nRead, nWrite\n\n\t\tdb.memCompGauge.Update(int64(stats.MemComp))\n\t\tdb.level0CompGauge.Update(int64(stats.Level0Comp))\n\t\tdb.nonlevel0CompGauge.Update(int64(stats.NonLevel0Comp))\n\t\tdb.seekCompGauge.Update(int64(stats.SeekComp))\n\n\t\tfor i, tables := range stats.LevelTablesCounts {\n\t\t\t// Append metrics for additional layers\n\t\t\tif i >= len(db.levelsGauge) {\n\t\t\t\tdb.levelsGauge = append(db.levelsGauge, metrics.NewRegisteredGauge(namespace+fmt.Sprintf(\"tables/level%v\", i), nil))\n\t\t\t}\n\t\t\tdb.levelsGauge[i].Update(int64(tables))\n\t\t}\n\n\t\t// Sleep a bit, then repeat the stats collection\n\t\tselect {\n\t\tcase errc = <-db.quitChan:\n\t\t\t// Quit requesting, stop hammering the database\n\t\tcase <-timer.C:\n\t\t\ttimer.Reset(refresh)\n\t\t\t// Timeout, gather a new set of stats\n\t\t}\n\t}\n\n\tif errc == nil {\n\t\terrc = <-db.quitChan\n\t}\n\terrc <- merr\n}\n\n// batch is a write-only leveldb batch that commits changes to its host database\n// when Write is called. A batch cannot be used concurrently.\ntype batch struct {\n\tdb   *leveldb.DB\n\tb    *leveldb.Batch\n\tsize int\n}\n\n// Put inserts the given value into the batch for later committing.\nfunc (b *batch) Put(key, value []byte) error {\n\tb.b.Put(key, value)\n\tb.size += len(key) + len(value)\n\treturn nil\n}\n\n// Delete inserts the key removal into the batch for later committing.\nfunc (b *batch) Delete(key []byte) error {\n\tb.b.Delete(key)\n\tb.size += len(key)\n\treturn nil\n}\n\n// DeleteRange removes all keys in the range [start, end) from the batch for\n// later committing, inclusive on start, exclusive on end.\n//\n// Note that this is a fallback implementation as leveldb does not natively\n// support range deletion in batches. It iterates through the database to find\n// keys in the range and adds them to the batch for deletion.\nfunc (b *batch) DeleteRange(start, end []byte) error {\n\t// Create an iterator to scan through the keys in the range\n\tslice := &util.Range{\n\t\tStart: start, // If nil, it represents the key before all keys\n\t\tLimit: end,   // If nil, it represents the key after all keys\n\t}\n\tit := b.db.NewIterator(slice, nil)\n\tdefer it.Release()\n\n\tvar count int\n\tfor it.Next() {\n\t\tcount++\n\t\tkey := it.Key()\n\t\tif count > 10000 { // should not block for more than a second\n\t\t\treturn ethdb.ErrTooManyKeys\n\t\t}\n\t\t// Add this key to the batch for deletion\n\t\tb.b.Delete(key)\n\t\tb.size += len(key)\n\t}\n\tif err := it.Error()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 561,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(levelMetrics.BytesCompacted)\n\t\t\tnWrite += int64(levelMetrics.BytesFlushed)\n\t\t\tcompWrite += int64(levelMetrics.BytesCompacted)\n\t\t\tcompRead += int64(levelMetrics.BytesRead)\n\t\t}\n\n\t\tnWrite += int64(stats.WAL.BytesWritten)\n\n\t\tcompWrites[i%2] = compWrite\n\t\tcompReads[i%2] = compRead\n\t\tnWrites[i%2] = nWrite\n\n\t\td.writeDelayNMeter.Mark(writeDelayCounts[i%2] - writeDelayCounts[(i-1)%2])\n\t\td.writeDelayMeter.Mark(writeDelayTimes[i%2] - writeDelayTimes[(i-1)%2])\n\t\t// Print a warning log if writing has been stalled for a while. The log will\n\t\t// be printed per minute to avoid overwhelming users.\n\t\tif d.writeStalled.Load() && writeDelayCounts[i%2] == writeDelayCounts[(i-1)%2] &&\n\t\t\ttime.Now().After(lastWriteStallReport.Add(degradationWarnInterval)) {\n\t\t\td.log.Warn(\"Database compacting, degraded performance\")\n\t\t\tlastWriteStallReport = time.Now()\n\t\t}\n\t\td.compTimeMeter.Mark(compTimes[i%2] - compTimes[(i-1)%2])\n\t\td.compReadMeter.Mark(compReads[i%2] - compReads[(i-1)%2])\n\t\td.compWriteMeter.Mark(compWrites[i%2] - compWrites[(i-1)%2])\n\t\td.diskSizeGauge.Update(int64(stats.DiskSpaceUsage()))\n\t\td.diskReadMeter.Mark(0) // pebble doesn't track non-compaction reads\n\t\td.diskWriteMeter.Mark(nWrites[i%2] - nWrites[(i-1)%2])\n\n\t\t// See https://github.com/cockroachdb/pebble/pull/1628#pullrequestreview-1026664054\n\t\tmanuallyAllocated := stats.BlockCache.Size + int64(stats.MemTable.Size) + int64(stats.MemTable.ZombieSize)\n\t\td.manualMemAllocGauge.Update(manuallyAllocated)\n\t\td.memCompGauge.Update(stats.Flush.Count)\n\t\td.nonlevel0CompGauge.Update(nonLevel0CompCount)\n\t\td.level0CompGauge.Update(level0CompCount)\n\t\td.seekCompGauge.Update(stats.Compact.ReadCount)\n\t\td.liveCompGauge.Update(stats.Compact.NumInProgress)\n\t\td.liveCompSizeGauge.Update(stats.Compact.InProgressBytes)\n\t\td.liveIterGauge.Update(stats.TableIters)\n\n\t\td.liveMemTablesGauge.Update(stats.MemTable.Count)\n\t\td.zombieMemTablesGauge.Update(stats.MemTable.ZombieCount)\n\t\td.estimatedCompDebtGauge.Update(int64(stats.Compact.EstimatedDebt))\n\t\td.tableCacheHitGauge.Update(stats.TableCache.Hits)\n\t\td.tableCacheMissGauge.Update(stats.TableCache.Misses)\n\t\td.blockCacheHitGauge.Update(stats.BlockCache.Hits)\n\t\td.blockCacheMissGauge.Update(stats.BlockCache.Misses)\n\t\td.filterHitGauge.Update(stats.Filter.Hits)\n\t\td.filterMissGauge.Update(stats.Filter.Misses)\n\n\t\tfor i, level := range stats.Levels {\n\t\t\t// Append metrics for additional layers\n\t\t\tif i >= len(d.levelsGauge) {\n\t\t\t\td.levelsGauge = append(d.levelsGauge, metrics.GetOrRegisterGauge(namespace+fmt.Sprintf(\"tables/level%v\", i), nil))\n\t\t\t}\n\t\t\td.levelsGauge[i].Update(level.NumFiles)\n\t\t}\n\n\t\t// Sleep a bit, then repeat the stats collection\n\t\tselect {\n\t\tcase errc = <-d.quitChan:\n\t\t\t// Quit requesting, stop hammering the database\n\t\tcase <-timer.C:\n\t\t\ttimer.Reset(refresh)\n\t\t\t// Timeout, gather a new set of stats\n\t\t}\n\t}\n\terrc <- nil\n}\n\n// batch is a write-only batch that commits changes to its host database\n// when Write is called. A batch cannot be used concurrently.\ntype batch struct {\n\tb    *pebble.Batch\n\tdb   *Database\n\tsize int\n}\n\n// Put inserts the given value into the batch for later committing.\nfunc (b *batch) Put(key, value []byte) error {\n\tif err := b.b.Set(key, value, nil)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 643,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(key) + len(value)\n\treturn nil\n}\n\n// Delete inserts the key removal into the batch for later committing.\nfunc (b *batch) Delete(key []byte) error {\n\tif err := b.b.Delete(key, nil)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 652,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(key)\n\treturn nil\n}\n\n// DeleteRange removes all keys in the range [start, end) from the batch for\n// later committing, inclusive on start, exclusive on end.\nfunc (b *batch) DeleteRange(start, end []byte) error {\n\t// There is no special flag to represent the end of key range\n\t// in pebble(nil in leveldb). Use an ugly hack to construct a\n\t// large key to represent it.\n\tif end == nil {\n\t\tend = ethdb.MaximumKey\n\t}\n\tif err := b.b.DeleteRange(start, end, nil)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/ethdb/pebble/pebble.go",
          "line": 669,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(start) + len(end)\n\treturn nil\n}\n\n// ValueSize retrieves the amount of data queued up for writing.\nfunc (b *batch) ValueSize() int {\n\treturn b.size\n}\n\n// Write flushes any accumulated data to disk.\nfunc (b *batch) Write() error {\n\tb.db.quitLock.RLock()\n\tdefer b.db.quitLock.RUnlock()\n\tif b.db.closed {\n\t\treturn pebble.ErrClosed\n\t}\n\treturn b.b.Commit(b.db.writeOptions)\n}\n\n// Reset resets the batch for reuse.\nfunc (b *batch) Reset() {\n\tb.b.Reset()\n\tb.size = 0\n}\n\n// Replay replays the batch contents.\nfunc (b *batch) Replay(w ethdb.KeyValueWriter) error {\n\treader := b.b.Reader()\n\tfor {\n\t\tkind, k, v, ok, err := reader.Next()\n\t\tif !ok || err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// The (k,v) slices might be overwritten if the batch is reset/reused,\n\t\t// and the receiver should copy them if they are to be retained long-term.\n\t\tif kind == pebble.InternalKeyKindSet {\n\t\t\tif err = w.Put(k, v)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethdb/memorydb/memorydb.go",
          "line": 318,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(key) + len(value)\n\treturn nil\n}\n\n// Delete inserts the key removal into the batch for later committing.\nfunc (b *batch) Delete(key []byte) error {\n\tb.writes = append(b.writes, keyvalue{key: string(key), delete: true})\n\tb.size += len(key)\n\treturn nil\n}\n\n// DeleteRange removes all keys in the range [start, end) from the batch for later committing.\nfunc (b *batch) DeleteRange(start, end []byte) error {\n\tb.writes = append(b.writes, keyvalue{\n\t\trangeFrom: bytes.Clone(start),\n\t\trangeTo:   bytes.Clone(end),\n\t\tdelete:    true,\n\t})\n\tb.size += len(start) + len(end)\n\treturn nil\n}\n\n// ValueSize retrieves the amount of data queued up for writing.\nfunc (b *batch) ValueSize() int {\n\treturn b.size\n}\n\n// Write flushes any accumulated data to the memory database.\nfunc (b *batch) Write() error {\n\tb.db.lock.Lock()\n\tdefer b.db.lock.Unlock()\n\n\tif b.db.db == nil {\n\t\treturn errMemorydbClosed\n\t}\n\tfor _, entry := range b.writes {\n\t\tif entry.delete {\n\t\t\tif entry.key != \"\" {\n\t\t\t\t// Single key deletion\n\t\t\t\tdelete(b.db.db, entry.key)\n\t\t\t} else {\n\t\t\t\t// Range deletion (inclusive of start, exclusive of end)\n\t\t\t\tfor key := range b.db.db {\n\t\t\t\t\tif entry.rangeFrom != nil && key < string(entry.rangeFrom) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tif entry.rangeTo != nil && key >= string(entry.rangeTo) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tdelete(b.db.db, key)\n\t\t\t\t}\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tb.db.db[entry.key] = entry.value\n\t}\n\treturn nil\n}\n\n// Reset resets the batch for reuse.\nfunc (b *batch) Reset() {\n\tb.writes = b.writes[:0]\n\tb.size = 0\n}\n\n// Replay replays the batch contents.\nfunc (b *batch) Replay(w ethdb.KeyValueWriter) error {\n\tfor _, entry := range b.writes {\n\t\tif entry.delete {\n\t\t\tif entry.key != \"\" {\n\t\t\t\t// Single key deletion\n\t\t\t\tif err := w.Delete([]byte(entry.key))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/ethdb/memorydb/memorydb.go",
          "line": 427,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\treturn it.index < len(it.keys)\n}\n\n// Error returns any accumulated error. Exhausting all the key/value pairs\n// is not considered to be an error. A memory iterator cannot encounter errors.\nfunc (it *iterator) Error() error {\n\treturn nil\n}\n\n// Key returns the key of the current key/value pair, or nil if done. The caller\n// should not modify the contents of the returned slice, and its contents may\n// change on the next call to Next.\nfunc (it *iterator) Key() []byte {\n\t// Short circuit if iterator is not in a valid position\n\tif it.index < 0 || it.index >= len(it.keys) {\n\t\treturn nil\n\t}\n\treturn []byte(it.keys[it.index])\n}\n\n// Value returns the value of the current key/value pair, or nil if done. The\n// caller should not modify the contents of the returned slice, and its contents\n// may change on the next call to Next.\nfunc (it *iterator) Value() []byte {\n\t// Short circuit if iterator is not in a valid position\n\tif it.index < 0 || it.index >= len(it.keys) {\n\t\treturn nil\n\t}\n\treturn it.values[it.index]\n}\n\n// Release releases associated resources. Release should always succeed and can\n// be called multiple times without causing error.\nfunc (it *iterator) Release() {\n\tit.index, it.keys, it.values = -1, nil, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/ethdb/dbtest/testsuite.go",
          "line": 924,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 {\n\t\tkeys = append(keys, randBytes(ksize))\n\t\tvals = append(vals, randBytes(vsize))\n\t}\n\tif order {\n\t\tslices.SortFunc(keys, bytes.Compare)\n\t}\n\treturn keys, vals\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/filters/filter_test.go",
          "line": 106,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, block.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 255,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 285,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 342,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 592,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 658,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 768,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/eth/filters/filter_system_test.go",
          "line": 96,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(b.db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.85,
          "confidence": 0.9765,
          "ensemble_confidence": 0.87885
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/gasprice/gasprice_test.go",
          "line": 223,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= b.Difficulty().Uint64()\n\t})\n\n\t// Construct testing chain\n\tgspec.Config.TerminalTotalDifficulty = new(big.Int).SetUint64(td)\n\tchain, err := core.NewBlockChain(db, gspec, engine, &core.BlockChainConfig{NoPrefetch: true})\n\tif err != nil {\n\t\tt.Fatalf(\"Failed to create local chain, %v\", err)\n\t}\n\tif i, err := chain.InsertChain(blocks)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/gasprice/feehistory.go",
          "line": 150,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= sorter[txIndex].gasUsed\n\t\t}\n\t\tbf.results.reward[i] = sorter[txIndex].reward\n\t}\n}\n\n// resolveBlockRange resolves the specified block range to absolute block numbers while also\n// enforcing backend specific limitations. The pending block and corresponding receipts are\n// also returned if requested and available.\n// Note: an error is only returned if retrieving the head header has failed. If there are no\n// retrievable blocks in the specified range then zero block count is returned with no error.\nfunc (oracle *Oracle) resolveBlockRange(ctx context.Context, reqEnd rpc.BlockNumber, blocks uint64) (*types.Block, []*types.Receipt, uint64, uint64, error) {\n\tvar (\n\t\theadBlock       *types.Header\n\t\tpendingBlock    *types.Block\n\t\tpendingReceipts types.Receipts\n\t\terr             error\n\t)\n\n\t// Get the chain's current head.\n\tif headBlock, err = oracle.backend.HeaderByNumber(ctx, rpc.LatestBlockNumber)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 61,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 179,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.Number.Uint64()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 227,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(payload.Number)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/catalyst/simulated_beacon.go",
          "line": 292,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/catalyst/simulated_beacon_test.go",
          "line": 200,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(block.Transactions())\n\t\t\tincludedWxs += len(block.Withdrawals())\n\t\t\t// ensure all withdrawals/txs included. this will take two blocks b/c number of withdrawals > 10\n\t\t\tif includedTxs == txCount && includedWxs == wxCount {\n\t\t\t\treturn\n\t\t\t}\n\t\t\tabort.Reset(10 * time.Second)\n\t\tcase <-abort.C:\n\t\t\tt.Fatalf(\"timed out without including all withdrawals/txs: have txs %d, want %d, have wxs %d, want %d\",\n\t\t\t\tincludedTxs, txCount, includedWxs, wxCount)\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/fetcher/tx_fetcher.go",
          "line": 336,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= addTxsBatchSize {\n\t\tend := i + addTxsBatchSize\n\t\tif end > len(txs) {\n\t\t\tend = len(txs)\n\t\t}\n\t\tvar (\n\t\t\tduplicate   int64\n\t\t\tunderpriced int64\n\t\t\totherreject int64\n\t\t)\n\t\tbatch := txs[i:end]\n\n\t\tfor j, err := range f.addTxs(peer, batch) {\n\t\t\t// Track the transaction hash if the price is too low for us.\n\t\t\t// Avoid re-request this transaction when we receive another\n\t\t\t// announcement.\n\t\t\tif errors.Is(err, txpool.ErrUnderpriced) || errors.Is(err, txpool.ErrReplaceUnderpriced) || errors.Is(err, txpool.ErrTxGasPriceTooLow) {\n\t\t\t\tf.underpriced.Add(batch[j].Hash(), batch[j].Time())\n\t\t\t}\n\t\t\t// Track a few interesting failure types\n\t\t\tswitch {\n\t\t\tcase err == nil: // Noop, but need to handle to not count these\n\n\t\t\tcase errors.Is(err, txpool.ErrAlreadyKnown):\n\t\t\t\tduplicate++\n\n\t\t\tcase errors.Is(err, txpool.ErrUnderpriced) || errors.Is(err, txpool.ErrReplaceUnderpriced) || errors.Is(err, txpool.ErrTxGasPriceTooLow):\n\t\t\t\tunderpriced++\n\n\t\t\tdefault:\n\t\t\t\totherreject++\n\t\t\t}\n\t\t\tadded = append(added, batch[j].Hash())\n\t\t\tmetas = append(metas, txMetadata{\n\t\t\t\tkind: batch[j].Type(),\n\t\t\t\tsize: uint32(batch[j].Size()),\n\t\t\t})\n\t\t}\n\t\tknownMeter.Mark(duplicate)\n\t\tunderpricedMeter.Mark(underpriced)\n\t\totherRejectMeter.Mark(otherreject)\n\n\t\t// If 'other reject' is >25% of the deliveries in any batch, sleep a bit.\n\t\tif otherreject > addTxsBatchSize/4 {\n\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\tlog.Debug(\"Peer delivering stale transactions\", \"peer\", peer, \"rejected\", otherreject)\n\t\t}\n\t}\n\tselect {\n\tcase f.cleanup <- &txDelivery{origin: peer, hashes: added, metas: metas, direct: direct}:\n\t\treturn nil\n\tcase <-f.quit:\n\t\treturn errTerminated\n\t}\n}\n\n// Drop should be called when a peer disconnects. It cleans up all the internal\n// data structures of the given node.\nfunc (f *TxFetcher) Drop(peer string) error {\n\tselect {\n\tcase f.drop <- &txDrop{peer: peer}:\n\t\treturn nil\n\tcase <-f.quit:\n\t\treturn errTerminated\n\t}\n}\n\n// Start boots up the announcement based synchroniser, accepting and processing\n// hash notifications and block fetches until termination requested.\nfunc (f *TxFetcher) Start() {\n\tgo f.loop()\n}\n\n// Stop terminates the announcement based synchroniser, canceling all pending\n// operations.\nfunc (f *TxFetcher) Stop() {\n\tclose(f.quit)\n}\n\nfunc (f *TxFetcher) loop() {\n\tvar (\n\t\twaitTimer    = new(mclock.Timer)\n\t\ttimeoutTimer = new(mclock.Timer)\n\n\t\twaitTrigger    = make(chan struct{}, 1)\n\t\ttimeoutTrigger = make(chan struct{}, 1)\n\t)\n\tfor {\n\t\tselect {\n\t\tcase ann := <-f.notify:\n\t\t\t// Drop part of the new announcements if there are too many accumulated.\n\t\t\t// Note, we could but do not filter already known transactions here as\n\t\t\t// the probability of something arriving between this call and the pre-\n\t\t\t// filter outside is essentially zero.\n\t\t\tused := len(f.waitslots[ann.origin]) + len(f.announces[ann.origin])\n\t\t\tif used >= maxTxAnnounces {\n\t\t\t\t// This can happen if a set of transactions are requested but not\n\t\t\t\t// all fulfilled, so the remainder are rescheduled without the cap\n\t\t\t\t// check. Should be fine as the limit is in the thousands and the\n\t\t\t\t// request size in the hundreds.\n\t\t\t\ttxAnnounceDOSMeter.Mark(int64(len(ann.hashes)))\n\t\t\t\tbreak\n\t\t\t}\n\t\t\twant := used + len(ann.hashes)\n\t\t\tif want > maxTxAnnounces {\n\t\t\t\ttxAnnounceDOSMeter.Mark(int64(want - maxTxAnnounces))\n\n\t\t\t\tann.hashes = ann.hashes[:maxTxAnnounces-used]\n\t\t\t\tann.metas = ann.metas[:maxTxAnnounces-used]\n\t\t\t}\n\t\t\t// All is well, schedule the remainder of the transactions\n\t\t\tvar (\n\t\t\t\tidleWait   = len(f.waittime) == 0\n\t\t\t\t_, oldPeer = f.announces[ann.origin]\n\t\t\t\thasBlob    bool\n\n\t\t\t\t// nextSeq returns the next available sequence number for tagging\n\t\t\t\t// transaction announcement and also bump it internally.\n\t\t\t\tnextSeq = func() uint64 {\n\t\t\t\t\tseq := f.txSeq\n\t\t\t\t\tf.txSeq++\n\t\t\t\t\treturn seq\n\t\t\t\t}\n\t\t\t)\n\t\t\tfor i, hash := range ann.hashes {\n\t\t\t\t// If the transaction is already downloading, add it to the list\n\t\t\t\t// of possible alternates (in case the current retrieval fails) and\n\t\t\t\t// also account it for the peer.\n\t\t\t\tif f.alternates[hash] != nil {\n\t\t\t\t\tf.alternates[hash][ann.origin] = struct{}{}\n\n\t\t\t\t\t// Stage 2 and 3 share the set of origins per tx\n\t\t\t\t\tif announces := f.announces[ann.origin]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/fetcher/tx_fetcher.go",
          "line": 957,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.size)\n\t\t\treturn bytes < maxTxRetrievalSize\n\t\t})\n\t\t// If any hashes were allocated, request them from the peer\n\t\tif len(hashes) > 0 {\n\t\t\tf.requests[peer] = &txRequest{hashes: hashes, time: f.clock.Now()}\n\t\t\ttxRequestOutMeter.Mark(int64(len(hashes)))\n\t\t\tp := peer\n\t\t\tgopool.Submit(func() {\n\t\t\t\t// Try to fetch the transactions, but in case of a request\n\t\t\t\t// failure (e.g. peer disconnected), reschedule the hashes.\n\t\t\t\tif err := f.fetchTxs(p, hashes)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/downloader/downloader.go",
          "line": 1147,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(proced)\n\t\t} else {\n\t\t\t// A malicious node might withhold advertised headers indefinitely\n\t\t\tif n := len(headers)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/downloader/downloader.go",
          "line": 1204,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(headers))\n\t\t}\n\t\t// If we're still skeleton filling snap sync, check pivot staleness\n\t\t// before continuing to the next skeleton filling\n\t\tif skeleton && pivot > 0 {\n\t\t\tpivoting = true\n\t\t}\n\t}\n}\n\n// fillHeaderSkeleton concurrently retrieves headers from all our available peers\n// and maps them to the provided skeleton header chain.\n//\n// Any partial results from the beginning of the skeleton is (if possible) forwarded\n// immediately to the header processor to keep the rest of the pipeline full even\n// in the case of header stalls.\n//\n// The method returns the entire filled skeleton and also the number of headers\n// already forwarded for processing.\nfunc (d *Downloader) fillHeaderSkeleton(from uint64, skeleton []*types.Header) ([]*types.Header, []common.Hash, int, error) {\n\tlog.Debug(\"Filling up skeleton\", \"from\", from)\n\td.queue.ScheduleSkeleton(from, skeleton)\n\n\terr := d.concurrentFetch((*headerQueue)(d), false)\n\tif err != nil {\n\t\tlog.Debug(\"Skeleton fill failed\", \"err\", err)\n\t}\n\tfilled, hashes, proced := d.queue.RetrieveHeaders()\n\tif err == nil {\n\t\tlog.Debug(\"Skeleton fill succeeded\", \"filled\", len(filled), \"processed\", proced)\n\t}\n\treturn filled, hashes, proced, err\n}\n\n// fetchBodies iteratively downloads the scheduled block bodies, taking any\n// available peers, reserving a chunk of blocks for each, waiting for delivery\n// and also periodically checking for timeouts.\nfunc (d *Downloader) fetchBodies(from uint64, beaconMode bool) error {\n\tlog.Debug(\"Downloading block bodies\", \"origin\", from)\n\terr := d.concurrentFetch((*bodyQueue)(d), beaconMode)\n\n\tlog.Debug(\"Block body download terminated\", \"err\", err)\n\treturn err\n}\n\n// fetchReceipts iteratively downloads the scheduled block receipts, taking any\n// available peers, reserving a chunk of receipts for each, waiting for delivery\n// and also periodically checking for timeouts.\nfunc (d *Downloader) fetchReceipts(from uint64, beaconMode bool) error {\n\tlog.Debug(\"Downloading receipts\", \"origin\", from)\n\terr := d.concurrentFetch((*receiptQueue)(d), beaconMode)\n\n\tlog.Debug(\"Receipt download terminated\", \"err\", err)\n\treturn err\n}\n\n// processHeaders takes batches of retrieved headers from an input channel and\n// keeps processing and scheduling them into the header chain and downloader's\n// queue until the stream ends or a failure occurs.\nfunc (d *Downloader) processHeaders(origin uint64, td, ttd *big.Int, beaconMode bool) error {\n\tvar (\n\t\tmode  = d.getMode()\n\t\ttimer = time.NewTimer(time.Second)\n\t)\n\tdefer timer.Stop()\n\n\tfor {\n\t\tselect {\n\t\tcase <-d.cancelCh:\n\t\t\treturn errCanceled\n\n\t\tcase task := <-d.headerProcCh:\n\t\t\t// Terminate header processing if we synced up\n\t\t\tif task == nil || len(task.headers) == 0 {\n\t\t\t\t// Notify everyone that headers are fully processed\n\t\t\t\tfor _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase ch <- false:\n\t\t\t\t\tcase <-d.cancelCh:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\t// Otherwise split the chunk of headers into batches and process them\n\t\t\theaders, hashes, scheduled := task.headers, task.hashes, false\n\n\t\t\tfor len(headers) > 0 {\n\t\t\t\t// Terminate if something failed in between processing chunks\n\t\t\t\tselect {\n\t\t\t\tcase <-d.cancelCh:\n\t\t\t\t\treturn errCanceled\n\t\t\t\tdefault:\n\t\t\t\t}\n\t\t\t\t// Select the next chunk of headers to import\n\t\t\t\tlimit := maxHeadersProcess\n\t\t\t\tif limit > len(headers) {\n\t\t\t\t\tlimit = len(headers)\n\t\t\t\t}\n\t\t\t\tchunkHeaders := headers[:limit]\n\t\t\t\tchunkHashes := hashes[:limit]\n\n\t\t\t\t// Split the headers around the chain cutoff\n\t\t\t\tvar cutoff int\n\t\t\t\tif mode == ethconfig.SnapSync && d.chainCutoffNumber != 0 {\n\t\t\t\t\tcutoff = sort.Search(len(chunkHeaders), func(i int) bool {\n\t\t\t\t\t\treturn chunkHeaders[i].Number.Uint64() >= d.chainCutoffNumber\n\t\t\t\t\t})\n\t\t\t\t}\n\t\t\t\t// Insert the header chain into the ancient store (with block bodies and\n\t\t\t\t// receipts set to nil) if they fall before the cutoff.\n\t\t\t\tif mode == ethconfig.SnapSync && cutoff != 0 {\n\t\t\t\t\tif n, err := d.blockchain.InsertHeadersBeforeCutoff(chunkHeaders[:cutoff])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/downloader/downloader.go",
          "line": 1347,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(limit)\n\t\t\t}\n\t\t\t// Update the highest block number we know if a higher one is found.\n\t\t\td.syncStatsLock.Lock()\n\t\t\tif d.syncStatsChainHeight < origin {\n\t\t\t\td.syncStatsChainHeight = origin - 1\n\t\t\t}\n\t\t\td.syncStatsLock.Unlock()\n\n\t\t\t// Signal the downloader of the availability of new tasks\n\t\t\tif scheduled {\n\t\t\t\tfor _, ch := range []chan bool{d.queue.blockWakeCh, d.queue.receiptWakeCh} {\n\t\t\t\t\tselect {\n\t\t\t\t\tcase ch <- true:\n\t\t\t\t\tdefault:\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n}\n\nfunc (d *Downloader) checkStalling(td *big.Int, beaconMode bool) error {\n\t// If we're in legacy sync mode, we need to check total difficulty\n\t// violations from malicious peers. That is not needed in beacon\n\t// mode and we can skip to terminating sync.\n\tif !beaconMode {\n\t\t// If no headers were retrieved at all, the peer violated its TD promise that it had a\n\t\t// better chain compared to ours. The only exception is if its promised blocks were\n\t\t// already imported by other means (e.g. fetcher):\n\t\t//\n\t\t// R <remote peer>, L <local node>: Both at block 10\n\t\t// R: Mine block 11, and propagate it to L\n\t\t// L: Queue block 11 for import\n\t\t// L: Notice that R's head and TD increased compared to ours, start sync\n\t\t// L: Import of block 11 finishes\n\t\t// L: Sync begins, and finds common ancestor at 11\n\t\t// L: Request new headers up from 11 (R's TD was higher, it must have something)\n\t\t// R: Nothing to give\n\t\thead := d.blockchain.CurrentBlock()\n\t\tif td.Cmp(d.blockchain.GetTd(head.Hash(), head.Number.Uint64())) > 0 {\n\t\t\treturn errStallingPeer\n\t\t}\n\t\t// If snap or light syncing, ensure promised headers are indeed delivered. This is\n\t\t// needed to detect scenarios where an attacker feeds a bad pivot and then bails out\n\t\t// of delivering the post-pivot blocks that would flag the invalid content.\n\t\t//\n\t\t// This check cannot be executed \"as is\" for full imports, since blocks may still be\n\t\t// queued for processing when the header download completes. However, as long as the\n\t\t// peer gave us something useful, we're already happy/progressed (above check).\n\t\tif d.getMode() == SnapSync {\n\t\t\thead := d.blockchain.CurrentHeader()\n\t\t\tif td.Cmp(d.blockchain.GetTd(head.Hash(), head.Number.Uint64())) > 0 {\n\t\t\t\treturn errStallingPeer\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// processFullSyncContent takes fetch results from the queue and imports them into the chain.\nfunc (d *Downloader) processFullSyncContent(ttd *big.Int, beaconMode bool) error {\n\tfor {\n\t\tresults := d.queue.Results(true)\n\t\tif len(results) == 0 {\n\t\t\treturn nil\n\t\t}\n\t\tstop := make(chan struct{})\n\t\tif d.chainInsertHook != nil {\n\t\t\td.chainInsertHook(results, stop)\n\t\t}\n\t\tif err := d.importBlockResults(results)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/downloader/queue.go",
          "line": 390,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uncle.Size()\n\t\t}\n\t\tsize += common.StorageSize(len(result.Receipts))\n\t\tfor _, tx := range result.Transactions {\n\t\t\tsize += common.StorageSize(tx.Size())\n\t\t}\n\t\tsize += common.StorageSize(result.Withdrawals.Size())\n\t\tq.resultSize = common.StorageSize(blockCacheSizeWeight)*size +\n\t\t\t(1-common.StorageSize(blockCacheSizeWeight))*q.resultSize\n\t}\n\t// Using the newly calibrated result size, figure out the new throttle limit\n\t// on the result cache\n\tthrottleThreshold := uint64((common.StorageSize(blockCacheMemory) + q.resultSize - 1) / q.resultSize)\n\tthrottleThreshold = q.resultCache.SetThrottleThreshold(throttleThreshold)\n\n\t// With results removed from the cache, wake throttled fetchers\n\tfor _, ch := range []chan bool{q.blockWakeCh, q.receiptWakeCh} {\n\t\tselect {\n\t\tcase ch <- true:\n\t\tdefault:\n\t\t}\n\t}\n\t// Log some info at certain times\n\tif time.Since(q.logTime) >= 60*time.Second {\n\t\tq.logTime = time.Now()\n\n\t\tinfo := q.Stats()\n\t\tinfo = append(info, \"throttle\", throttleThreshold)\n\t\tlog.Debug(\"Downloader queue stats\", info...)\n\t}\n\treturn results\n}\n\nfunc (q *queue) Stats() []interface{} {\n\tq.lock.RLock()\n\tdefer q.lock.RUnlock()\n\n\treturn q.stats()\n}\n\nfunc (q *queue) stats() []interface{} {\n\treturn []interface{}{\n\t\t\"receiptTasks\", q.receiptTaskQueue.Size(),\n\t\t\"blockTasks\", q.blockTaskQueue.Size(),\n\t\t\"itemSize\", q.resultSize,\n\t}\n}\n\n// ReserveHeaders reserves a set of headers for the given peer, skipping any\n// previously failed batches.\nfunc (q *queue) ReserveHeaders(p *peerConnection, count int) *fetchRequest {\n\tq.lock.Lock()\n\tdefer q.lock.Unlock()\n\n\t// Short circuit if the peer's already downloading something (sanity check to\n\t// not corrupt state)\n\tif _, ok := q.headerPendPool[p.id]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/downloader/queue.go",
          "line": 763,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= MaxHeaderFetch\n\t}\n\tif ready > 0 {\n\t\t// Headers are ready for delivery, gather them and push forward (non blocking)\n\t\tprocessHeaders := make([]*types.Header, ready)\n\t\tcopy(processHeaders, q.headerResults[q.headerProced:q.headerProced+ready])\n\n\t\tprocessHashes := make([]common.Hash, ready)\n\t\tcopy(processHashes, q.headerHashes[q.headerProced:q.headerProced+ready])\n\n\t\tselect {\n\t\tcase headerProcCh <- &headerTask{\n\t\t\theaders: processHeaders,\n\t\t\thashes:  processHashes,\n\t\t}:\n\t\t\tlogger.Trace(\"Pre-scheduled new headers\", \"count\", len(processHeaders), \"from\", processHeaders[0].Number)\n\t\t\tq.headerProced += len(processHeaders)\n\t\tdefault:\n\t\t}\n\t}\n\t// Check for termination and return\n\tif len(q.headerTaskPool) == 0 {\n\t\tq.headerContCh <- false\n\t}\n\treturn len(headers), nil\n}\n\n// DeliverBodies injects a block body retrieval response into the results queue.\n// The method returns the number of blocks bodies accepted from the delivery and\n// also wakes any threads waiting for data delivery.\nfunc (q *queue) DeliverBodies(id string, txLists [][]*types.Transaction, txListHashes []common.Hash,\n\tuncleLists [][]*types.Header, uncleListHashes []common.Hash,\n\twithdrawalLists [][]*types.Withdrawal, withdrawalListHashes []common.Hash, sidecars []types.BlobSidecars,\n) (int, error) {\n\tq.lock.Lock()\n\tdefer q.lock.Unlock()\n\n\tvalidate := func(index int, header *types.Header) error {\n\t\tif txListHashes[index] != header.TxHash {\n\t\t\treturn errInvalidBody\n\t\t}\n\t\tif uncleListHashes[index] != header.UncleHash {\n\t\t\treturn errInvalidBody\n\t\t}\n\t\tif header.WithdrawalsHash == nil {\n\t\t\t// nil hash means that withdrawals should not be present in body\n\t\t\tif withdrawalLists[index] != nil {\n\t\t\t\treturn errInvalidBody\n\t\t\t}\n\t\t} else { // non-nil hash: body must have withdrawals\n\t\t\tif withdrawalLists[index] == nil {\n\t\t\t\treturn errInvalidBody\n\t\t\t}\n\t\t\tif withdrawalListHashes[index] != *header.WithdrawalsHash {\n\t\t\t\treturn errInvalidBody\n\t\t\t}\n\t\t}\n\t\t// Blocks must have a number of blobs corresponding to the header gas usage,\n\t\t// and zero before the Cancun hardfork.\n\t\tvar blobs int\n\t\tfor _, tx := range txLists[index] {\n\t\t\t// Count the number of blobs to validate against the header's blobGasUsed\n\t\t\tblobs += len(tx.BlobHashes())\n\n\t\t\t// Validate the data blobs individually too\n\t\t\tif tx.Type() == types.BlobTxType {\n\t\t\t\tif len(tx.BlobHashes()) == 0 {\n\t\t\t\t\treturn errInvalidBody\n\t\t\t\t}\n\t\t\t\tfor _, hash := range tx.BlobHashes() {\n\t\t\t\t\tif !kzg4844.IsValidVersionedHash(hash[:]) {\n\t\t\t\t\t\treturn errInvalidBody\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif tx.BlobTxSidecar() != nil {\n\t\t\t\t\treturn errInvalidBody\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif header.BlobGasUsed != nil {\n\t\t\tif want := *header.BlobGasUsed / params.BlobTxBlobGasPerBlob",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/downloader/resultstore.go",
          "line": 181,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(limit)\n\tr.indexIncomplete.Add(int32(-limit))\n\n\treturn results\n}\n\n// Prepare initialises the offset with the given block number\nfunc (r *resultStore) Prepare(offset uint64) {\n\tr.lock.Lock()\n\tdefer r.lock.Unlock()\n\n\tif r.resultOffset < offset {\n\t\tr.resultOffset = offset\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/downloader/downloader_test.go",
          "line": 1225,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= span + 1\n\t\t}\n\t\treturn r\n\t}\n\tfor i, tt := range testCases {\n\t\tfrom, count, span, max := calculateRequestSpan(tt.remoteHeight, tt.localHeight)\n\t\tdata := reqs(int(from), count, span)\n\n\t\tif max != uint64(data[len(data)-1]) {\n\t\t\tt.Errorf(\"test %d: wrong last value %d != %d\", i, data[len(data)-1], max)\n\t\t}\n\t\tfailed := false\n\t\tif len(data) != len(tt.expected) {\n\t\t\tfailed = true\n\t\t\tt.Errorf(\"test %d: length wrong, expected %d got %d\", i, len(tt.expected), len(data))\n\t\t} else {\n\t\t\tfor j, n := range data {\n\t\t\t\tif n != tt.expected[j] {\n\t\t\t\t\tfailed = true\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\tif failed {\n\t\t\tres := strings.ReplaceAll(fmt.Sprint(data), \" \", \",\")\n\t\t\texp := strings.ReplaceAll(fmt.Sprint(tt.expected), \" \", \",\")\n\t\t\tt.Logf(\"got: %v\\n\", res)\n\t\t\tt.Logf(\"exp: %v\\n\", exp)\n\t\t\tt.Errorf(\"test %d: wrong values\", i)\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/downloader/queue_test.go",
          "line": 297,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= l\n\t\t}\n\t}()\n\twg.Add(1)\n\tgo func() {\n\t\t// collect results\n\t\tdefer wg.Done()\n\t\ttot := 0\n\t\tfor {\n\t\t\tres := q.Results(true)\n\t\t\ttot += len(res)\n\t\t\tfmt.Printf(\"got %d results, %d tot\\n\", len(res), tot)\n\t\t\t// Now we can forget about these\n\t\t\tworld.forget(res[len(res)-1].Header.Number.Uint64())\n\t\t}\n\t}()\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\t// reserve body fetch\n\t\ti := 4\n\t\tfor {\n\t\t\tpeer := dummyPeer(fmt.Sprintf(\"peer-%d\", i))\n\t\t\tf, _, _ := q.ReserveBodies(peer, rand.Intn(30))\n\t\t\tif f != nil {\n\t\t\t\tvar (\n\t\t\t\t\temptyList []*types.Header\n\t\t\t\t\ttxset     [][]*types.Transaction\n\t\t\t\t\tuncleset  [][]*types.Header\n\t\t\t\t)\n\t\t\t\tnumToSkip := rand.Intn(len(f.Headers))\n\t\t\t\tfor _, hdr := range f.Headers[0 : len(f.Headers)-numToSkip] {\n\t\t\t\t\ttxset = append(txset, world.getTransactions(hdr.Number.Uint64()))\n\t\t\t\t\tuncleset = append(uncleset, emptyList)\n\t\t\t\t}\n\t\t\t\tvar (\n\t\t\t\t\ttxsHashes   = make([]common.Hash, len(txset))\n\t\t\t\t\tuncleHashes = make([]common.Hash, len(uncleset))\n\t\t\t\t)\n\t\t\t\thasher := trie.NewStackTrie(nil)\n\t\t\t\tfor i, txs := range txset {\n\t\t\t\t\ttxsHashes[i] = types.DeriveSha(types.Transactions(txs), hasher)\n\t\t\t\t}\n\t\t\t\tfor i, uncles := range uncleset {\n\t\t\t\t\tuncleHashes[i] = types.CalcUncleHash(uncles)\n\t\t\t\t}\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t\t_, err := q.DeliverBodies(peer.id, txset, txsHashes, uncleset, uncleHashes, nil, nil, nil)\n\t\t\t\tif err != nil {\n\t\t\t\t\tfmt.Printf(\"delivered %d bodies %v\\n\", len(txset), err)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\ti++\n\t\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\t}\n\t\t}\n\t}()\n\tgo func() {\n\t\tdefer wg.Done()\n\t\t// reserve receiptfetch\n\t\tpeer := dummyPeer(\"peer-3\")\n\t\tfor {\n\t\t\tf, _, _ := q.ReserveReceipts(peer, rand.Intn(50))\n\t\t\tif f != nil {\n\t\t\t\tvar rcs []types.Receipts\n\t\t\t\tfor _, hdr := range f.Headers {\n\t\t\t\t\trcs = append(rcs, world.getReceipts(hdr.Number.Uint64()))\n\t\t\t\t}\n\t\t\t\thasher := trie.NewStackTrie(nil)\n\t\t\t\thashes := make([]common.Hash, len(rcs))\n\t\t\t\tfor i, receipt := range rcs {\n\t\t\t\t\thashes[i] = types.DeriveSha(receipt, hasher)\n\t\t\t\t}\n\t\t\t\t_, err := q.DeliverReceipts(peer.id, types.EncodeBlockReceiptLists(rcs), hashes)\n\t\t\t\tif err != nil {\n\t\t\t\t\tfmt.Printf(\"delivered %d receipts %v\\n\", len(rcs), err)\n\t\t\t\t}\n\t\t\t\ttime.Sleep(100 * time.Millisecond)\n\t\t\t} else {\n\t\t\t\ttime.Sleep(200 * time.Millisecond)\n\t\t\t}\n\t\t}\n\t}()\n\twg.Add(1)\n\tgo func() {\n\t\tdefer wg.Done()\n\t\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/downloader/peer.go",
          "line": 224,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/downloader/peer.go",
          "line": 241,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/tracker.go",
          "line": 68,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t\tt.oldest += uint64(count)\n\t\tcopy(t.used, t.used[count:])\n\n\t\t// Clean up the array tail since they are useless now.\n\t\tfor i := t.limit - count",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/api.go",
          "line": 434,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(txs))\n\t\t}\n\t})\n\n\t// Keep reading the trace results and stream them to result channel.\n\tretCh := make(chan *blockTraceResult)\n\tgopool.Submit(func() {\n\t\tdefer close(retCh)\n\t\tvar (\n\t\t\tnext = start.NumberU64() + 1\n\t\t\tdone = make(map[uint64]*blockTraceResult)\n\t\t)\n\t\tfor res := range resCh {\n\t\t\t// Queue up next received result\n\t\t\tresult := &blockTraceResult{\n\t\t\t\tBlock:  hexutil.Uint64(res.block.NumberU64()),\n\t\t\t\tHash:   res.block.Hash(),\n\t\t\t\tTraces: res.results,\n\t\t\t}\n\t\t\tdone[uint64(result.Block)] = result\n\n\t\t\t// Stream completed traces to the result channel\n\t\t\tfor result, ok := done[next]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/tracers/api.go",
          "line": 418,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(next.ParentHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/tracers/api.go",
          "line": 578,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/tracers/api.go",
          "line": 661,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/tracers/api.go",
          "line": 886,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block.ParentHash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/tracers/api.go",
          "line": 1096,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(n)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.82,
          "confidence": 0.9738,
          "ensemble_confidence": 0.87642
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 1106,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t})\n\tbackend.refHook = func() { ref.Add(1) }\n\tbackend.relHook = func() { rel.Add(1) }\n\tapi := NewAPI(backend)\n\n\tsingle := `{\"txHash\":\"0x0000000000000000000000000000000000000000000000000000000000000000\",\"result\":{\"gas\":21000,\"failed\":false,\"returnValue\":\"0x\",\"structLogs\":[]}}`\n\tvar cases = []struct {\n\t\tstart  uint64\n\t\tend    uint64\n\t\tconfig *TraceConfig\n\t}{\n\t\t{0, 50, nil},  // the entire chain range, blocks [1, 50]\n\t\t{10, 20, nil}, // the middle chain range, blocks [11, 20]\n\t}\n\tfor _, c := range cases {\n\t\tref.Store(0)\n\t\trel.Store(0)\n\n\t\tfrom, _ := api.blockByNumber(context.Background(), rpc.BlockNumber(c.start))\n\t\tto, _ := api.blockByNumber(context.Background(), rpc.BlockNumber(c.end))\n\t\tresCh := api.traceChain(from, to, c.config, nil)\n\n\t\tnext := c.start + 1\n\t\tfor result := range resCh {\n\t\t\tif have, want := uint64(result.Block), next",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 1145,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t\tif next != c.end+1 {\n\t\t\tt.Error(\"Missing tracing block\")\n\t\t}\n\n\t\tif nref, nrel := ref.Load(), rel.Load()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 778,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BLOCKHASH(0)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.82,
          "confidence": 0.9738,
          "ensemble_confidence": 0.87642
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 780,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BLOCKHASH(0x1336)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 782,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BLOCKHASH(0x1337)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/tracers/api_test.go",
          "line": 214,
          "category": "topological_instabilities",
          "pattern": "balances?\\[.*\\]\\s*=.*(?!transfer)",
          "match": "Balance[addr] = (*hexutil.Big)(new)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/bsc/handler.go",
          "line": 178,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/bsc/handshake.go",
          "line": 25,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/bsc/dispatcher.go",
          "line": 60,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/bsc/peer.go",
          "line": 138,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\treturn p.periodCounter > uint(secondsPerPeriod*receiveRateLimitPerSecond)\n}\n\n// broadcastVotes is a write loop that schedules votes broadcasts\n// to the remote peer. The goal is to have an async writer that does not lock up\n// node internals and at the same time rate limits queued data.\nfunc (p *Peer) broadcastVotes() {\n\tfor {\n\t\tselect {\n\t\tcase votes := <-p.voteBroadcast:\n\t\t\tif err := p.sendVotes(votes)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/bsc/peer.go",
          "line": 112,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 311,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(common.HashLength + len(account))\n\t\taccounts = append(accounts, &AccountData{\n\t\t\tHash: hash,\n\t\t\tBody: account,\n\t\t})\n\t\t// If we've exceeded the request threshold, abort\n\t\tif bytes.Compare(hash[:], req.Limit[:]) >= 0 {\n\t\t\tbreak\n\t\t}\n\t\tif size > req.Bytes {\n\t\t\tbreak\n\t\t}\n\t}\n\tit.Release()\n\n\t// Generate the Merkle proofs for the first and last account\n\tproof := trienode.NewProofSet()\n\tif err := tr.Prove(req.Origin[:], proof)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 407,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(common.HashLength + len(slot))\n\t\t\tstorage = append(storage, &StorageData{\n\t\t\t\tHash: hash,\n\t\t\t\tBody: slot,\n\t\t\t})\n\t\t\t// If we've exceeded the request threshold, abort\n\t\t\tif bytes.Compare(hash[:], limit[:]) >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(storage) > 0 {\n\t\t\tslots = append(slots, storage)\n\t\t}\n\t\tit.Release()\n\n\t\t// Generate the Merkle proofs for the first and last storage slot, but\n\t\t// only if the response was capped. If the entire storage trie included\n\t\t// in the response, no need for any proofs.\n\t\tif origin != (common.Hash{}) || (abort && len(storage) > 0) {\n\t\t\t// Request started at a non-zero hash or was capped prematurely, add\n\t\t\t// the endpoint Merkle proofs\n\t\t\taccTrie, err := trie.NewStateTrie(trie.StateTrieID(req.Root), chain.TrieDB())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tacc, err := accTrie.GetAccountByHash(account)\n\t\t\tif err != nil || acc == nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tid := trie.StorageTrieID(req.Root, account, acc.Root)\n\t\t\tstTrie, err := trie.NewStateTrie(id, chain.TrieDB())\n\t\t\tif err != nil {\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tproof := trienode.NewProofSet()\n\t\t\tif err := stTrie.Prove(origin[:], proof)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 483,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(blob))\n\t\t}\n\t\tif bytes > req.Bytes {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn codes\n}\n\n// ServiceGetTrieNodesQuery assembles the response to a trie nodes query.\n// It is exposed to allow external packages to test protocol behavior.\nfunc ServiceGetTrieNodesQuery(chain *core.BlockChain, req *GetTrieNodesPacket, start time.Time) ([][]byte, error) {\n\tif req.Bytes > softResponseLimit {\n\t\treq.Bytes = softResponseLimit\n\t}\n\t// Make sure we have the state associated with the request\n\ttriedb := chain.TrieDB()\n\n\taccTrie, err := trie.NewStateTrie(trie.StateTrieID(req.Root), triedb)\n\tif err != nil {\n\t\t// We don't have the requested state available, bail out\n\t\treturn nil, nil\n\t}\n\t// The 'reader' might be nil, in which case we cannot serve storage slots\n\t// via snapshot.\n\tvar reader database.StateReader\n\tif chain.Snapshots() != nil {\n\t\treader = chain.Snapshots().Snapshot(req.Root)\n\t}\n\tif reader == nil {\n\t\treader, _ = triedb.StateReader(req.Root)\n\t}\n\t// Retrieve trie nodes until the packet size limit is reached\n\tvar (\n\t\tnodes [][]byte\n\t\tbytes uint64\n\t\tloads int // Trie hash expansions to count database reads\n\t)\n\tfor _, pathset := range req.Paths {\n\t\tswitch len(pathset) {\n\t\tcase 0:\n\t\t\t// Ensure we penalize invalid requests\n\t\t\treturn nil, fmt.Errorf(\"%w: zero-item pathset requested\", errBadRequest)\n\n\t\tcase 1:\n\t\t\t// If we're only retrieving an account trie node, fetch it directly\n\t\t\tblob, resolved, err := accTrie.GetNode(pathset[0])\n\t\t\tloads += resolved // always account database reads, even for failures\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tnodes = append(nodes, blob)\n\t\t\tbytes += uint64(len(blob))\n\n\t\tdefault:\n\t\t\tvar stRoot common.Hash\n\n\t\t\t// Storage slots requested, open the storage trie and retrieve from there\n\t\t\tif reader == nil {\n\t\t\t\t// We don't have the requested state snapshotted yet (or it is stale),\n\t\t\t\t// but can look up the account via the trie instead.\n\t\t\t\taccount, err := accTrie.GetAccountByHash(common.BytesToHash(pathset[0]))\n\t\t\t\tloads += 8 // We don't know the exact cost of lookup, this is an estimate\n\t\t\t\tif err != nil || account == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tstRoot = account.Root\n\t\t\t} else {\n\t\t\t\taccount, err := reader.Account(common.BytesToHash(pathset[0]))\n\t\t\t\tloads++ // always account database reads, even for failures\n\t\t\t\tif err != nil || account == nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tstRoot = common.BytesToHash(account.Root)\n\t\t\t}\n\t\t\tid := trie.StorageTrieID(req.Root, common.BytesToHash(pathset[0]), stRoot)\n\t\t\tstTrie, err := trie.NewStateTrie(id, triedb)\n\t\t\tloads++ // always account database reads, even for failures\n\t\t\tif err != nil {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tfor _, path := range pathset[1:] {\n\t\t\t\tblob, resolved, err := stTrie.GetNode(path)\n\t\t\t\tloads += resolved // always account database reads, even for failures\n\t\t\t\tif err != nil {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tnodes = append(nodes, blob)\n\t\t\t\tbytes += uint64(len(blob))\n\n\t\t\t\t// Sanity check limits to avoid DoS on the store trie loads\n\t\t\t\tif bytes > req.Bytes || loads > maxTrieNodeLookups || time.Since(start) > maxTrieNodeTimeSpent {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Abort request processing if we've exceeded our limits\n\t\tif bytes > req.Bytes || loads > maxTrieNodeLookups || time.Since(start) > maxTrieNodeTimeSpent {\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nodes, nil\n}\n\n// NodeInfo represents a short summary of the `snap` sub-protocol metadata\n// known about the host peer.\ntype NodeInfo struct{}\n\n// nodeInfo retrieves some `snap` protocol metadata about the running host node.\nfunc nodeInfo(chain *core.BlockChain) *NodeInfo {\n\treturn &NodeInfo{}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 161,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 193,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 227,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/eth/protocols/snap/handler.go",
          "line": 254,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 780,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(key) + len(value))\n\t\t\t\t\t},\n\t\t\t\t}\n\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\ttask.genTrie = newHashTrie(task.genBatch)\n\t\t\t\t}\n\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\ttask.genTrie = newPathTrie(common.Hash{}, task.Next != common.Hash{}, s.db, task.genBatch)\n\t\t\t\t}\n\t\t\t\t// Restore leftover storage tasks\n\t\t\t\tfor accountHash, subtasks := range task.SubTasks {\n\t\t\t\t\tfor _, subtask := range subtasks {\n\t\t\t\t\t\tsubtask.genBatch = ethdb.HookedBatch{\n\t\t\t\t\t\t\tBatch: stateDiskDB.NewBatch(),\n\t\t\t\t\t\t\tOnPut: func(key []byte, value []byte) {\n\t\t\t\t\t\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\t\t\tsubtask.genTrie = newHashTrie(subtask.genBatch)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\t\t\tsubtask.genTrie = newPathTrie(accountHash, subtask.Next != common.Hash{}, s.db, subtask.genBatch)\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\ts.lock.Lock()\n\t\t\tdefer s.lock.Unlock()\n\n\t\t\ts.snapped = len(s.tasks) == 0\n\n\t\t\ts.accountSynced = progress.AccountSynced\n\t\t\ts.accountBytes = progress.AccountBytes\n\t\t\ts.bytecodeSynced = progress.BytecodeSynced\n\t\t\ts.bytecodeBytes = progress.BytecodeBytes\n\t\t\ts.storageSynced = progress.StorageSynced\n\t\t\ts.storageBytes = progress.StorageBytes\n\n\t\t\ts.trienodeHealSynced = progress.TrienodeHealSynced\n\t\t\ts.trienodeHealBytes = progress.TrienodeHealBytes\n\t\t\ts.bytecodeHealSynced = progress.BytecodeHealSynced\n\t\t\ts.bytecodeHealBytes = progress.BytecodeHealBytes\n\t\t\treturn\n\t\t}\n\t}\n\t// Either we've failed to decode the previous state, or there was none.\n\t// Start a fresh sync by chunking up the account range and scheduling\n\t// them for retrieval.\n\ts.tasks = nil\n\ts.accountSynced, s.accountBytes = 0, 0\n\ts.bytecodeSynced, s.bytecodeBytes = 0, 0\n\ts.storageSynced, s.storageBytes = 0, 0\n\ts.trienodeHealSynced, s.trienodeHealBytes = 0, 0\n\ts.bytecodeHealSynced, s.bytecodeHealBytes = 0, 0\n\n\tvar next common.Hash\n\tstep := new(big.Int).Sub(\n\t\tnew(big.Int).Div(\n\t\t\tnew(big.Int).Exp(common.Big2, common.Big256, nil),\n\t\t\tbig.NewInt(int64(accountConcurrency)),\n\t\t), common.Big1,\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 852,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(key) + len(value))\n\t\t\t},\n\t\t}\n\t\tvar tr genTrie\n\t\tif s.scheme == rawdb.HashScheme {\n\t\t\ttr = newHashTrie(batch)\n\t\t}\n\t\tif s.scheme == rawdb.PathScheme {\n\t\t\ttr = newPathTrie(common.Hash{}, next != common.Hash{}, s.db, batch)\n\t\t}\n\t\ts.tasks = append(s.tasks, &accountTask{\n\t\t\tNext:           next,\n\t\t\tLast:           last,\n\t\t\tSubTasks:       make(map[common.Hash][]*storageTask),\n\t\t\tgenBatch:       batch,\n\t\t\tstateCompleted: make(map[common.Hash]struct{}),\n\t\t\tgenTrie:        tr,\n\t\t})\n\t\tlog.Debug(\"Created account sync task\", \"from\", next, \"last\", last)\n\t\tnext = common.BigToHash(new(big.Int).Add(last.Big(), common.Big1))\n\t}\n}\n\n// saveSyncStatus marshals the remaining sync tasks into leveldb.\nfunc (s *Syncer) saveSyncStatus() {\n\t// Serialize any partial progress to disk before spinning down\n\tfor _, task := range s.tasks {\n\t\t// Claim the right boundary as incomplete before flushing the\n\t\t// accumulated nodes in batch, the nodes on right boundary\n\t\t// will be discarded and cleaned up by this call.\n\t\ttask.genTrie.commit(false)\n\t\tif err := task.genBatch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2021,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= codes\n\ts.bytecodeBytes += bytes\n\n\tlog.Debug(\"Persisted set of bytecodes\", \"count\", codes, \"bytes\", bytes)\n\n\t// If this delivery completed the last pending task, forward the account task\n\t// to the next chunk\n\tif res.task.pend == 0 {\n\t\ts.forwardAccountTask(res.task)\n\t\treturn\n\t}\n\t// Some accounts are still incomplete, leave as is for the storage and contract\n\t// task assigners to pick up and fill.\n}\n\n// processStorageResponse integrates an already validated storage response\n// into the account tasks.\nfunc (s *Syncer) processStorageResponse(res *storageResponse) {\n\t// Switch the subtask from pending to idle\n\tif res.subTask != nil {\n\t\tres.subTask.req = nil\n\t}\n\n\tvar usingMultDatabase bool\n\tbatch := ethdb.HookedBatch{\n\t\tBatch: s.db.GetStateStore().NewBatch(),\n\t\tOnPut: func(key []byte, value []byte) {\n\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t},\n\t}\n\tvar snapBatch ethdb.HookedBatch\n\tif s.db.HasSeparateStateStore() {\n\t\tusingMultDatabase = true\n\t\tsnapBatch = ethdb.HookedBatch{\n\t\t\tBatch: s.db.NewBatch(),\n\t\t\tOnPut: func(key []byte, value []byte) {\n\t\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t\t},\n\t\t}\n\t}\n\n\tvar (\n\t\tslots           int\n\t\toldStorageBytes = s.storageBytes\n\t)\n\t// Iterate over all the accounts and reconstruct their storage tries from the\n\t// delivered slots\n\tfor i, account := range res.accounts {\n\t\t// If the account was not delivered, reschedule it\n\t\tif i >= len(res.hashes) {\n\t\t\tres.mainTask.stateTasks[account] = res.roots[i]\n\t\t\tcontinue\n\t\t}\n\t\t// State was delivered, if complete mark as not needed any more, otherwise\n\t\t// mark the account as needing healing\n\t\tfor j, hash := range res.mainTask.res.hashes {\n\t\t\tif account != hash {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tacc := res.mainTask.res.accounts[j]\n\n\t\t\t// If the packet contains multiple contract storage slots, all\n\t\t\t// but the last are surely complete. The last contract may be\n\t\t\t// chunked, so check it's continuation flag.\n\t\t\tif res.subTask == nil && res.mainTask.needState[j] && (i < len(res.hashes)-1 || !res.cont) {\n\t\t\t\tres.mainTask.needState[j] = false\n\t\t\t\tres.mainTask.pend--\n\t\t\t\tres.mainTask.stateCompleted[account] = struct{}{} // mark it as completed\n\t\t\t\tsmallStorageGauge.Inc(1)\n\t\t\t}\n\t\t\t// If the last contract was chunked, mark it as needing healing\n\t\t\t// to avoid writing it out to disk prematurely.\n\t\t\tif res.subTask == nil && !res.mainTask.needHeal[j] && i == len(res.hashes)-1 && res.cont {\n\t\t\t\tres.mainTask.needHeal[j] = true\n\t\t\t}\n\t\t\t// If the last contract was chunked, we need to switch to large\n\t\t\t// contract handling mode\n\t\t\tif res.subTask == nil && i == len(res.hashes)-1 && res.cont {\n\t\t\t\t// If we haven't yet started a large-contract retrieval, create\n\t\t\t\t// the subtasks for it within the main account task\n\t\t\t\tif tasks, ok := res.mainTask.SubTasks[account]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2134,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(key) + len(value))\n\t\t\t\t\t\t},\n\t\t\t\t\t}\n\t\t\t\t\tvar tr genTrie\n\t\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\t\ttr = newHashTrie(batch)\n\t\t\t\t\t}\n\t\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\t\t// Keep the left boundary as it's the first range.\n\t\t\t\t\t\ttr = newPathTrie(account, false, s.db, batch)\n\t\t\t\t\t}\n\t\t\t\t\ttasks = append(tasks, &storageTask{\n\t\t\t\t\t\tNext:     common.Hash{},\n\t\t\t\t\t\tLast:     r.End(),\n\t\t\t\t\t\troot:     acc.Root,\n\t\t\t\t\t\tgenBatch: batch,\n\t\t\t\t\t\tgenTrie:  tr,\n\t\t\t\t\t})\n\t\t\t\t\tfor r.Next() {\n\t\t\t\t\t\tbatch := ethdb.HookedBatch{\n\t\t\t\t\t\t\tBatch: s.db.GetStateStore().NewBatch(),\n\t\t\t\t\t\t\tOnPut: func(key []byte, value []byte) {\n\t\t\t\t\t\t\t\ts.storageBytes += common.StorageSize(len(key) + len(value))\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t}\n\t\t\t\t\t\tvar tr genTrie\n\t\t\t\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\t\t\t\ttr = newHashTrie(batch)\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t\t\t\ttr = newPathTrie(account, true, s.db, batch)\n\t\t\t\t\t\t}\n\t\t\t\t\t\ttasks = append(tasks, &storageTask{\n\t\t\t\t\t\t\tNext:     r.Start(),\n\t\t\t\t\t\t\tLast:     r.End(),\n\t\t\t\t\t\t\troot:     acc.Root,\n\t\t\t\t\t\t\tgenBatch: batch,\n\t\t\t\t\t\t\tgenTrie:  tr,\n\t\t\t\t\t\t})\n\t\t\t\t\t}\n\t\t\t\t\tfor _, task := range tasks {\n\t\t\t\t\t\tlog.Debug(\"Created storage sync task\", \"account\", account, \"root\", acc.Root, \"from\", task.Next, \"last\", task.Last)\n\t\t\t\t\t}\n\t\t\t\t\tres.mainTask.SubTasks[account] = tasks\n\n\t\t\t\t\t// Since we've just created the sub-tasks, this response\n\t\t\t\t\t// is surely for the first one (zero origin)\n\t\t\t\t\tres.subTask = tasks[0]\n\t\t\t\t}\n\t\t\t}\n\t\t\t// If we're in large contract delivery mode, forward the subtask\n\t\t\tif res.subTask != nil {\n\t\t\t\t// Ensure the response doesn't overflow into the subsequent task\n\t\t\t\tlast := res.subTask.Last.Big()\n\t\t\t\t// Find the first overflowing key. While at it, mark res as complete\n\t\t\t\t// if we find the range to include or pass the 'last'\n\t\t\t\tindex := sort.Search(len(res.hashes[i]), func(k int) bool {\n\t\t\t\t\tcmp := res.hashes[i][k].Big().Cmp(last)\n\t\t\t\t\tif cmp >= 0 {\n\t\t\t\t\t\tres.cont = false\n\t\t\t\t\t}\n\t\t\t\t\treturn cmp > 0\n\t\t\t\t})\n\t\t\t\tif index >= 0 {\n\t\t\t\t\t// cut off excess\n\t\t\t\t\tres.hashes[i] = res.hashes[i][:index]\n\t\t\t\t\tres.slots[i] = res.slots[i][:index]\n\t\t\t\t}\n\t\t\t\t// Forward the relevant storage chunk (even if created just now)\n\t\t\t\tif res.cont {\n\t\t\t\t\tres.subTask.Next = incHash(res.hashes[i][len(res.hashes[i])-1])\n\t\t\t\t} else {\n\t\t\t\t\tres.subTask.done = true\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t\t// Iterate over all the complete contracts, reconstruct the trie nodes and\n\t\t// push them to disk. If the contract is chunked, the trie nodes will be\n\t\t// reconstructed later.\n\t\tslots += len(res.hashes[i])\n\n\t\tif i < len(res.hashes)-1 || res.subTask == nil {\n\t\t\t// no need to make local reassignment of account: this closure does not outlive the loop\n\t\t\tvar tr genTrie\n\t\t\tif s.scheme == rawdb.HashScheme {\n\t\t\t\ttr = newHashTrie(batch)\n\t\t\t}\n\t\t\tif s.scheme == rawdb.PathScheme {\n\t\t\t\t// Keep the left boundary as it's complete\n\t\t\t\ttr = newPathTrie(account, false, s.db, batch)\n\t\t\t}\n\t\t\tfor j := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2283,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(slots)\n\n\tlog.Debug(\"Persisted set of storage slots\", \"accounts\", len(res.hashes), \"slots\", slots, \"bytes\", s.storageBytes-oldStorageBytes)\n\n\t// If this delivery completed the last pending task, forward the account task\n\t// to the next chunk\n\tif res.mainTask.pend == 0 {\n\t\ts.forwardAccountTask(res.mainTask)\n\t\treturn\n\t}\n\t// Some accounts are still incomplete, leave as is for the storage and contract\n\t// task assigners to pick up and fill.\n}\n\n// processTrienodeHealResponse integrates an already validated trienode response\n// into the healer tasks.\nfunc (s *Syncer) processTrienodeHealResponse(res *trienodeHealResponse) {\n\tvar (\n\t\tstart = time.Now()\n\t\tfills int\n\t)\n\tfor i, hash := range res.hashes {\n\t\tnode := res.nodes[i]\n\n\t\t// If the trie node was not delivered, reschedule it\n\t\tif node == nil {\n\t\t\tres.task.trieTasks[res.paths[i]] = res.hashes[i]\n\t\t\tcontinue\n\t\t}\n\t\tfills++\n\n\t\t// Push the trie node into the state syncer\n\t\ts.trienodeHealSynced++\n\t\ts.trienodeHealBytes += common.StorageSize(len(node))\n\n\t\terr := s.healer.scheduler.ProcessNode(trie.NodeSyncResult{Path: res.paths[i], Data: node})\n\t\tswitch err {\n\t\tcase nil:\n\t\tcase trie.ErrAlreadyProcessed:\n\t\t\ts.trienodeHealDups++\n\t\tcase trie.ErrNotRequested:\n\t\t\ts.trienodeHealNops++\n\t\tdefault:\n\t\t\tlog.Error(\"Invalid trienode processed\", \"hash\", hash, \"err\", err)\n\t\t}\n\t}\n\ts.commitHealer(false)\n\n\t// Calculate the processing rate of one filled trie node\n\trate := float64(fills) / (float64(time.Since(start)) / float64(time.Second))\n\n\t// Update the currently measured trienode queueing and processing throughput.\n\t//\n\t// The processing rate needs to be updated uniformly independent if we've\n\t// processed 1x100 trie nodes or 100x1 to keep the rate consistent even in\n\t// the face of varying network packets. As such, we cannot just measure the\n\t// time it took to process N trie nodes and update once, we need one update\n\t// per trie node.\n\t//\n\t// Naively, that would be:\n\t//\n\t//   for i:=0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2416,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(node))\n\n\t\terr := s.healer.scheduler.ProcessCode(trie.CodeSyncResult{Hash: hash, Data: node})\n\t\tswitch err {\n\t\tcase nil:\n\t\tcase trie.ErrAlreadyProcessed:\n\t\t\ts.bytecodeHealDups++\n\t\tcase trie.ErrNotRequested:\n\t\t\ts.bytecodeHealNops++\n\t\tdefault:\n\t\t\tlog.Error(\"Invalid bytecode processed\", \"hash\", hash, \"err\", err)\n\t\t}\n\t}\n\ts.commitHealer(false)\n}\n\n// forwardAccountTask takes a filled account task and persists anything available\n// into the database, after which it forwards the next account marker so that the\n// task's next chunk may be filled.\nfunc (s *Syncer) forwardAccountTask(task *accountTask) {\n\t// Remove any pending delivery\n\tres := task.res\n\tif res == nil {\n\t\treturn // nothing to forward\n\t}\n\ttask.res = nil\n\n\t// Persist the received account segments. These flat state maybe\n\t// outdated during the sync, but it can be fixed later during the\n\t// snapshot generation.\n\toldAccountBytes := s.accountBytes\n\n\tbatch := ethdb.HookedBatch{\n\t\tBatch: s.db.NewBatch(),\n\t\tOnPut: func(key []byte, value []byte) {\n\t\t\ts.accountBytes += common.StorageSize(len(key) + len(value))\n\t\t},\n\t}\n\tfor i, hash := range res.hashes {\n\t\tif task.needCode[i] || task.needState[i] {\n\t\t\tbreak\n\t\t}\n\t\tslim := types.SlimAccountRLP(*res.accounts[i])\n\t\trawdb.WriteAccountSnapshot(batch, hash, slim)\n\n\t\tif !task.needHeal[i] {\n\t\t\t// If the storage task is complete, drop it into the stack trie\n\t\t\t// to generate account trie nodes for it\n\t\t\tfull, err := types.FullAccountRLP(slim) // TODO(karalabe): Slim parsing can be omitted\n\t\t\tif err != nil {\n\t\t\t\tpanic(err) // Really shouldn't ever happen\n\t\t\t}\n\t\t\ttask.genTrie.update(hash[:], full)\n\t\t} else {\n\t\t\t// If the storage task is incomplete, explicitly delete the corresponding\n\t\t\t// account item from the account trie to ensure that all nodes along the\n\t\t\t// path to the incomplete storage trie are cleaned up.\n\t\t\tif err := task.genTrie.delete(hash[:])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2482,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(res.accounts))\n\n\t// Task filling persisted, push it the chunk marker forward to the first\n\t// account still missing data.\n\tfor i, hash := range res.hashes {\n\t\tif task.needCode[i] || task.needState[i] {\n\t\t\treturn\n\t\t}\n\t\ttask.Next = incHash(hash)\n\n\t\t// Remove the completion flag once the account range is pushed\n\t\t// forward. The leftover accounts will be skipped in the next\n\t\t// cycle.\n\t\tdelete(task.stateCompleted, hash)\n\t}\n\t// All accounts marked as complete, track if the entire task is done\n\ttask.done = !res.cont\n\n\t// Error out if there is any leftover completion flag.\n\tif task.done && len(task.stateCompleted) != 0 {\n\t\tpanic(fmt.Errorf(\"storage completion flags should be emptied, %d left\", len(task.stateCompleted)))\n\t}\n\t// Stack trie could have generated trie nodes, push them to disk (we need to\n\t// flush after finalizing task.done. It's fine even if we crash and lose this\n\t// write as it will only cause more data to be downloaded during heal.\n\tif task.done {\n\t\ttask.genTrie.commit(task.Last == common.MaxHash)\n\t\tif err := task.genBatch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2528,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(account))\n\t}\n\tfor _, node := range proof {\n\t\tsize += common.StorageSize(len(node))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering range of accounts\", \"hashes\", len(hashes), \"accounts\", len(accounts), \"proofs\", len(proof), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2641,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(code))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering set of bytecodes\", \"bytecodes\", len(bytecodes), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2744,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(common.HashLength * len(hashset))\n\t\thashCount += len(hashset)\n\t}\n\tfor _, slotset := range slots {\n\t\tfor _, slot := range slotset {\n\t\t\tsize += common.StorageSize(len(slot))\n\t\t}\n\t\tslotCount += len(slotset)\n\t}\n\tfor _, node := range proof {\n\t\tsize += common.StorageSize(len(node))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering ranges of storage slots\", \"accounts\", len(hashes), \"hashes\", hashCount, \"slots\", slotCount, \"proofs\", len(proof), \"size\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2888,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(node))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering set of healing trienodes\", \"trienodes\", len(trienodes), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2995,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(code))\n\t}\n\tlogger := peer.Log().New(\"reqid\", id)\n\tlogger.Trace(\"Delivering set of healing bytecodes\", \"bytecodes\", len(bytecodes), \"bytes\", size)\n\n\t// Whether or not the response is valid, we can mark the peer as idle and\n\t// notify the scheduler to assign a new task. If the response is invalid,\n\t// we'll drop the peer in a bit.\n\tdefer func() {\n\t\ts.lock.Lock()\n\t\tdefer s.lock.Unlock()\n\t\tif _, ok := s.peers[peer.ID()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 3100,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\ts.accountHealedBytes += common.StorageSize(1 + common.HashLength + len(blob))\n\t}\n\tif len(paths) == 2 {\n\t\trawdb.WriteStorageSnapshot(s.stateWriter, common.BytesToHash(paths[0]), common.BytesToHash(paths[1]), value)\n\t\ts.storageHealed += 1\n\t\ts.storageHealedBytes += common.StorageSize(1 + 2*common.HashLength + len(value))\n\t}\n\tif s.stateWriter.ValueSize() > ethdb.IdealBatchSize {\n\t\ts.stateWriter.Write() // It's fine to ignore the error here\n\t\ts.stateWriter.Reset()\n\t}\n\treturn nil\n}\n\n// hashSpace is the total size of the 256 bit hash space for accounts.\nvar hashSpace = new(big.Int).Exp(common.Big2, common.Big256, nil)\n\n// report calculates various status reports and provides it to the user.\nfunc (s *Syncer) report(force bool) {\n\tif len(s.tasks) > 0 {\n\t\ts.reportSyncProgress(force)\n\t\treturn\n\t}\n\ts.reportHealProgress(force)\n}\n\n// reportSyncProgress calculates various status reports and provides it to the user.\nfunc (s *Syncer) reportSyncProgress(force bool) {\n\t// Don't report all the events, just occasionally\n\tif !force && time.Since(s.logTime) < 8*time.Second {\n\t\treturn\n\t}\n\t// Don't report anything until we have a meaningful progress\n\tsynced := s.accountBytes + s.bytecodeBytes + s.storageBytes\n\tif synced == 0 {\n\t\treturn\n\t}\n\taccountGaps := new(big.Int)\n\tfor _, task := range s.tasks {\n\t\taccountGaps.Add(accountGaps, new(big.Int).Sub(task.Last.Big(), task.Next.Big()))\n\t}\n\taccountFills := new(big.Int).Sub(hashSpace, accountGaps)\n\tif accountFills.BitLen() == 0 {\n\t\treturn\n\t}\n\ts.logTime = time.Now()\n\testBytes := float64(new(big.Int).Div(\n\t\tnew(big.Int).Mul(new(big.Int).SetUint64(uint64(synced)), hashSpace),\n\t\taccountFills,\n\t).Uint64())\n\t// Don't report anything until we have a meaningful progress\n\tif estBytes < 1.0 {\n\t\treturn\n\t}\n\t// Cap the estimated state size using the synced size to avoid negative values\n\tif estBytes < float64(synced) {\n\t\testBytes = float64(synced)\n\t}\n\telapsed := time.Since(s.startTime)\n\testTime := elapsed / time.Duration(synced) * time.Duration(estBytes)\n\n\t// Create a mega progress report\n\tvar (\n\t\tprogress = fmt.Sprintf(\"%.2f%%\", float64(synced)*100/estBytes)\n\t\taccounts = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.accountSynced), s.accountBytes.TerminalString())\n\t\tstorage  = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.storageSynced), s.storageBytes.TerminalString())\n\t\tbytecode = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.bytecodeSynced), s.bytecodeBytes.TerminalString())\n\t)\n\tlog.Info(\"Syncing: state download in progress\", \"synced\", progress, \"state\", synced,\n\t\t\"accounts\", accounts, \"slots\", storage, \"codes\", bytecode, \"eta\", common.PrettyDuration(estTime-elapsed))\n}\n\n// reportHealProgress calculates various status reports and provides it to the user.\nfunc (s *Syncer) reportHealProgress(force bool) {\n\t// Don't report all the events, just occasionally\n\tif !force && time.Since(s.logTime) < 8*time.Second {\n\t\treturn\n\t}\n\ts.logTime = time.Now()\n\n\t// Create a mega progress report\n\tvar (\n\t\ttrienode = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.trienodeHealSynced), s.trienodeHealBytes.TerminalString())\n\t\tbytecode = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.bytecodeHealSynced), s.bytecodeHealBytes.TerminalString())\n\t\taccounts = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.accountHealed), s.accountHealedBytes.TerminalString())\n\t\tstorage  = fmt.Sprintf(\"%v@%v\", log.FormatLogfmtUint64(s.storageHealed), s.storageHealedBytes.TerminalString())\n\t)\n\tlog.Info(\"Syncing: state healing in progress\", \"accounts\", accounts, \"slots\", storage,\n\t\t\"codes\", bytecode, \"nodes\", trienode, \"pending\", s.healer.scheduler.Pending())\n}\n\n// estimateRemainingSlots tries to determine roughly how many slots are left in\n// a contract storage, based on the number of keys and the last hash. This method\n// assumes that the hashes are lexicographically ordered and evenly distributed.\nfunc estimateRemainingSlots(hashes int, last common.Hash) (uint64, error) {\n\tif last == (common.Hash{}) {\n\t\treturn 0, errors.New(\"last hash empty\")\n\t}\n\tspace := new(big.Int).Mul(math.MaxBig256, big.NewInt(int64(hashes)))\n\tspace.Div(space, last.Big())\n\tif !space.IsUint64() {\n\t\t// Gigantic address space probably due to too few or malicious slots\n\t\treturn 0, errors.New(\"too few slots for estimation\")\n\t}\n\treturn space.Uint64() - uint64(hashes), nil\n}\n\n// capacitySort implements the Sort interface, allowing sorting by peer message\n// throughput. Note, callers should use sort.Reverse to get the desired effect\n// of highest capacity being at the front.\ntype capacitySort struct {\n\tids  []string\n\tcaps []int\n}\n\nfunc (s *capacitySort) Len() int {\n\treturn len(s.ids)\n}\n\nfunc (s *capacitySort) Less(i, j int) bool {\n\treturn s.caps[i] < s.caps[j]\n}\n\nfunc (s *capacitySort) Swap(i, j int) {\n\ts.ids[i], s.ids[j] = s.ids[j], s.ids[i]\n\ts.caps[i], s.caps[j] = s.caps[j], s.caps[i]\n}\n\n// healRequestSort implements the Sort interface, allowing sorting trienode\n// heal requests, which is a prerequisite for merging storage-requests.\ntype healRequestSort struct {\n\tpaths     []string\n\thashes    []common.Hash\n\tsyncPaths []trie.SyncPath\n}\n\nfunc (t *healRequestSort) Len() int {\n\treturn len(t.hashes)\n}\n\nfunc (t *healRequestSort) Less(i, j int) bool {\n\ta := t.syncPaths[i]\n\tb := t.syncPaths[j]\n\tswitch bytes.Compare(a[0], b[0]) {\n\tcase -1:\n\t\treturn true\n\tcase 1:\n\t\treturn false\n\t}\n\t// identical first part\n\tif len(a) < len(b) {\n\t\treturn true\n\t}\n\tif len(b) < len(a) {\n\t\treturn false\n\t}\n\tif len(a) == 2 {\n\t\treturn bytes.Compare(a[1], b[1]) < 0\n\t}\n\treturn false\n}\n\nfunc (t *healRequestSort) Swap(i, j int) {\n\tt.paths[i], t.paths[j] = t.paths[j], t.paths[i]\n\tt.hashes[i], t.hashes[j] = t.hashes[j], t.hashes[i]\n\tt.syncPaths[i], t.syncPaths[j] = t.syncPaths[j], t.syncPaths[i]\n}\n\n// Merge merges the pathsets, so that several storage requests concerning the\n// same account are merged into one, to reduce bandwidth.\n// OBS: This operation is moot if t has not first been sorted.\nfunc (t *healRequestSort) Merge() []TrieNodePathSet {\n\tvar result []TrieNodePathSet\n\tfor _, path := range t.syncPaths {\n\t\tpathset := TrieNodePathSet(path)\n\t\tif len(path) == 1 {\n\t\t\t// It's an account reference.\n\t\t\tresult = append(result, pathset)\n\t\t} else {\n\t\t\t// It's a storage reference.\n\t\t\tend := len(result) - 1\n\t\t\tif len(result) == 0 || !bytes.Equal(pathset[0], result[end][0]) {\n\t\t\t\t// The account doesn't match last, create a new entry.\n\t\t\t\tresult = append(result, pathset)\n\t\t\t} else {\n\t\t\t\t// It's the same account as the previous one, add to the storage\n\t\t\t\t// paths of that request.\n\t\t\t\tresult[end] = append(result[end], pathset[1])\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\n// sortByAccountPath takes hashes and paths, and sorts them. After that, it generates\n// the TrieNodePaths and merges paths which belongs to the same account path.\nfunc sortByAccountPath(paths []string, hashes []common.Hash) ([]string, []common.Hash, []trie.SyncPath, []TrieNodePathSet) {\n\tsyncPaths := make([]trie.SyncPath, len(paths))\n\tfor i, path := range paths {\n\t\tsyncPaths[i] = trie.NewSyncPath([]byte(path))\n\t}\n\tn := &healRequestSort{paths, hashes, syncPaths}\n\tsort.Sort(n)\n\tpathsets := n.Merge()\n\treturn n.paths, n.hashes, n.syncPaths, pathsets\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 2361,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= trienodeHealThrottleIncrease\n\t\t} else {\n\t\t\ts.trienodeHealThrottle /= trienodeHealThrottleDecrease\n\t\t}\n\t\tif s.trienodeHealThrottle > maxTrienodeHealThrottle {\n\t\t\ts.trienodeHealThrottle = maxTrienodeHealThrottle\n\t\t} else if s.trienodeHealThrottle < minTrienodeHealThrottle {\n\t\t\ts.trienodeHealThrottle = minTrienodeHealThrottle\n\t\t}\n\t\ts.trienodeHealThrottled = time.Now()\n\n\t\tlog.Debug(\"Updated trie node heal throttler\", \"rate\", s.trienodeHealRate, \"pending\", pending, \"throttle\", s.trienodeHealThrottle)\n\t}\n}\n\nfunc (s *Syncer) commitHealer(force bool) {\n\tif !force && s.healer.scheduler.MemSize() < ethdb.IdealBatchSize {\n\t\treturn\n\t}\n\tbatch := s.db.NewBatch()\n\tvar stateBatch ethdb.Batch\n\tvar err error\n\tif s.db.HasSeparateStateStore() {\n\t\tstateBatch = s.db.GetStateStore().NewBatch()\n\t\terr = s.healer.scheduler.Commit(batch, stateBatch)\n\t} else {\n\t\terr = s.healer.scheduler.Commit(batch, nil)\n\t}\n\tif err != nil {\n\t\tlog.Crit(\"Failed to commit healing data\", \"err\", err)\n\t}\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 570,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc/eth/protocols/snap/sync.go",
          "line": 598,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/snap/gentrie_test.go",
          "line": 47,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\treturn\n\t}\n\tvar path []byte\n\tif account {\n\t\t_, path = rawdb.ResolveAccountTrieNodeKey(key)\n\t} else {\n\t\t_, owner, inner := rawdb.ResolveStorageTrieNode(key)\n\t\tpath = append(owner.Bytes(), inner...)\n\t}\n\tr.paths = append(r.paths, string(path))\n\n\tif len(value) == 0 {\n\t\tr.hashes = append(r.hashes, common.Hash{})\n\t} else {\n\t\tr.hashes = append(r.hashes, crypto.Keccak256Hash(value))\n\t}\n}\n\n// updates returns a set of effective mutations. Multiple mutations targeting\n// the same node path will be merged in FIFO order.\nfunc (r *replayer) modifies() map[string]common.Hash {\n\tset := make(map[string]common.Hash)\n\tfor i, path := range r.paths {\n\t\tset[path] = r.hashes[i]\n\t}\n\treturn set\n}\n\n// updates returns the number of updates.\nfunc (r *replayer) updates() int {\n\tvar count int\n\tfor _, hash := range r.modifies() {\n\t\tif hash == (common.Hash{}) {\n\t\t\tcontinue\n\t\t}\n\t\tcount++\n\t}\n\treturn count\n}\n\n// Put inserts the given value into the key-value data store.\nfunc (r *replayer) Put(key []byte, value []byte) error {\n\tr.decode(key, value)\n\treturn nil\n}\n\n// Delete removes the key from the key-value data store.\nfunc (r *replayer) Delete(key []byte) error {\n\tr.decode(key, nil)\n\treturn nil\n}\n\nfunc byteToHex(str []byte) []byte {\n\tl := len(str) * 2\n\tvar nibbles = make([]byte, l)\n\tfor i, b := range str {\n\t\tnibbles[i*2] = b / 16\n\t\tnibbles[i*2+1] = b % 16\n\t}\n\treturn nibbles\n}\n\n// innerNodes returns the internal nodes narrowed by two boundaries along with\n// the leftmost and rightmost sub-trie roots.\nfunc innerNodes(first, last []byte, includeLeft, includeRight bool, nodes map[string]common.Hash, t *testing.T) (map[string]common.Hash, []byte, []byte) {\n\tvar (\n\t\tleftRoot  []byte\n\t\trightRoot []byte\n\t\tfirstHex  = byteToHex(first)\n\t\tlastHex   = byteToHex(last)\n\t\tinner     = make(map[string]common.Hash)\n\t)\n\tfor path, hash := range nodes {\n\t\tif hash == (common.Hash{}) {\n\t\t\tt.Fatalf(\"Unexpected deletion, %v\", []byte(path))\n\t\t}\n\t\t// Filter out the siblings on the left side or the left boundary nodes.\n\t\tif !includeLeft && (bytes.Compare(firstHex, []byte(path)) > 0 || bytes.HasPrefix(firstHex, []byte(path))) {\n\t\t\tcontinue\n\t\t}\n\t\t// Filter out the siblings on the right side or the right boundary nodes.\n\t\tif !includeRight && (bytes.Compare(lastHex, []byte(path)) < 0 || bytes.HasPrefix(lastHex, []byte(path))) {\n\t\t\tcontinue\n\t\t}\n\t\tinner[path] = hash\n\n\t\t// Track the path of the leftmost sub trie root\n\t\tif leftRoot == nil || bytes.Compare(leftRoot, []byte(path)) > 0 {\n\t\t\tleftRoot = []byte(path)\n\t\t}\n\t\t// Track the path of the rightmost sub trie root\n\t\tif rightRoot == nil ||\n\t\t\t(bytes.Compare(rightRoot, []byte(path)) < 0) ||\n\t\t\t(bytes.Compare(rightRoot, []byte(path)) > 0 && bytes.HasPrefix(rightRoot, []byte(path))) {\n\t\t\trightRoot = []byte(path)\n\t\t}\n\t}\n\treturn inner, leftRoot, rightRoot\n}\n\nfunc buildPartial(owner common.Hash, db ethdb.KeyValueReader, batch ethdb.Batch, entries []*kv, first, last int) *replayer {\n\ttr := newPathTrie(owner, first != 0, db, batch)\n\tfor i := first",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/snap/sync_test.go",
          "line": 269,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(32 + len(entry.v))\n\t\t}\n\t\t// If we've exceeded the request threshold, abort\n\t\tif bytes.Compare(entry.k, limit[:]) >= 0 {\n\t\t\tbreak\n\t\t}\n\t}\n\t// Unless we send the entire trie, we need to supply proofs\n\t// Actually, we need to supply proofs either way! This seems to be an implementation\n\t// quirk in go-ethereum\n\tproof := trienode.NewProofSet()\n\tif err := t.accountTrie.Prove(origin[:], proof)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/snap/sync_test.go",
          "line": 341,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(32 + len(entry.v))\n\t\t\tif bytes.Compare(entry.k, limitHash[:]) >= 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t}\n\t\tif len(keys) > 0 {\n\t\t\thashes = append(hashes, keys)\n\t\t\tslots = append(slots, vals)\n\t\t}\n\t\t// Generate the Merkle proofs for the first and last storage slot, but\n\t\t// only if the response was capped. If the entire storage trie included\n\t\t// in the response, no need for any proofs.\n\t\tif originHash != (common.Hash{}) || (abort && len(keys) > 0) {\n\t\t\t// If we're aborting, we need to prove the first and last item\n\t\t\t// This terminates the response (and thus the loop)\n\t\t\tproof := trienode.NewProofSet()\n\t\t\tstTrie := t.storageTries[account]\n\n\t\t\t// Here's a potential gotcha: when constructing the proof, we cannot\n\t\t\t// use the 'origin' slice directly, but must use the full 32-byte\n\t\t\t// hash form.\n\t\t\tif err := stTrie.Prove(originHash[:], proof)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/snap/sync_test.go",
          "line": 398,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(32 + len(entry.v))\n\t\t\tif size > max {\n\t\t\t\texit = true\n\t\t\t}\n\t\t}\n\t\tif i == len(accounts)-1 {\n\t\t\texit = true\n\t\t}\n\t\thashes = append(hashes, keys)\n\t\tslots = append(slots, vals)\n\n\t\tif exit {\n\t\t\t// If we're aborting, we need to prove the first and last item\n\t\t\t// This terminates the response (and thus the loop)\n\t\t\tproof := trienode.NewProofSet()\n\t\t\tstTrie := t.storageTries[account]\n\n\t\t\t// Here's a potential gotcha: when constructing the proof, we cannot\n\t\t\t// use the 'origin' slice directly, but must use the full 32-byte\n\t\t\t// hash form.\n\t\t\tif err := stTrie.Prove(origin[:], proof)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 80,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 99,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 114,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/protocols/snap/peer.go",
          "line": 127,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/handshake_test.go",
          "line": 81,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/handshake.go",
          "line": 69,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/eth/handshake.go",
          "line": 94,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/eth/handshake.go",
          "line": 154,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/dispatcher.go",
          "line": 200,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 97,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(rlpData))\n\t\t}\n\t\t// Advance to the next header of the query\n\t\tswitch {\n\t\tcase hashMode && query.Reverse:\n\t\t\t// Hash based traversal towards the genesis block\n\t\t\tancestor := query.Skip + 1\n\t\t\tif ancestor == 0 {\n\t\t\t\tunknown = true\n\t\t\t} else {\n\t\t\t\tquery.Origin.Hash, query.Origin.Number = chain.GetAncestor(query.Origin.Hash, query.Origin.Number, ancestor, &maxNonCanonical)\n\t\t\t\tunknown = (query.Origin.Hash == common.Hash{})\n\t\t\t}\n\t\tcase hashMode && !query.Reverse:\n\t\t\t// Hash based traversal towards the leaf block\n\t\t\tvar (\n\t\t\t\tcurrent = origin.Number.Uint64()\n\t\t\t\tnext    = current + query.Skip + 1\n\t\t\t)\n\t\t\tif next <= current {\n\t\t\t\tinfos, _ := json.MarshalIndent(peer.Peer.Info(), \"\", \"  \")\n\t\t\t\tpeer.Log().Warn(\"GetBlockHeaders skip overflow attack\", \"current\", current, \"skip\", query.Skip, \"next\", next, \"attacker\", infos)\n\t\t\t\tunknown = true\n\t\t\t} else {\n\t\t\t\tif header := chain.GetHeaderByNumber(next)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 259,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(enc)\n\t}\n\treturn bodies\n}\n\nfunc handleGetReceipts68(backend Backend, msg Decoder, peer *Peer) error {\n\t// Decode the block receipts retrieval message\n\tvar query GetReceiptsPacket\n\tif err := msg.Decode(&query)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 316,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(results)\n\t}\n\treturn receipts\n}\n\n// serviceGetReceiptsQuery69 assembles the response to a receipt query.\n// It does not send the bloom filters for the receipts\nfunc serviceGetReceiptsQuery69(chain *core.BlockChain, query GetReceiptsRequest) []rlp.RawValue {\n\t// Gather state data until the fetch or network limits is reached\n\tvar (\n\t\tbytes    int\n\t\treceipts []rlp.RawValue\n\t)\n\tfor lookups, hash := range query {\n\t\tif bytes >= softResponseLimit || len(receipts) >= maxReceiptsServe ||\n\t\t\tlookups >= 2*maxReceiptsServe {\n\t\t\tbreak\n\t\t}\n\t\t// Retrieve the requested block's receipts\n\t\tresults := chain.GetReceiptsRLP(hash)\n\t\tif results == nil {\n\t\t\tif header := chain.GetHeaderByHash(hash)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 353,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(results)\n\t}\n\treturn receipts\n}\n\nfunc handleNewBlockhashes(backend Backend, msg Decoder, peer *Peer) error {\n\t// A batch of new block announcements just arrived\n\tann := new(NewBlockHashesPacket)\n\tif err := msg.Decode(ann)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/protocols/eth/handlers.go",
          "line": 530,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(encoded)\n\t}\n\treturn hashes, txs\n}\n\nfunc handleTransactions(backend Backend, msg Decoder, peer *Peer) error {\n\t// Transactions arrived, make sure we have a valid and fresh chain to handle them\n\tif !backend.AcceptTxs() {\n\t\treturn nil\n\t}\n\t// Transactions can be processed, parse all of them and deliver to the pool\n\tvar txs TransactionsPacket\n\tif err := msg.Decode(&txs)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 225,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 253,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 277,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 294,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 314,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 335,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 344,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 352,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 488,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 499,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/eth/protocols/eth/peer.go",
          "line": 300,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(block *types.Block)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 363,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 378,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 472,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 546,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/eth/protocols/eth/handler_test.go",
          "line": 844,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/protocols/eth/broadcast.go",
          "line": 86,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(tx.Size())\n\t\t\t\t}\n\t\t\t\thashesCount++\n\t\t\t}\n\t\t\tqueue = queue[:copy(queue, queue[hashesCount:])]\n\n\t\t\t// If there's anything available to transfer, fire up an async writer\n\t\t\tif len(txs) > 0 {\n\t\t\t\tdone = make(chan struct{})\n\t\t\t\tgo func() {\n\t\t\t\t\tif err := p.SendTransactions(txs)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/protocols/eth/broadcast.go",
          "line": 160,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength\n\t\t\t\t}\n\t\t\t}\n\t\t\t// Shift and trim queue\n\t\t\tqueue = queue[:copy(queue, queue[count:])]\n\n\t\t\t// If there's anything available to transfer, fire up an async writer\n\t\t\tif len(pending) > 0 {\n\t\t\t\tdone = make(chan struct{})\n\t\t\t\tgopool.Submit(func() {\n\t\t\t\t\tif err := p.sendPooledTransactionHashes(pending, pendingTypes, pendingSizes)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/logger/logger.go",
          "line": 182,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32 {\n\t\t\tend := i + 32\n\t\t\tif end > len(s.Memory) {\n\t\t\t\tend = len(s.Memory)\n\t\t\t}\n\t\t\tmemory = append(memory, fmt.Sprintf(\"%x\", s.Memory[i:end]))\n\t\t}\n\t\tmsg.Memory = &memory\n\t}\n\tif len(s.Storage) > 0 {\n\t\tstorage := make(map[string]string)\n\t\tfor i, storageValue := range s.Storage {\n\t\t\tstorage[fmt.Sprintf(\"%x\", i)] = fmt.Sprintf(\"%x\", storageValue)\n\t\t}\n\t\tmsg.Storage = &storage\n\t}\n\telement, _ := json.Marshal(msg)\n\treturn element\n}\n\n// StructLogger is an EVM state logger and implements EVMLogger.\n//\n// StructLogger can capture state based on the given Log configuration and also keeps\n// a track record of modified storage which is used in reporting snapshots of the\n// contract their storage.\n//\n// A StructLogger can either yield it's output immediately (streaming) or store for\n// later output.\ntype StructLogger struct {\n\tcfg Config\n\tenv *tracing.VMContext\n\n\tstorage map[common.Address]Storage\n\toutput  []byte\n\terr     error\n\tusedGas uint64\n\n\twriter     io.Writer         // If set, the logger will stream instead of store logs\n\tlogs       []json.RawMessage // buffer of json-encoded logs\n\tresultSize int\n\n\tinterrupt atomic.Bool // Atomic flag to signal execution interruption\n\treason    error       // Textual reason for the interruption\n\tskip      bool        // skip processing hooks.\n}\n\n// NewStreamingStructLogger returns a new streaming logger.\nfunc NewStreamingStructLogger(cfg *Config, writer io.Writer) *StructLogger {\n\tl := NewStructLogger(cfg)\n\tl.writer = writer\n\treturn l\n}\n\n// NewStructLogger construct a new (non-streaming) struct logger.\nfunc NewStructLogger(cfg *Config) *StructLogger {\n\tlogger := &StructLogger{\n\t\tstorage: make(map[common.Address]Storage),\n\t\tlogs:    make([]json.RawMessage, 0),\n\t}\n\tif cfg != nil {\n\t\tlogger.cfg = *cfg\n\t}\n\treturn logger\n}\n\nfunc (l *StructLogger) Hooks() *tracing.Hooks {\n\treturn &tracing.Hooks{\n\t\tOnTxStart:                 l.OnTxStart,\n\t\tOnTxEnd:                   l.OnTxEnd,\n\t\tOnSystemCallStartV2:       l.OnSystemCallStart,\n\t\tOnSystemCallEnd:           l.OnSystemCallEnd,\n\t\tOnExit:                    l.OnExit,\n\t\tOnOpcode:                  l.OnOpcode,\n\t\tOnSystemTxFixIntrinsicGas: l.OnSystemTxFixIntrinsicGas,\n\t}\n}\n\n// OnOpcode logs a new structured log message and pushes it out to the environment\n//\n// OnOpcode also tracks SLOAD/SSTORE ops to track storage change.\nfunc (l *StructLogger) OnOpcode(pc uint64, opcode byte, gas, cost uint64, scope tracing.OpContext, rData []byte, depth int, err error) {\n\t// If tracing was interrupted, exit\n\tif l.interrupt.Load() {\n\t\treturn\n\t}\n\t// Processing a system call.\n\tif l.skip {\n\t\treturn\n\t}\n\t// check if already accumulated the size of the response.\n\tif l.cfg.Limit != 0 && l.resultSize > l.cfg.Limit {\n\t\treturn\n\t}\n\tvar (\n\t\top           = vm.OpCode(opcode)\n\t\tmemory       = scope.MemoryData()\n\t\tcontractAddr = scope.Address()\n\t\tstack        = scope.StackData()\n\t\tstackLen     = len(stack)\n\t)\n\tlog := StructLog{pc, op, gas, cost, nil, len(memory), nil, nil, nil, depth, l.env.StateDB.GetRefund(), err}\n\tif l.cfg.EnableMemory {\n\t\tlog.Memory = memory\n\t}\n\tif !l.cfg.DisableStack {\n\t\tlog.Stack = scope.StackData()\n\t}\n\tif l.cfg.EnableReturnData {\n\t\tlog.ReturnData = rData\n\t}\n\n\t// Copy a snapshot of the current storage to a new container\n\tvar storage Storage\n\tif !l.cfg.DisableStorage && (op == vm.SLOAD || op == vm.SSTORE) {\n\t\t// initialise new changed values storage container for this contract\n\t\t// if not present.\n\t\tif l.storage[contractAddr] == nil {\n\t\t\tl.storage[contractAddr] = make(Storage)\n\t\t}\n\t\t// capture SLOAD opcodes and record the read entry in the local storage\n\t\tif op == vm.SLOAD && stackLen >= 1 {\n\t\t\tvar (\n\t\t\t\taddress = common.Hash(stack[stackLen-1].Bytes32())\n\t\t\t\tvalue   = l.env.StateDB.GetState(contractAddr, address)\n\t\t\t)\n\t\t\tl.storage[contractAddr][address] = value\n\t\t\tstorage = maps.Clone(l.storage[contractAddr])\n\t\t} else if op == vm.SSTORE && stackLen >= 2 {\n\t\t\t// capture SSTORE opcodes and record the written entry in the local storage.\n\t\t\tvar (\n\t\t\t\tvalue   = common.Hash(stack[stackLen-2].Bytes32())\n\t\t\t\taddress = common.Hash(stack[stackLen-1].Bytes32())\n\t\t\t)\n\t\t\tl.storage[contractAddr][address] = value\n\t\t\tstorage = maps.Clone(l.storage[contractAddr])\n\t\t}\n\t}\n\tlog.Storage = storage\n\n\t// create a log\n\tif l.writer == nil {\n\t\tentry := log.toLegacyJSON()\n\t\tl.resultSize += len(entry)\n\t\tl.logs = append(l.logs, entry)\n\t\treturn\n\t}\n\tlog.WriteTo(l.writer)\n}\n\n// OnExit is called a call frame finishes processing.\nfunc (l *StructLogger) OnExit(depth int, output []byte, gasUsed uint64, err error, reverted bool) {\n\tif depth != 0 {\n\t\treturn\n\t}\n\tif l.skip {\n\t\treturn\n\t}\n\tl.output = output\n\tl.err = err\n\t// TODO @holiman, should we output the per-scope output?\n\t//if l.cfg.Debug {\n\t//\tfmt.Printf(\"%#x\\n\", output)\n\t//\tif err != nil {\n\t//\t\tfmt.Printf(\" error: %v\\n\", err)\n\t//\t}\n\t//}\n}\n\nfunc (l *StructLogger) GetResult() (json.RawMessage, error) {\n\t// Tracing aborted\n\tif l.reason != nil {\n\t\treturn nil, l.reason\n\t}\n\tfailed := l.err != nil\n\treturnData := common.CopyBytes(l.output)\n\t// Return data when successful and revert reason when reverted, otherwise empty.\n\tif failed && !errors.Is(l.err, vm.ErrExecutionReverted) {\n\t\treturnData = []byte{}\n\t}\n\treturn json.Marshal(&ExecutionResult{\n\t\tGas:         l.usedGas,\n\t\tFailed:      failed,\n\t\tReturnValue: returnData,\n\t\tStructLogs:  l.logs,\n\t})\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (l *StructLogger) Stop(err error) {\n\tl.reason = err\n\tl.interrupt.Store(true)\n}\n\nfunc (l *StructLogger) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tl.env = env\n}\nfunc (l *StructLogger) OnSystemCallStart(env *tracing.VMContext) {\n\tl.skip = true\n}\n\nfunc (l *StructLogger) OnSystemCallEnd() {\n\tl.skip = false\n}\n\nfunc (l *StructLogger) OnTxEnd(receipt *types.Receipt, err error) {\n\tif err != nil {\n\t\t// Don't override vm error\n\t\tif l.err == nil {\n\t\t\tl.err = err\n\t\t}\n\t\treturn\n\t}\n\tif receipt != nil {\n\t\tl.usedGas = receipt.GasUsed\n\t}\n}\n\nfunc (l *StructLogger) OnSystemTxFixIntrinsicGas(intrinsicGas uint64) {\n\tl.usedGas -= intrinsicGas\n}\n\n// Error returns the VM error captured by the trace.\nfunc (l *StructLogger) Error() error { return l.err }\n\n// Output returns the VM return value captured by the trace.\nfunc (l *StructLogger) Output() []byte { return l.output }\n\n// WriteTrace writes a formatted trace to the given writer\n// @deprecated\nfunc WriteTrace(writer io.Writer, logs []StructLog) {\n\tfor _, log := range logs {\n\t\tlog.WriteTo(writer)\n\t}\n}\n\ntype mdLogger struct {\n\tout  io.Writer\n\tcfg  *Config\n\tenv  *tracing.VMContext\n\tskip bool\n}\n\n// NewMarkdownLogger creates a logger which outputs information in a format adapted\n// for human readability, and is also a valid markdown table\nfunc NewMarkdownLogger(cfg *Config, writer io.Writer) *mdLogger {\n\tl := &mdLogger{out: writer, cfg: cfg}\n\tif l.cfg == nil {\n\t\tl.cfg = &Config{}\n\t}\n\treturn l\n}\n\nfunc (t *mdLogger) Hooks() *tracing.Hooks {\n\treturn &tracing.Hooks{\n\t\tOnTxStart:           t.OnTxStart,\n\t\tOnSystemCallStartV2: t.OnSystemCallStart,\n\t\tOnSystemCallEnd:     t.OnSystemCallEnd,\n\t\tOnEnter:             t.OnEnter,\n\t\tOnExit:              t.OnExit,\n\t\tOnOpcode:            t.OnOpcode,\n\t\tOnFault:             t.OnFault,\n\t}\n}\n\nfunc (t *mdLogger) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tt.env = env\n}\n\nfunc (t *mdLogger) OnSystemCallStart(env *tracing.VMContext) {\n\tt.skip = true\n}\n\nfunc (t *mdLogger) OnSystemCallEnd() {\n\tt.skip = false\n}\n\nfunc (t *mdLogger) OnEnter(depth int, typ byte, from common.Address, to common.Address, input []byte, gas uint64, value *big.Int) {\n\tif t.skip {\n\t\treturn\n\t}\n\tif depth != 0 {\n\t\treturn\n\t}\n\tif create := vm.OpCode(typ) == vm.CREATE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/tracers/logger/logger.go",
          "line": 400,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= intrinsicGas\n}\n\n// Error returns the VM error captured by the trace.\nfunc (l *StructLogger) Error() error { return l.err }\n\n// Output returns the VM return value captured by the trace.\nfunc (l *StructLogger) Output() []byte { return l.output }\n\n// WriteTrace writes a formatted trace to the given writer\n// @deprecated\nfunc WriteTrace(writer io.Writer, logs []StructLog) {\n\tfor _, log := range logs {\n\t\tlog.WriteTo(writer)\n\t}\n}\n\ntype mdLogger struct {\n\tout  io.Writer\n\tcfg  *Config\n\tenv  *tracing.VMContext\n\tskip bool\n}\n\n// NewMarkdownLogger creates a logger which outputs information in a format adapted\n// for human readability, and is also a valid markdown table\nfunc NewMarkdownLogger(cfg *Config, writer io.Writer) *mdLogger {\n\tl := &mdLogger{out: writer, cfg: cfg}\n\tif l.cfg == nil {\n\t\tl.cfg = &Config{}\n\t}\n\treturn l\n}\n\nfunc (t *mdLogger) Hooks() *tracing.Hooks {\n\treturn &tracing.Hooks{\n\t\tOnTxStart:           t.OnTxStart,\n\t\tOnSystemCallStartV2: t.OnSystemCallStart,\n\t\tOnSystemCallEnd:     t.OnSystemCallEnd,\n\t\tOnEnter:             t.OnEnter,\n\t\tOnExit:              t.OnExit,\n\t\tOnOpcode:            t.OnOpcode,\n\t\tOnFault:             t.OnFault,\n\t}\n}\n\nfunc (t *mdLogger) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tt.env = env\n}\n\nfunc (t *mdLogger) OnSystemCallStart(env *tracing.VMContext) {\n\tt.skip = true\n}\n\nfunc (t *mdLogger) OnSystemCallEnd() {\n\tt.skip = false\n}\n\nfunc (t *mdLogger) OnEnter(depth int, typ byte, from common.Address, to common.Address, input []byte, gas uint64, value *big.Int) {\n\tif t.skip {\n\t\treturn\n\t}\n\tif depth != 0 {\n\t\treturn\n\t}\n\tif create := vm.OpCode(typ) == vm.CREATE",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/native/erc7562.go",
          "line": 272,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\n\tif errors.Is(err, vm.ErrCodeStoreOutOfGas) || errors.Is(err, vm.ErrOutOfGas) {\n\t\tcall.OutOfGas = true\n\t}\n\tcall.GasUsed = gasUsed\n\tcall.processOutput(output, err, reverted)\n\t// Nest call into parent.\n\tt.callstackWithOpcodes[size-1].Calls = append(t.callstackWithOpcodes[size-1].Calls, call)\n}\n\nfunc (t *erc7562Tracer) OnTxEnd(receipt *types.Receipt, err error) {\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\t// Error happened during tx validation.\n\tif err != nil {\n\t\treturn\n\t}\n\tt.callstackWithOpcodes[0].GasUsed = receipt.GasUsed\n\tif t.config.WithLog {\n\t\t// Logs are not emitted when the call fails\n\t\tt.clearFailedLogs(&t.callstackWithOpcodes[0], false)\n\t}\n}\n\nfunc (t *erc7562Tracer) OnLog(log1 *types.Log) {\n\t// Only logs need to be captured via opcode processing\n\tif !t.config.WithLog {\n\t\treturn\n\t}\n\t// Skip if tracing was interrupted\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tl := callLog{\n\t\tAddress:  log1.Address,\n\t\tTopics:   log1.Topics,\n\t\tData:     log1.Data,\n\t\tPosition: hexutil.Uint(len(t.callstackWithOpcodes[len(t.callstackWithOpcodes)-1].Calls)),\n\t}\n\tt.callstackWithOpcodes[len(t.callstackWithOpcodes)-1].Logs = append(t.callstackWithOpcodes[len(t.callstackWithOpcodes)-1].Logs, l)\n}\n\n// GetResult returns the json-encoded nested list of call traces, and any\n// error arising from the encoding or forceful termination (via `Stop`).\nfunc (t *erc7562Tracer) GetResult() (json.RawMessage, error) {\n\tif t.interrupt.Load() {\n\t\treturn nil, t.reason\n\t}\n\tif len(t.callstackWithOpcodes) != 1 {\n\t\treturn nil, errors.New(\"incorrect number of top-level calls\")\n\t}\n\n\tkeccak := make([][]byte, 0, len(t.callstackWithOpcodes[0].KeccakPreimages))\n\tfor k := range t.keccakPreimages {\n\t\tkeccak = append(keccak, []byte(k))\n\t}\n\tt.callstackWithOpcodes[0].KeccakPreimages = keccak\n\tslices.SortFunc(keccak, func(a, b []byte) int {\n\t\treturn bytes.Compare(a, b)\n\t})\n\n\tenc, err := json.Marshal(t.callstackWithOpcodes[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\treturn enc, t.reason\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (t *erc7562Tracer) Stop(err error) {\n\tt.reason = err\n\tt.interrupt.Store(true)\n}\n\n// clearFailedLogs clears the logs of a callframe and all its children\n// in case of execution failure.\nfunc (t *erc7562Tracer) clearFailedLogs(cf *callFrameWithOpcodes, parentFailed bool) {\n\tfailed := cf.failed() || parentFailed\n\t// Clear own logs\n\tif failed {\n\t\tcf.Logs = nil\n\t}\n\tfor i := range cf.Calls {\n\t\tt.clearFailedLogs(&cf.Calls[i], failed)\n\t}\n}\n\nfunc (t *erc7562Tracer) OnOpcode(pc uint64, op byte, gas, cost uint64, scope tracing.OpContext, rData []byte, depth int, err error) {\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tvar (\n\t\topcode          = vm.OpCode(op)\n\t\topcodeWithStack *opcodeWithPartialStack\n\t\tstackSize       = len(scope.StackData())\n\t\tstackLimit      = min(stackSize, t.config.StackTopItemsSize)\n\t\tstackTopItems   = make([]uint256.Int, stackLimit)\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/native/4byte.go",
          "line": 85,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n}\n\nfunc (t *fourByteTracer) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\t// Update list of precompiles based on current block\n\trules := t.chainConfig.Rules(env.BlockNumber, env.Random != nil, env.Time)\n\tt.activePrecompiles = vm.ActivePrecompiles(rules)\n}\n\n// OnEnter is called when EVM enters a new scope (via call, create or selfdestruct).\nfunc (t *fourByteTracer) OnEnter(depth int, opcode byte, from common.Address, to common.Address, input []byte, gas uint64, value *big.Int) {\n\t// Skip if tracing was interrupted\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tif len(input) < 4 {\n\t\treturn\n\t}\n\top := vm.OpCode(opcode)\n\t// primarily we want to avoid CREATE/CREATE2/SELFDESTRUCT\n\tif op != vm.DELEGATECALL && op != vm.STATICCALL &&\n\t\top != vm.CALL && op != vm.CALLCODE {\n\t\treturn\n\t}\n\t// Skip any pre-compile invocations, those are just fancy opcodes\n\tif t.isPrecompiled(to) {\n\t\treturn\n\t}\n\tt.store(input[0:4], len(input)-4)\n}\n\n// GetResult returns the json-encoded nested list of call traces, and any\n// error arising from the encoding or forceful termination (via `Stop`).\nfunc (t *fourByteTracer) GetResult() (json.RawMessage, error) {\n\tres, err := json.Marshal(t.ids)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn res, t.reason\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (t *fourByteTracer) Stop(err error) {\n\tt.reason = err\n\tt.interrupt.Store(true)\n}\n\nfunc bytesToHex(s []byte) string {\n\treturn \"0x\" + common.Bytes2Hex(s)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/native/call.go",
          "line": 204,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\n\tcall.GasUsed = gasUsed\n\tcall.processOutput(output, err, reverted)\n\t// Nest call into parent.\n\tt.callstack[size-1].Calls = append(t.callstack[size-1].Calls, call)\n}\n\nfunc (t *callTracer) captureEnd(output []byte, gasUsed uint64, err error, reverted bool) {\n\tif len(t.callstack) != 1 {\n\t\treturn\n\t}\n\tt.callstack[0].processOutput(output, err, reverted)\n}\n\nfunc (t *callTracer) OnTxStart(env *tracing.VMContext, tx *types.Transaction, from common.Address) {\n\tt.gasLimit = tx.Gas()\n}\n\nfunc (t *callTracer) OnTxEnd(receipt *types.Receipt, err error) {\n\t// Error happened during tx validation.\n\tif err != nil {\n\t\treturn\n\t}\n\tif receipt != nil {\n\t\tt.callstack[0].GasUsed = receipt.GasUsed\n\t}\n\tif t.config.WithLog {\n\t\t// Logs are not emitted when the call fails\n\t\tclearFailedLogs(&t.callstack[0], false)\n\t}\n}\n\nfunc (t *callTracer) OnSystemTxFixIntrinsicGas(intrinsicGas uint64) {\n\tt.callstack[0].GasUsed -= intrinsicGas\n}\n\nfunc (t *callTracer) OnLog(log *types.Log) {\n\t// Only logs need to be captured via opcode processing\n\tif !t.config.WithLog {\n\t\treturn\n\t}\n\t// Avoid processing nested calls when only caring about top call\n\tif t.config.OnlyTopCall && t.depth > 0 {\n\t\treturn\n\t}\n\t// Skip if tracing was interrupted\n\tif t.interrupt.Load() {\n\t\treturn\n\t}\n\tl := callLog{\n\t\tAddress:  log.Address,\n\t\tTopics:   log.Topics,\n\t\tData:     log.Data,\n\t\tPosition: hexutil.Uint(len(t.callstack[len(t.callstack)-1].Calls)),\n\t}\n\tt.callstack[len(t.callstack)-1].Logs = append(t.callstack[len(t.callstack)-1].Logs, l)\n}\n\n// GetResult returns the json-encoded nested list of call traces, and any\n// error arising from the encoding or forceful termination (via `Stop`).\nfunc (t *callTracer) GetResult() (json.RawMessage, error) {\n\tif len(t.callstack) != 1 {\n\t\treturn nil, errors.New(\"incorrect number of top-level calls\")\n\t}\n\n\tres, err := json.Marshal(t.callstack[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn res, t.reason\n}\n\n// Stop terminates execution of the tracer at the first opportune moment.\nfunc (t *callTracer) Stop(err error) {\n\tt.reason = err\n\tt.interrupt.Store(true)\n}\n\n// clearFailedLogs clears the logs of a callframe and all its children\n// in case of execution failure.\nfunc clearFailedLogs(cf *callFrame, parentFailed bool) {\n\tfailed := cf.failed() || parentFailed\n\t// Clear own logs\n\tif failed {\n\t\tcf.Logs = nil\n\t}\n\tfor i := range cf.Calls {\n\t\tclearFailedLogs(&cf.Calls[i], failed)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/js/bigint.go",
          "line": 20,
          "category": "ensemble_overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath) | -\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+=1}while(carry>0){r[i++]=carry%base",
          "severity": "HIGH",
          "model": "ensemble_fusion",
          "classical_detected": true,
          "omega_detected": false,
          "ensemble_confidence": 0.255,
          "fusion_score": 9.0
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/js/tracer_test.go",
          "line": 113,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/eth/tracers/js/tracer_test.go",
          "line": 143,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/eth/tracers/live/supply.go",
          "line": 265,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\n\t// In case of a revert, we can drop the call and all its subcalls.\n\t// Caution, that this has to happen after popping the call from the stack.\n\tif reverted {\n\t\treturn\n\t}\n\ts.txCallstack[size-1].calls = append(s.txCallstack[size-1].calls, call)\n}\n\nfunc (s *supplyTracer) onClose() {\n\tif err := s.logger.Close()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/enr/enr.go",
          "line": 109,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rlp.BytesSize(r.signature)\n\tfor _, p := range r.pairs {\n\t\tsize += rlp.StringSize(p.k)\n\t\tsize += uint64(len(p.v))\n\t}\n\treturn rlp.ListSize(size)\n}\n\n// Seq returns the sequence number.\nfunc (r *Record) Seq() uint64 {\n\treturn r.seq\n}\n\n// SetSeq updates the record sequence number. This invalidates any signature on the record.\n// Calling SetSeq is usually not required because setting any key in a signed record\n// increments the sequence number.\nfunc (r *Record) SetSeq(s uint64) {\n\tr.signature = nil\n\tr.raw = nil\n\tr.seq = s\n}\n\n// Load retrieves the value of a key/value pair. The given Entry must be a pointer and will\n// be set to the value of the entry in the record.\n//\n// Errors returned by Load are wrapped in KeyError. You can distinguish decoding errors\n// from missing keys using the IsNotFound function.\nfunc (r *Record) Load(e Entry) error {\n\ti := sort.Search(len(r.pairs), func(i int) bool { return r.pairs[i].k >= e.ENRKey() })\n\tif i < len(r.pairs) && r.pairs[i].k == e.ENRKey() {\n\t\tif err := rlp.DecodeBytes(r.pairs[i].v, e)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/v5_udp_test.go",
          "line": 930,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/ntp.go",
          "line": 108,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= drifts[i]\n\t}\n\treturn drift / time.Duration(measurements), nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/v4_lookup_test.go",
          "line": 259,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(keys)\n\t}\n\treturn n\n}\n\nfunc (tn *preminedTestnet) nodes() []*enode.Node {\n\tresult := make([]*enode.Node, 0, tn.len())\n\tfor dist, keys := range tn.dists {\n\t\tfor index := range keys {\n\t\t\tresult = append(result, tn.node(dist, index))\n\t\t}\n\t}\n\tsortByID(result)\n\treturn result\n}\n\nfunc (tn *preminedTestnet) node(dist, index int) *enode.Node {\n\tkey := tn.dists[dist][index]\n\trec := new(enr.Record)\n\trec.Set(enr.IP{127, byte(dist >> 8), byte(dist), byte(index)})\n\trec.Set(enr.UDP(5000))\n\tenode.SignV4(rec, key)\n\tn, _ := enode.New(enode.ValidSchemes, rec)\n\treturn n\n}\n\nfunc (tn *preminedTestnet) nodeByAddr(addr netip.AddrPort) (*enode.Node, *ecdsa.PrivateKey) {\n\tip := addr.Addr().As4()\n\tdist := int(ip[1])<<8 + int(ip[2])\n\tindex := int(ip[3])\n\tkey := tn.dists[dist][index]\n\treturn tn.node(dist, index), key\n}\n\nfunc (tn *preminedTestnet) nodesAtDistance(dist int) []v4wire.Node {\n\tresult := make([]v4wire.Node, len(tn.dists[dist]))\n\tfor i := range result {\n\t\tresult[i] = nodeToRPC(tn.node(dist, i))\n\t}\n\treturn result\n}\n\nfunc (tn *preminedTestnet) neighborsAtDistances(base *enode.Node, distances []uint, elems int) []*enode.Node {\n\tvar result []*enode.Node\n\tfor d := range lookupTestnet.dists {\n\t\tfor i := range lookupTestnet.dists[d] {\n\t\t\tn := lookupTestnet.node(d, i)\n\t\t\td := enode.LogDist(base.ID(), n.ID())\n\t\t\tif slices.Contains(distances, uint(d)) {\n\t\t\t\tresult = append(result, n)\n\t\t\t\tif len(result) >= elems {\n\t\t\t\t\treturn result\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn result\n}\n\nfunc (tn *preminedTestnet) closest(n int) (nodes []*enode.Node) {\n\tfor d := range tn.dists {\n\t\tfor i := range tn.dists[d] {\n\t\t\tnodes = append(nodes, tn.node(d, i))\n\t\t}\n\t}\n\tslices.SortFunc(nodes, func(a, b *enode.Node) int {\n\t\treturn enode.DistCmp(tn.target.ID(), a.ID(), b.ID())\n\t})\n\treturn nodes[:n]\n}\n\nvar _ = (*preminedTestnet).mine // avoid linter warning about mine being dead code.\n\n// mine generates a testnet struct literal with nodes at\n// various distances to the network's target.\nfunc (tn *preminedTestnet) mine() {\n\t// Clear existing slices first (useful when re-mining).\n\tfor i := range tn.dists {\n\t\ttn.dists[i] = nil\n\t}\n\n\ttargetSha := tn.target.ID()\n\tfound, need := 0, 40\n\tfor found < need {\n\t\tk := newkey()\n\t\tld := enode.LogDist(targetSha, v4wire.EncodePubkey(&k.PublicKey).ID())\n\t\tif len(tn.dists[ld]) < 8 {\n\t\t\ttn.dists[ld] = append(tn.dists[ld], k)\n\t\t\tfound++\n\t\t\tfmt.Printf(\"found ID with ld %d (%d/%d)\\n\", ld, found, need)\n\t\t}\n\t}\n\tfmt.Printf(\"&preminedTestnet{\\n\")\n\tfmt.Printf(\"\ttarget: hexEncPubkey(\\\"%x\\\"),\\n\", tn.target[:])\n\tfmt.Printf(\"\tdists: [%d][]*ecdsa.PrivateKey{\\n\", len(tn.dists))\n\tfor ld, ns := range tn.dists {\n\t\tif len(ns) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tfmt.Printf(\"\t\t%d: {\\n\", ld)\n\t\tfor _, key := range ns {\n\t\t\tfmt.Printf(\"\t\t\thexEncPrivkey(\\\"%x\\\"),\\n\", crypto.FromECDSA(key))\n\t\t}\n\t\tfmt.Printf(\"\t\t},\\n\")\n\t}\n\tfmt.Printf(\"\t},\\n\")\n\tfmt.Printf(\"}\\n\")\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 974,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= r.Size()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 602,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 672,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/p2p/discover/v5_udp.go",
          "line": 681,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/v4_udp_test.go",
          "line": 132,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 341,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 703,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 781,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 787,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/p2p/discover/v4_udp.go",
          "line": 820,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/table.go",
          "line": 313,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(b.entries)\n\t}\n\treturn n\n}\n\n// addFoundNode adds a node which may not be live. If the bucket has space available,\n// adding the node succeeds immediately. Otherwise, the node is added to the replacements\n// list.\n//\n// The caller must not hold tab.mutex.\nfunc (tab *Table) addFoundNode(n *enode.Node, forceSetLive bool) bool {\n\top := addNodeOp{node: n, isInbound: false, forceSetLive: forceSetLive, syncExecution: true}\n\tselect {\n\tcase tab.addNodeCh <- op:\n\t\treturn <-tab.addNodeHandled\n\tcase <-tab.closeReq:\n\t\treturn false\n\t}\n}\n\n// addInboundNode adds a node from an inbound contact. If the bucket has no space, the\n// node is added to the replacements list.\n//\n// There is an additional safety measure: if the table is still initializing the node is\n// not added. This prevents an attack where the table could be filled by just sending ping\n// repeatedly.\n//\n// The caller must not hold tab.mutex.\nfunc (tab *Table) addInboundNode(n *enode.Node) {\n\top := addNodeOp{node: n, isInbound: true}\n\tselect {\n\tcase tab.addNodeCh <- op:\n\t\treturn\n\tcase <-tab.closeReq:\n\t\treturn\n\t}\n}\n\n// Only for testing purposes\nfunc (tab *Table) addInboundNodeSync(n *enode.Node) bool {\n\top := addNodeOp{node: n, isInbound: true, syncExecution: true}\n\tselect {\n\tcase tab.addNodeCh <- op:\n\t\treturn <-tab.addNodeHandled\n\tcase <-tab.closeReq:\n\t\treturn false\n\t}\n}\n\nfunc (tab *Table) trackRequest(n *enode.Node, success bool, foundNodes []*enode.Node) {\n\top := trackRequestOp{n, foundNodes, success}\n\tselect {\n\tcase tab.trackRequestCh <- op:\n\tcase <-tab.closeReq:\n\t}\n}\n\n// loop is the main loop of Table.\nfunc (tab *Table) loop() {\n\tvar (\n\t\trefresh         = time.NewTimer(tab.nextRefreshTime())\n\t\trefreshDone     = make(chan struct{})           // where doRefresh reports completion\n\t\twaiting         = []chan struct{}{tab.initDone} // holds waiting callers while doRefresh runs\n\t\trevalTimer      = mclock.NewAlarm(tab.cfg.Clock)\n\t\treseedRandTimer = time.NewTicker(10 * time.Minute)\n\t)\n\tdefer refresh.Stop()\n\tdefer revalTimer.Stop()\n\tdefer reseedRandTimer.Stop()\n\n\t// Start initial refresh.\n\tgopool.Submit(func() {\n\t\ttab.doRefresh(refreshDone)\n\t})\nloop:\n\tfor {\n\t\tnextTime := tab.revalidation.run(tab, tab.cfg.Clock.Now())\n\t\trevalTimer.Schedule(nextTime)\n\n\t\tselect {\n\t\tcase <-reseedRandTimer.C:\n\t\t\ttab.rand.seed()\n\n\t\tcase <-revalTimer.C():\n\n\t\tcase r := <-tab.revalResponseCh:\n\t\t\ttab.revalidation.handleResponse(tab, r)\n\n\t\tcase op := <-tab.addNodeCh:\n\t\t\t// only happens in tests\n\t\t\tif op.syncExecution {\n\t\t\t\tok := tab.handleAddNode(op)\n\t\t\t\ttab.addNodeHandled <- ok\n\t\t\t} else {\n\t\t\t\t// async execution as handleAddNode is blocking\n\t\t\t\tgo func() {\n\t\t\t\t\ttab.handleAddNode(op)\n\t\t\t\t}()\n\t\t\t}\n\n\t\tcase op := <-tab.trackRequestCh:\n\t\t\ttab.handleTrackRequest(op)\n\n\t\tcase <-refresh.C:\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tgopool.Submit(func() {\n\t\t\t\t\ttab.doRefresh(refreshDone)\n\t\t\t\t})\n\t\t\t}\n\n\t\tcase req := <-tab.refreshReq:\n\t\t\twaiting = append(waiting, req)\n\t\t\tif refreshDone == nil {\n\t\t\t\trefreshDone = make(chan struct{})\n\t\t\t\tgopool.Submit(\n\t\t\t\t\tfunc() {\n\t\t\t\t\t\ttab.doRefresh(refreshDone)\n\t\t\t\t\t})\n\t\t\t}\n\n\t\tcase <-refreshDone:\n\t\t\tfor _, ch := range waiting {\n\t\t\t\tclose(ch)\n\t\t\t}\n\t\t\twaiting, refreshDone = nil, nil\n\t\t\trefresh.Reset(tab.nextRefreshTime())\n\n\t\tcase <-tab.closeReq:\n\t\t\tbreak loop\n\t\t}\n\t}\n\n\tif refreshDone != nil {\n\t\t<-refreshDone\n\t}\n\tfor _, ch := range waiting {\n\t\tclose(ch)\n\t}\n\tclose(tab.closed)\n}\n\n// doRefresh performs a lookup for a random target to keep buckets full. seed nodes are\n// inserted if the table is empty (initial bootstrap or discarded faulty peers).\nfunc (tab *Table) doRefresh(done chan struct{}) {\n\tdefer close(done)\n\n\t// Load nodes from the database and insert\n\t// them. This should yield a few previously seen nodes that are\n\t// (hopefully) still alive.\n\ttab.loadSeedNodes()\n\n\t// Run self lookup to discover new neighbor nodes.\n\ttab.net.lookupSelf()\n\n\t// The Kademlia paper specifies that the bucket refresh should\n\t// perform a lookup in the least recently used bucket. We cannot\n\t// adhere to this because the findnode target is a 512bit value\n\t// (not hash-sized) and it is not easily possible to generate a\n\t// sha3 preimage that falls into a chosen bucket.\n\t// We perform a few lookups with a random target instead.\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/rlpx/rlpx.go",
          "line": 182,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 16 - padding\n\t}\n\n\t// Read the frame content.\n\tframe, err := h.rbuf.read(conn, int(rsize))\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Validate frame MAC.\n\tframeMAC, err := h.rbuf.read(conn, 16)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\twantFrameMAC := h.ingressMAC.computeFrame(frame)\n\tif !hmac.Equal(wantFrameMAC, frameMAC) {\n\t\treturn nil, errors.New(\"bad frame MAC\")\n\t}\n\n\t// Decrypt the frame data.\n\th.dec.XORKeyStream(frame, frame)\n\treturn frame[:fsize], nil\n}\n\n// Write writes a message to the connection.\n//\n// Write returns the written size of the message data. This may be less than or equal to\n// len(data) depending on whether snappy compression is enabled.\nfunc (c *Conn) Write(code uint64, data []byte) (uint32, error) {\n\tif c.session == nil {\n\t\tpanic(\"can't WriteMsg before handshake\")\n\t}\n\tif len(data) > maxUint24 {\n\t\treturn 0, errPlainMessageTooLarge\n\t}\n\tif c.snappyWriteBuffer != nil {\n\t\t// Ensure the buffer has sufficient size.\n\t\t// Package snappy will allocate its own buffer if the provided\n\t\t// one is smaller than MaxEncodedLen.\n\t\tc.snappyWriteBuffer = growslice(c.snappyWriteBuffer, snappy.MaxEncodedLen(len(data)))\n\t\tdata = snappy.Encode(c.snappyWriteBuffer, data)\n\t}\n\n\twireSize := uint32(len(data))\n\terr := c.session.writeFrame(c.conn, code, data)\n\treturn wireSize, err\n}\n\nfunc (h *sessionState) writeFrame(conn io.Writer, code uint64, data []byte) error {\n\th.wbuf.reset()\n\n\t// Write header.\n\tfsize := rlp.IntSize(code) + len(data)\n\tif fsize > maxUint24 {\n\t\treturn errPlainMessageTooLarge\n\t}\n\theader := h.wbuf.appendZero(16)\n\tputUint24(uint32(fsize), header)\n\tcopy(header[3:], zeroHeader)\n\th.enc.XORKeyStream(header, header)\n\n\t// Write header MAC.\n\th.wbuf.Write(h.egressMAC.computeHeader(header))\n\n\t// Encode and encrypt the frame data.\n\toffset := len(h.wbuf.data)\n\th.wbuf.data = rlp.AppendUint64(h.wbuf.data, code)\n\th.wbuf.Write(data)\n\tif padding := fsize % 16",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/rlpx/buffer.go",
          "line": 68,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rn\n\tb.data = b.data[:offset+n]\n\treturn b.data[offset : offset+n], nil\n}\n\n// grow ensures the buffer has at least n bytes of unused space.\nfunc (b *readBuffer) grow(n int) {\n\tif cap(b.data)-b.end >= n {\n\t\treturn\n\t}\n\tneed := n - (cap(b.data) - b.end)\n\toffset := len(b.data)\n\tb.data = append(b.data[:cap(b.data)], make([]byte, need)...)\n\tb.data = b.data[:offset]\n}\n\n// writeBuffer implements buffering for network writes. This is essentially\n// a convenience wrapper around a byte slice.\ntype writeBuffer struct {\n\tdata []byte\n}\n\nfunc (b *writeBuffer) reset() {\n\tb.data = b.data[:0]\n}\n\nfunc (b *writeBuffer) appendZero(n int) []byte {\n\toffset := len(b.data)\n\tb.data = append(b.data, make([]byte, n)...)\n\treturn b.data[offset : offset+n]\n}\n\nfunc (b *writeBuffer) Write(data []byte) (int, error) {\n\tb.data = append(b.data, data...)\n\treturn len(data), nil\n}\n\nconst maxUint24 = int(^uint32(0) >> 8)\n\nfunc readUint24(b []byte) uint32 {\n\treturn uint32(b[2]) | uint32(b[1])<<8 | uint32(b[0])<<16\n}\n\nfunc putUint24(v uint32, b []byte) {\n\tb[0] = byte(v >> 16)\n\tb[1] = byte(v >> 8)\n\tb[2] = byte(v)\n}\n\n// growslice ensures b has the wanted length by either expanding it to its capacity\n// or allocating a new slice if b has insufficient capacity.\nfunc growslice(b []byte, wantLength int) []byte {\n\tif len(b) >= wantLength {\n\t\treturn b\n\t}\n\tif cap(b) >= wantLength {\n\t\treturn b[:cap(b)]\n\t}\n\treturn make([]byte, wantLength)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/nat/stun.go",
          "line": 110,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\":%d\", stunV2.DefaultPort)\n\t}\n\n\tlog.Trace(\"Attempting STUN binding request\", \"server\", server)\n\tconn, err := stunV2.Dial(\"udp4\", server)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tdefer conn.Close()\n\n\tmessage, err := stunV2.Build(stunV2.TransactionID, stunV2.BindingRequest)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tvar responseError error\n\tvar mappedAddr stunV2.XORMappedAddress\n\terr = conn.Do(message, func(event stunV2.Event) {\n\t\tif event.Error != nil {\n\t\t\tresponseError = event.Error\n\t\t\treturn\n\t\t}\n\t\tif err := mappedAddr.GetFrom(event.Message)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/enode/node.go",
          "line": 388,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 8\n\t\t} else {\n\t\t\tlz += bits.LeadingZeros8(x)\n\t\t\tbreak\n\t\t}\n\t}\n\treturn len(a)*8 - lz\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/msgrate/msgrate.go",
          "line": 337,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= val\n\t\t}\n\t\ttt.lock.RUnlock()\n\t}\n\tfor key, val := range capacities {\n\t\tcapacities[key] = val / float64(len(t.trackers))\n\t}\n\treturn capacities\n}\n\n// TargetRoundTrip returns the current target round trip time for a request to\n// complete in.The returned RTT is slightly under the estimated RTT. The reason\n// is that message rate estimation is a 2 dimensional problem which is solvable\n// for any RTT. The goal is to gravitate towards smaller RTTs instead of large\n// messages, to result in a stabler download stream.\nfunc (t *Trackers) TargetRoundTrip() time.Duration {\n\t// Recalculate the internal caches if it's been a while\n\tt.tune()\n\n\t// Caches surely recent, return target roundtrip\n\tt.lock.RLock()\n\tdefer t.lock.RUnlock()\n\n\treturn time.Duration(float64(t.roundtrip) * rttPushdownFactor)\n}\n\n// TargetTimeout returns the timeout allowance for a single request to finish\n// under. The timeout is proportional to the roundtrip, but also takes into\n// consideration the tracker's confidence in said roundtrip and scales it\n// accordingly. The final value is capped to avoid runaway requests.\nfunc (t *Trackers) TargetTimeout() time.Duration {\n\t// Recalculate the internal caches if it's been a while\n\tt.tune()\n\n\t// Caches surely recent, return target timeout\n\tt.lock.RLock()\n\tdefer t.lock.RUnlock()\n\n\treturn t.targetTimeout()\n}\n\n// targetTimeout is the internal lockless version of TargetTimeout to be used\n// during QoS tuning.\nfunc (t *Trackers) targetTimeout() time.Duration {\n\ttimeout := min(time.Duration(ttlScaling*float64(t.roundtrip)/t.confidence), t.OverrideTTLLimit)\n\treturn timeout\n}\n\n// tune gathers the individual tracker statistics and updates the estimated\n// request round trip time.\nfunc (t *Trackers) tune() {\n\t// Tune may be called concurrently all over the place, but we only want to\n\t// periodically update and even then only once. First check if it was updated\n\t// recently and abort if so.\n\tt.lock.RLock()\n\tdirty := time.Since(t.tuned) > t.roundtrip\n\tt.lock.RUnlock()\n\tif !dirty {\n\t\treturn\n\t}\n\t// If an update is needed, obtain a write lock but make sure we don't update\n\t// it on all concurrent threads one by one.\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tif dirty := time.Since(t.tuned) > t.roundtrip",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/netutil/net.go",
          "line": 306,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= i\n\t}\n\treturn int(n)\n}\n\n// key returns the map key for ip.\nfunc (s *DistinctNetSet) key(ip netip.Addr) netip.Prefix {\n\t// Lazily initialize storage.\n\tif s.members == nil {\n\t\ts.members = make(map[netip.Prefix]uint)\n\t}\n\tp, err := ip.Prefix(int(s.Subnet))\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn p\n}\n\n// String implements fmt.Stringer\nfunc (s DistinctNetSet) String() string {\n\tkeys := slices.SortedFunc(maps.Keys(s.members), func(a, b netip.Prefix) int {\n\t\treturn strings.Compare(a.String(), b.String())\n\t})\n\n\tvar buf bytes.Buffer\n\tbuf.WriteString(\"{\")\n\tfor i, k := range keys {\n\t\tfmt.Fprintf(&buf, \"%v\u00d7%d\", k, s.members[k])\n\t\tif i != len(keys)-1 {\n\t\t\tbuf.WriteString(\" \")\n\t\t}\n\t}\n\tbuf.WriteString(\"}\")\n\treturn buf.String()\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/p2p/discover/v5wire/encoding.go",
          "line": 286,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= authsizeExtra\n\tif authsize > int(^uint16(0)) {\n\t\tpanic(fmt.Errorf(\"BUG: auth size %d overflows uint16\", authsize))\n\t}\n\treturn Header{\n\t\tStaticHeader: StaticHeader{\n\t\t\tProtocolID: c.protocolID,\n\t\t\tVersion:    version,\n\t\t\tFlag:       flag,\n\t\t\tAuthSize:   uint16(authsize),\n\t\t},\n\t}\n}\n\n// encodeRandom encodes a packet with random content.\nfunc (c *Codec) encodeRandom(toID enode.ID) (Header, []byte, error) {\n\thead := c.makeHeader(toID, flagMessage, 0)\n\n\t// Encode auth data.\n\tauth := messageAuthData{SrcID: c.localnode.ID()}\n\tif _, err := crand.Read(head.Nonce[:])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/usbwallet/trezor.go",
          "line": 272,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= byte(chainID.Uint64()*2 + 35)\n\t\t}\n\t}\n\n\t// Inject the final signature into the transaction and sanity check the sender\n\tsigned, err := tx.WithSignature(signer, signature)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\tsender, err := types.Sender(signer, signed)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\treturn sender, signed, nil\n}\n\n// trezorExchange performs a data exchange with the Trezor wallet, sending it a\n// message and retrieving the response. If multiple responses are possible, the\n// method will also return the index of the destination object used.\nfunc (w *trezorDriver) trezorExchange(req proto.Message, results ...proto.Message) (int, error) {\n\t// Construct the original message payload to chunk up\n\tdata, err := proto.Marshal(req)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tpayload := make([]byte, 8+len(data))\n\tcopy(payload, []byte{0x23, 0x23})\n\tbinary.BigEndian.PutUint16(payload[2:], trezor.Type(req))\n\tbinary.BigEndian.PutUint32(payload[4:], uint32(len(data)))\n\tcopy(payload[8:], data)\n\n\t// Stream all the chunks to the device\n\tchunk := make([]byte, 64)\n\tchunk[0] = 0x3f // Report ID magic number\n\n\tfor len(payload) > 0 {\n\t\t// Construct the new message to stream, padding with zeroes if needed\n\t\tif len(payload) > 63 {\n\t\t\tcopy(chunk[1:], payload[:63])\n\t\t\tpayload = payload[63:]\n\t\t} else {\n\t\t\tcopy(chunk[1:], payload)\n\t\t\tcopy(chunk[1+len(payload):], make([]byte, 63-len(payload)))\n\t\t\tpayload = nil\n\t\t}\n\t\t// Send over to the device\n\t\tw.log.Trace(\"Data chunk sent to the Trezor\", \"chunk\", hexutil.Bytes(chunk))\n\t\tif _, err := w.device.Write(chunk)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/usbwallet/hub.go",
          "line": 246,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/usbwallet/ledger.go",
          "line": 403,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= byte(chainID.Uint64()*2 + 35)\n\t\t}\n\t}\n\tsigned, err := tx.WithSignature(signer, signature)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\tsender, err := types.Sender(signer, signed)\n\tif err != nil {\n\t\treturn common.Address{}, nil, err\n\t}\n\treturn sender, signed, nil\n}\n\n// ledgerSignTypedMessage sends the transaction to the Ledger wallet, and waits for the user\n// to confirm or deny the transaction.\n//\n// The signing protocol is defined as follows:\n//\n//\tCLA | INS | P1 | P2                          | Lc  | Le\n//\t----+-----+----+-----------------------------+-----+---\n//\t E0 | 0C  | 00 | implementation version : 00 | variable | variable\n//\n// Where the input is:\n//\n//\tDescription                                      | Length\n//\t-------------------------------------------------+----------\n//\tNumber of BIP 32 derivations to perform (max 10) | 1 byte\n//\tFirst derivation index (big endian)              | 4 bytes\n//\t...                                              | 4 bytes\n//\tLast derivation index (big endian)               | 4 bytes\n//\tdomain hash                                      | 32 bytes\n//\tmessage hash                                     | 32 bytes\n//\n// And the output data is:\n//\n//\tDescription | Length\n//\t------------+---------\n//\tsignature V | 1 byte\n//\tsignature R | 32 bytes\n//\tsignature S | 32 bytes\nfunc (w *ledgerDriver) ledgerSignTypedMessage(derivationPath []uint32, domainHash []byte, messageHash []byte) ([]byte, error) {\n\t// Flatten the derivation path into the Ledger request\n\tpath := make([]byte, 1+4*len(derivationPath))\n\tpath[0] = byte(len(derivationPath))\n\tfor i, component := range derivationPath {\n\t\tbinary.BigEndian.PutUint32(path[1+4*i:], component)\n\t}\n\t// Create the 712 message\n\tpayload := append(path, domainHash...)\n\tpayload = append(payload, messageHash...)\n\n\t// Send the request and wait for the response\n\tvar (\n\t\top    = ledgerP1InitTypedMessageData\n\t\treply []byte\n\t\terr   error\n\t)\n\n\t// Send the message over, ensuring it's processed correctly\n\treply, err = w.ledgerExchange(ledgerOpSignTypedMessage, op, 0, payload)\n\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Extract the Ethereum signature and do a sanity validation\n\tif len(reply) != crypto.SignatureLength {\n\t\treturn nil, errors.New(\"reply lacks signature\")\n\t}\n\tsignature := append(reply[1:], reply[0])\n\treturn signature, nil\n}\n\n// ledgerExchange performs a data exchange with the Ledger wallet, sending it a\n// message and retrieving the response.\n//\n// The common transport header is defined as follows:\n//\n//\tDescription                           | Length\n//\t--------------------------------------+----------\n//\tCommunication channel ID (big endian) | 2 bytes\n//\tCommand tag                           | 1 byte\n//\tPacket sequence index (big endian)    | 2 bytes\n//\tPayload                               | arbitrary\n//\n// The Communication channel ID allows commands multiplexing over the same\n// physical link. It is not used for the time being, and should be set to 0101\n// to avoid compatibility issues with implementations ignoring a leading 00 byte.\n//\n// The Command tag describes the message content. Use TAG_APDU (0x05) for standard\n// APDU payloads, or TAG_PING (0x02) for a simple link test.\n//\n// The Packet sequence index describes the current sequence for fragmented payloads.\n// The first fragment index is 0x00.\n//\n// APDU Command payloads are encoded as follows:\n//\n//\tDescription              | Length\n//\t-----------------------------------\n//\tAPDU length (big endian) | 2 bytes\n//\tAPDU CLA                 | 1 byte\n//\tAPDU INS                 | 1 byte\n//\tAPDU P1                  | 1 byte\n//\tAPDU P2                  | 1 byte\n//\tAPDU length              | 1 byte\n//\tOptional APDU data       | arbitrary\nfunc (w *ledgerDriver) ledgerExchange(opcode ledgerOpcode, p1 ledgerParam1, p2 ledgerParam2, data []byte) ([]byte, error) {\n\t// Construct the message payload, possibly split into multiple chunks\n\tapdu := make([]byte, 2, 7+len(data))\n\n\tbinary.BigEndian.PutUint16(apdu, uint16(5+len(data)))\n\tapdu = append(apdu, []byte{0xe0, byte(opcode), byte(p1), byte(p2), byte(len(data))}...)\n\tapdu = append(apdu, data...)\n\n\t// Stream all the chunks to the device\n\theader := []byte{0x01, 0x01, 0x05, 0x00, 0x00} // Channel ID and command tag appended\n\tchunk := make([]byte, 64)\n\tspace := len(chunk) - len(header)\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/usbwallet/wallet.go",
          "line": 174,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/keystore/account_cache_test.go",
          "line": 222,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tcache.delete(wantAccounts[i])\n\t}\n\tcache.delete(accounts.Account{Address: common.HexToAddress(\"fd9bd350f08ee3c0c19b85a8e16114a11a60aa4e\"), URL: accounts.URL{Scheme: KeyStoreScheme, Path: \"something\"}})\n\n\t// Check content again after deletion.\n\twantAccountsAfterDelete := []accounts.Account{\n\t\twantAccounts[1],\n\t\twantAccounts[3],\n\t\twantAccounts[5],\n\t}\n\tlist = cache.accounts()\n\tif !reflect.DeepEqual(list, wantAccountsAfterDelete) {\n\t\tt.Fatalf(\"got accounts after delete: %s\\nwant %s\", spew.Sdump(list), spew.Sdump(wantAccountsAfterDelete))\n\t}\n\tfor _, a := range wantAccountsAfterDelete {\n\t\tif !cache.hasAddress(a.Address) {\n\t\t\tt.Errorf(\"expected hasAccount(%x) to return true\", a.Address)\n\t\t}\n\t}\n\tif cache.hasAddress(wantAccounts[0].Address) {\n\t\tt.Errorf(\"expected hasAccount(%x) to return false\", wantAccounts[0].Address)\n\t}\n}\n\nfunc TestCacheFind(t *testing.T) {\n\tt.Parallel()\n\tdir := filepath.Join(\"testdata\", \"dir\")\n\tcache, _ := newAccountCache(dir)\n\tcache.watcher.running = true // prevent unexpected reloads\n\n\taccs := []accounts.Account{\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"095e7baea6a6c7c4c2dfeb977efac326af552d87\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"a.key\")},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"2cac1adea150210703ba75ed097ddfe24e14f213\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"b.key\")},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"d49ff4eeb0b2686ed89c0fc0f2b6ea533ddbbd5e\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"c.key\")},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"d49ff4eeb0b2686ed89c0fc0f2b6ea533ddbbd5e\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"c2.key\")},\n\t\t},\n\t}\n\tfor _, a := range accs {\n\t\tcache.add(a)\n\t}\n\n\tnomatchAccount := accounts.Account{\n\t\tAddress: common.HexToAddress(\"f466859ead1932d743d622cb74fc058882e8648a\"),\n\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Join(dir, \"something\")},\n\t}\n\ttests := []struct {\n\t\tQuery      accounts.Account\n\t\tWantResult accounts.Account\n\t\tWantError  error\n\t}{\n\t\t// by address\n\t\t{Query: accounts.Account{Address: accs[0].Address}, WantResult: accs[0]},\n\t\t// by file\n\t\t{Query: accounts.Account{URL: accs[0].URL}, WantResult: accs[0]},\n\t\t// by basename\n\t\t{Query: accounts.Account{URL: accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Base(accs[0].URL.Path)}}, WantResult: accs[0]},\n\t\t// by file and address\n\t\t{Query: accs[0], WantResult: accs[0]},\n\t\t// ambiguous address, tie resolved by file\n\t\t{Query: accs[2], WantResult: accs[2]},\n\t\t// ambiguous address error\n\t\t{\n\t\t\tQuery: accounts.Account{Address: accs[2].Address},\n\t\t\tWantError: &AmbiguousAddrError{\n\t\t\t\tAddr:    accs[2].Address,\n\t\t\t\tMatches: []accounts.Account{accs[2], accs[3]},\n\t\t\t},\n\t\t},\n\t\t// no match error\n\t\t{Query: nomatchAccount, WantError: ErrNoMatch},\n\t\t{Query: accounts.Account{URL: nomatchAccount.URL}, WantError: ErrNoMatch},\n\t\t{Query: accounts.Account{URL: accounts.URL{Scheme: KeyStoreScheme, Path: filepath.Base(nomatchAccount.URL.Path)}}, WantError: ErrNoMatch},\n\t\t{Query: accounts.Account{Address: nomatchAccount.Address}, WantError: ErrNoMatch},\n\t}\n\tfor i, test := range tests {\n\t\ta, err := cache.find(test.Query)\n\t\tif !reflect.DeepEqual(err, test.WantError) {\n\t\t\tt.Errorf(\"test %d: error mismatch for query %v\\ngot %q\\nwant %q\", i, test.Query, err, test.WantError)\n\t\t\tcontinue\n\t\t}\n\t\tif a != test.WantResult {\n\t\t\tt.Errorf(\"test %d: result mismatch for query %v\\ngot %v\\nwant %v\", i, test.Query, a, test.WantResult)\n\t\t\tcontinue\n\t\t}\n\t}\n}\n\n// TestUpdatedKeyfileContents tests that updating the contents of a keystore file\n// is noticed by the watcher, and the account cache is updated accordingly\nfunc TestUpdatedKeyfileContents(t *testing.T) {\n\tt.Parallel()\n\n\t// Create a temporary keystore to test with\n\tdir := t.TempDir()\n\n\tks := NewKeyStore(dir, LightScryptN, LightScryptP)\n\n\tlist := ks.Accounts()\n\tif len(list) > 0 {\n\t\tt.Error(\"initial account list not empty:\", list)\n\t}\n\tif !waitWatcherStart(ks) {\n\t\tt.Fatal(\"keystore watcher didn't start in time\")\n\t}\n\t// Copy a key file into it\n\tfile := filepath.Join(dir, \"aaa\")\n\n\t// Place one of our testfiles in there\n\tif err := cp.CopyFile(file, cachetestAccounts[0].URL.Path)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/keystore/account_cache_test.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 2 {\n\t\tlist = ks.Accounts()\n\t\tif reflect.DeepEqual(list, wantAccounts) {\n\t\t\t// ks should have also received change notifications\n\t\t\tselect {\n\t\t\tcase <-ks.changes:\n\t\t\tdefault:\n\t\t\t\tt.Fatalf(\"wasn't notified of new accounts\")\n\t\t\t}\n\t\t\treturn\n\t\t}\n\t\ttime.Sleep(d)\n\t}\n\tt.Errorf(\"\\ngot  %v\\nwant %v\", list, wantAccounts)\n}\n\nfunc TestCacheInitialReload(t *testing.T) {\n\tt.Parallel()\n\tcache, _ := newAccountCache(cachetestDir)\n\taccounts := cache.accounts()\n\tif !reflect.DeepEqual(accounts, cachetestAccounts) {\n\t\tt.Fatalf(\"got initial accounts: %swant %s\", spew.Sdump(accounts), spew.Sdump(cachetestAccounts))\n\t}\n}\n\nfunc TestCacheAddDeleteOrder(t *testing.T) {\n\tt.Parallel()\n\tcache, _ := newAccountCache(\"testdata/no-such-dir\")\n\tcache.watcher.running = true // prevent unexpected reloads\n\n\taccs := []accounts.Account{\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"095e7baea6a6c7c4c2dfeb977efac326af552d87\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"-309830980\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"2cac1adea150210703ba75ed097ddfe24e14f213\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"ggg\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"8bda78331c916a08481428e4b07c96d3e916d165\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"zzzzzz-the-very-last-one.keyXXX\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"d49ff4eeb0b2686ed89c0fc0f2b6ea533ddbbd5e\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"SOMETHING.key\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"7ef5a6135f1fd6a02593eedc869c6d41d934aef8\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"UTC--2016-03-22T12-57-55.920751759Z--7ef5a6135f1fd6a02593eedc869c6d41d934aef8\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"f466859ead1932d743d622cb74fc058882e8648a\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"aaa\"},\n\t\t},\n\t\t{\n\t\t\tAddress: common.HexToAddress(\"289d485d9771714cce91d3393d764e1311907acc\"),\n\t\t\tURL:     accounts.URL{Scheme: KeyStoreScheme, Path: \"zzz\"},\n\t\t},\n\t}\n\tfor _, a := range accs {\n\t\tcache.add(a)\n\t}\n\t// Add some of them twice to check that they don't get reinserted.\n\tcache.add(accs[0])\n\tcache.add(accs[2])\n\n\t// Check that the account list is sorted by filename.\n\twantAccounts := make([]accounts.Account, len(accs))\n\tcopy(wantAccounts, accs)\n\tslices.SortFunc(wantAccounts, byURL)\n\tlist := cache.accounts()\n\tif !reflect.DeepEqual(list, wantAccounts) {\n\t\tt.Fatalf(\"got accounts: %s\\nwant %s\", spew.Sdump(accs), spew.Sdump(wantAccounts))\n\t}\n\tfor _, a := range accs {\n\t\tif !cache.hasAddress(a.Address) {\n\t\t\tt.Errorf(\"expected hasAccount(%x) to return true\", a.Address)\n\t\t}\n\t}\n\tif cache.hasAddress(common.HexToAddress(\"fd9bd350f08ee3c0c19b85a8e16114a11a60aa4e\")) {\n\t\tt.Errorf(\"expected hasAccount(%x) to return false\", common.HexToAddress(\"fd9bd350f08ee3c0c19b85a8e16114a11a60aa4e\"))\n\t}\n\n\t// Delete a few keys from the cache.\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/keystore/account_cache.go",
          "line": 57,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a.URL.Path\n\t\tif i < len(err.Matches)-1 {\n\t\t\tfiles += \", \"\n\t\t}\n\t}\n\treturn fmt.Sprintf(\"multiple keys match address (%s)\", files)\n}\n\n// accountCache is a live index of all accounts in the keystore.\ntype accountCache struct {\n\tkeydir   string\n\twatcher  *watcher\n\tmu       sync.Mutex\n\tall      []accounts.Account\n\tbyAddr   map[common.Address][]accounts.Account\n\tthrottle *time.Timer\n\tnotify   chan struct{}\n\tfileC    fileCache\n}\n\nfunc newAccountCache(keydir string) (*accountCache, chan struct{}) {\n\tac := &accountCache{\n\t\tkeydir: keydir,\n\t\tbyAddr: make(map[common.Address][]accounts.Account),\n\t\tnotify: make(chan struct{}, 1),\n\t\tfileC:  fileCache{all: mapset.NewThreadUnsafeSet[string]()},\n\t}\n\tac.watcher = newWatcher(ac)\n\treturn ac, ac.notify\n}\n\nfunc (ac *accountCache) accounts() []accounts.Account {\n\tac.maybeReload()\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\tcpy := make([]accounts.Account, len(ac.all))\n\tcopy(cpy, ac.all)\n\treturn cpy\n}\n\nfunc (ac *accountCache) hasAddress(addr common.Address) bool {\n\tac.maybeReload()\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\treturn len(ac.byAddr[addr]) > 0\n}\n\nfunc (ac *accountCache) add(newAccount accounts.Account) {\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\n\ti := sort.Search(len(ac.all), func(i int) bool { return ac.all[i].URL.Cmp(newAccount.URL) >= 0 })\n\tif i < len(ac.all) && ac.all[i] == newAccount {\n\t\treturn\n\t}\n\t// newAccount is not in the cache.\n\tac.all = append(ac.all, accounts.Account{})\n\tcopy(ac.all[i+1:], ac.all[i:])\n\tac.all[i] = newAccount\n\tac.byAddr[newAccount.Address] = append(ac.byAddr[newAccount.Address], newAccount)\n}\n\n// note: removed needs to be unique here (i.e. both File and Address must be set).\nfunc (ac *accountCache) delete(removed accounts.Account) {\n\tac.mu.Lock()\n\tdefer ac.mu.Unlock()\n\n\tac.all = removeAccount(ac.all, removed)\n\tif ba := removeAccount(ac.byAddr[removed.Address], removed)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/keystore/keystore.go",
          "line": 170,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/keystore/passphrase_test.go",
          "line": 56,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"new data appended\" // nolint: gosec\n\t\tif keyjson, err = EncryptKey(key, password, veryLightScryptN, veryLightScryptP)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/argument.go",
          "line": 210,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= getTypeSize(arg.Type)/32 - 1\n\t\t} else if arg.Type.T == TupleTy && !isDynamicType(arg.Type) {\n\t\t\t// If we have a static tuple, like (uint256, bool, uint256), these are\n\t\t\t// coded as just like uint256,bool,uint256\n\t\t\tvirtualArgs += getTypeSize(arg.Type)/32 - 1\n\t\t}\n\t\tretval = append(retval, marshalledValue)\n\t\tindex++\n\t}\n\treturn retval, nil\n}\n\n// PackValues performs the operation Go format -> Hexdata.\n// It is the semantic opposite of UnpackValues.\nfunc (arguments Arguments) PackValues(args []any) ([]byte, error) {\n\treturn arguments.Pack(args...)\n}\n\n// Pack performs the operation Go format -> Hexdata.\nfunc (arguments Arguments) Pack(args ...any) ([]byte, error) {\n\t// Make sure arguments match up and pack them\n\tabiArgs := arguments\n\tif len(args) != len(abiArgs) {\n\t\treturn nil, fmt.Errorf(\"argument count mismatch: got %d for %d\", len(args), len(abiArgs))\n\t}\n\t// variable input is the output appended at the end of packed\n\t// output. This is used for strings and bytes types input.\n\tvar variableInput []byte\n\n\t// input offset is the bytes offset for packed output\n\tinputOffset := 0\n\tfor _, abiArg := range abiArgs {\n\t\tinputOffset += getTypeSize(abiArg.Type)\n\t}\n\tvar ret []byte\n\tfor i, a := range args {\n\t\tinput := abiArgs[i]\n\t\t// pack the input\n\t\tpacked, err := input.Type.pack(reflect.ValueOf(a))\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// check for dynamic types\n\t\tif isDynamicType(input.Type) {\n\t\t\t// set the offset\n\t\t\tret = append(ret, packNum(reflect.ValueOf(inputOffset))...)\n\t\t\t// calculate next offset\n\t\t\tinputOffset += len(packed)\n\t\t\t// append to variable input\n\t\t\tvariableInput = append(variableInput, packed...)\n\t\t} else {\n\t\t\t// append the packed value to the input\n\t\t\tret = append(ret, packed...)\n\t\t}\n\t}\n\t// append the variable input at the end of the packed input\n\tret = append(ret, variableInput...)\n\n\treturn ret, nil\n}\n\n// ToCamelCase converts an under-score string to a camel-case string\nfunc ToCamelCase(input string) string {\n\tparts := strings.Split(input, \"_\")\n\tfor i, s := range parts {\n\t\tif len(s) > 0 {\n\t\t\tparts[i] = strings.ToUpper(s[:1]) + s[1:]\n\t\t}\n\t}\n\treturn strings.Join(parts, \"\")\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/abifuzzer_test.go",
          "line": 104,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(`, \"stateMutability\": \"%v\" `, *stateMutability)\n\t}\n\tif payable != nil {\n\t\tsig += fmt.Sprintf(`, \"payable\": %v `, *payable)\n\t}\n\tif len(inputs) > 0 {\n\t\tsig += `, \"inputs\" : [ {`\n\t\tfor i, inp := range inputs {\n\t\t\tsig += fmt.Sprintf(`\"name\" : \"%v\", \"type\" : \"%v\" `, inp.name, inp.typ)\n\t\t\tif i+1 < len(inputs) {\n\t\t\t\tsig += \",\"\n\t\t\t}\n\t\t}\n\t\tsig += \"} ]\"\n\t\tsig += `, \"outputs\" : [ {`\n\t\tfor i, inp := range inputs {\n\t\t\tsig += fmt.Sprintf(`\"name\" : \"%v\", \"type\" : \"%v\" `, inp.name, inp.typ)\n\t\t\tif i+1 < len(inputs) {\n\t\t\t\tsig += \",\"\n\t\t\t}\n\t\t}\n\t\tsig += \"} ]\"\n\t}\n\tsig += `}]`\n\t//fmt.Printf(\"sig: %s\\n\", sig)\n\treturn JSON(strings.NewReader(sig))\n}\n\nfunc fuzzAbi(input []byte) {\n\tvar (\n\t\tfuzzer    = fuzz.NewFromGoFuzz(input)\n\t\tname      = oneOf(fuzzer, names)\n\t\tstateM    = oneOfOrNil(fuzzer, stateMut)\n\t\tpayable   = oneOfOrNil(fuzzer, pays)\n\t\targuments []arg\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/abi/abifuzzer_test.go",
          "line": 145,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"[]\"\n\t\tcase 1: // 10% chance to make it an array\n\t\t\targTyp += fmt.Sprintf(\"[%d]\", 1+upTo(fuzzer, 30))\n\t\tdefault:\n\t\t}\n\t\targuments = append(arguments, arg{name: argName, typ: argTyp})\n\t}\n\tabi, err := createABI(name, stateM, payable, arguments)\n\tif err != nil {\n\t\t//fmt.Printf(\"err: %v\\n\", err)\n\t\tpanic(err)\n\t}\n\tstructs, _ := unpackPack(abi, name, input)\n\t_ = packUnpack(abi, name, &structs)\n}\n\nfunc upTo(fuzzer *fuzz.Fuzzer, max int) int {\n\tvar i int\n\tfuzzer.Fuzz(&i)\n\tif i < 0 {\n\t\treturn (-1 - i) % max\n\t}\n\treturn i % max\n}\n\nfunc oneOf(fuzzer *fuzz.Fuzzer, options []string) string {\n\treturn options[upTo(fuzzer, len(options))]\n}\n\nfunc oneOfOrNil(fuzzer *fuzz.Fuzzer, options []string) *string {\n\tif i := upTo(fuzzer, len(options)+1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/type.go",
          "line": 173,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"(\"\n\t\tfor idx, c := range components {\n\t\t\tcType, err := NewType(c.Type, c.InternalType, c.Components)\n\t\t\tif err != nil {\n\t\t\t\treturn Type{}, err\n\t\t\t}\n\t\t\tname := ToCamelCase(c.Name)\n\t\t\tif name == \"\" {\n\t\t\t\treturn Type{}, errors.New(\"abi: purely anonymous or underscored field is not supported\")\n\t\t\t}\n\t\t\tfieldName := ResolveNameConflict(name, func(s string) bool { return used[s] })\n\t\t\tused[fieldName] = true\n\t\t\tif !isValidFieldName(fieldName) {\n\t\t\t\treturn Type{}, fmt.Errorf(\"field %d has invalid name\", idx)\n\t\t\t}\n\t\t\tfields = append(fields, reflect.StructField{\n\t\t\t\tName: fieldName, // reflect.StructOf will panic for any exported field.\n\t\t\t\tType: cType.GetType(),\n\t\t\t\tTag:  reflect.StructTag(\"json:\\\"\" + c.Name + \"\\\"\"),\n\t\t\t})\n\t\t\telems = append(elems, &cType)\n\t\t\tnames = append(names, c.Name)\n\t\t\texpression += cType.stringKind\n\t\t\tif idx != len(components)-1 {\n\t\t\t\texpression += \",\"\n\t\t\t}\n\t\t}\n\t\texpression += \")\"\n\n\t\ttyp.TupleType = reflect.StructOf(fields)\n\t\ttyp.TupleElems = elems\n\t\ttyp.TupleRawNames = names\n\t\ttyp.T = TupleTy\n\t\ttyp.stringKind = expression\n\n\t\tconst structPrefix = \"struct \"\n\t\t// After solidity 0.5.10, a new field of abi \"internalType\"\n\t\t// is introduced. From that we can obtain the struct name\n\t\t// user defined in the source code.\n\t\tif internalType != \"\" && strings.HasPrefix(internalType, structPrefix) {\n\t\t\t// Foo.Bar type definition is not allowed in golang,\n\t\t\t// convert the format to FooBar\n\t\t\ttyp.TupleRawName = strings.ReplaceAll(internalType[len(structPrefix):], \".\", \"\")\n\t\t}\n\n\tcase \"function\":\n\t\ttyp.T = FunctionTy\n\t\ttyp.Size = 24\n\tdefault:\n\t\tif strings.HasPrefix(internalType, \"contract \") {\n\t\t\ttyp.Size = 20\n\t\t\ttyp.T = AddressTy\n\t\t} else {\n\t\t\treturn Type{}, fmt.Errorf(\"unsupported arg type: %s\", t)\n\t\t}\n\t}\n\n\treturn\n}\n\n// GetType returns the reflection type of the ABI type.\nfunc (t Type) GetType() reflect.Type {\n\tswitch t.T {\n\tcase IntTy:\n\t\treturn reflectIntType(false, t.Size)\n\tcase UintTy:\n\t\treturn reflectIntType(true, t.Size)\n\tcase BoolTy:\n\t\treturn reflect.TypeOf(false)\n\tcase StringTy:\n\t\treturn reflect.TypeOf(\"\")\n\tcase SliceTy:\n\t\treturn reflect.SliceOf(t.Elem.GetType())\n\tcase ArrayTy:\n\t\treturn reflect.ArrayOf(t.Size, t.Elem.GetType())\n\tcase TupleTy:\n\t\treturn t.TupleType\n\tcase AddressTy:\n\t\treturn reflect.TypeOf(common.Address{})\n\tcase FixedBytesTy:\n\t\treturn reflect.ArrayOf(t.Size, reflect.TypeOf(byte(0)))\n\tcase BytesTy:\n\t\treturn reflect.SliceOf(reflect.TypeOf(byte(0)))\n\tcase HashTy:\n\t\t// hashtype currently not used\n\t\treturn reflect.ArrayOf(32, reflect.TypeOf(byte(0)))\n\tcase FixedPointTy:\n\t\t// fixedpoint type currently not used\n\t\treturn reflect.ArrayOf(32, reflect.TypeOf(byte(0)))\n\tcase FunctionTy:\n\t\treturn reflect.ArrayOf(24, reflect.TypeOf(byte(0)))\n\tdefault:\n\t\tpanic(\"Invalid type\")\n\t}\n}\n\n// String implements Stringer.\nfunc (t Type) String() (out string) {\n\treturn t.stringKind\n}\n\nfunc (t Type) pack(v reflect.Value) ([]byte, error) {\n\t// dereference pointer first if it's a pointer\n\tv = indirect(v)\n\tif err := typeCheck(t, v)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/abi/type.go",
          "line": 307,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(val)\n\t\t\ttail = append(tail, val...)\n\t\t}\n\t\treturn append(ret, tail...), nil\n\tcase TupleTy:\n\t\t// (T1,...,Tk) for k >= 0 and any types T1, \u2026, Tk\n\t\t// enc(X) = head(X(1)) ... head(X(k)) tail(X(1)) ... tail(X(k))\n\t\t// where X = (X(1), ..., X(k)) and head and tail are defined for Ti being a static\n\t\t// type as\n\t\t//     head(X(i)) = enc(X(i)) and tail(X(i)) = \"\" (the empty string)\n\t\t// and as\n\t\t//     head(X(i)) = enc(len(head(X(1)) ... head(X(k)) tail(X(1)) ... tail(X(i-1))))\n\t\t//     tail(X(i)) = enc(X(i))\n\t\t// otherwise, i.e. if Ti is a dynamic type.\n\t\tfieldmap, err := mapArgNamesToStructFields(t.TupleRawNames, v)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\t// Calculate prefix occupied size.\n\t\toffset := 0\n\t\tfor _, elem := range t.TupleElems {\n\t\t\toffset += getTypeSize(*elem)\n\t\t}\n\t\tvar ret, tail []byte\n\t\tfor i, elem := range t.TupleElems {\n\t\t\tfield := v.FieldByName(fieldmap[t.TupleRawNames[i]])\n\t\t\tif !field.IsValid() {\n\t\t\t\treturn nil, fmt.Errorf(\"field %s for tuple not found in the given struct\", t.TupleRawNames[i])\n\t\t\t}\n\t\t\tval, err := elem.pack(field)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tif isDynamicType(*elem) {\n\t\t\t\tret = append(ret, packNum(reflect.ValueOf(offset))...)\n\t\t\t\ttail = append(tail, val...)\n\t\t\t\toffset += len(val)\n\t\t\t} else {\n\t\t\t\tret = append(ret, val...)\n\t\t\t}\n\t\t}\n\t\treturn append(ret, tail...), nil\n\n\tdefault:\n\t\treturn packElement(t, v)\n\t}\n}\n\n// requiresLengthPrefix returns whether the type requires any sort of length\n// prefixing.\nfunc (t Type) requiresLengthPrefix() bool {\n\treturn t.T == StringTy || t.T == BytesTy || t.T == SliceTy\n}\n\n// isDynamicType returns true if the type is dynamic.\n// The following types are called \u201cdynamic\u201d:\n// * bytes\n// * string\n// * T[] for any T\n// * T[k] for any dynamic T and any k >= 0\n// * (T1,...,Tk) if Ti is dynamic for some 1 <= i <= k\nfunc isDynamicType(t Type) bool {\n\tif t.T == TupleTy {\n\t\tfor _, elem := range t.TupleElems {\n\t\t\tif isDynamicType(*elem) {\n\t\t\t\treturn true\n\t\t\t}\n\t\t}\n\t\treturn false\n\t}\n\treturn t.T == StringTy || t.T == BytesTy || t.T == SliceTy || (t.T == ArrayTy && isDynamicType(*t.Elem))\n}\n\n// getTypeSize returns the size that this type needs to occupy.\n// We distinguish static and dynamic types. Static types are encoded in-place\n// and dynamic types are encoded at a separately allocated location after the\n// current block.\n// So for a static variable, the size returned represents the size that the\n// variable actually occupies.\n// For a dynamic variable, the returned size is fixed 32 bytes, which is used\n// to store the location reference for actual value storage.\nfunc getTypeSize(t Type) int {\n\tif t.T == ArrayTy && !isDynamicType(*t.Elem) {\n\t\t// Recursively calculate type size if it is a nested array\n\t\tif t.Elem.T == ArrayTy || t.Elem.T == TupleTy {\n\t\t\treturn t.Size * getTypeSize(*t.Elem)\n\t\t}\n\t\treturn t.Size * 32\n\t} else if t.T == TupleTy && !isDynamicType(t) {\n\t\ttotal := 0\n\t\tfor _, elem := range t.TupleElems {\n\t\t\ttotal += getTypeSize(*elem)\n\t\t}\n\t\treturn total\n\t}\n\treturn 32\n}\n\n// isLetter reports whether a given 'rune' is classified as a Letter.\n// This method is copied from reflect/type.go\nfunc isLetter(ch rune) bool {\n\treturn 'a' <= ch && ch <= 'z' || 'A' <= ch && ch <= 'Z' || ch == '_' || ch >= utf8.RuneSelf && unicode.IsLetter(ch)\n}\n\n// isValidFieldName checks if a string is a valid (struct) field name or not.\n//\n// According to the language spec, a field name should be an identifier.\n//\n// identifier = letter { letter | unicode_digit } .\n// letter = unicode_letter | \"_\" .\n// This method is copied from reflect/type.go\nfunc isValidFieldName(fieldName string) bool {\n\tfor i, c := range fieldName {\n\t\tif i == 0 && !isLetter(c) {\n\t\t\treturn false\n\t\t}\n\n\t\tif !(isLetter(c) || unicode.IsDigit(c)) {\n\t\t\treturn false\n\t\t}\n\t}\n\n\treturn len(fieldName) > 0\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/unpack.go",
          "line": 211,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= getTypeSize(*elem)/32 - 1\n\t\t} else if elem.T == TupleTy && !isDynamicType(*elem) {\n\t\t\t// If we have a static tuple, like (uint256, bool, uint256), these are\n\t\t\t// coded as just like uint256,bool,uint256\n\t\t\tvirtualArgs += getTypeSize(*elem)/32 - 1\n\t\t}\n\t\tretval.Field(index).Set(reflect.ValueOf(marshalledValue))\n\t}\n\treturn retval.Interface(), nil\n}\n\n// toGoType parses the output bytes and recursively assigns the value of these bytes\n// into a go type with accordance with the ABI spec.\nfunc toGoType(index int, t Type, output []byte) (interface{}, error) {\n\tif index+32 > len(output) {\n\t\treturn nil, fmt.Errorf(\"abi: cannot marshal in to go type: length insufficient %d require %d\", len(output), index+32)\n\t}\n\n\tvar (\n\t\treturnOutput  []byte\n\t\tbegin, length int\n\t\terr           error\n\t)\n\n\t// if we require a length prefix, find the beginning word and size returned.\n\tif t.requiresLengthPrefix() {\n\t\tbegin, length, err = lengthPrefixPointsTo(index, output)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t} else {\n\t\treturnOutput = output[index : index+32]\n\t}\n\n\tswitch t.T {\n\tcase TupleTy:\n\t\tif isDynamicType(t) {\n\t\t\tbegin, err := tuplePointsTo(index, output)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\treturn forTupleUnpack(t, output[begin:])\n\t\t}\n\t\treturn forTupleUnpack(t, output[index:])\n\tcase SliceTy:\n\t\treturn forEachUnpack(t, output[begin:], 0, length)\n\tcase ArrayTy:\n\t\tif isDynamicType(*t.Elem) {\n\t\t\toffset := binary.BigEndian.Uint64(returnOutput[len(returnOutput)-8:])\n\t\t\tif offset > uint64(len(output)) {\n\t\t\t\treturn nil, fmt.Errorf(\"abi: toGoType offset greater than output length: offset: %d, len(output): %d\", offset, len(output))\n\t\t\t}\n\t\t\treturn forEachUnpack(t, output[offset:], 0, t.Size)\n\t\t}\n\t\treturn forEachUnpack(t, output[index:], 0, t.Size)\n\tcase StringTy: // variable arrays are written at the end of the return bytes\n\t\treturn string(output[begin : begin+length]), nil\n\tcase IntTy, UintTy:\n\t\treturn ReadInteger(t, returnOutput)\n\tcase BoolTy:\n\t\treturn readBool(returnOutput)\n\tcase AddressTy:\n\t\treturn common.BytesToAddress(returnOutput), nil\n\tcase HashTy:\n\t\treturn common.BytesToHash(returnOutput), nil\n\tcase BytesTy:\n\t\treturn output[begin : begin+length], nil\n\tcase FixedBytesTy:\n\t\treturn ReadFixedBytes(t, returnOutput)\n\tcase FunctionTy:\n\t\treturn readFunctionType(t, returnOutput)\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"abi: unknown type %v\", t.T)\n\t}\n}\n\n// lengthPrefixPointsTo interprets a 32 byte slice as an offset and then determines which indices to look to decode the type.\nfunc lengthPrefixPointsTo(index int, output []byte) (start int, length int, err error) {\n\tbigOffsetEnd := new(big.Int).SetBytes(output[index : index+32])\n\tbigOffsetEnd.Add(bigOffsetEnd, common.Big32)\n\toutputLength := big.NewInt(int64(len(output)))\n\n\tif bigOffsetEnd.Cmp(outputLength) > 0 {\n\t\treturn 0, 0, fmt.Errorf(\"abi: cannot marshal in to go slice: offset %v would go over slice boundary (len=%v)\", bigOffsetEnd, outputLength)\n\t}\n\n\tif bigOffsetEnd.BitLen() > 63 {\n\t\treturn 0, 0, fmt.Errorf(\"abi offset larger than int64: %v\", bigOffsetEnd)\n\t}\n\n\toffsetEnd := int(bigOffsetEnd.Uint64())\n\tlengthBig := new(big.Int).SetBytes(output[offsetEnd-32 : offsetEnd])\n\n\ttotalSize := new(big.Int).Add(bigOffsetEnd, lengthBig)\n\tif totalSize.BitLen() > 63 {\n\t\treturn 0, 0, fmt.Errorf(\"abi: length larger than int64: %v\", totalSize)\n\t}\n\n\tif totalSize.Cmp(outputLength) > 0 {\n\t\treturn 0, 0, fmt.Errorf(\"abi: cannot marshal in to go type: length insufficient %v require %v\", outputLength, totalSize)\n\t}\n\tstart = int(bigOffsetEnd.Uint64())\n\tlength = int(lengthBig.Uint64())\n\treturn\n}\n\n// tuplePointsTo resolves the location reference for dynamic tuple.\nfunc tuplePointsTo(index int, output []byte) (start int, err error) {\n\toffset := new(big.Int).SetBytes(output[index : index+32])\n\toutputLen := big.NewInt(int64(len(output)))\n\n\tif offset.Cmp(outputLen) > 0 {\n\t\treturn 0, fmt.Errorf(\"abi: cannot marshal in to go slice: offset %v would go over slice boundary (len=%v)\", offset, outputLen)\n\t}\n\tif offset.BitLen() > 63 {\n\t\treturn 0, fmt.Errorf(\"abi offset larger than int64: %v\", offset)\n\t}\n\treturn int(offset.Uint64()), nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/method.go",
          "line": 107,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\" %v\", output.Name)\n\t\t}\n\t}\n\t// calculate the signature and method id. Note only function\n\t// has meaningful signature and id.\n\tvar (\n\t\tsig string\n\t\tid  []byte\n\t)\n\tif funType == Function {\n\t\tsig = fmt.Sprintf(\"%v(%v)\", rawName, strings.Join(types, \",\"))\n\t\tid = crypto.Keccak256([]byte(sig))[:4]\n\t}\n\tidentity := fmt.Sprintf(\"function %v\", rawName)\n\tswitch funType {\n\tcase Fallback:\n\t\tidentity = \"fallback\"\n\tcase Receive:\n\t\tidentity = \"receive\"\n\tcase Constructor:\n\t\tidentity = \"constructor\"\n\t}\n\tvar str string\n\t// Extract meaningful state mutability of solidity method.\n\t// If it's empty string or default value \"nonpayable\", never print it.\n\tif mutability == \"\" || mutability == \"nonpayable\" {\n\t\tstr = fmt.Sprintf(\"%v(%v) returns(%v)\", identity, strings.Join(inputNames, \", \"), strings.Join(outputNames, \", \"))\n\t} else {\n\t\tstr = fmt.Sprintf(\"%v(%v) %s returns(%v)\", identity, strings.Join(inputNames, \", \"), mutability, strings.Join(outputNames, \", \"))\n\t}\n\n\treturn Method{\n\t\tName:            name,\n\t\tRawName:         rawName,\n\t\tType:            funType,\n\t\tStateMutability: mutability,\n\t\tConstant:        isConst,\n\t\tPayable:         isPayable,\n\t\tInputs:          inputs,\n\t\tOutputs:         outputs,\n\t\tstr:             str,\n\t\tSig:             sig,\n\t\tID:              id,\n\t}\n}\n\nfunc (method Method) String() string {\n\treturn method.str\n}\n\n// IsConstant returns the indicator whether the method is read-only.\nfunc (method Method) IsConstant() bool {\n\treturn method.StateMutability == \"view\" || method.StateMutability == \"pure\" || method.Constant\n}\n\n// IsPayable returns the indicator whether the method can process\n// plain ether transfers.\nfunc (method Method) IsPayable() bool {\n\treturn method.StateMutability == \"payable\" || method.Payable\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/scwallet/hub.go",
          "line": 261,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/scwallet/wallet.go",
          "line": 414,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/external/backend.go",
          "line": 168,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 27 // Transform V from 27/28 to 0/1 for Clique and Parlia use\n\t}\n\treturn res, nil\n}\n\nfunc (api *ExternalSigner) SignText(account accounts.Account, text []byte) ([]byte, error) {\n\tvar signature hexutil.Bytes\n\tvar signAddress = common.NewMixedcaseAddress(account.Address)\n\tif err := api.client.Call(&signature, \"account_signData\",\n\t\taccounts.MimetypeTextPlain,\n\t\t&signAddress, // Need to use the pointer here, because of how MarshalJSON is defined\n\t\thexutil.Encode(text))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/external/backend.go",
          "line": 185,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 27 // Transform V from Ethereum-legacy to 0/1\n\t}\n\treturn signature, nil\n}\n\n// signTransactionResult represents the signinig result returned by clef.\ntype signTransactionResult struct {\n\tRaw hexutil.Bytes      `json:\"raw\"`\n\tTx  *types.Transaction `json:\"tx\"`\n}\n\n// SignTx sends the transaction to the external signer.\n// If chainID is nil, or tx.ChainID is zero, the chain ID will be assigned\n// by the external signer. For non-legacy transactions, the chain ID of the\n// transaction overrides the chainID parameter.\nfunc (api *ExternalSigner) SignTx(account accounts.Account, tx *types.Transaction, chainID *big.Int) (*types.Transaction, error) {\n\tdata := hexutil.Bytes(tx.Data())\n\tvar to *common.MixedcaseAddress\n\tif tx.To() != nil {\n\t\tt := common.NewMixedcaseAddress(*tx.To())\n\t\tto = &t\n\t}\n\targs := &apitypes.SendTxArgs{\n\t\tInput: &data,\n\t\tNonce: hexutil.Uint64(tx.Nonce()),\n\t\tValue: hexutil.Big(*tx.Value()),\n\t\tGas:   hexutil.Uint64(tx.Gas()),\n\t\tTo:    to,\n\t\tFrom:  common.NewMixedcaseAddress(account.Address),\n\t}\n\tswitch tx.Type() {\n\tcase types.LegacyTxType, types.AccessListTxType:\n\t\targs.GasPrice = (*hexutil.Big)(tx.GasPrice())\n\tcase types.DynamicFeeTxType, types.BlobTxType, types.SetCodeTxType:\n\t\targs.MaxFeePerGas = (*hexutil.Big)(tx.GasFeeCap())\n\t\targs.MaxPriorityFeePerGas = (*hexutil.Big)(tx.GasTipCap())\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unsupported tx type %d\", tx.Type())\n\t}\n\t// We should request the default chain id that we're operating with\n\t// (the chain we're executing on)\n\tif chainID != nil && chainID.Sign() != 0 {\n\t\targs.ChainID = (*hexutil.Big)(chainID)\n\t}\n\tif tx.Type() != types.LegacyTxType {\n\t\t// However, if the user asked for a particular chain id, then we should\n\t\t// use that instead.\n\t\tif tx.ChainId().Sign() != 0 {\n\t\t\targs.ChainID = (*hexutil.Big)(tx.ChainId())\n\t\t}\n\t\taccessList := tx.AccessList()\n\t\targs.AccessList = &accessList\n\t}\n\tif tx.Type() == types.BlobTxType {\n\t\targs.BlobHashes = tx.BlobHashes()\n\t\tsidecar := tx.BlobTxSidecar()\n\t\tif sidecar == nil {\n\t\t\treturn nil, errors.New(\"blobs must be present for signing\")\n\t\t}\n\t\targs.Blobs = sidecar.Blobs\n\t\targs.Commitments = sidecar.Commitments\n\t\targs.Proofs = sidecar.Proofs\n\t}\n\n\tvar res signTransactionResult\n\tif err := api.client.Call(&res, \"account_signTransaction\", args)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/accounts/external/backend.go",
          "line": 160,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/accounts/external/backend.go",
          "line": 176,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/accounts/external/backend.go",
          "line": 250,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/accounts/external/backend.go",
          "line": 269,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/accounts/external/backend.go",
          "line": 277,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/abigen/bind_test.go",
          "line": 1862,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t}\n\t\t\tif count != 1 {\n\t\t\t\tt.Fatal(\"Unexpected contract event number\")\n\t\t\t}\n\t\t\t`,\n\t\tnil,\n\t\tnil,\n\t\tnil,\n\t\tnil,\n\t},\n\t// Test errors introduced in v0.8.4\n\t{\n\t\t`NewErrors`,\n\t\t`\n\t\tpragma solidity >0.8.4",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 160,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 170,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 180,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/accounts/abi/bind/v2/base_test.go",
          "line": 570,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/bind/v2/dep_tree_test.go",
          "line": 186,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 20 {\n\t\t\t\tvar dep common.Address\n\t\t\t\tdep.SetBytes(deployer[i : i+20])\n\t\t\t\tif _, ok := overridesAddrs[dep]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/bind/v2/base.go",
          "line": 167,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/abi/bind/v2/base.go",
          "line": 184,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/accounts/abi/bind/v2/lib_test.go",
          "line": 111,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/accounts/abi/bind/v2/lib_test.go",
          "line": 179,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/trienode/proof.go",
          "line": 58,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(value)\n\n\treturn nil\n}\n\n// Delete removes a node from the set\nfunc (db *ProofSet) Delete(key []byte) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\tdelete(db.nodes, string(key))\n\treturn nil\n}\n\nfunc (db *ProofSet) DeleteRange(start, end []byte) error {\n\tpanic(\"not supported\")\n}\n\n// Get returns a stored node\nfunc (db *ProofSet) Get(key []byte) ([]byte, error) {\n\tdb.lock.RLock()\n\tdefer db.lock.RUnlock()\n\n\tif entry, ok := db.nodes[string(key)]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/trie/trienode/proof.go",
          "line": 163,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(node)\n\t}\n\treturn size\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/trie/trienode/node.go",
          "line": 96,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t} else {\n\t\tset.updates += 1\n\t}\n\tset.Nodes[string(path)] = n\n}\n\n// MergeSet merges this 'set' with 'other'. It assumes that the sets are disjoint,\n// and thus does not deduplicate data (count deletes, dedup leaves etc).\nfunc (set *NodeSet) MergeSet(other *NodeSet) error {\n\tif set.Owner != other.Owner {\n\t\treturn fmt.Errorf(\"nodesets belong to different owner are not mergeable %x-%x\", set.Owner, other.Owner)\n\t}\n\tmaps.Copy(set.Nodes, other.Nodes)\n\n\tset.deletes += other.deletes\n\tset.updates += other.updates\n\n\t// Since we assume the sets are disjoint, we can safely append leaves\n\t// like this without deduplication.\n\tset.Leaves = append(set.Leaves, other.Leaves...)\n\treturn nil\n}\n\n// Merge adds a set of nodes into the set.\nfunc (set *NodeSet) Merge(owner common.Hash, nodes map[string]*Node) error {\n\tif set.Owner != owner {\n\t\treturn fmt.Errorf(\"nodesets belong to different owner are not mergeable %x-%x\", set.Owner, owner)\n\t}\n\tfor path, node := range nodes {\n\t\tprev, ok := set.Nodes[path]\n\t\tif ok {\n\t\t\t// overwrite happens, revoke the counter\n\t\t\tif prev.IsDeleted() {\n\t\t\t\tset.deletes -= 1\n\t\t\t} else {\n\t\t\t\tset.updates -= 1\n\t\t\t}\n\t\t}\n\t\tif node.IsDeleted() {\n\t\t\tset.deletes += 1\n\t\t} else {\n\t\t\tset.updates += 1\n\t\t}\n\t\tset.Nodes[path] = node\n\t}\n\treturn nil\n}\n\n// AddLeaf adds the provided leaf node into set. TODO(rjl493456442) how can\n// we get rid of it?\nfunc (set *NodeSet) AddLeaf(parent common.Hash, blob []byte) {\n\tset.Leaves = append(set.Leaves, &leaf{Blob: blob, Parent: parent})\n}\n\n// Size returns the number of dirty nodes in set.\nfunc (set *NodeSet) Size() (int, int) {\n\treturn set.updates, set.deletes\n}\n\n// HashSet returns a set of trie nodes keyed by node hash.\nfunc (set *NodeSet) HashSet() map[common.Hash][]byte {\n\tret := make(map[common.Hash][]byte, len(set.Nodes))\n\tfor _, n := range set.Nodes {\n\t\tret[n.Hash] = n.Blob\n\t}\n\treturn ret\n}\n\n// Summary returns a string-representation of the NodeSet.\nfunc (set *NodeSet) Summary() string {\n\tvar out = new(strings.Builder)\n\tfmt.Fprintf(out, \"nodeset owner: %v\\n\", set.Owner)\n\tfor path, n := range set.Nodes {\n\t\t// Deletion\n\t\tif n.IsDeleted() {\n\t\t\tfmt.Fprintf(out, \"  [-]: %x\\n\", path)\n\t\t\tcontinue\n\t\t}\n\t\t// Insertion or update\n\t\tfmt.Fprintf(out, \"  [+/*]: %x -> %v \\n\", path, n.Hash)\n\t}\n\tfor _, n := range set.Leaves {\n\t\tfmt.Fprintf(out, \"[leaf]: %v\\n\", n)\n\t}\n\treturn out.String()\n}\n\n// MergedNodeSet represents a merged node set for a group of tries.\ntype MergedNodeSet struct {\n\tSets map[common.Hash]*NodeSet\n}\n\n// NewMergedNodeSet initializes an empty merged set.\nfunc NewMergedNodeSet() *MergedNodeSet {\n\treturn &MergedNodeSet{Sets: make(map[common.Hash]*NodeSet)}\n}\n\n// NewWithNodeSet constructs a merged nodeset with the provided single set.\nfunc NewWithNodeSet(set *NodeSet) *MergedNodeSet {\n\tmerged := NewMergedNodeSet()\n\tmerged.Merge(set)\n\treturn merged\n}\n\n// Merge merges the provided dirty nodes of a trie into the set. The assumption\n// is held that no duplicated set belonging to the same trie will be merged twice.\nfunc (set *MergedNodeSet) Merge(other *NodeSet) error {\n\tsubset, present := set.Sets[other.Owner]\n\tif present {\n\t\treturn subset.Merge(other.Owner, other.Nodes)\n\t}\n\tset.Sets[other.Owner] = other\n\treturn nil\n}\n\n// Flatten returns a two-dimensional map for internal nodes.\nfunc (set *MergedNodeSet) Flatten() map[common.Hash]map[string]*Node {\n\tnodes := make(map[common.Hash]map[string]*Node, len(set.Sets))\n\tfor owner, set := range set.Sets {\n\t\tnodes[owner] = set.Nodes\n\t}\n\treturn nodes\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/trie/trienode/node.go",
          "line": 130,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\t\t} else {\n\t\t\t\tset.updates -= 1\n\t\t\t}\n\t\t}\n\t\tif node.IsDeleted() {\n\t\t\tset.deletes += 1\n\t\t} else {\n\t\t\tset.updates += 1\n\t\t}\n\t\tset.Nodes[path] = node\n\t}\n\treturn nil\n}\n\n// AddLeaf adds the provided leaf node into set. TODO(rjl493456442) how can\n// we get rid of it?\nfunc (set *NodeSet) AddLeaf(parent common.Hash, blob []byte) {\n\tset.Leaves = append(set.Leaves, &leaf{Blob: blob, Parent: parent})\n}\n\n// Size returns the number of dirty nodes in set.\nfunc (set *NodeSet) Size() (int, int) {\n\treturn set.updates, set.deletes\n}\n\n// HashSet returns a set of trie nodes keyed by node hash.\nfunc (set *NodeSet) HashSet() map[common.Hash][]byte {\n\tret := make(map[common.Hash][]byte, len(set.Nodes))\n\tfor _, n := range set.Nodes {\n\t\tret[n.Hash] = n.Blob\n\t}\n\treturn ret\n}\n\n// Summary returns a string-representation of the NodeSet.\nfunc (set *NodeSet) Summary() string {\n\tvar out = new(strings.Builder)\n\tfmt.Fprintf(out, \"nodeset owner: %v\\n\", set.Owner)\n\tfor path, n := range set.Nodes {\n\t\t// Deletion\n\t\tif n.IsDeleted() {\n\t\t\tfmt.Fprintf(out, \"  [-]: %x\\n\", path)\n\t\t\tcontinue\n\t\t}\n\t\t// Insertion or update\n\t\tfmt.Fprintf(out, \"  [+/*]: %x -> %v \\n\", path, n.Hash)\n\t}\n\tfor _, n := range set.Leaves {\n\t\tfmt.Fprintf(out, \"[leaf]: %v\\n\", n)\n\t}\n\treturn out.String()\n}\n\n// MergedNodeSet represents a merged node set for a group of tries.\ntype MergedNodeSet struct {\n\tSets map[common.Hash]*NodeSet\n}\n\n// NewMergedNodeSet initializes an empty merged set.\nfunc NewMergedNodeSet() *MergedNodeSet {\n\treturn &MergedNodeSet{Sets: make(map[common.Hash]*NodeSet)}\n}\n\n// NewWithNodeSet constructs a merged nodeset with the provided single set.\nfunc NewWithNodeSet(set *NodeSet) *MergedNodeSet {\n\tmerged := NewMergedNodeSet()\n\tmerged.Merge(set)\n\treturn merged\n}\n\n// Merge merges the provided dirty nodes of a trie into the set. The assumption\n// is held that no duplicated set belonging to the same trie will be merged twice.\nfunc (set *MergedNodeSet) Merge(other *NodeSet) error {\n\tsubset, present := set.Sets[other.Owner]\n\tif present {\n\t\treturn subset.Merge(other.Owner, other.Nodes)\n\t}\n\tset.Sets[other.Owner] = other\n\treturn nil\n}\n\n// Flatten returns a two-dimensional map for internal nodes.\nfunc (set *MergedNodeSet) Flatten() map[common.Hash]map[string]*Node {\n\tnodes := make(map[common.Hash]map[string]*Node, len(set.Sets))\n\tfor owner, set := range set.Sets {\n\t\tnodes[owner] = set.Nodes\n\t}\n\treturn nodes\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/lru/blob_lru_test.go",
          "line": 40,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(v))\n\t\tif want > 100 {\n\t\t\twant = 100\n\t\t}\n\t\tif have := lru.size",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/lru/blob_lru.go",
          "line": 70,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(len(v))\n\t\t}\n\t\tc.size = targetSize\n\t}\n\tc.lru.Add(key, value)\n\treturn evicted\n}\n\n// Get looks up a key's value from the cache.\nfunc (c *SizeConstrainedCache[K, V]) Get(key K) (V, bool) {\n\tc.lock.Lock()\n\tdefer c.lock.Unlock()\n\n\treturn c.lru.Get(key)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/hexutil/hexutil.go",
          "line": 157,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tdec := new(big.Int).SetBits(words)\n\treturn dec, nil\n}\n\n// MustDecodeBig decodes a hex string with 0x prefix as a quantity.\n// It panics for invalid input.\nfunc MustDecodeBig(input string) *big.Int {\n\tdec, err := DecodeBig(input)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn dec\n}\n\n// EncodeBig encodes bigint as a hex string with 0x prefix.\nfunc EncodeBig(bigint *big.Int) string {\n\tif sign := bigint.Sign()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/common/hexutil/hexutil.go",
          "line": 156,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 16\n\t\t\twords[i] += big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tdec := new(big.Int).SetBits(words)\n\treturn dec, nil\n}\n\n// MustDecodeBig decodes a hex string with 0x prefix as a quantity.\n// It panics for invalid input.\nfunc MustDecodeBig(input string) *big.Int {\n\tdec, err := DecodeBig(input)\n\tif err != nil {\n\t\tpanic(err)\n\t}\n\treturn dec\n}\n\n// EncodeBig encodes bigint as a hex string with 0x prefix.\nfunc EncodeBig(bigint *big.Int) string {\n\tif sign := bigint.Sign()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/hexutil/json.go",
          "line": 189,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tvar dec big.Int\n\tdec.SetBits(words)\n\t*b = (Big)(dec)\n\treturn nil\n}\n\n// ToInt converts b to a big.Int.\nfunc (b *Big) ToInt() *big.Int {\n\treturn (*big.Int)(b)\n}\n\n// String returns the hex encoding of b.\nfunc (b *Big) String() string {\n\treturn EncodeBig(b.ToInt())\n}\n\n// ImplementsGraphQLType returns true if Big implements the provided GraphQL type.\nfunc (b Big) ImplementsGraphQLType(name string) bool { return name == \"BigInt\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Big) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\tvar num big.Int\n\t\tnum.SetInt64(int64(input))\n\t\t*b = Big(num)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for BigInt\", input)\n\t}\n\treturn err\n}\n\n// U256 marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype U256 uint256.Int\n\n// MarshalText implements encoding.TextMarshaler\nfunc (b U256) MarshalText() ([]byte, error) {\n\tu256 := (*uint256.Int)(&b)\n\treturn []byte(u256.Hex()), nil\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *U256) UnmarshalJSON(input []byte) error {\n\t// The uint256.Int.UnmarshalJSON method accepts \"dec\", \"0xhex\"",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/common/hexutil/json.go",
          "line": 306,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nib\n\t}\n\t*b = Uint64(dec)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint64) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\n// ImplementsGraphQLType returns true if Uint64 implements the provided GraphQL type.\nfunc (b Uint64) ImplementsGraphQLType(name string) bool { return name == \"Long\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Uint64) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\t*b = Uint64(input)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for Long\", input)\n\t}\n\treturn err\n}\n\n// Uint marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype Uint uint\n\n// MarshalText implements encoding.TextMarshaler.\nfunc (b Uint) MarshalText() ([]byte, error) {\n\treturn Uint64(b).MarshalText()\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *Uint) UnmarshalJSON(input []byte) error {\n\tif !isString(input) {\n\t\treturn errNonString(uintT)\n\t}\n\treturn wrapTypeError(b.UnmarshalText(input[1:len(input)-1]), uintT)\n}\n\n// UnmarshalText implements encoding.TextUnmarshaler.\nfunc (b *Uint) UnmarshalText(input []byte) error {\n\tvar u64 Uint64\n\terr := u64.UnmarshalText(input)\n\tif u64 > Uint64(^uint(0)) || err == ErrUint64Range {\n\t\treturn ErrUintRange\n\t} else if err != nil {\n\t\treturn err\n\t}\n\t*b = Uint(u64)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\nfunc isString(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '\"' && input[len(input)-1] == '\"'\n}\n\nfunc bytesHave0xPrefix(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '0' && (input[1] == 'x' || input[1] == 'X')\n}\n\nfunc checkText(input []byte, wantPrefix bool) ([]byte, error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif bytesHave0xPrefix(input) {\n\t\tinput = input[2:]\n\t} else if wantPrefix {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tif len(input)%2 != 0 {\n\t\treturn nil, ErrOddLength\n\t}\n\treturn input, nil\n}\n\nfunc checkNumberText(input []byte) (raw []byte, err error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif !bytesHave0xPrefix(input) {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tinput = input[2:]\n\tif len(input) == 0 {\n\t\treturn nil, ErrEmptyNumber\n\t}\n\tif len(input) > 1 && input[0] == '0' {\n\t\treturn nil, ErrLeadingZero\n\t}\n\treturn input, nil\n}\n\nfunc wrapTypeError(err error, typ reflect.Type) error {\n\tif _, ok := err.(*decError)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/common/hexutil/json.go",
          "line": 188,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 16\n\t\t\twords[i] += big.Word(nib)\n\t\t}\n\t\tend = start\n\t}\n\tvar dec big.Int\n\tdec.SetBits(words)\n\t*b = (Big)(dec)\n\treturn nil\n}\n\n// ToInt converts b to a big.Int.\nfunc (b *Big) ToInt() *big.Int {\n\treturn (*big.Int)(b)\n}\n\n// String returns the hex encoding of b.\nfunc (b *Big) String() string {\n\treturn EncodeBig(b.ToInt())\n}\n\n// ImplementsGraphQLType returns true if Big implements the provided GraphQL type.\nfunc (b Big) ImplementsGraphQLType(name string) bool { return name == \"BigInt\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Big) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\tvar num big.Int\n\t\tnum.SetInt64(int64(input))\n\t\t*b = Big(num)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for BigInt\", input)\n\t}\n\treturn err\n}\n\n// U256 marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype U256 uint256.Int\n\n// MarshalText implements encoding.TextMarshaler\nfunc (b U256) MarshalText() ([]byte, error) {\n\tu256 := (*uint256.Int)(&b)\n\treturn []byte(u256.Hex()), nil\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *U256) UnmarshalJSON(input []byte) error {\n\t// The uint256.Int.UnmarshalJSON method accepts \"dec\", \"0xhex\"",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/common/hexutil/json.go",
          "line": 305,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= 16\n\t\tdec += nib\n\t}\n\t*b = Uint64(dec)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint64) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\n// ImplementsGraphQLType returns true if Uint64 implements the provided GraphQL type.\nfunc (b Uint64) ImplementsGraphQLType(name string) bool { return name == \"Long\" }\n\n// UnmarshalGraphQL unmarshals the provided GraphQL query data.\nfunc (b *Uint64) UnmarshalGraphQL(input interface{}) error {\n\tvar err error\n\tswitch input := input.(type) {\n\tcase string:\n\t\treturn b.UnmarshalText([]byte(input))\n\tcase int32:\n\t\t*b = Uint64(input)\n\tdefault:\n\t\terr = fmt.Errorf(\"unexpected type %T for Long\", input)\n\t}\n\treturn err\n}\n\n// Uint marshals/unmarshals as a JSON string with 0x prefix.\n// The zero value marshals as \"0x0\".\ntype Uint uint\n\n// MarshalText implements encoding.TextMarshaler.\nfunc (b Uint) MarshalText() ([]byte, error) {\n\treturn Uint64(b).MarshalText()\n}\n\n// UnmarshalJSON implements json.Unmarshaler.\nfunc (b *Uint) UnmarshalJSON(input []byte) error {\n\tif !isString(input) {\n\t\treturn errNonString(uintT)\n\t}\n\treturn wrapTypeError(b.UnmarshalText(input[1:len(input)-1]), uintT)\n}\n\n// UnmarshalText implements encoding.TextUnmarshaler.\nfunc (b *Uint) UnmarshalText(input []byte) error {\n\tvar u64 Uint64\n\terr := u64.UnmarshalText(input)\n\tif u64 > Uint64(^uint(0)) || err == ErrUint64Range {\n\t\treturn ErrUintRange\n\t} else if err != nil {\n\t\treturn err\n\t}\n\t*b = Uint(u64)\n\treturn nil\n}\n\n// String returns the hex encoding of b.\nfunc (b Uint) String() string {\n\treturn EncodeUint64(uint64(b))\n}\n\nfunc isString(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '\"' && input[len(input)-1] == '\"'\n}\n\nfunc bytesHave0xPrefix(input []byte) bool {\n\treturn len(input) >= 2 && input[0] == '0' && (input[1] == 'x' || input[1] == 'X')\n}\n\nfunc checkText(input []byte, wantPrefix bool) ([]byte, error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif bytesHave0xPrefix(input) {\n\t\tinput = input[2:]\n\t} else if wantPrefix {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tif len(input)%2 != 0 {\n\t\treturn nil, ErrOddLength\n\t}\n\treturn input, nil\n}\n\nfunc checkNumberText(input []byte) (raw []byte, err error) {\n\tif len(input) == 0 {\n\t\treturn nil, nil // empty strings are allowed\n\t}\n\tif !bytesHave0xPrefix(input) {\n\t\treturn nil, ErrMissingPrefix\n\t}\n\tinput = input[2:]\n\tif len(input) == 0 {\n\t\treturn nil, ErrEmptyNumber\n\t}\n\tif len(input) > 1 && input[0] == '0' {\n\t\treturn nil, ErrLeadingZero\n\t}\n\treturn input, nil\n}\n\nfunc wrapTypeError(err error, typ reflect.Type) error {\n\tif _, ok := err.(*decError)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/prque/lazyqueue_test.go",
          "line": 100,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rand.Int63n(testPriorityStep*2-1) + 1\n\t\tif items[i].p > maxPri {\n\t\t\tmaxPri = items[i].p\n\t\t}\n\t\titems[i].last = clock.Now()\n\t\tif items[i].p > items[i].maxp {\n\t\t\tq.Update(items[i].index)\n\t\t}\n\t\tif rand.Intn(100) == 0 {\n\t\t\tp := q.PopItem().(*lazyItem)\n\t\t\tif p.p != maxPri {\n\t\t\t\tlock.Unlock()\n\t\t\t\tclose(stopCh)\n\t\t\t\tt.Fatalf(\"incorrect item (best known priority %d, popped %d)\", maxPri, p.p)\n\t\t\t}\n\t\t\tq.Push(p)\n\t\t}\n\t\tlock.Unlock()\n\t\tclock.Run(testStepPeriod)\n\t\tclock.WaitForTimers(1)\n\t}\n\n\tclose(stopCh)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/common/prque/sstack.go",
          "line": 58,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= blockSize\n\t\ts.offset = 0\n\t} else if s.offset == blockSize {\n\t\ts.active = s.blocks[s.size/blockSize]\n\t\ts.offset = 0\n\t}\n\tif s.setIndex != nil {\n\t\ts.setIndex(data.(*item[P, V]).value, s.size)\n\t}\n\ts.active[s.offset] = data.(*item[P, V])\n\ts.offset++\n\ts.size++\n}\n\n// Pop a value off the stack and returns it. Currently no shrinking is done.\n// Required by heap.Interface.\nfunc (s *sstack[P, V]) Pop() (res any) {\n\ts.size--\n\ts.offset--\n\tif s.offset < 0 {\n\t\ts.offset = blockSize - 1\n\t\ts.active = s.blocks[s.size/blockSize]\n\t}\n\tres, s.active[s.offset] = s.active[s.offset], nil\n\tif s.setIndex != nil {\n\t\ts.setIndex(res.(*item[P, V]).value, -1)\n\t}\n\treturn\n}\n\n// Len returns the length of the stack. Required by sort.Interface.\nfunc (s *sstack[P, V]) Len() int {\n\treturn s.size\n}\n\n// Less compares the priority of two elements of the stack (higher is first).\n// Required by sort.Interface.\nfunc (s *sstack[P, V]) Less(i, j int) bool {\n\treturn s.blocks[i/blockSize][i%blockSize].priority > s.blocks[j/blockSize][j%blockSize].priority\n}\n\n// Swap two elements in the stack. Required by sort.Interface.\nfunc (s *sstack[P, V]) Swap(i, j int) {\n\tib, io, jb, jo := i/blockSize, i%blockSize, j/blockSize, j%blockSize\n\ta, b := s.blocks[jb][jo], s.blocks[ib][io]\n\tif s.setIndex != nil {\n\t\ts.setIndex(a.value, i)\n\t\ts.setIndex(b.value, j)\n\t}\n\ts.blocks[ib][io], s.blocks[jb][jo] = a, b\n}\n\n// Reset the stack, effectively clearing its contents.\nfunc (s *sstack[P, V]) Reset() {\n\t*s = *newSstack[P, V](s.setIndex)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/rlp/rlpgen/gen.go",
          "line": 576,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" || \"\n\t\t\t}\n\t\t\tcond += zeroV[j]\n\t\t}\n\t\tfmt.Fprintf(b, \"if %s {\\n\", cond)\n\t\tfmt.Fprint(b, field.elem.genWrite(ctx, selector))\n\t\tfmt.Fprintf(b, \"}\\n\")\n\t}\n}\n\nfunc (op structOp) genDecode(ctx *genContext) (string, string) {\n\t// Get the string representation of the type.\n\t// Here, named types are handled separately because the output\n\t// would contain a copy of the struct definition otherwise.\n\tvar typeName string\n\tif op.named != nil {\n\t\ttypeName = types.TypeString(op.named, ctx.qualify)\n\t} else {\n\t\ttypeName = types.TypeString(op.typ, ctx.qualify)\n\t}\n\n\t// Create struct object.\n\tvar resultV = ctx.temp()\n\tvar b bytes.Buffer\n\tfmt.Fprintf(&b, \"var %s %s\\n\", resultV, typeName)\n\n\t// Decode fields.\n\tfmt.Fprintf(&b, \"{\\n\")\n\tfmt.Fprintf(&b, \"if _, err := dec.List()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/flags/helpers.go",
          "line": 207,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= wlen\n\n\t\tif sp == -1 {\n\t\t\tbreak\n\t\t}\n\t\ts = s[wlen+1:]\n\t}\n\n\treturn output.String()\n}\n\n// AutoEnvVars extends all the specific CLI flags with automatically generated\n// env vars by capitalizing the flag, replacing . with _ and prefixing it with\n// the specified string.\n//\n// Note, the prefix should *not* contain the separator underscore, that will be\n// added automatically.\nfunc AutoEnvVars(flags []cli.Flag, prefix string) {\n\tfor _, flag := range flags {\n\t\tenvvar := strings.ToUpper(prefix + \"_\" + strings.ReplaceAll(strings.ReplaceAll(flag.Names()[0], \".\", \"_\"), \"-\", \"_\"))\n\n\t\tswitch flag := flag.(type) {\n\t\tcase *cli.StringFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.StringSliceFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.BoolFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.IntFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.Int64Flag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.Uint64Flag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.Float64Flag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.DurationFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *cli.PathFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *BigFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\n\t\tcase *DirectoryFlag:\n\t\t\tflag.EnvVars = append(flag.EnvVars, envvar)\n\t\t}\n\t}\n}\n\n// CheckEnvVars iterates over all the environment variables and checks if any of\n// them look like a CLI flag but is not consumed. This can be used to detect old\n// or mistyped names.\nfunc CheckEnvVars(ctx *cli.Context, flags []cli.Flag, prefix string) {\n\tknown := make(map[string]string)\n\tfor _, flag := range flags {\n\t\tdocflag, ok := flag.(cli.DocGenerationFlag)\n\t\tif !ok {\n\t\t\tcontinue\n\t\t}\n\t\tfor _, envvar := range docflag.GetEnvVars() {\n\t\t\tknown[envvar] = flag.Names()[0]\n\t\t}\n\t}\n\tkeyvals := os.Environ()\n\tsort.Strings(keyvals)\n\n\tfor _, keyval := range keyvals {\n\t\tkey := strings.Split(keyval, \"=\")[0]\n\t\tif !strings.HasPrefix(key, prefix) {\n\t\t\tcontinue\n\t\t}\n\t\tif flag, ok := known[key]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/flags/helpers.go",
          "line": 318,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"=\" + option\n\t\t\t\t\tset = append(set, \"--\"+name)\n\t\t\t\t}\n\t\t\t\t// shift arguments and continue\n\t\t\t\ti++\n\t\t\t\tcontinue\n\n\t\t\tcase cli.Flag:\n\t\t\tdefault:\n\t\t\t\tpanic(fmt.Sprintf(\"invalid argument, not cli.Flag or string extension: %T\", args[i+1]))\n\t\t\t}\n\t\t}\n\t\t// Mark the flag if it's set\n\t\tif ctx.IsSet(flag.Names()[0]) {\n\t\t\tset = append(set, \"--\"+name)\n\t\t}\n\t}\n\tif len(set) > 1 {\n\t\tfmt.Fprintf(os.Stderr, \"Flags %v can't be used at the same time\", strings.Join(set, \", \"))\n\t\tos.Exit(1)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/utesting/utesting.go",
          "line": 205,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= nn\n\t\t\treturn n, err\n\t\t}\n\n\t\tline := b[:end+1]\n\t\tnn, err := w.out.Write(line)\n\t\tn += nn\n\t\tif err != nil {\n\t\t\treturn n, err\n\t\t}\n\t\tb = b[end+1:]\n\t\tw.inLine = false\n\t}\n\treturn n, err\n}\n\n// flush ensures the current line is terminated.\nfunc (w *indentWriter) flush() {\n\tif w.inLine {\n\t\tfmt.Println(w.out)\n\t\tw.inLine = false\n\t}\n}\n\n// CountFailures returns the number of failed tests in the result slice.\nfunc CountFailures(rr []Result) int {\n\tcount := 0\n\tfor _, r := range rr {\n\t\tif r.Failed {\n\t\t\tcount++\n\t\t}\n\t}\n\treturn count\n}\n\n// Run executes a single test.\nfunc Run(test Test) (bool, string) {\n\toutput := new(bytes.Buffer)\n\tfailed := runTest(test, output)\n\treturn failed, output.String()\n}\n\nfunc runTest(test Test, output io.Writer) bool {\n\tt := &T{output: output}\n\tdone := make(chan struct{})\n\tgo func() {\n\t\tdefer close(done)\n\t\tdefer func() {\n\t\t\tif err := recover()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/utesting/utesting.go",
          "line": 312,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"\\n\"\n\t}\n\tfmt.Fprintf(t.output, format, vs...)\n}\n\n// Error is equivalent to Log followed by Fail.\nfunc (t *T) Error(vs ...interface{}) {\n\tt.Log(vs...)\n\tt.Fail()\n}\n\n// Errorf is equivalent to Logf followed by Fail.\nfunc (t *T) Errorf(format string, vs ...interface{}) {\n\tt.Logf(format, vs...)\n\tt.Fail()\n}\n\n// Fatal is equivalent to Log followed by FailNow.\nfunc (t *T) Fatal(vs ...interface{}) {\n\tt.Log(vs...)\n\tt.FailNow()\n}\n\n// Fatalf is equivalent to Logf followed by FailNow.\nfunc (t *T) Fatalf(format string, vs ...interface{}) {\n\tt.Logf(format, vs...)\n\tt.FailNow()\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/era/iterator.go",
          "line": 158,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif it.Body, n, it.err = newSnappyReader(it.e.s, TypeCompressedBody, off)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/era/iterator.go",
          "line": 163,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif it.Receipts, n, it.err = newSnappyReader(it.e.s, TypeCompressedReceipts, off)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/internal/era/iterator.go",
          "line": 168,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif it.TotalDifficulty, _, it.err = it.e.s.ReaderAt(TypeTotalDifficulty, off)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/internal/era/iterator.go",
          "line": 173,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\treturn true\n}\n\n// Number returns the current number block the iterator will return.\nfunc (it *RawIterator) Number() uint64 {\n\treturn it.next - 1\n}\n\n// Error returns the error status of the iterator. It should be called before\n// reading from any of the iterator's values.\nfunc (it *RawIterator) Error() error {\n\tif it.err == io.EOF {\n\t\treturn nil\n\t}\n\treturn it.err\n}\n\n// clear sets all the outputs to nil.\nfunc (it *RawIterator) clear() {\n\tit.Header = nil\n\tit.Body = nil\n\tit.Receipts = nil\n\tit.TotalDifficulty = nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/era/era.go",
          "line": 80,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\teras = append(eras, entry.Name())\n\t}\n\treturn eras, nil\n}\n\ntype ReadAtSeekCloser interface {\n\tio.ReaderAt\n\tio.Seeker\n\tio.Closer\n}\n\n// Era reads and Era1 file.\ntype Era struct {\n\tf   ReadAtSeekCloser // backing era1 file\n\ts   *e2store.Reader  // e2store reader over f\n\tm   metadata         // start, count, length info\n\tmu  *sync.Mutex      // lock for buf\n\tbuf [8]byte          // buffer reading entry offsets\n}\n\n// From returns an Era backed by f.\nfunc From(f ReadAtSeekCloser) (*Era, error) {\n\tm, err := readMetadata(f)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &Era{\n\t\tf:  f,\n\t\ts:  e2store.NewReader(f),\n\t\tm:  m,\n\t\tmu: new(sync.Mutex),\n\t}, nil\n}\n\n// Open returns an Era backed by the given filename.\nfunc Open(filename string) (*Era, error) {\n\tf, err := os.Open(filename)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn From(f)\n}\n\nfunc (e *Era) Close() error {\n\treturn e.f.Close()\n}\n\n// GetBlockByNumber returns the block for the given block number.\nfunc (e *Era) GetBlockByNumber(num uint64) (*types.Block, error) {\n\tif e.m.start > num || e.m.start+e.m.count <= num {\n\t\treturn nil, fmt.Errorf(\"out-of-bounds: %d not in [%d, %d)\", num, e.m.start, e.m.start+e.m.count)\n\t}\n\toff, err := e.readOffset(num)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tr, n, err := newSnappyReader(e.s, TypeCompressedHeader, off)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar header types.Header\n\tif err := rlp.Decode(r, &header)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/era/era.go",
          "line": 145,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tr, _, err = newSnappyReader(e.s, TypeCompressedBody, off)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar body types.Body\n\tif err := rlp.Decode(r, &body)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/internal/era/era.go",
          "line": 231,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\n\t// Skip over header and body.\n\toff, err = e.s.SkipN(off, 2)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\t// Read total difficulty after first block.\n\tif r, _, err = e.s.ReaderAt(TypeTotalDifficulty, off)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/era/builder.go",
          "line": 127,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\t}\n\tif len(b.indexes) >= MaxEra1Size {\n\t\treturn fmt.Errorf(\"exceeds maximum batch size of %d\", MaxEra1Size)\n\t}\n\n\tb.indexes = append(b.indexes, uint64(b.written))\n\tb.hashes = append(b.hashes, hash)\n\tb.tds = append(b.tds, td)\n\n\t// Write block data.\n\tif err := b.snappyWrite(TypeCompressedHeader, header)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/era/builder.go",
          "line": 151,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif err != nil {\n\t\treturn err\n\t}\n\n\treturn nil\n}\n\n// Finalize computes the accumulator and block index values, then writes the\n// corresponding e2store entries.\nfunc (b *Builder) Finalize() (common.Hash, error) {\n\tif b.startNum == nil {\n\t\treturn common.Hash{}, errors.New(\"finalize called on empty builder\")\n\t}\n\t// Compute accumulator root and write entry.\n\troot, err := ComputeAccumulator(b.hashes, b.tds)\n\tif err != nil {\n\t\treturn common.Hash{}, fmt.Errorf(\"error calculating accumulator root: %w\", err)\n\t}\n\tn, err := b.w.Write(TypeAccumulator, root[:])\n\tb.written += n\n\tif err != nil {\n\t\treturn common.Hash{}, fmt.Errorf(\"error writing accumulator: %w\", err)\n\t}\n\t// Get beginning of index entry to calculate block relative offset.\n\tbase := int64(b.written)\n\n\t// Construct block index. Detailed format described in Builder\n\t// documentation, but it is essentially encoded as:\n\t// \"start | index | index | ... | count\"\n\tvar (\n\t\tcount = len(b.indexes)\n\t\tindex = make([]byte, 16+count*8)\n\t)\n\tbinary.LittleEndian.PutUint64(index, *b.startNum)\n\t// Each offset is relative from the position it is encoded in the\n\t// index. This means that even if the same block was to be included in\n\t// the index twice (this would be invalid anyways), the relative offset\n\t// would be different. The idea with this is that after reading a\n\t// relative offset, the corresponding block can be quickly read by\n\t// performing a seek relative to the current position.\n\tfor i, offset := range b.indexes {\n\t\trelative := int64(offset) - base\n\t\tbinary.LittleEndian.PutUint64(index[8+i*8:], uint64(relative))\n\t}\n\tbinary.LittleEndian.PutUint64(index[8+count*8:], uint64(count))\n\n\t// Finally, write the block index entry.\n\tif _, err := b.w.Write(TypeBlockIndex, index)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/internal/era/builder.go",
          "line": 221,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\tif err != nil {\n\t\treturn fmt.Errorf(\"error writing e2store entry: %w\", err)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/version/version.go",
          "line": 41,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"-\" + version.Meta\n\t}\n\treturn v\n}()\n\nfunc WithCommit(gitCommit, gitDate string) string {\n\tvsn := WithMeta\n\tif len(gitCommit) >= 8 {\n\t\tvsn += \"-\" + gitCommit[:8]\n\t}\n\tif (version.Meta != \"stable\") && (gitDate != \"\") {\n\t\tvsn += \"-\" + gitDate\n\t}\n\treturn vsn\n}\n\n// Archive holds the textual version string used for Geth archives. e.g.\n// \"1.8.11-dea1ce05\" for stable releases, or \"1.8.13-unstable-21c059b6\" for unstable\n// releases.\nfunc Archive(gitCommit string) string {\n\tvsn := Semantic\n\tif version.Meta != \"stable\" {\n\t\tvsn += \"-\" + version.Meta\n\t}\n\tif len(gitCommit) >= 8 {\n\t\tvsn += \"-\" + gitCommit[:8]\n\t}\n\treturn vsn\n}\n\n// ClientName creates a software name/version identifier according to common\n// conventions in the Ethereum p2p network.\nfunc ClientName(clientIdentifier string) string {\n\tgit, _ := VCS()\n\treturn fmt.Sprintf(\"%s/v%v/%v-%v/%v\",\n\t\tstrings.Title(clientIdentifier),\n\t\tWithCommit(git.Commit, git.Date),\n\t\truntime.GOOS, runtime.GOARCH,\n\t\truntime.Version(),\n\t)\n}\n\n// Info returns build and platform information about the current binary.\n//\n// If the package that is currently executing is a prefixed by our go-ethereum\n// module path, it will print out commit and date VCS information. Otherwise,\n// it will assume it's imported by a third-party and will return the imported\n// version and whether it was replaced by another module.\nfunc Info() (version, vcs string) {\n\tversion = WithMeta\n\tbuildInfo, ok := debug.ReadBuildInfo()\n\tif !ok {\n\t\treturn version, \"\"\n\t}\n\tversion = versionInfo(buildInfo)\n\tif status, ok := VCS()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/version/version.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"%s@%s\", mod.Path, mod.Version)\n\tif mod.Replace != nil {\n\t\t// If our package was replaced by something else, also note that.\n\t\tversion += fmt.Sprintf(\" (replaced by %s@%s)\", mod.Replace.Path, mod.Replace.Version)\n\t}\n\treturn version\n}\n\n// findModule returns the module at path.\nfunc findModule(info *debug.BuildInfo, path string) *debug.Module {\n\tif info.Path == ourPath {\n\t\treturn &info.Main\n\t}\n\tfor _, mod := range info.Deps {\n\t\tif mod.Path == path {\n\t\t\treturn mod\n\t\t}\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/download/download.go",
          "line": 276,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(n)\n\tpct := w.written * 10 / w.size * 10\n\tif pct != w.lastpct {\n\t\tif w.lastpct != 0 {\n\t\t\tfmt.Print(\"...\")\n\t\t}\n\t\tfmt.Print(pct, \"%\")\n\t\tw.lastpct = pct\n\t}\n\treturn n, err\n}\n\nfunc (w *downloadWriter) Close() error {\n\tif w.lastpct > 0 {\n\t\tfmt.Println() // Finish the progress line.\n\t}\n\tflushErr := w.dstBuf.Flush()\n\tcloseErr := w.file.Close()\n\tif flushErr != nil {\n\t\treturn flushErr\n\t}\n\treturn closeErr\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/jsre/completion.go",
          "line": 84,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"(\"\n\t\t\t} else {\n\t\t\t\tresults[0] += \".\"\n\t\t\t}\n\t\t}\n\t}\n\n\tsort.Strings(results)\n\treturn results\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/cmdtest/test_cmd.go",
          "line": 174,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tsubmatch := string(output[matches[i]:matches[i+1]])\n\t\tsubmatches = append(submatches, submatch)\n\t}\n\treturn re, submatches\n}\n\n// ExpectExit expects the child process to exit within 5s without\n// printing any additional text on stdout.\nfunc (tt *TestCmd) ExpectExit() {\n\tvar output []byte\n\ttt.withKillTimeout(func() {\n\t\toutput, _ = io.ReadAll(tt.stdout)\n\t})\n\ttt.WaitExit()\n\tif tt.Cleanup != nil {\n\t\ttt.Cleanup()\n\t}\n\tif len(output) > 0 {\n\t\ttt.Errorf(\"Unmatched stdout text:\\n%s\", output)\n\t}\n}\n\nfunc (tt *TestCmd) WaitExit() {\n\ttt.Err = tt.cmd.Wait()\n}\n\nfunc (tt *TestCmd) Interrupt() {\n\ttt.Err = tt.cmd.Process.Signal(os.Interrupt)\n}\n\n// ExitStatus exposes the process' OS exit code\n// It will only return a valid value after the process has finished.\nfunc (tt *TestCmd) ExitStatus() int {\n\tif tt.Err != nil {\n\t\texitErr := tt.Err.(*exec.ExitError)\n\t\tif exitErr != nil {\n\t\t\tif status, ok := exitErr.Sys().(syscall.WaitStatus)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/ethapi/simulate.go",
          "line": 327,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= result.UsedGas\n\t\treceipts[i] = core.MakeReceipt(evm, result, sim.state, blockContext.BlockNumber, common.Hash{}, blockContext.Time, tx, gasUsed, root)\n\t\tblobGasUsed += receipts[i].BlobGasUsed\n\t\tlogs := tracer.Logs()\n\t\tcallRes := simCallResult{ReturnValue: result.Return(), Logs: logs, GasUsed: hexutil.Uint64(result.UsedGas)}\n\t\tif result.Failed() {\n\t\t\tcallRes.Status = hexutil.Uint64(types.ReceiptStatusFailed)\n\t\t\tif errors.Is(result.Err, vm.ErrExecutionReverted) {\n\t\t\t\t// If the result contains a revert reason, try to unpack it.\n\t\t\t\trevertErr := newRevertError(result.Revert())\n\t\t\t\tcallRes.Error = &callError{Message: revertErr.Error(), Code: errCodeReverted, Data: revertErr.ErrorData().(string)}\n\t\t\t} else {\n\t\t\t\tcallRes.Error = &callError{Message: result.Err.Error(), Code: errCodeVMError}\n\t\t\t}\n\t\t} else {\n\t\t\tcallRes.Status = hexutil.Uint64(types.ReceiptStatusSuccessful)\n\t\t\tallLogs = append(allLogs, callRes.Logs...)\n\t\t}\n\t\tcallResults[i] = callRes\n\t}\n\theader.GasUsed = gasUsed\n\tif sim.chainConfig.IsCancun(header.Number, header.Time) {\n\t\theader.BlobGasUsed = &blobGasUsed\n\t}\n\tvar requests [][]byte\n\t// Process EIP-7685 requests\n\tif sim.chainConfig.IsPrague(header.Number, header.Time) {\n\t\trequests = [][]byte{}\n\t\t// EIP-6110\n\t\tif err := core.ParseDepositLogs(&requests, allLogs, sim.chainConfig)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/ethapi/simulate.go",
          "line": 292,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(header.ParentHash, evm)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/ethapi/transaction_opts_utils.go",
          "line": 29,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t} else if account.StorageSlots != nil {\n\t\t\tcounter += len(account.StorageSlots)\n\t\t}\n\t}\n\tif counter > MaxNumberOfEntries {\n\t\treturn errors.New(\"knownAccounts too large\")\n\t}\n\treturn TxOptsCheckStorage(o, statedb)\n}\n\nfunc TxOptsCheckStorage(o types.TransactionOpts, statedb *state.StateDB) error {\n\tfor address, accountStorage := range o.KnownAccounts {\n\t\tif accountStorage.StorageRoot != nil {\n\t\t\trootHash := statedb.GetStorageRoot(address)\n\t\t\tif rootHash != *accountStorage.StorageRoot {\n\t\t\t\treturn errors.New(\"storage root hash condition not met\")\n\t\t\t}\n\t\t} else if len(accountStorage.StorageSlots) > 0 {\n\t\t\tfor slot, value := range accountStorage.StorageSlots {\n\t\t\t\tstored := statedb.GetState(address, slot)\n\t\t\t\tif !bytes.Equal(stored.Bytes(), value.Bytes()) {\n\t\t\t\t\treturn errors.New(\"storage slot value condition not met\")\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/ethapi/api.go",
          "line": 2098,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 27 // Transform V from 0/1 to 27/28 according to the yellow paper\n\t}\n\treturn signature, err\n}\n\n// SignTransactionResult represents a RLP encoded signed transaction.\ntype SignTransactionResult struct {\n\tRaw hexutil.Bytes      `json:\"raw\"`\n\tTx  *types.Transaction `json:\"tx\"`\n}\n\n// SignTransaction will sign the given transaction with the from account.\n// The node needs to have the private key of the account corresponding with\n// the given from address and it needs to be unlocked.\nfunc (api *TransactionAPI) SignTransaction(ctx context.Context, args TransactionArgs) (*SignTransactionResult, error) {\n\targs.blobSidecarAllowed = true\n\n\tif args.Gas == nil {\n\t\treturn nil, errors.New(\"gas not specified\")\n\t}\n\tif args.GasPrice == nil && (args.MaxPriorityFeePerGas == nil || args.MaxFeePerGas == nil) {\n\t\treturn nil, errors.New(\"missing gasPrice or maxFeePerGas/maxPriorityFeePerGas\")\n\t}\n\tif args.Nonce == nil {\n\t\treturn nil, errors.New(\"nonce not specified\")\n\t}\n\tif err := args.setDefaults(ctx, api.b, false)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/ethapi/api.go",
          "line": 661,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(ctx context.Context, blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/ethapi/api_test.go",
          "line": 1270,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/build/gotool.go",
          "line": 124,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \".zip\"\n\t} else {\n\t\tfile += \".tar.gz\"\n\t}\n\turl := \"https://golang.org/dl/\" + file\n\tdst := filepath.Join(ucache, file)\n\tif err := csdb.DownloadFile(url, dst)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/debug/flags.go",
          "line": 49,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*=5,p2p=4)\",\n\t\tValue:    \"\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tvmoduleFlag = &cli.StringFlag{\n\t\tName:     \"vmodule\",\n\t\tUsage:    \"Per-module verbosity: comma-separated list of <pattern>=<level> (e.g. eth/*=5,p2p=4)\",\n\t\tValue:    \"\",\n\t\tHidden:   true,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogjsonFlag = &cli.BoolFlag{\n\t\tName:     \"log.json\",\n\t\tUsage:    \"Format logs with JSON\",\n\t\tHidden:   true,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogFormatFlag = &cli.StringFlag{\n\t\tName:     \"log.format\",\n\t\tUsage:    \"Log format to use (json|logfmt|terminal)\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogFileFlag = &cli.StringFlag{\n\t\tName:     \"log.file\",\n\t\tUsage:    \"Write logs to a file\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogRotateFlag = &cli.BoolFlag{\n\t\tName:     \"log.rotate\",\n\t\tUsage:    \"Enables log file rotation\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogMaxSizeMBsFlag = &cli.IntFlag{\n\t\tName:     \"log.maxsize\",\n\t\tUsage:    \"Maximum size in MBs of a single log file\",\n\t\tValue:    100,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogMaxBackupsFlag = &cli.IntFlag{\n\t\tName:     \"log.maxbackups\",\n\t\tUsage:    \"Maximum number of log files to retain\",\n\t\tValue:    10,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogMaxAgeFlag = &cli.IntFlag{\n\t\tName:     \"log.maxage\",\n\t\tUsage:    \"Maximum number of days to retain a log file\",\n\t\tValue:    30,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tlogCompressFlag = &cli.BoolFlag{\n\t\tName:     \"log.compress\",\n\t\tUsage:    \"Compress the log files\",\n\t\tValue:    false,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tpprofFlag = &cli.BoolFlag{\n\t\tName:     \"pprof\",\n\t\tUsage:    \"Enable the pprof HTTP server\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tpprofPortFlag = &cli.IntFlag{\n\t\tName:     \"pprof.port\",\n\t\tUsage:    \"pprof HTTP server listening port\",\n\t\tValue:    6060,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tpprofAddrFlag = &cli.StringFlag{\n\t\tName:     \"pprof.addr\",\n\t\tUsage:    \"pprof HTTP server listening interface\",\n\t\tValue:    \"127.0.0.1\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tmemprofilerateFlag = &cli.IntFlag{\n\t\tName:     \"pprof.memprofilerate\",\n\t\tUsage:    \"Turn on memory profiling with the given rate\",\n\t\tValue:    runtime.MemProfileRate,\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tblockprofilerateFlag = &cli.IntFlag{\n\t\tName:     \"pprof.blockprofilerate\",\n\t\tUsage:    \"Turn on block profiling with the given rate\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\tcpuprofileFlag = &cli.StringFlag{\n\t\tName:     \"pprof.cpuprofile\",\n\t\tUsage:    \"Write CPU profile to the given file\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n\ttraceFlag = &cli.StringFlag{\n\t\tName:     \"go-execution-trace\",\n\t\tUsage:    \"Write Go execution trace to the given file\",\n\t\tCategory: flags.LoggingCategory,\n\t}\n)\n\n// Flags holds all command-line flags required for debugging.\nvar Flags = []cli.Flag{\n\tverbosityFlag,\n\tlogVmoduleFlag,\n\tvmoduleFlag,\n\tlogjsonFlag,\n\tlogFormatFlag,\n\tlogFileFlag,\n\tlogRotateFlag,\n\tlogMaxSizeMBsFlag,\n\tlogMaxBackupsFlag,\n\tlogMaxAgeFlag,\n\tlogCompressFlag,\n\tpprofFlag,\n\tpprofAddrFlag,\n\tpprofPortFlag,\n\tmemprofilerateFlag,\n\tblockprofilerateFlag,\n\tcpuprofileFlag,\n\ttraceFlag,\n}\n\nvar (\n\tglogger       *log.GlogHandler\n\tlogOutputFile io.WriteCloser\n)\n\nfunc init() {\n\tglogger = log.NewGlogHandler(log.NewTerminalHandler(os.Stderr, false))\n}\n\n// Setup initializes profiling and logging based on the CLI flags.\n// It should be called as early as possible in the program.\nfunc Setup(ctx *cli.Context) error {\n\tvar (\n\t\thandler        slog.Handler\n\t\tterminalOutput = io.Writer(os.Stderr)\n\t\toutput         io.Writer\n\t\tlogFmtFlag     = ctx.String(logFormatFlag.Name)\n\t)\n\tvar (\n\t\tlogFile  = ctx.String(logFileFlag.Name)\n\t\trotation = ctx.Bool(logRotateFlag.Name)\n\t)\n\tif len(logFile) > 0 {\n\t\tif err := validateLogLocation(filepath.Dir(logFile))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 87,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(n)\n\treturn &e, nil\n}\n\n// ReadAt reads one Entry from r at the specified offset.\nfunc (r *Reader) ReadAt(entry *Entry, off int64) (int, error) {\n\ttyp, length, err := r.ReadMetadataAt(off)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tentry.Type = typ\n\n\t// Check length bounds.\n\tif length > valueSizeLimit {\n\t\treturn headerSize, fmt.Errorf(\"item larger than item size limit %d: have %d\", valueSizeLimit, length)\n\t}\n\tif length == 0 {\n\t\treturn headerSize, nil\n\t}\n\n\t// Read value.\n\tval := make([]byte, length)\n\tif n, err := r.r.ReadAt(val, off+headerSize)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 110,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= headerSize\n\t\t// An entry with a non-zero length should not return EOF when\n\t\t// reading the value.\n\t\tif err == io.EOF {\n\t\t\treturn n, io.ErrUnexpectedEOF\n\t\t}\n\t\treturn n, err\n\t}\n\tentry.Value = val\n\treturn int(headerSize + length), nil\n}\n\n// ReaderAt returns an io.Reader delivering value data for the entry at\n// the specified offset. If the entry type does not match the expected type, an\n// error is returned.\nfunc (r *Reader) ReaderAt(expectedType uint16, off int64) (io.Reader, int, error) {\n\t// problem = need to return length+headerSize not just value length via section reader\n\ttyp, length, err := r.ReadMetadataAt(off)\n\tif err != nil {\n\t\treturn nil, headerSize, err\n\t}\n\tif typ != expectedType {\n\t\treturn nil, headerSize, fmt.Errorf(\"wrong type, want %d have %d\", expectedType, typ)\n\t}\n\tif length > valueSizeLimit {\n\t\treturn nil, headerSize, fmt.Errorf(\"item larger than item size limit %d: have %d\", valueSizeLimit, length)\n\t}\n\treturn io.NewSectionReader(r.r, off+headerSize, int64(length)), headerSize + int(length), nil\n}\n\n// LengthAt reads the header at off and returns the total length of the entry,\n// including header.\nfunc (r *Reader) LengthAt(off int64) (int64, error) {\n\t_, length, err := r.ReadMetadataAt(off)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\treturn int64(length) + headerSize, nil\n}\n\n// ReadMetadataAt reads the header metadata at the given offset.\nfunc (r *Reader) ReadMetadataAt(off int64) (typ uint16, length uint32, err error) {\n\tb := make([]byte, headerSize)\n\tif n, err := r.r.ReadAt(b, off)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 192,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(headerSize + length)\n\t}\n}\n\n// FindAll returns all entries with the matching type.\nfunc (r *Reader) FindAll(want uint16) ([]*Entry, error) {\n\tvar (\n\t\toff     int64\n\t\ttyp     uint16\n\t\tlength  uint32\n\t\tentries []*Entry\n\t\terr     error\n\t)\n\tfor {\n\t\ttyp, length, err = r.ReadMetadataAt(off)\n\t\tif err == io.EOF {\n\t\t\treturn entries, nil\n\t\t} else if err != nil {\n\t\t\treturn entries, err\n\t\t}\n\t\tif typ == want {\n\t\t\te := new(Entry)\n\t\t\tif _, err := r.ReadAt(e, off)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 219,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(headerSize + length)\n\t}\n}\n\n// SkipN skips `n` entries starting from `offset` and returns the new offset.\nfunc (r *Reader) SkipN(offset int64, n uint64) (int64, error) {\n\tfor i := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/internal/era/e2store/e2store.go",
          "line": 230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length\n\t}\n\treturn offset, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 170,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(common.HashLength + len(node))\n}\n\n// node retrieves an encoded cached trie node from memory. If it cannot be found\n// cached, the method queries the persistent database for the content.\nfunc (db *Database) node(hash common.Hash) ([]byte, error) {\n\t// It doesn't make sense to retrieve the metaroot\n\tif hash == (common.Hash{}) {\n\t\treturn nil, errors.New(\"not found\")\n\t}\n\t// Retrieve the node from the clean cache if available\n\tif db.cleans != nil {\n\t\tif enc := db.cleans.Get(nil, hash[:])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 236,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\treturn\n\t}\n\t// The reference is for external storage trie, don't duplicate if\n\t// the reference is already existent.\n\tif db.dirties[parent].external == nil {\n\t\tdb.dirties[parent].external = make(map[common.Hash]struct{})\n\t}\n\tif _, ok := db.dirties[parent].external[child]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 249,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength\n}\n\n// Dereference removes an existing reference from a root node.\nfunc (db *Database) Dereference(root common.Hash) {\n\t// Sanity check to ensure that the meta-root is not removed\n\tif root == (common.Hash{}) {\n\t\tlog.Error(\"Attempted to dereference the trie cache meta root\")\n\t\treturn\n\t}\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\tdb.dereference(root)\n\n\tdb.gcnodes += uint64(nodes - len(db.dirties))\n\tdb.gcsize += storage - db.dirtiesSize\n\tdb.gctime += time.Since(start)\n\n\tmemcacheGCTimeTimer.Update(time.Since(start))\n\tmemcacheGCBytesMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheGCNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Dereferenced trie from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"gcnodes\", db.gcnodes, \"gcsize\", db.gcsize, \"gctime\", db.gctime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n}\n\n// dereference is the private locked version of Dereference.\nfunc (db *Database) dereference(hash common.Hash) {\n\t// If the node does not exist, it's a previously committed node.\n\tnode, ok := db.dirties[hash]\n\tif !ok {\n\t\treturn\n\t}\n\t// If there are no more references to the node, delete it and cascade\n\tif node.parents > 0 {\n\t\t// This is a special cornercase where a node loaded from disk (i.e. not in the\n\t\t// memcache any more) gets reinjected as a new node (short node split into full,\n\t\t// then reverted into short), causing a cached node to have no parents. That is\n\t\t// no problem in itself, but don't make maxint parents out of it.\n\t\tnode.parents--\n\t}\n\tif node.parents == 0 {\n\t\t// Remove the node from the flush-list\n\t\tswitch hash {\n\t\tcase db.oldest:\n\t\t\tdb.oldest = node.flushNext\n\t\t\tif node.flushNext != (common.Hash{}) {\n\t\t\t\tdb.dirties[node.flushNext].flushPrev = common.Hash{}\n\t\t\t}\n\t\tcase db.newest:\n\t\t\tdb.newest = node.flushPrev\n\t\t\tif node.flushPrev != (common.Hash{}) {\n\t\t\t\tdb.dirties[node.flushPrev].flushNext = common.Hash{}\n\t\t\t}\n\t\tdefault:\n\t\t\tdb.dirties[node.flushPrev].flushNext = node.flushNext\n\t\t\tdb.dirties[node.flushNext].flushPrev = node.flushPrev\n\t\t}\n\t\t// Dereference all children and delete the node\n\t\tnode.forChildren(func(child common.Hash) {\n\t\t\tdb.dereference(child)\n\t\t})\n\t\tdelete(db.dirties, hash)\n\t\tdb.dirtiesSize -= common.StorageSize(common.HashLength + len(node.node))\n\t\tif node.external != nil {\n\t\t\tdb.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t}\n}\n\n// Cap iteratively flushes old but still referenced trie nodes until the total\n// memory usage goes below the given threshold.\nfunc (db *Database) Cap(limit common.StorageSize) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tbatch := db.diskdb.NewBatch()\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\n\t// db.dirtiesSize only contains the useful data in the cache, but when reporting\n\t// the total memory consumption, the maintenance metadata is also needed to be\n\t// counted.\n\tsize := db.dirtiesSize + common.StorageSize(len(db.dirties)*cachedNodeSize)\n\tsize += db.childrenSize\n\n\t// Keep committing nodes from the flush-list until we're below allowance\n\toldest := db.oldest\n\tfor size > limit && oldest != (common.Hash{}) {\n\t\t// Fetch the oldest referenced node and push into the batch\n\t\tnode := db.dirties[oldest]\n\t\trawdb.WriteLegacyTrieNode(batch, oldest, node.node)\n\n\t\t// If we exceeded the ideal batch size, commit and reset\n\t\tif batch.ValueSize() >= ethdb.IdealBatchSize {\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 383,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(nodes - len(db.dirties))\n\tdb.flushsize += storage - db.dirtiesSize\n\tdb.flushtime += time.Since(start)\n\n\tmemcacheFlushTimeTimer.Update(time.Since(start))\n\tmemcacheFlushBytesMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheFlushNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Persisted nodes from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"flushnodes\", db.flushnodes, \"flushsize\", db.flushsize, \"flushtime\", db.flushtime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n\n\treturn nil\n}\n\n// Commit iterates over all the children of a particular node, writes them out\n// to disk, forcefully tearing down all references in both directions. As a side\n// effect, all pre-images accumulated up to this point are also written.\nfunc (db *Database) Commit(node common.Hash, report bool) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tstart := time.Now()\n\tbatch := db.diskdb.NewBatch()\n\n\t// Move the trie itself into the batch, flushing if enough data is accumulated\n\tnodes, storage := len(db.dirties), db.dirtiesSize\n\n\tuncacher := &cleaner{db}\n\tif err := db.commit(node, batch, uncacher)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 314,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node))\n\t\tif node.external != nil {\n\t\t\tdb.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t}\n}\n\n// Cap iteratively flushes old but still referenced trie nodes until the total\n// memory usage goes below the given threshold.\nfunc (db *Database) Cap(limit common.StorageSize) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tbatch := db.diskdb.NewBatch()\n\tnodes, storage, start := len(db.dirties), db.dirtiesSize, time.Now()\n\n\t// db.dirtiesSize only contains the useful data in the cache, but when reporting\n\t// the total memory consumption, the maintenance metadata is also needed to be\n\t// counted.\n\tsize := db.dirtiesSize + common.StorageSize(len(db.dirties)*cachedNodeSize)\n\tsize += db.childrenSize\n\n\t// Keep committing nodes from the flush-list until we're below allowance\n\toldest := db.oldest\n\tfor size > limit && oldest != (common.Hash{}) {\n\t\t// Fetch the oldest referenced node and push into the batch\n\t\tnode := db.dirties[oldest]\n\t\trawdb.WriteLegacyTrieNode(batch, oldest, node.node)\n\n\t\t// If we exceeded the ideal batch size, commit and reset\n\t\tif batch.ValueSize() >= ethdb.IdealBatchSize {\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 358,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node) + cachedNodeSize)\n\t\tif node.external != nil {\n\t\t\tsize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t\toldest = node.flushNext\n\t}\n\t// Flush out any remainder data from the last batch\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 375,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node))\n\t\tif node.external != nil {\n\t\t\tdb.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t\t}\n\t}\n\tif db.oldest != (common.Hash{}) {\n\t\tdb.dirties[db.oldest].flushPrev = common.Hash{}\n\t}\n\tdb.flushnodes += uint64(nodes - len(db.dirties))\n\tdb.flushsize += storage - db.dirtiesSize\n\tdb.flushtime += time.Since(start)\n\n\tmemcacheFlushTimeTimer.Update(time.Since(start))\n\tmemcacheFlushBytesMeter.Mark(int64(storage - db.dirtiesSize))\n\tmemcacheFlushNodesMeter.Mark(int64(nodes - len(db.dirties)))\n\n\tlog.Debug(\"Persisted nodes from memory database\", \"nodes\", nodes-len(db.dirties), \"size\", storage-db.dirtiesSize, \"time\", time.Since(start),\n\t\t\"flushnodes\", db.flushnodes, \"flushsize\", db.flushsize, \"flushtime\", db.flushtime, \"livenodes\", len(db.dirties), \"livesize\", db.dirtiesSize)\n\n\treturn nil\n}\n\n// Commit iterates over all the children of a particular node, writes them out\n// to disk, forcefully tearing down all references in both directions. As a side\n// effect, all pre-images accumulated up to this point are also written.\nfunc (db *Database) Commit(node common.Hash, report bool) error {\n\tdb.lock.Lock()\n\tdefer db.lock.Unlock()\n\n\t// Create a database batch to flush persistent data out. It is important that\n\t// outside code doesn't see an inconsistent state (referenced data removed from\n\t// memory cache during commit but not yet in persistent storage). This is ensured\n\t// by only uncaching existing data when the database write finalizes.\n\tstart := time.Now()\n\tbatch := db.diskdb.NewBatch()\n\n\t// Move the trie itself into the batch, flushing if enough data is accumulated\n\tnodes, storage := len(db.dirties), db.dirtiesSize\n\n\tuncacher := &cleaner{db}\n\tif err := db.commit(node, batch, uncacher)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/triedb/hashdb/database.go",
          "line": 519,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= common.StorageSize(common.HashLength + len(node.node))\n\tif node.external != nil {\n\t\tc.db.childrenSize -= common.StorageSize(len(node.external) * common.HashLength)\n\t}\n\t// Move the flushed node into the clean cache to prevent insta-reloads\n\tif c.db.cleans != nil {\n\t\tc.db.cleans.Set(hash[:], rlp)\n\t\tmemcacheCleanWriteMeter.Mark(int64(len(rlp)))\n\t}\n\treturn nil\n}\n\nfunc (c *cleaner) Delete(key []byte) error {\n\tpanic(\"not implemented\")\n}\n\n// Update inserts the dirty nodes in provided nodeset into database and link the\n// account trie with multiple storage tries if necessary.\nfunc (db *Database) Update(root common.Hash, parent common.Hash, block uint64, nodes *trienode.MergedNodeSet) error {\n\t// Ensure the parent state is present and signal a warning if not.\n\tif parent != types.EmptyRootHash {\n\t\tif blob, _ := db.node(parent)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 530,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t\t\t\tcontinue\n\t\t\t} else if cmp == 0 {\n\t\t\t\t// the snapshot key can be overwritten\n\t\t\t\tcreated--\n\t\t\t\tif write = !bytes.Equal(kvvals[0], iter.Value)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 549,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t}\n\tif iter.Err != nil {\n\t\t// Trie errors should never happen. Still, in case of a bug, expose the\n\t\t// error here, as the outer code will presume errors are interrupts, not\n\t\t// some deeper issues.\n\t\tlog.Error(\"State snapshotter failed to iterate trie\", \"err\", iter.Err)\n\t\treturn false, nil, iter.Err\n\t}\n\t// Delete all stale snapshot states remaining\n\tistart := time.Now()\n\tfor _, key := range kvkeys {\n\t\tif err := onState(key, nil, false, true)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 564,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tinternal += time.Since(istart)\n\n\t// Update metrics for counting trie iteration\n\tif kind == snapStorage {\n\t\tstorageTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t} else {\n\t\taccountTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t}\n\tlogger.Trace(\"Regenerated state range\", \"root\", trieId.Root, \"last\", hexutil.Encode(last),\n\t\t\"count\", count, \"created\", created, \"updated\", updated, \"untouched\", untouched, \"deleted\", deleted)\n\n\t// If there are either more trie items, or there are more snap items\n\t// (in the next segment), then we need to keep working\n\treturn !trieMore && !result.diskMore, last, nil\n}\n\n// checkAndFlush checks if an interruption signal is received or the\n// batch size has exceeded the allowance.\nfunc (g *generator) checkAndFlush(ctx *generatorContext, current []byte) error {\n\tvar abort chan struct{}\n\tselect {\n\tcase abort = <-g.abort:\n\tdefault:\n\t}\n\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize || abort != nil {\n\t\tif bytes.Compare(current, g.progress) < 0 {\n\t\t\tlog.Error(\"Snapshot generator went backwards\", \"current\", fmt.Sprintf(\"%x\", current), \"genMarker\", fmt.Sprintf(\"%x\", g.progress))\n\t\t}\n\t\t// Persist the progress marker regardless of whether the batch is empty or not.\n\t\t// It may happen that all the flat states in the database are correct, so the\n\t\t// generator indeed makes progress even if there is nothing to commit.\n\t\tjournalProgress(ctx.batch, current, g.stats)\n\n\t\t// Flush out the database writes atomically\n\t\tif err := ctx.batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 645,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + 2*common.HashLength + len(val))\n\t\tg.stats.slots++\n\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := g.checkAndFlush(ctx, append(account[:], key...))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 680,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= ctx.removeStorageBefore(account)\n\n\t\tstart := time.Now()\n\t\tif delete {\n\t\t\trawdb.DeleteAccountSnapshot(ctx.batch, account)\n\t\t\twipedAccountMeter.Mark(1)\n\t\t\taccountWriteCounter.Inc(time.Since(start).Nanoseconds())\n\n\t\t\tctx.removeStorageAt(account)\n\t\t\treturn nil\n\t\t}\n\t\t// Retrieve the current account and flatten it into the internal format\n\t\tvar acc types.StateAccount\n\t\tif err := rlp.DecodeBytes(val, &acc)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 713,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tg.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(g.progress) > common.HashLength {\n\t\t\tmarker = g.progress\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := g.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 757,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= ctx.removeRemainingStorage()\n\t\t\tbreak\n\t\t}\n\t}\n\treturn nil\n}\n\n// generate is a background thread that iterates over the state and storage tries,\n// constructing the state snapshot. All the arguments are purely for statistics\n// gathering and logging, since the method surfs the blocks as they arrive, often\n// being restarted.\nfunc (g *generator) generate(ctx *generatorContext) {\n\tg.stats.log(\"Resuming snapshot generation\", ctx.root, g.progress)\n\tdefer ctx.close()\n\n\t// Persist the initial marker and state snapshot root if progress is none\n\tif len(g.progress) == 0 {\n\t\tbatch := g.db.NewBatch()\n\t\trawdb.WriteSnapshotRoot(batch, ctx.root)\n\t\tjournalProgress(batch, g.progress, g.stats)\n\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/triedb/pathdb/generate.go",
          "line": 701,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n\t\t\t\t}\n\t\t\t\tif acc.Root == types.EmptyRootHash {\n\t\t\t\t\tdataLen -= 32\n\t\t\t\t}\n\t\t\t\trecoveredAccountMeter.Mark(1)\n\t\t\t} else {\n\t\t\t\tdata := types.SlimAccountRLP(acc)\n\t\t\t\tdataLen = len(data)\n\t\t\t\trawdb.WriteAccountSnapshot(ctx.batch, account, data)\n\t\t\t\tgeneratedAccountMeter.Mark(1)\n\t\t\t}\n\t\t\tg.stats.storage += common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tg.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(g.progress) > common.HashLength {\n\t\t\tmarker = g.progress\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := g.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/database.go",
          "line": 679,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(diff.size())\n\t\t}\n\t\tif disk, ok := layer.(*diskLayer)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/history_index_block.go",
          "line": 207,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= x\n\t\t}\n\t\tif result > id {\n\t\t\treturn result, nil\n\t\t}\n\t\tpos += n\n\t}\n\t// The element which is greater than specified id is not found.\n\tif index == len(br.restarts) {\n\t\treturn math.MaxUint64, nil\n\t}\n\t// The element which is the first one greater than the specified id\n\t// is exactly the one located at the restart point.\n\titem, _ := binary.Uvarint(br.data[br.restarts[index]:])\n\treturn item, nil\n}\n\ntype blockWriter struct {\n\tdesc     *indexBlockDesc // Descriptor of the block\n\trestarts []uint16        // Offsets into the data slice, marking the start of each section\n\tscratch  []byte          // Buffer used for encoding full integers or value differences\n\tdata     []byte          // Aggregated encoded data slice\n}\n\nfunc newBlockWriter(blob []byte, desc *indexBlockDesc) (*blockWriter, error) {\n\tscratch := make([]byte, binary.MaxVarintLen64)\n\tif len(blob) == 0 {\n\t\treturn &blockWriter{\n\t\t\tdesc:    desc,\n\t\t\tscratch: scratch,\n\t\t\tdata:    make([]byte, 0, 1024),\n\t\t}, nil\n\t}\n\trestarts, data, err := parseIndexBlock(blob)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn &blockWriter{\n\t\tdesc:     desc,\n\t\trestarts: restarts,\n\t\tscratch:  scratch,\n\t\tdata:     data, // safe to own the slice\n\t}, nil\n}\n\n// append adds a new element to the block. The new element must be greater than\n// the previous one. The provided ID is assumed to always be greater than 0.\nfunc (b *blockWriter) append(id uint64) error {\n\tif id == 0 {\n\t\treturn errors.New(\"invalid zero id\")\n\t}\n\tif id <= b.desc.max {\n\t\treturn fmt.Errorf(\"append element out of order, last: %d, this: %d\", b.desc.max, id)\n\t}\n\t// Rotate the current restart section if it's full\n\tif b.desc.entries%indexBlockRestartLen == 0 {\n\t\t// Save the offset within the data slice as the restart point\n\t\t// for the next section.\n\t\tb.restarts = append(b.restarts, uint16(len(b.data)))\n\n\t\t// The restart point item can either be encoded in variable\n\t\t// size or fixed size. Although variable-size encoding is\n\t\t// slightly slower (2ns per operation), it is still relatively\n\t\t// fast, therefore, it's picked for better space efficiency.\n\t\t//\n\t\t// The first element in a restart range is encoded using its\n\t\t// full value.\n\t\tn := binary.PutUvarint(b.scratch[0:], id)\n\t\tb.data = append(b.data, b.scratch[:n]...)\n\t} else {\n\t\t// The current section is not full, append the element.\n\t\t// The element which is not the first one in the section\n\t\t// is encoded using the value difference from the preceding\n\t\t// element.\n\t\tn := binary.PutUvarint(b.scratch[0:], id-b.desc.max)\n\t\tb.data = append(b.data, b.scratch[:n]...)\n\t}\n\tb.desc.entries++\n\n\t// The state history ID must be greater than 0.\n\t//if b.desc.min == 0 {\n\t//\tb.desc.min = id\n\t//}\n\tb.desc.max = id\n\treturn nil\n}\n\n// scanSection traverses the specified section and terminates if fn returns true.\nfunc (b *blockWriter) scanSection(section int, fn func(uint64, int) bool) {\n\tvar (\n\t\tvalue uint64\n\t\tstart = int(b.restarts[section])\n\t\tpos   = start\n\t\tlimit int\n\t)\n\tif section == len(b.restarts)-1 {\n\t\tlimit = len(b.data)\n\t} else {\n\t\tlimit = int(b.restarts[section+1])\n\t}\n\tfor pos < limit {\n\t\tx, n := binary.Uvarint(b.data[pos:])\n\t\tif pos == start {\n\t\t\tvalue = x\n\t\t} else {\n\t\t\tvalue += x\n\t\t}\n\t\tif fn(value, pos) {\n\t\t\treturn\n\t\t}\n\t\tpos += n\n\t}\n}\n\n// sectionLast returns the last element in the specified section.\nfunc (b *blockWriter) sectionLast(section int) uint64 {\n\tvar n uint64\n\tb.scanSection(section, func(v uint64, _ int) bool {\n\t\tn = v\n\t\treturn false\n\t})\n\treturn n\n}\n\n// sectionSearch looks up the specified value in the given section,\n// the position and the preceding value will be returned if found.\nfunc (b *blockWriter) sectionSearch(section int, n uint64) (found bool, prev uint64, pos int) {\n\tb.scanSection(section, func(v uint64, p int) bool {\n\t\tif n == v {\n\t\t\tpos = p\n\t\t\tfound = true\n\t\t\treturn true // terminate iteration\n\t\t}\n\t\tprev = v\n\t\treturn false // continue iteration\n\t})\n\treturn found, prev, pos\n}\n\n// pop removes the last element from the block. The assumption is held that block\n// writer must be non-empty.\nfunc (b *blockWriter) pop(id uint64) error {\n\tif id == 0 {\n\t\treturn errors.New(\"invalid zero id\")\n\t}\n\tif id != b.desc.max {\n\t\treturn fmt.Errorf(\"pop element out of order, last: %d, this: %d\", b.desc.max, id)\n\t}\n\t// If there is only one entry left, the entire block should be reset\n\tif b.desc.entries == 1 {\n\t\t//b.desc.min = 0\n\t\tb.desc.max = 0\n\t\tb.desc.entries = 0\n\t\tb.restarts = nil\n\t\tb.data = b.data[:0]\n\t\treturn nil\n\t}\n\t// Pop the last restart section if the section becomes empty after removing\n\t// one element.\n\tif b.desc.entries%indexBlockRestartLen == 1 {\n\t\tb.data = b.data[:b.restarts[len(b.restarts)-1]]\n\t\tb.restarts = b.restarts[:len(b.restarts)-1]\n\t\tb.desc.max = b.sectionLast(len(b.restarts) - 1)\n\t\tb.desc.entries -= 1\n\t\treturn nil\n\t}\n\t// Look up the element preceding the one to be popped, in order to update\n\t// the maximum element in the block.\n\tfound, prev, pos := b.sectionSearch(len(b.restarts)-1, id)\n\tif !found {\n\t\treturn fmt.Errorf(\"pop element is not found, last: %d, this: %d\", b.desc.max, id)\n\t}\n\tb.desc.max = prev\n\tb.data = b.data[:pos]\n\tb.desc.entries -= 1\n\treturn nil\n}\n\nfunc (b *blockWriter) empty() bool {\n\treturn b.desc.empty()\n}\n\nfunc (b *blockWriter) full() bool {\n\treturn b.desc.full()\n}\n\n// finish finalizes the index block encoding by appending the encoded restart points\n// and the restart counter to the end of the block.\n//\n// This function is safe to be called multiple times.\nfunc (b *blockWriter) finish() []byte {\n\tvar buf []byte\n\tfor _, number := range b.restarts {\n\t\tbinary.BigEndian.PutUint16(b.scratch[:2], number)\n\t\tbuf = append(buf, b.scratch[:2]...)\n\t}\n\tbuf = append(buf, byte(len(b.restarts)))\n\treturn append(b.data, buf...)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/history_index_block.go",
          "line": 370,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\treturn nil\n\t}\n\t// Look up the element preceding the one to be popped, in order to update\n\t// the maximum element in the block.\n\tfound, prev, pos := b.sectionSearch(len(b.restarts)-1, id)\n\tif !found {\n\t\treturn fmt.Errorf(\"pop element is not found, last: %d, this: %d\", b.desc.max, id)\n\t}\n\tb.desc.max = prev\n\tb.data = b.data[:pos]\n\tb.desc.entries -= 1\n\treturn nil\n}\n\nfunc (b *blockWriter) empty() bool {\n\treturn b.desc.empty()\n}\n\nfunc (b *blockWriter) full() bool {\n\treturn b.desc.full()\n}\n\n// finish finalizes the index block encoding by appending the encoded restart points\n// and the restart counter to the end of the block.\n//\n// This function is safe to be called multiple times.\nfunc (b *blockWriter) finish() []byte {\n\tvar buf []byte\n\tfor _, number := range b.restarts {\n\t\tbinary.BigEndian.PutUint16(b.scratch[:2], number)\n\t\tbuf = append(buf, b.scratch[:2]...)\n\t}\n\tbuf = append(buf, byte(len(b.restarts)))\n\treturn append(b.data, buf...)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/iterator_test.go",
          "line": 165,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t}\n\t\t}\n\t\tstorage[hash] = accStorage\n\t\tnilStorage[hash] = nilstorage\n\t}\n\tstates := newStates(accounts, storage, false)\n\tfor account := range accounts {\n\t\tit := newDiffStorageIterator(account, common.Hash{}, states.storageList(account), nil)\n\t\tverifyIterator(t, 100, it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n\n\tdb := rawdb.NewMemoryDatabase()\n\tbatch := db.NewBatch()\n\tstates.write(batch, nil, nil)\n\tbatch.Write()\n\tfor account := range accounts {\n\t\tit := newDiskStorageIterator(db, account, common.Hash{})\n\t\tverifyIterator(t, 100-nilStorage[account], it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n}\n\ntype testIterator struct {\n\tvalues []byte\n}\n\nfunc newTestIterator(values ...byte) *testIterator {\n\treturn &testIterator{values}\n}\n\nfunc (ti *testIterator) Seek(common.Hash) {\n\tpanic(\"implement me\")\n}\n\nfunc (ti *testIterator) Next() bool {\n\tti.values = ti.values[1:]\n\treturn len(ti.values) > 0\n}\n\nfunc (ti *testIterator) Error() error {\n\treturn nil\n}\n\nfunc (ti *testIterator) Hash() common.Hash {\n\treturn common.BytesToHash([]byte{ti.values[0]})\n}\n\nfunc (ti *testIterator) Account() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Slot() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Release() {}\n\nfunc TestFastIteratorBasics(t *testing.T) {\n\ttype testCase struct {\n\t\tlists   [][]byte\n\t\texpKeys []byte\n\t}\n\tfor i, tc := range []testCase{\n\t\t{lists: [][]byte{{0, 1, 8}, {1, 2, 8}, {2, 9}, {4},\n\t\t\t{7, 14, 15}, {9, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 4, 7, 8, 9, 13, 14, 15, 16}},\n\t\t{lists: [][]byte{{0, 8}, {1, 2, 8}, {7, 14, 15}, {8, 9},\n\t\t\t{9, 10}, {10, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 7, 8, 9, 10, 13, 14, 15, 16}},\n\t} {\n\t\tvar iterators []*weightedIterator\n\t\tfor i, data := range tc.lists {\n\t\t\tit := newTestIterator(data...)\n\t\t\titerators = append(iterators, &weightedIterator{it, i})\n\t\t}\n\t\tfi := &fastIterator{\n\t\t\titerators: iterators,\n\t\t\tinitiated: false,\n\t\t}\n\t\tcount := 0\n\t\tfor fi.Next() {\n\t\t\tif got, exp := fi.Hash()[31], tc.expKeys[count]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/states_test.go",
          "line": 442,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2*common.HashLength + 2 + 2                                                  /* storage a change */\n\tmergeSize += 2*common.HashLength + 2 - 1                                                  /* storage b change */\n\tmergeSize += 2*2*common.HashLength - 1                                                    /* storage data removal of 0xc */\n\n\tif a.size != uint64(mergeSize) {\n\t\tt.Fatalf(\"Unexpected size, want: %d, got: %d\", mergeSize, a.size)\n\t}\n\n\t// Revert the set to original status\n\ta.revertTo(\n\t\tmap[common.Hash][]byte{\n\t\t\t{0xa}: {0xa0},\n\t\t\t{0xb}: {0xb0},\n\t\t\t{0xc}: {0xc0},\n\t\t},\n\t\tmap[common.Hash]map[common.Hash][]byte{\n\t\t\t{0xa}: {\n\t\t\t\tcommon.Hash{0x1}: {0x10},\n\t\t\t\tcommon.Hash{0x2}: {0x20},\n\t\t\t\tcommon.Hash{0x3}: nil, // revert slot creation\n\t\t\t},\n\t\t\t{0xb}: {\n\t\t\t\tcommon.Hash{0x1}: {0x10, 0x11, 0x12},\n\t\t\t\tcommon.Hash{0x2}: nil, // revert slot creation\n\t\t\t},\n\t\t\t{0xc}: {\n\t\t\t\tcommon.Hash{0x1}: {0x10},\n\t\t\t\tcommon.Hash{0x2}: {0x20}, // resurrected slot\n\t\t\t\tcommon.Hash{0x3}: {0x30}, // resurrected slot\n\t\t\t},\n\t\t},\n\t)\n\trevertSize := expSizeA + 2*common.HashLength + 2*common.HashLength // delete-marker of a.3 and b.2 slot\n\trevertSize += 2 * (2*common.HashLength + 1)                        // resurrected slot, c.2, c.3\n\tif a.size != uint64(revertSize) {\n\t\tt.Fatalf(\"Unexpected size, want: %d, got: %d\", revertSize, a.size)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/flush.go",
          "line": 64,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(subset)\n\t}\n\treturn total\n}\n\n// writeStates flushes state mutations into the provided database batch as a whole.\n//\n// This function assumes the background generator is already terminated and states\n// before the supplied marker has been correctly generated.\n//\n// TODO(rjl493456442) do we really need this generation marker? The state updates\n// after the marker can also be written and will be fixed by generator later if\n// it's outdated.\nfunc writeStates(batch ethdb.Batch, genMarker []byte, accountData map[common.Hash][]byte, storageData map[common.Hash]map[common.Hash][]byte, clean *fastcache.Cache) (int, int) {\n\tvar (\n\t\taccounts int\n\t\tslots    int\n\t)\n\tfor addrHash, blob := range accountData {\n\t\t// Skip any account not yet covered by the snapshot. The account\n\t\t// at the generation marker position (addrHash == genMarker[:common.HashLength])\n\t\t// should still be updated, as it would be skipped in the next\n\t\t// generation cycle.\n\t\tif genMarker != nil && bytes.Compare(addrHash[:], genMarker) > 0 {\n\t\t\tcontinue\n\t\t}\n\t\taccounts += 1\n\t\tif len(blob) == 0 {\n\t\t\trawdb.DeleteAccountSnapshot(batch, addrHash)\n\t\t\tif clean != nil {\n\t\t\t\tclean.Set(addrHash[:], nil)\n\t\t\t}\n\t\t} else {\n\t\t\trawdb.WriteAccountSnapshot(batch, addrHash, blob)\n\t\t\tif clean != nil {\n\t\t\t\tclean.Set(addrHash[:], blob)\n\t\t\t}\n\t\t}\n\t}\n\tfor addrHash, storages := range storageData {\n\t\t// Skip any account not covered yet by the snapshot\n\t\tif genMarker != nil && bytes.Compare(addrHash[:], genMarker) > 0 {\n\t\t\tcontinue\n\t\t}\n\t\tmidAccount := genMarker != nil && bytes.Equal(addrHash[:], genMarker[:common.HashLength])\n\n\t\tfor storageHash, blob := range storages {\n\t\t\t// Skip any storage slot not yet covered by the snapshot. The storage slot\n\t\t\t// at the generation marker position (addrHash == genMarker[:common.HashLength]\n\t\t\t// and storageHash == genMarker[common.HashLength:]) should still be updated,\n\t\t\t// as it would be skipped in the next generation cycle.\n\t\t\tif midAccount && bytes.Compare(storageHash[:], genMarker[common.HashLength:]) > 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tslots += 1\n\t\t\tif len(blob) == 0 {\n\t\t\t\trawdb.DeleteStorageSnapshot(batch, addrHash, storageHash)\n\t\t\t\tif clean != nil {\n\t\t\t\t\tclean.Set(append(addrHash[:], storageHash[:]...), nil)\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\trawdb.WriteStorageSnapshot(batch, addrHash, storageHash, blob)\n\t\t\t\tif clean != nil {\n\t\t\t\t\tclean.Set(append(addrHash[:], storageHash[:]...), blob)\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn accounts, slots\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/history.go",
          "line": 336,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint32(len(slots))\n\t\t}\n\t\taccountData = append(accountData, h.accounts[addr]...)\n\t\taccountIndexes = append(accountIndexes, accIndex.encode()...)\n\t}\n\treturn accountData, storageData, accountIndexes, storageIndexes\n}\n\n// decoder wraps the byte streams for decoding with extra meta fields.\ntype decoder struct {\n\taccountData    []byte // the buffer for concatenated account data\n\tstorageData    []byte // the buffer for concatenated storage data\n\taccountIndexes []byte // the buffer for concatenated account index\n\tstorageIndexes []byte // the buffer for concatenated storage index\n\n\tlastAccount       *common.Address // the address of last resolved account\n\tlastAccountRead   uint32          // the read-cursor position of account data\n\tlastSlotIndexRead uint32          // the read-cursor position of storage slot index\n\tlastSlotDataRead  uint32          // the read-cursor position of storage slot data\n}\n\n// verify validates the provided byte streams for decoding state history. A few\n// checks will be performed to quickly detect data corruption. The byte stream\n// is regarded as corrupted if:\n//\n// - account indexes buffer is empty(empty state set is invalid)\n// - account indexes/storage indexer buffer is not aligned\n//\n// note, these situations are allowed:\n//\n// - empty account data: all accounts were not present\n// - empty storage set: no slots are modified\nfunc (r *decoder) verify() error {\n\tif len(r.accountIndexes)%accountIndexSize != 0 || len(r.accountIndexes) == 0 {\n\t\treturn fmt.Errorf(\"invalid account index, len: %d\", len(r.accountIndexes))\n\t}\n\tif len(r.storageIndexes)%slotIndexSize != 0 {\n\t\treturn fmt.Errorf(\"invalid storage index, len: %d\", len(r.storageIndexes))\n\t}\n\treturn nil\n}\n\n// readAccount parses the account from the byte stream with specified position.\nfunc (r *decoder) readAccount(pos int) (accountIndex, []byte, error) {\n\t// Decode account index from the index byte stream.\n\tvar index accountIndex\n\tif (pos+1)*accountIndexSize > len(r.accountIndexes) {\n\t\treturn accountIndex{}, nil, errors.New(\"account data buffer is corrupted\")\n\t}\n\tindex.decode(r.accountIndexes[pos*accountIndexSize : (pos+1)*accountIndexSize])\n\n\t// Perform validation before parsing account data, ensure\n\t// - account is sorted in order in byte stream\n\t// - account data is strictly encoded with no gap inside\n\t// - account data is not out-of-slice\n\tif r.lastAccount != nil { // zero address is possible\n\t\tif bytes.Compare(r.lastAccount.Bytes(), index.address.Bytes()) >= 0 {\n\t\t\treturn accountIndex{}, nil, errors.New(\"account is not in order\")\n\t\t}\n\t}\n\tif index.offset != r.lastAccountRead {\n\t\treturn accountIndex{}, nil, errors.New(\"account data buffer is gaped\")\n\t}\n\tlast := index.offset + uint32(index.length)\n\tif uint32(len(r.accountData)) < last {\n\t\treturn accountIndex{}, nil, errors.New(\"account data buffer is corrupted\")\n\t}\n\tdata := r.accountData[index.offset:last]\n\n\tr.lastAccount = &index.address\n\tr.lastAccountRead = last\n\n\treturn index, data, nil\n}\n\n// readStorage parses the storage slots from the byte stream with specified account.\nfunc (r *decoder) readStorage(accIndex accountIndex) ([]common.Hash, map[common.Hash][]byte, error) {\n\tvar (\n\t\tlast    *common.Hash\n\t\tcount   = int(accIndex.storageSlots)\n\t\tlist    = make([]common.Hash, 0, count)\n\t\tstorage = make(map[common.Hash][]byte, count)\n\t)\n\tfor j := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/history.go",
          "line": 593,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(blobs))\n\t}\n\treturn nil\n}\n\n// truncateFromHead removes the extra state histories from the head with the given\n// parameters. It returns the number of items removed from the head.\nfunc truncateFromHead(db ethdb.Batcher, store ethdb.AncientStore, nhead uint64) (int, error) {\n\tohead, err := store.Ancients()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\totail, err := store.Tail()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\t// Ensure that the truncation target falls within the specified range.\n\tif ohead < nhead || nhead < otail {\n\t\treturn 0, fmt.Errorf(\"out of range, tail: %d, head: %d, target: %d\", otail, ohead, nhead)\n\t}\n\t// Short circuit if nothing to truncate.\n\tif ohead == nhead {\n\t\treturn 0, nil\n\t}\n\t// Load the meta objects in range [nhead+1, ohead]\n\tblobs, err := rawdb.ReadStateHistoryMetaList(store, nhead+1, ohead-nhead)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbatch := db.NewBatch()\n\tfor _, blob := range blobs {\n\t\tvar m meta\n\t\tif err := m.decode(blob)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/triedb/pathdb/history.go",
          "line": 592,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(len(blobs))\n\t\tstart += uint64(len(blobs))\n\t}\n\treturn nil\n}\n\n// truncateFromHead removes the extra state histories from the head with the given\n// parameters. It returns the number of items removed from the head.\nfunc truncateFromHead(db ethdb.Batcher, store ethdb.AncientStore, nhead uint64) (int, error) {\n\tohead, err := store.Ancients()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\totail, err := store.Tail()\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\t// Ensure that the truncation target falls within the specified range.\n\tif ohead < nhead || nhead < otail {\n\t\treturn 0, fmt.Errorf(\"out of range, tail: %d, head: %d, target: %d\", otail, ohead, nhead)\n\t}\n\t// Short circuit if nothing to truncate.\n\tif ohead == nhead {\n\t\treturn 0, nil\n\t}\n\t// Load the meta objects in range [nhead+1, ohead]\n\tblobs, err := rawdb.ReadStateHistoryMetaList(store, nhead+1, ohead-nhead)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tbatch := db.NewBatch()\n\tfor _, blob := range blobs {\n\t\tvar m meta\n\t\tif err := m.decode(blob)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/verifier.go",
          "line": 121,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tstat.head = account\n}\n\n// finishAccounts updates the generator stats for the finished account range.\nfunc (stat *generateStats) finishAccounts(done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.accounts += done\n}\n\n// progressContract updates the generator stats for a specific in-progress contract.\nfunc (stat *generateStats) progressContract(account common.Hash, slot common.Hash, done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.slots += done\n\tstat.slotsHead[account] = slot\n\tif _, ok := stat.slotsStart[account]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/verifier.go",
          "line": 150,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tdelete(stat.slotsHead, account)\n\tdelete(stat.slotsStart, account)\n}\n\n// report prints the cumulative progress statistic smartly.\nfunc (stat *generateStats) report() {\n\tstat.lock.RLock()\n\tdefer stat.lock.RUnlock()\n\n\tctx := []interface{}{\n\t\t\"accounts\", stat.accounts,\n\t\t\"slots\", stat.slots,\n\t\t\"elapsed\", common.PrettyDuration(time.Since(stat.start)),\n\t}\n\tif stat.accounts > 0 {\n\t\t// If there's progress on the account trie, estimate the time to finish crawling it\n\t\tif done := binary.BigEndian.Uint64(stat.head[:8]) / stat.accounts",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/history_indexer.go",
          "line": 99,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tb.accounts[addrHash] = append(b.accounts[addrHash], historyID)\n\n\t\tfor _, slotKey := range h.storageList[address] {\n\t\t\tb.counter += 1\n\t\t\tif _, ok := b.storages[addrHash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/history_indexer.go",
          "line": 172,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(slots)\n\t\tfor storageHash, idList := range slots {\n\t\t\teg.Go(func() error {\n\t\t\t\tif !b.delete {\n\t\t\t\t\tiw, err := newIndexWriter(b.db, newStorageIdent(addrHash, storageHash))\n\t\t\t\t\tif err != nil {\n\t\t\t\t\t\treturn err\n\t\t\t\t\t}\n\t\t\t\t\tfor _, n := range idList {\n\t\t\t\t\t\tif err := iw.append(n)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/triedb/pathdb/history_indexer.go",
          "line": 539,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\n\t\t\t// Occasionally report the indexing progress\n\t\t\tif time.Since(logged) > time.Second*8 {\n\t\t\t\tlogged = time.Now()\n\n\t\t\t\tvar (\n\t\t\t\t\tleft  = lastID - current + 1\n\t\t\t\t\tdone  = current - beginID\n\t\t\t\t\tspeed = done/uint64(time.Since(start)/time.Millisecond+1) + 1 // +1s to avoid division by zero\n\t\t\t\t)\n\t\t\t\t// Override the ETA if larger than the largest until now\n\t\t\t\teta := time.Duration(left/speed) * time.Millisecond\n\t\t\t\tlog.Info(\"Indexing state history\", \"processed\", done, \"left\", left, \"elapsed\", common.PrettyDuration(time.Since(start)), \"eta\", common.PrettyDuration(eta))\n\t\t\t}\n\t\t}\n\t\ti.indexed.Store(current - 1) // update indexing progress\n\n\t\t// Check interruption signal and abort process if it's fired\n\t\tif interrupt != nil {\n\t\t\tif signal := interrupt.Load()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 44,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n}\n\n// report uploads the cached statistics to meters.\nfunc (c *counter) report(count, size *metrics.Meter) {\n\tcount.Mark(int64(c.n))\n\tsize.Mark(int64(c.size))\n}\n\n// stateSet represents a collection of state modifications associated with a\n// transition (e.g., a block execution) or multiple aggregated transitions.\n//\n// A stateSet can only reside within a diffLayer or the buffer of a diskLayer,\n// serving as the envelope for the set. Lock protection is not required for\n// accessing or mutating the account set and storage set, as the associated\n// envelope is always marked as stale before any mutation is applied. Any\n// subsequent state access will be denied due to the stale flag. Therefore,\n// state access and mutation won't happen at the same time with guarantee.\ntype stateSet struct {\n\taccountData map[common.Hash][]byte                 // Keyed accounts for direct retrieval (nil means deleted)\n\tstorageData map[common.Hash]map[common.Hash][]byte // Keyed storage slots for direct retrieval. one per account (nil means deleted)\n\tsize        uint64                                 // Memory size of the state data (accountData and storageData)\n\n\taccountListSorted []common.Hash                 // List of account for iteration. If it exists, it's sorted, otherwise it's nil\n\tstorageListSorted map[common.Hash][]common.Hash // List of storage slots for iterated retrievals, one per account. Any existing lists are sorted if non-nil\n\n\trawStorageKey bool // indicates whether the storage set uses the raw slot key or the hash\n\n\t// Lock for guarding the two lists above. These lists might be accessed\n\t// concurrently and lock protection is essential to avoid concurrent\n\t// slice or map read/write.\n\tlistLock sync.RWMutex\n}\n\n// newStates constructs the state set with the provided account and storage data.\nfunc newStates(accounts map[common.Hash][]byte, storages map[common.Hash]map[common.Hash][]byte, rawStorageKey bool) *stateSet {\n\t// Don't panic for the lazy callers, initialize the nil maps instead.\n\tif accounts == nil {\n\t\taccounts = make(map[common.Hash][]byte)\n\t}\n\tif storages == nil {\n\t\tstorages = make(map[common.Hash]map[common.Hash][]byte)\n\t}\n\ts := &stateSet{\n\t\taccountData:       accounts,\n\t\tstorageData:       storages,\n\t\trawStorageKey:     rawStorageKey,\n\t\tstorageListSorted: make(map[common.Hash][]common.Hash),\n\t}\n\ts.size = s.check()\n\treturn s\n}\n\n// account returns the account data associated with the specified address hash.\nfunc (s *stateSet) account(hash common.Hash) ([]byte, bool) {\n\t// If the account is known locally, return it\n\tif data, ok := s.accountData[hash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 147,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.HashLength + len(blob)\n\t}\n\tfor accountHash, slots := range s.storageData {\n\t\tif slots == nil {\n\t\t\tpanic(fmt.Sprintf(\"storage %#x nil\", accountHash)) // nil slots is not permitted\n\t\t}\n\t\tfor _, blob := range slots {\n\t\t\tsize += 2*common.HashLength + len(blob)\n\t\t}\n\t}\n\treturn uint64(size)\n}\n\n// accountList returns a sorted list of all accounts in this state set, including\n// the deleted ones.\n//\n// Note, the returned slice is not a copy, so do not modify it.\nfunc (s *stateSet) accountList() []common.Hash {\n\t// If an old list already exists, return it\n\ts.listLock.RLock()\n\tlist := s.accountListSorted\n\ts.listLock.RUnlock()\n\n\tif list != nil {\n\t\treturn list\n\t}\n\t// No old sorted account list exists, generate a new one. It's possible that\n\t// multiple threads waiting for the write lock may regenerate the list\n\t// multiple times, which is acceptable.\n\ts.listLock.Lock()\n\tdefer s.listLock.Unlock()\n\n\tlist = slices.SortedFunc(maps.Keys(s.accountData), common.Hash.Cmp)\n\ts.accountListSorted = list\n\treturn list\n}\n\n// StorageList returns a sorted list of all storage slot hashes in this state set\n// for the given account. The returned list will include the hash of deleted\n// storage slot.\n//\n// Note, the returned slice is not a copy, so do not modify it.\nfunc (s *stateSet) storageList(accountHash common.Hash) []common.Hash {\n\ts.listLock.RLock()\n\tif _, ok := s.storageData[accountHash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(data) - len(origin)\n\t\t\taccountOverwrites.add(common.HashLength + len(origin))\n\t\t} else {\n\t\t\tdelta += common.HashLength + len(data)\n\t\t}\n\t\ts.accountData[accountHash] = data\n\t}\n\t// Apply all the updated storage slots (individually)\n\tfor accountHash, storage := range other.storageData {\n\t\t// If storage didn't exist in the set, overwrite blindly\n\t\tif _, ok := s.storageData[accountHash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 256,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2*common.HashLength + len(data)\n\t\t\t}\n\t\t\ts.storageData[accountHash] = slots\n\t\t\tcontinue\n\t\t}\n\t\t// Storage exists in both local and external set, merge the slots\n\t\tslots := s.storageData[accountHash]\n\t\tfor storageHash, data := range storage {\n\t\t\tif origin, ok := slots[storageHash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 265,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(data) - len(origin)\n\t\t\t\tstorageOverwrites.add(2*common.HashLength + len(origin))\n\t\t\t} else {\n\t\t\t\tdelta += 2*common.HashLength + len(data)\n\t\t\t}\n\t\t\tslots[storageHash] = data\n\t\t}\n\t}\n\taccountOverwrites.report(gcAccountMeter, gcAccountBytesMeter)\n\tstorageOverwrites.report(gcStorageMeter, gcStorageBytesMeter)\n\ts.clearLists()\n\ts.updateSize(delta)\n}\n\n// revertTo takes the original value of accounts and storages as input and reverts\n// the latest state transition applied on the state set.\n//\n// Notably, this operation may result in the set containing more entries after a\n// revert. For example, if account x did not exist and was created during transition\n// w, reverting w will retain an x=nil entry in the set. And also if account x along\n// with its storage slots was deleted in the transition w, reverting w will retain\n// a list of additional storage slots with their original value.\nfunc (s *stateSet) revertTo(accountOrigin map[common.Hash][]byte, storageOrigin map[common.Hash]map[common.Hash][]byte) {\n\tvar delta int // size tracking\n\tfor addrHash, blob := range accountOrigin {\n\t\tdata, ok := s.accountData[addrHash]\n\t\tif !ok {\n\t\t\tpanic(fmt.Sprintf(\"non-existent account for reverting, %x\", addrHash))\n\t\t}\n\t\tif len(data) == 0 && len(blob) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"invalid account mutation (null to null), %x\", addrHash))\n\t\t}\n\t\tdelta += len(blob) - len(data)\n\t\ts.accountData[addrHash] = blob\n\t}\n\t// Overwrite the storage data with original value blindly\n\tfor addrHash, storage := range storageOrigin {\n\t\tslots := s.storageData[addrHash]\n\t\tif len(slots) == 0 {\n\t\t\tpanic(fmt.Sprintf(\"non-existent storage set for reverting, %x\", addrHash))\n\t\t}\n\t\tfor storageHash, blob := range storage {\n\t\t\tdata, ok := slots[storageHash]\n\t\t\tif !ok {\n\t\t\t\tpanic(fmt.Sprintf(\"non-existent storage slot for reverting, %x-%x\", addrHash, storageHash))\n\t\t\t}\n\t\t\tif len(blob) == 0 && len(data) == 0 {\n\t\t\t\tpanic(fmt.Sprintf(\"invalid storage slot mutation (null to null), %x-%x\", addrHash, storageHash))\n\t\t\t}\n\t\t\tdelta += len(blob) - len(data)\n\t\t\tslots[storageHash] = blob\n\t\t}\n\t}\n\ts.clearLists()\n\ts.updateSize(delta)\n}\n\n// updateSize updates the total cache size by the given delta.\nfunc (s *stateSet) updateSize(delta int) {\n\tsize := int64(s.size) + int64(delta)\n\tif size >= 0 {\n\t\ts.size = uint64(size)\n\t\treturn\n\t}\n\tlog.Error(\"Stateset size underflow\", \"prev\", common.StorageSize(s.size), \"delta\", common.StorageSize(delta))\n\ts.size = 0\n}\n\n// encode serializes the content of state set into the provided writer.\nfunc (s *stateSet) encode(w io.Writer) error {\n\t// Encode accounts\n\tif err := rlp.Encode(w, s.rawStorageKey)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/triedb/pathdb/states.go",
          "line": 440,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(slots) * len(rawdb.SnapshotStoragePrefix)\n\t}\n\treturn m + int(s.size)\n}\n\n// StateSetWithOrigin wraps the state set with additional original values of the\n// mutated states.\ntype StateSetWithOrigin struct {\n\t*stateSet\n\n\t// accountOrigin represents the account data before the state transition,\n\t// corresponding to both the accountData and destructSet. It's keyed by the\n\t// account address. The nil value means the account was not present before.\n\taccountOrigin map[common.Address][]byte\n\n\t// storageOrigin represents the storage data before the state transition,\n\t// corresponding to storageData and deleted slots of destructSet. It's keyed\n\t// by the account address and slot key hash. The nil value means the slot was\n\t// not present.\n\tstorageOrigin map[common.Address]map[common.Hash][]byte\n\n\t// memory size of the state data (accountOrigin and storageOrigin)\n\tsize uint64\n}\n\n// NewStateSetWithOrigin constructs the state set with the provided data.\nfunc NewStateSetWithOrigin(accounts map[common.Hash][]byte, storages map[common.Hash]map[common.Hash][]byte, accountOrigin map[common.Address][]byte, storageOrigin map[common.Address]map[common.Hash][]byte, rawStorageKey bool) *StateSetWithOrigin {\n\t// Don't panic for the lazy callers, initialize the nil maps instead.\n\tif accountOrigin == nil {\n\t\taccountOrigin = make(map[common.Address][]byte)\n\t}\n\tif storageOrigin == nil {\n\t\tstorageOrigin = make(map[common.Address]map[common.Hash][]byte)\n\t}\n\t// Count the memory size occupied by the set. Note that each slot key here\n\t// uses 2*common.HashLength to keep consistent with the calculation method\n\t// of stateSet.\n\tvar size int\n\tfor _, data := range accountOrigin {\n\t\tsize += common.HashLength + len(data)\n\t}\n\tfor _, slots := range storageOrigin {\n\t\tfor _, data := range slots {\n\t\t\tsize += 2*common.HashLength + len(data)\n\t\t}\n\t}\n\tset := newStates(accounts, storages, rawStorageKey)\n\treturn &StateSetWithOrigin{\n\t\tstateSet:      set,\n\t\taccountOrigin: accountOrigin,\n\t\tstorageOrigin: storageOrigin,\n\t\tsize:          set.size + uint64(size),\n\t}\n}\n\n// encode serializes the content of state set into the provided writer.\nfunc (s *StateSetWithOrigin) encode(w io.Writer) error {\n\t// Encode state set\n\tif err := s.stateSet.encode(w)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 69,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(n.Blob) + len(path))\n\t}\n\tfor _, subset := range s.storageNodes {\n\t\tfor path, n := range subset {\n\t\t\tsize += uint64(common.HashLength + len(n.Blob) + len(path))\n\t\t}\n\t}\n\ts.size = size\n}\n\n// updateSize updates the total cache size by the given delta.\nfunc (s *nodeSet) updateSize(delta int64) {\n\tsize := int64(s.size) + delta\n\tif size >= 0 {\n\t\ts.size = uint64(size)\n\t\treturn\n\t}\n\tlog.Error(\"Nodeset size underflow\", \"prev\", common.StorageSize(s.size), \"delta\", common.StorageSize(delta))\n\ts.size = 0\n}\n\n// node retrieves the trie node with node path and its trie identifier.\nfunc (s *nodeSet) node(owner common.Hash, path []byte) (*trienode.Node, bool) {\n\t// Account trie node\n\tif owner == (common.Hash{}) {\n\t\tn, ok := s.accountNodes[string(path)]\n\t\treturn n, ok\n\t}\n\t// Storage trie node\n\tsubset, ok := s.storageNodes[owner]\n\tif !ok {\n\t\treturn nil, false\n\t}\n\tn, ok := subset[string(path)]\n\treturn n, ok\n}\n\n// merge integrates the provided dirty nodes into the set. The provided nodeset\n// will remain unchanged, as it may still be referenced by other layers.\nfunc (s *nodeSet) merge(set *nodeSet) {\n\tvar (\n\t\tdelta     int64   // size difference resulting from node merging\n\t\toverwrite counter // counter of nodes being overwritten\n\t)\n\n\t// Merge account nodes\n\tfor path, n := range set.accountNodes {\n\t\tif orig, exist := s.accountNodes[path]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 117,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(len(n.Blob) + len(path))\n\t\t} else {\n\t\t\tdelta += int64(len(n.Blob) - len(orig.Blob))\n\t\t\toverwrite.add(len(orig.Blob) + len(path))\n\t\t}\n\t\ts.accountNodes[path] = n\n\t}\n\n\t// Merge storage nodes\n\tfor owner, subset := range set.storageNodes {\n\t\tcurrent, exist := s.storageNodes[owner]\n\t\tif !exist {\n\t\t\tfor path, n := range subset {\n\t\t\t\tdelta += int64(common.HashLength + len(n.Blob) + len(path))\n\t\t\t}\n\t\t\t// Perform a shallow copy of the map for the subset instead of claiming it\n\t\t\t// directly from the provided nodeset to avoid potential concurrent map\n\t\t\t// read/write issues. The nodes belonging to the original diff layer remain\n\t\t\t// accessible even after merging. Therefore, ownership of the nodes map\n\t\t\t// should still belong to the original layer, and any modifications to it\n\t\t\t// should be prevented.\n\t\t\ts.storageNodes[owner] = maps.Clone(subset)\n\t\t\tcontinue\n\t\t}\n\t\tfor path, n := range subset {\n\t\t\tif orig, exist := current[path]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 143,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= int64(common.HashLength + len(n.Blob) + len(path))\n\t\t\t} else {\n\t\t\t\tdelta += int64(len(n.Blob) - len(orig.Blob))\n\t\t\t\toverwrite.add(common.HashLength + len(orig.Blob) + len(path))\n\t\t\t}\n\t\t\tcurrent[path] = n\n\t\t}\n\t\ts.storageNodes[owner] = current\n\t}\n\toverwrite.report(gcTrieNodeMeter, gcTrieNodeBytesMeter)\n\ts.updateSize(delta)\n}\n\n// revertTo merges the provided trie nodes into the set. This should reverse the\n// changes made by the most recent state transition.\nfunc (s *nodeSet) revertTo(db ethdb.KeyValueReader, nodes map[common.Hash]map[string]*trienode.Node) {\n\tvar delta int64\n\tfor owner, subset := range nodes {\n\t\tif owner == (common.Hash{}) {\n\t\t\t// Account trie nodes\n\t\t\tfor path, n := range subset {\n\t\t\t\torig, ok := s.accountNodes[path]\n\t\t\t\tif !ok {\n\t\t\t\t\tblob := rawdb.ReadAccountTrieNode(db, []byte(path))\n\t\t\t\t\tif bytes.Equal(blob, n.Blob) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tpanic(fmt.Sprintf(\"non-existent account node (%v) blob: %v\", path, crypto.Keccak256Hash(n.Blob).Hex()))\n\t\t\t\t}\n\t\t\t\ts.accountNodes[path] = n\n\t\t\t\tdelta += int64(len(n.Blob)) - int64(len(orig.Blob))\n\t\t\t}\n\t\t} else {\n\t\t\t// Storage trie nodes\n\t\t\tcurrent, ok := s.storageNodes[owner]\n\t\t\tif !ok {\n\t\t\t\tpanic(fmt.Sprintf(\"non-existent subset (%x)\", owner))\n\t\t\t}\n\t\t\tfor path, n := range subset {\n\t\t\t\torig, ok := current[path]\n\t\t\t\tif !ok {\n\t\t\t\t\tblob := rawdb.ReadStorageTrieNode(db, owner, []byte(path))\n\t\t\t\t\tif bytes.Equal(blob, n.Blob) {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tpanic(fmt.Sprintf(\"non-existent storage node (%x %v) blob: %v\", owner, path, crypto.Keccak256Hash(n.Blob).Hex()))\n\t\t\t\t}\n\t\t\t\tcurrent[path] = n\n\t\t\t\tdelta += int64(len(n.Blob)) - int64(len(orig.Blob))\n\t\t\t}\n\t\t}\n\t}\n\ts.updateSize(delta)\n}\n\n// journalNode represents a trie node persisted in the journal.\ntype journalNode struct {\n\tPath []byte // Path of the node in the trie\n\tBlob []byte // RLP-encoded trie node blob, nil means the node is deleted\n}\n\n// journalNodes represents a list trie nodes belong to a single account\n// or the main account trie.\ntype journalNodes struct {\n\tOwner common.Hash\n\tNodes []journalNode\n}\n\n// encode serializes the content of trie nodes into the provided writer.\nfunc (s *nodeSet) encode(w io.Writer) error {\n\tnodes := make([]journalNodes, 0, len(s.storageNodes)+1)\n\n\t// Encode account nodes\n\tif len(s.accountNodes) > 0 {\n\t\tentry := journalNodes{Owner: common.Hash{}}\n\t\tfor path, node := range s.accountNodes {\n\t\t\tentry.Nodes = append(entry.Nodes, journalNode{\n\t\t\t\tPath: []byte(path),\n\t\t\t\tBlob: node.Blob,\n\t\t\t})\n\t\t}\n\t\tnodes = append(nodes, entry)\n\t}\n\t// Encode storage nodes\n\tfor owner, subset := range s.storageNodes {\n\t\tentry := journalNodes{Owner: owner}\n\t\tfor path, node := range subset {\n\t\t\tentry.Nodes = append(entry.Nodes, journalNode{\n\t\t\t\tPath: []byte(path),\n\t\t\t\tBlob: node.Blob,\n\t\t\t})\n\t\t}\n\t\tnodes = append(nodes, entry)\n\t}\n\treturn rlp.Encode(w, nodes)\n}\n\n// decode deserializes the content from the rlp stream into the nodeset.\nfunc (s *nodeSet) decode(r *rlp.Stream) error {\n\tvar encoded []journalNodes\n\tif err := r.Decode(&encoded)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/triedb/pathdb/nodes.go",
          "line": 298,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(s.accountNodes) * len(rawdb.TrieNodeAccountPrefix) // database key prefix\n\tfor _, nodes := range s.storageNodes {\n\t\tm += len(nodes) * (len(rawdb.TrieNodeStoragePrefix)) // database key prefix\n\t}\n\treturn m + int(s.size)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/triedb/pathdb/history_inspect.go",
          "line": 74,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 {\n\t\t// The entire history object is decoded, although it's unnecessary for\n\t\t// account inspection. TODO(rjl493456442) optimization is worthwhile.\n\t\th, err := readHistory(freezer, id)\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tif id == start {\n\t\t\tstats.Start = h.meta.block\n\t\t}\n\t\tif id == end {\n\t\t\tstats.End = h.meta.block\n\t\t}\n\t\tonHistory(h, stats)\n\n\t\tif time.Since(logged) > time.Second*8 {\n\t\t\tlogged = time.Now()\n\t\t\teta := float64(time.Since(init)) / float64(id-start+1) * float64(end-id)\n\t\t\tlog.Info(\"Inspecting state history\", \"checked\", id-start+1, \"left\", end-id, \"elapsed\", common.PrettyDuration(time.Since(init)), \"eta\", common.PrettyDuration(eta))\n\t\t}\n\t}\n\tlog.Info(\"Inspected state history\", \"total\", end-start+1, \"elapsed\", common.PrettyDuration(time.Since(init)))\n\treturn stats, nil\n}\n\n// accountHistory inspects the account history within the range.\nfunc accountHistory(freezer ethdb.AncientReader, address common.Address, start, end uint64) (*HistoryStats, error) {\n\treturn inspectHistory(freezer, start, end, func(h *history, stats *HistoryStats) {\n\t\tblob, exists := h.accounts[address]\n\t\tif !exists {\n\t\t\treturn\n\t\t}\n\t\tstats.Blocks = append(stats.Blocks, h.meta.block)\n\t\tstats.Origins = append(stats.Origins, blob)\n\t})\n}\n\n// storageHistory inspects the storage history within the range.\nfunc storageHistory(freezer ethdb.AncientReader, address common.Address, slot common.Hash, start uint64, end uint64) (*HistoryStats, error) {\n\tslotHash := crypto.Keccak256Hash(slot.Bytes())\n\treturn inspectHistory(freezer, start, end, func(h *history, stats *HistoryStats) {\n\t\tslots, exists := h.storages[address]\n\t\tif !exists {\n\t\t\treturn\n\t\t}\n\t\tkey := slotHash\n\t\tif h.meta.version != stateHistoryV0 {\n\t\t\tkey = slot\n\t\t}\n\t\tblob, exists := slots[key]\n\t\tif !exists {\n\t\t\treturn\n\t\t}\n\t\tstats.Blocks = append(stats.Blocks, h.meta.block)\n\t\tstats.Origins = append(stats.Origins, blob)\n\t})\n}\n\n// historyRange returns the block number range of local state histories.\nfunc historyRange(freezer ethdb.AncientReader) (uint64, uint64, error) {\n\t// Load the id of the first history object in local store.\n\ttail, err := freezer.Tail()\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tfirst := tail + 1\n\n\t// Load the id of the last history object in local store.\n\thead, err := freezer.Ancients()\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tlast := head - 1\n\n\tfh, err := readHistory(freezer, first)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\tlh, err := readHistory(freezer, last)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\treturn fh.meta.block, lh.meta.block, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vote/vote_journal.go",
          "line": 82,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif err = walLog.Write(lastIndex, vote)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 178,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 284,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 348,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(blockHash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vote/vote_pool.go",
          "line": 291,
          "category": "spectral_anomalies",
          "pattern": "votes\\[.*\\]\\s*=",
          "match": "Votes[blockHash] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vote/vote_pool_test.go",
          "line": 342,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tif curNumber == 294 {\n\t\t\tfutureBlockHash = bs[0].Hash()\n\t\t\tfutureVotesMap := votePool.futureVotes\n\t\t\tvoteBox := futureVotesMap[common.Hash{}]\n\t\t\tfutureVotesMap[futureBlockHash] = voteBox\n\t\t\tdelete(futureVotesMap, common.Hash{})\n\t\t\tfutureVotesPq := votePool.futureVotesPq\n\t\t\tfutureVotesPq.Peek().TargetHash = futureBlockHash\n\t\t}\n\t\tif _, err := chain.InsertChain(bs)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vote/vote_pool_test.go",
          "line": 364,
          "category": "spectral_anomalies",
          "pattern": "votes\\[.*\\]\\s*=",
          "match": "Votes[futureBlockHash] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/tracing/journal_test.go",
          "line": 273,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/tracing/gen_balance_change_reason_stringer.go",
          "line": 46,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 210\n\t\treturn _BalanceChangeReason_name_1[_BalanceChangeReason_index_1[i]:_BalanceChangeReason_index_1[i+1]]\n\tdefault:\n\t\treturn \"BalanceChangeReason(\" + strconv.FormatInt(int64(i), 10) + \")\"\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/monitor/malicious_vote_monitor.go",
          "line": 40,
          "category": "spectral_anomalies",
          "pattern": "votes\\[.*\\]\\s*=",
          "match": "Votes[newVote.VoteAddress] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/deposit.go",
          "line": 49,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 48 + 16 + 32\n\t// WithdrawalCredentials is 32 bytes. Read that value then skip over next\n\t// length.\n\tcopy(request[withdrawalCredOffset:], data[b:b+32])\n\tb += 32 + 32\n\t// Amount is 8 bytes, but it is padded to 32. Skip over it and the next\n\t// length.\n\tcopy(request[amountOffset:], data[b:b+8])\n\tb += 8 + 24 + 32\n\t// Signature is 96 bytes. Skip over it and the next length.\n\tcopy(request[signatureOffset:], data[b:b+96])\n\tb += 96 + 32\n\t// Index is 8 bytes.\n\tcopy(request[indexOffset:], data[b:b+8])\n\treturn request, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/types_test.go",
          "line": 31,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(p)\n\treturn len(p), nil\n}\n\nfunc BenchmarkEncodeRLP(b *testing.B) {\n\tbenchRLP(b, true)\n}\n\nfunc BenchmarkDecodeRLP(b *testing.B) {\n\tbenchRLP(b, false)\n}\n\nfunc benchRLP(b *testing.B, encode bool) {\n\tkey, _ := crypto.HexToECDSA(\"b71c71a67e1177ad4e901695e1b4b9ee17ae16c6668d313eac2f96dbcda3f291\")\n\tto := common.HexToAddress(\"0x00000000000000000000000000000000deadbeef\")\n\tsigner := NewLondonSigner(big.NewInt(1337))\n\tfor _, tc := range []struct {\n\t\tname string\n\t\tobj  interface{}\n\t}{\n\t\t{\n\t\t\t\"legacy-header\",\n\t\t\t&Header{\n\t\t\t\tDifficulty: big.NewInt(10000000000),\n\t\t\t\tNumber:     big.NewInt(1000),\n\t\t\t\tGasLimit:   8_000_000,\n\t\t\t\tGasUsed:    8_000_000,\n\t\t\t\tTime:       555,\n\t\t\t\tExtra:      make([]byte, 32),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"london-header\",\n\t\t\t&Header{\n\t\t\t\tDifficulty: big.NewInt(10000000000),\n\t\t\t\tNumber:     big.NewInt(1000),\n\t\t\t\tGasLimit:   8_000_000,\n\t\t\t\tGasUsed:    8_000_000,\n\t\t\t\tTime:       555,\n\t\t\t\tExtra:      make([]byte, 32),\n\t\t\t\tBaseFee:    big.NewInt(10000000000),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"receipt-for-storage\",\n\t\t\t&ReceiptForStorage{\n\t\t\t\tStatus:            ReceiptStatusSuccessful,\n\t\t\t\tCumulativeGasUsed: 0x888888888,\n\t\t\t\tLogs:              make([]*Log, 0),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"receipt-full\",\n\t\t\t&Receipt{\n\t\t\t\tStatus:            ReceiptStatusSuccessful,\n\t\t\t\tCumulativeGasUsed: 0x888888888,\n\t\t\t\tLogs:              make([]*Log, 0),\n\t\t\t},\n\t\t},\n\t\t{\n\t\t\t\"legacy-transaction\",\n\t\t\tMustSignNewTx(key, signer,\n\t\t\t\t&LegacyTx{\n\t\t\t\t\tNonce:    1,\n\t\t\t\t\tGasPrice: big.NewInt(500),\n\t\t\t\t\tGas:      1000000,\n\t\t\t\t\tTo:       &to,\n\t\t\t\t\tValue:    big.NewInt(1),\n\t\t\t\t}),\n\t\t},\n\t\t{\n\t\t\t\"access-transaction\",\n\t\t\tMustSignNewTx(key, signer,\n\t\t\t\t&AccessListTx{\n\t\t\t\t\tNonce:    1,\n\t\t\t\t\tGasPrice: big.NewInt(500),\n\t\t\t\t\tGas:      1000000,\n\t\t\t\t\tTo:       &to,\n\t\t\t\t\tValue:    big.NewInt(1),\n\t\t\t\t}),\n\t\t},\n\t\t{\n\t\t\t\"1559-transaction\",\n\t\t\tMustSignNewTx(key, signer,\n\t\t\t\t&DynamicFeeTx{\n\t\t\t\t\tNonce:     1,\n\t\t\t\t\tGas:       1000000,\n\t\t\t\t\tTo:        &to,\n\t\t\t\t\tValue:     big.NewInt(1),\n\t\t\t\t\tGasTipCap: big.NewInt(500),\n\t\t\t\t\tGasFeeCap: big.NewInt(500),\n\t\t\t\t}),\n\t\t},\n\t} {\n\t\tif encode {\n\t\t\tb.Run(tc.name, func(b *testing.B) {\n\t\t\t\tb.ReportAllocs()\n\t\t\t\tvar null = &devnull{}\n\t\t\t\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/transaction.go",
          "line": 592,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rlp.ListSize(sc.encodedSize())\n\t}\n\n\t// For typed transactions, the encoding also includes the leading type byte.\n\tif tx.Type() != LegacyTxType {\n\t\tsize += 1\n\t}\n\n\ttx.size.Store(size)\n\treturn size\n}\n\n// WithSignature returns a new transaction with the given signature.\n// This signature needs to be in the [R || S || V] format where V is 0 or 1.\nfunc (tx *Transaction) WithSignature(signer Signer, sig []byte) (*Transaction, error) {\n\tr, s, v, err := signer.SignatureValues(tx, sig)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif r == nil || s == nil || v == nil {\n\t\treturn nil, fmt.Errorf(\"%w: r: %s, s: %s, v: %s\", ErrInvalidSig, r, s, v)\n\t}\n\tcpy := tx.inner.copy()\n\tcpy.setSignatureValues(signer.ChainID(), v, r, s)\n\treturn &Transaction{inner: cpy, time: tx.time}, nil\n}\n\n// Transactions implements DerivableList for transactions.\ntype Transactions []*Transaction\n\n// Len returns the length of s.\nfunc (s Transactions) Len() int { return len(s) }\n\n// EncodeIndex encodes the i'th transaction to w. Note that this does not check for errors\n// because we assume that *Transaction will only ever contain valid txs that were either\n// constructed by decoding or via public API in this package.\nfunc (s Transactions) EncodeIndex(i int, w *bytes.Buffer) {\n\ttx := s[i]\n\tif tx.Type() == LegacyTxType {\n\t\trlp.Encode(w, tx.inner)\n\t} else {\n\t\ttx.encodeTyped(w)\n\t}\n}\n\n// TxDifference returns a new set of transactions that are present in a but not in b.\nfunc TxDifference(a, b Transactions) Transactions {\n\tkeep := make(Transactions, 0, len(a))\n\n\tremove := make(map[common.Hash]struct{}, b.Len())\n\tfor _, tx := range b {\n\t\tremove[tx.Hash()] = struct{}{}\n\t}\n\n\tfor _, tx := range a {\n\t\tif _, ok := remove[tx.Hash()]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/receipt.go",
          "line": 260,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(len(r.Logs)) * common.StorageSize(unsafe.Sizeof(Log{}))\n\tfor _, log := range r.Logs {\n\t\tsize += common.StorageSize(len(log.Topics)*common.HashLength + len(log.Data))\n\t}\n\treturn size\n}\n\n// DeriveReceiptContext holds the contextual information needed to derive a receipt\ntype DeriveReceiptContext struct {\n\tBlockHash    common.Hash\n\tBlockNumber  uint64\n\tBlockTime    uint64\n\tBaseFee      *big.Int\n\tBlobGasPrice *big.Int\n\tGasUsed      uint64\n\tLogIndex     uint // Number of logs in the block until this receipt\n\tTx           *Transaction\n\tTxIndex      uint\n}\n\n// DeriveFields fills the receipt with computed fields based on consensus\n// data and contextual infos like containing block and transactions.\nfunc (r *Receipt) DeriveFields(signer Signer, context DeriveReceiptContext) {\n\t// The transaction type and hash can be retrieved from the transaction itself\n\tr.Type = context.Tx.Type()\n\tr.TxHash = context.Tx.Hash()\n\tr.GasUsed = context.GasUsed\n\tr.EffectiveGasPrice = context.Tx.inner.effectiveGasPrice(new(big.Int), context.BaseFee)\n\n\t// EIP-4844 blob transaction fields\n\tif context.Tx.Type() == BlobTxType {\n\t\tr.BlobGasUsed = context.Tx.BlobGas()\n\t\tr.BlobGasPrice = context.BlobGasPrice\n\t}\n\n\t// Block location fields\n\tr.BlockHash = context.BlockHash\n\tr.BlockNumber = new(big.Int).SetUint64(context.BlockNumber)\n\tr.TransactionIndex = context.TxIndex\n\n\t// The contract address can be derived from the transaction itself\n\tif context.Tx.To() == nil {\n\t\t// Deriving the signer is expensive, only do if it's actually needed\n\t\tfrom, _ := Sender(signer, context.Tx)\n\t\tr.ContractAddress = crypto.CreateAddress(from, context.Tx.Nonce())\n\t} else {\n\t\tr.ContractAddress = common.Address{}\n\t}\n\t// The derived log fields can simply be set from the block and transaction\n\tlogIndex := context.LogIndex\n\tfor j := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/types/receipt.go",
          "line": 411,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint(len(rs[i].Logs))\n\t}\n\treturn nil\n}\n\n// EncodeBlockReceiptLists encodes a list of block receipt lists into RLP.\nfunc EncodeBlockReceiptLists(receipts []Receipts) []rlp.RawValue {\n\tvar storageReceipts []*ReceiptForStorage\n\tresult := make([]rlp.RawValue, len(receipts))\n\tfor i, receipt := range receipts {\n\t\tstorageReceipts = storageReceipts[:0]\n\t\tfor _, r := range receipt {\n\t\t\tstorageReceipts = append(storageReceipts, (*ReceiptForStorage)(r))\n\t\t}\n\t\tbytes, err := rlp.EncodeToBytes(storageReceipts)\n\t\tif err != nil {\n\t\t\tlog.Crit(\"Failed to encode block receipts\", \"err\", err)\n\t\t}\n\t\tresult[i] = bytes\n\t}\n\treturn result\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/tx_blob.go",
          "line": 101,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rlp.BytesSize(sc.Blobs[i][:])\n\t}\n\tfor i := range sc.Commitments {\n\t\tcommitments += rlp.BytesSize(sc.Commitments[i][:])\n\t}\n\tfor i := range sc.Proofs {\n\t\tproofs += rlp.BytesSize(sc.Proofs[i][:])\n\t}\n\treturn rlp.ListSize(blobs) + rlp.ListSize(commitments) + rlp.ListSize(proofs)\n}\n\n// ValidateBlobCommitmentHashes checks whether the given hashes correspond to the\n// commitments in the sidecar\nfunc (sc *BlobTxSidecar) ValidateBlobCommitmentHashes(hashes []common.Hash) error {\n\tif len(sc.Commitments) != len(hashes) {\n\t\treturn fmt.Errorf(\"invalid number of %d blob commitments compared to %d blob hashes\", len(sc.Commitments), len(hashes))\n\t}\n\thasher := sha256.New()\n\tfor i, vhash := range hashes {\n\t\tcomputed := kzg4844.CalcBlobHashV1(hasher, &sc.Commitments[i])\n\t\tif vhash != computed {\n\t\t\treturn fmt.Errorf(\"blob %d: computed hash %#x mismatches transaction one %#x\", i, computed, vhash)\n\t\t}\n\t}\n\treturn nil\n}\n\n// blobTxWithBlobs represents blob tx with its corresponding sidecar.\n// This is an interface because sidecars are versioned.\ntype blobTxWithBlobs interface {\n\ttx() *BlobTx\n\tassign(*BlobTxSidecar) error\n}\n\ntype blobTxWithBlobsV0 struct {\n\tBlobTx      *BlobTx\n\tBlobs       []kzg4844.Blob\n\tCommitments []kzg4844.Commitment\n\tProofs      []kzg4844.Proof\n}\n\ntype blobTxWithBlobsV1 struct {\n\tBlobTx      *BlobTx\n\tVersion     byte\n\tBlobs       []kzg4844.Blob\n\tCommitments []kzg4844.Commitment\n\tProofs      []kzg4844.Proof\n}\n\nfunc (btx *blobTxWithBlobsV0) tx() *BlobTx {\n\treturn btx.BlobTx\n}\n\nfunc (btx *blobTxWithBlobsV0) assign(sc *BlobTxSidecar) error {\n\tsc.Version = 0\n\tsc.Blobs = btx.Blobs\n\tsc.Commitments = btx.Commitments\n\tsc.Proofs = btx.Proofs\n\treturn nil\n}\n\nfunc (btx *blobTxWithBlobsV1) tx() *BlobTx {\n\treturn btx.BlobTx\n}\n\nfunc (btx *blobTxWithBlobsV1) assign(sc *BlobTxSidecar) error {\n\t// NOTE(BSC): Upstream geth supports both Version 0 and 1 sidecars.\n\t// BSC only supports Version 0, as EIP-7594 (cell proofs) is not enabled yet.\n\tdisableEIP7594 := true\n\tif disableEIP7594 || btx.Version != 1 {\n\t\treturn fmt.Errorf(\"unsupported blob tx version %d\", btx.Version)\n\t}\n\tsc.Version = 1\n\tsc.Blobs = btx.Blobs\n\tsc.Commitments = btx.Commitments\n\tsc.Proofs = btx.Proofs\n\treturn nil\n}\n\n// copy creates a deep copy of the transaction data and initializes all fields.\nfunc (tx *BlobTx) copy() TxData {\n\tcpy := &BlobTx{\n\t\tNonce: tx.Nonce,\n\t\tTo:    tx.To,\n\t\tData:  common.CopyBytes(tx.Data),\n\t\tGas:   tx.Gas,\n\t\t// These are copied below.\n\t\tAccessList: make(AccessList, len(tx.AccessList)),\n\t\tBlobHashes: make([]common.Hash, len(tx.BlobHashes)),\n\t\tValue:      new(uint256.Int),\n\t\tChainID:    new(uint256.Int),\n\t\tGasTipCap:  new(uint256.Int),\n\t\tGasFeeCap:  new(uint256.Int),\n\t\tBlobFeeCap: new(uint256.Int),\n\t\tV:          new(uint256.Int),\n\t\tR:          new(uint256.Int),\n\t\tS:          new(uint256.Int),\n\t}\n\tcopy(cpy.AccessList, tx.AccessList)\n\tcopy(cpy.BlobHashes, tx.BlobHashes)\n\n\tif tx.Value != nil {\n\t\tcpy.Value.Set(tx.Value)\n\t}\n\tif tx.ChainID != nil {\n\t\tcpy.ChainID.Set(tx.ChainID)\n\t}\n\tif tx.GasTipCap != nil {\n\t\tcpy.GasTipCap.Set(tx.GasTipCap)\n\t}\n\tif tx.GasFeeCap != nil {\n\t\tcpy.GasFeeCap.Set(tx.GasFeeCap)\n\t}\n\tif tx.BlobFeeCap != nil {\n\t\tcpy.BlobFeeCap.Set(tx.BlobFeeCap)\n\t}\n\tif tx.V != nil {\n\t\tcpy.V.Set(tx.V)\n\t}\n\tif tx.R != nil {\n\t\tcpy.R.Set(tx.R)\n\t}\n\tif tx.S != nil {\n\t\tcpy.S.Set(tx.S)\n\t}\n\tif tx.Sidecar != nil {\n\t\tcpy.Sidecar = &BlobTxSidecar{\n\t\t\tVersion:     tx.Sidecar.Version,\n\t\t\tBlobs:       slices.Clone(tx.Sidecar.Blobs),\n\t\t\tCommitments: slices.Clone(tx.Sidecar.Commitments),\n\t\t\tProofs:      slices.Clone(tx.Sidecar.Proofs),\n\t\t}\n\t}\n\treturn cpy\n}\n\n// accessors for innerTx.\nfunc (tx *BlobTx) txType() byte           { return BlobTxType }\nfunc (tx *BlobTx) chainID() *big.Int      { return tx.ChainID.ToBig() }\nfunc (tx *BlobTx) accessList() AccessList { return tx.AccessList }\nfunc (tx *BlobTx) data() []byte           { return tx.Data }\nfunc (tx *BlobTx) gas() uint64            { return tx.Gas }\nfunc (tx *BlobTx) gasFeeCap() *big.Int    { return tx.GasFeeCap.ToBig() }\nfunc (tx *BlobTx) gasTipCap() *big.Int    { return tx.GasTipCap.ToBig() }\nfunc (tx *BlobTx) gasPrice() *big.Int     { return tx.GasFeeCap.ToBig() }\nfunc (tx *BlobTx) value() *big.Int        { return tx.Value.ToBig() }\nfunc (tx *BlobTx) nonce() uint64          { return tx.Nonce }\nfunc (tx *BlobTx) to() *common.Address    { tmp := tx.To",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/tx_access_list.go",
          "line": 42,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(tuple.StorageKeys)\n\t}\n\treturn sum\n}\n\n// AccessListTx is the data of EIP-2930 access list transactions.\ntype AccessListTx struct {\n\tChainID    *big.Int        // destination chain ID\n\tNonce      uint64          // nonce of sender account\n\tGasPrice   *big.Int        // wei per gas\n\tGas        uint64          // gas limit\n\tTo         *common.Address `rlp:\"nil\"` // nil means contract creation\n\tValue      *big.Int        // wei amount\n\tData       []byte          // contract invocation input data\n\tAccessList AccessList      // EIP-2930 access list\n\tV, R, S    *big.Int        // signature values\n}\n\n// copy creates a deep copy of the transaction data and initializes all fields.\nfunc (tx *AccessListTx) copy() TxData {\n\tcpy := &AccessListTx{\n\t\tNonce: tx.Nonce,\n\t\tTo:    copyAddressPtr(tx.To),\n\t\tData:  common.CopyBytes(tx.Data),\n\t\tGas:   tx.Gas,\n\t\t// These are copied below.\n\t\tAccessList: make(AccessList, len(tx.AccessList)),\n\t\tValue:      new(big.Int),\n\t\tChainID:    new(big.Int),\n\t\tGasPrice:   new(big.Int),\n\t\tV:          new(big.Int),\n\t\tR:          new(big.Int),\n\t\tS:          new(big.Int),\n\t}\n\tcopy(cpy.AccessList, tx.AccessList)\n\tif tx.Value != nil {\n\t\tcpy.Value.Set(tx.Value)\n\t}\n\tif tx.ChainID != nil {\n\t\tcpy.ChainID.Set(tx.ChainID)\n\t}\n\tif tx.GasPrice != nil {\n\t\tcpy.GasPrice.Set(tx.GasPrice)\n\t}\n\tif tx.V != nil {\n\t\tcpy.V.Set(tx.V)\n\t}\n\tif tx.R != nil {\n\t\tcpy.R.Set(tx.R)\n\t}\n\tif tx.S != nil {\n\t\tcpy.S.Set(tx.S)\n\t}\n\treturn cpy\n}\n\n// accessors for innerTx.\nfunc (tx *AccessListTx) txType() byte           { return AccessListTxType }\nfunc (tx *AccessListTx) chainID() *big.Int      { return tx.ChainID }\nfunc (tx *AccessListTx) accessList() AccessList { return tx.AccessList }\nfunc (tx *AccessListTx) data() []byte           { return tx.Data }\nfunc (tx *AccessListTx) gas() uint64            { return tx.Gas }\nfunc (tx *AccessListTx) gasPrice() *big.Int     { return tx.GasPrice }\nfunc (tx *AccessListTx) gasTipCap() *big.Int    { return tx.GasPrice }\nfunc (tx *AccessListTx) gasFeeCap() *big.Int    { return tx.GasPrice }\nfunc (tx *AccessListTx) value() *big.Int        { return tx.Value }\nfunc (tx *AccessListTx) nonce() uint64          { return tx.Nonce }\nfunc (tx *AccessListTx) to() *common.Address    { return tx.To }\n\nfunc (tx *AccessListTx) effectiveGasPrice(dst *big.Int, baseFee *big.Int) *big.Int {\n\treturn dst.Set(tx.GasPrice)\n}\n\nfunc (tx *AccessListTx) rawSignatureValues() (v, r, s *big.Int) {\n\treturn tx.V, tx.R, tx.S\n}\n\nfunc (tx *AccessListTx) setSignatureValues(chainID, v, r, s *big.Int) {\n\ttx.ChainID, tx.V, tx.R, tx.S = chainID, v, r, s\n}\n\nfunc (tx *AccessListTx) encode(b *bytes.Buffer) error {\n\treturn rlp.Encode(b, tx)\n}\n\nfunc (tx *AccessListTx) decode(input []byte) error {\n\treturn rlp.DecodeBytes(input, tx)\n}\n\nfunc (tx *AccessListTx) sigHash(chainID *big.Int) common.Hash {\n\treturn prefixedRlpHash(\n\t\tAccessListTxType,\n\t\t[]any{\n\t\t\tchainID,\n\t\t\ttx.Nonce,\n\t\t\ttx.GasPrice,\n\t\t\ttx.Gas,\n\t\t\ttx.To,\n\t\t\ttx.Value,\n\t\t\ttx.Data,\n\t\t\ttx.AccessList,\n\t\t})\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/bloom9_test.go",
          "line": 126,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tcopy(rLarge[i:], rSmall)\n\t}\n\tb.Run(\"small-createbloom\", func(b *testing.B) {\n\t\tb.ReportAllocs()\n\t\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/types/block.go",
          "line": 518,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeCounter(len(b))\n\treturn len(b), nil\n}\n\nfunc CalcUncleHash(uncles []*Header) common.Hash {\n\tif len(uncles) == 0 {\n\t\treturn EmptyUncleHash\n\t}\n\treturn rlpHash(uncles)\n}\n\n// CalcRequestsHash creates the block requestsHash value for a list of requests.\nfunc CalcRequestsHash(requests [][]byte) common.Hash {\n\th1, h2 := sha256.New(), sha256.New()\n\tvar buf common.Hash\n\tfor _, item := range requests {\n\t\tif len(item) > 1 { // skip items with only requestType and no data.\n\t\t\th1.Reset()\n\t\t\th1.Write(item)\n\t\t\th2.Write(h1.Sum(buf[:0]))\n\t\t}\n\t}\n\th2.Sum(buf[:0])\n\treturn buf\n}\n\n// NewBlockWithHeader creates a block with the given header data. The\n// header data is copied, changes to header and to the field values\n// will not affect the block.\nfunc NewBlockWithHeader(header *Header) *Block {\n\treturn &Block{header: CopyHeader(header)}\n}\n\n// WithSeal returns a new block with the data from b but the header replaced with\n// the sealed one.\nfunc (b *Block) WithSeal(header *Header) *Block {\n\t// fill sidecars metadata\n\tfor _, sidecar := range b.sidecars {\n\t\tsidecar.BlockNumber = header.Number\n\t\tsidecar.BlockHash = header.Hash()\n\t}\n\treturn &Block{\n\t\theader:       CopyHeader(header),\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twithdrawals:  b.withdrawals,\n\t\twitness:      b.witness,\n\t\tsidecars:     b.sidecars,\n\t}\n}\n\n// WithBody returns a new block with the original header and a deep copy of the\n// provided body.\nfunc (b *Block) WithBody(body Body) *Block {\n\tblock := &Block{\n\t\theader:       b.header,\n\t\ttransactions: slices.Clone(body.Transactions),\n\t\tuncles:       make([]*Header, len(body.Uncles)),\n\t\twithdrawals:  slices.Clone(body.Withdrawals),\n\t\twitness:      b.witness,\n\t\tsidecars:     b.sidecars,\n\t}\n\tfor i := range body.Uncles {\n\t\tblock.uncles[i] = CopyHeader(body.Uncles[i])\n\t}\n\treturn block\n}\n\n// WithWithdrawals returns a copy of the block containing the given withdrawals.\nfunc (b *Block) WithWithdrawals(withdrawals []*Withdrawal) *Block {\n\tblock := &Block{\n\t\theader:       b.header,\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twitness:      b.witness,\n\t\tsidecars:     b.sidecars,\n\t}\n\tif withdrawals != nil {\n\t\tblock.withdrawals = make([]*Withdrawal, len(withdrawals))\n\t\tcopy(block.withdrawals, withdrawals)\n\t}\n\treturn block\n}\n\n// WithSidecars returns a block containing the given blobs.\nfunc (b *Block) WithSidecars(sidecars BlobSidecars) *Block {\n\tblock := &Block{\n\t\theader:       b.header,\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twithdrawals:  b.withdrawals,\n\t\twitness:      b.witness,\n\t}\n\tif sidecars != nil {\n\t\tblock.sidecars = make(BlobSidecars, len(sidecars))\n\t\tcopy(block.sidecars, sidecars)\n\t}\n\treturn block\n}\n\nfunc (b *Block) WithWitness(witness *ExecutionWitness) *Block {\n\treturn &Block{\n\t\theader:       b.header,\n\t\ttransactions: b.transactions,\n\t\tuncles:       b.uncles,\n\t\twithdrawals:  b.withdrawals,\n\t\twitness:      witness,\n\t\tsidecars:     b.sidecars,\n\t}\n}\n\n// Hash returns the keccak256 hash of b's header.\n// The hash is computed on the first call and cached thereafter.\nfunc (b *Block) Hash() common.Hash {\n\tif hash := b.hash.Load()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/table_test.go",
          "line": 111,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\tcount++\n\t\t}\n\t\tif count != expCount {\n\t\t\tt.Fatalf(\"Wrong number of elems, exp %d got %d\", expCount, count)\n\t\t}\n\t\titer.Release()\n\t}\n\t// Test iterators\n\tcheck(db.NewIterator(nil, nil), 6, 0)\n\t// Test iterators with prefix\n\tcheck(db.NewIterator([]byte{0xff, 0xff}, nil), 3, 3)\n\t// Test iterators with start point\n\tcheck(db.NewIterator(nil, []byte{0xff, 0xff, 0x02}), 2, 4)\n\t// Test iterators with prefix and start point\n\tcheck(db.NewIterator([]byte{0xee}, nil), 0, 0)\n\tcheck(db.NewIterator(nil, []byte{0x00}), 6, 0)\n\n\t// Test range deletion\n\tdb.DeleteRange(nil, nil)\n\tfor _, entry := range entries {\n\t\t_, err := db.Get(entry.key)\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Unexpected item after deletion\")\n\t\t}\n\t}\n\t// Test range deletion by batch\n\tbatch = db.NewBatch()\n\tfor _, entry := range entries {\n\t\tbatch.Put(entry.key, entry.value)\n\t}\n\tbatch.Write()\n\tbatch.Reset()\n\tbatch.DeleteRange(nil, nil)\n\tbatch.Write()\n\tfor _, entry := range entries {\n\t\t_, err := db.Get(entry.key)\n\t\tif err == nil {\n\t\t\tt.Fatal(\"Unexpected item after deletion\")\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 145,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 161,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 262,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(nfdb)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.85,
          "confidence": 0.9765,
          "ensemble_confidence": 0.87885
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/rawdb/chain_freezer.go",
          "line": 289,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(nfdb)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.85,
          "confidence": 0.9765,
          "ensemble_confidence": 0.87885
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 72,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(data))\n\t\t// If we've spent too much time already, notify the user of what we're doing\n\t\tif time.Since(logged) > 8*time.Second {\n\t\t\tlog.Info(\"Initializing database from freezer\", \"total\", frozen, \"number\", i, \"hash\", hash, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\tlogged = time.Now()\n\t\t}\n\t}\n\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 229,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(delivery.hashes)\n\t\t\t// If enough data was accumulated in memory or we're at the last block, dump to disk\n\t\t\tif batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\t\tWriteTxIndexTail(batch, lastNum) // Also write the tail here\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 324,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(delivery.hashes)\n\t\t\tblocks++\n\n\t\t\t// If enough data was accumulated in memory or we're at the last block, dump to disk\n\t\t\t// A batch counts the size of deletion as '1', so we need to flush more\n\t\t\t// often than that.\n\t\t\tif blocks%1000 == 0 {\n\t\t\t\tWriteTxIndexTail(batch, nextNum)\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/rawdb/chain_iterator.go",
          "line": 85,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8899999999999999,
          "confidence": 0.9801,
          "ensemble_confidence": 0.88209
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/freezer_test.go",
          "line": 146,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeBatchSize {\n\t\t\t_, err := f.ModifyAncients(func(op ethdb.AncientWriteOp) error {\n\t\t\t\tfor i := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/database.go",
          "line": 554,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\ts.count++\n}\n\nfunc (s *stat) Size() string {\n\treturn s.size.String()\n}\n\nfunc (s *stat) Count() string {\n\treturn s.count.String()\n}\n\nfunc AncientInspect(db ethdb.Database) error {\n\tancientTail, err := db.Tail()\n\tif err != nil {\n\t\treturn err\n\t}\n\tancientHead, err := db.Ancients()\n\tif err != nil {\n\t\treturn err\n\t}\n\tstats := [][]string{\n\t\t{\"Offset/StartBlockNumber\", \"Offset/StartBlockNumber of ancientDB\", counter(ancientTail).String()},\n\t\t{\"Amount of remained items in AncientStore\", \"Remaining items of ancientDB\", counter(ancientHead - ancientTail).String()},\n\t\t{\"The last BlockNumber within ancientDB\", \"The last BlockNumber\", counter(ancientHead - 1).String()},\n\t}\n\ttable := tablewriter.NewWriter(os.Stdout)\n\ttable.SetHeader([]string{\"Database\", \"Category\", \"Items\"})\n\ttable.SetFooter([]string{\"\", \"AncientStore information\", \"\"})\n\ttable.AppendBulk(stats)\n\ttable.Render()\n\n\treturn nil\n}\n\ntype DataType int\n\nconst (\n\tStateDataType DataType = iota\n\tChainDataType\n\tUnknown\n)\n\nfunc DataTypeByKey(key []byte) DataType {\n\tswitch {\n\t// state\n\tcase IsLegacyTrieNode(key, key),\n\t\tbytes.HasPrefix(key, stateIDPrefix) && len(key) == len(stateIDPrefix)+common.HashLength,\n\t\tIsAccountTrieNode(key),\n\t\tIsStorageTrieNode(key):\n\t\treturn StateDataType\n\n\tdefault:\n\t\tfor _, meta := range [][]byte{\n\t\t\tfastTrieProgressKey, persistentStateIDKey, trieJournalKey, snapSyncStatusFlagKey} {\n\t\t\tif bytes.Equal(key, meta) {\n\t\t\t\treturn StateDataType\n\t\t\t}\n\t\t}\n\t\treturn ChainDataType\n\t}\n}\n\n// InspectDatabase traverses the entire database and checks the size\n// of all different categories of data.\nfunc InspectDatabase(db ethdb.Database, keyPrefix, keyStart []byte) error {\n\tit := db.NewIterator(keyPrefix, keyStart)\n\tdefer it.Release()\n\n\tvar trieIter ethdb.Iterator\n\tif db.HasSeparateStateStore() {\n\t\ttrieIter = db.GetStateStore().NewIterator(keyPrefix, nil)\n\t\tdefer trieIter.Release()\n\t}\n\n\tvar (\n\t\tcount  int64\n\t\tstart  = time.Now()\n\t\tlogged = time.Now()\n\n\t\t// Key-value store statistics\n\t\theaders            stat\n\t\tbodies             stat\n\t\treceipts           stat\n\t\ttds                stat\n\t\tnumHashPairings    stat\n\t\tblobSidecars       stat\n\t\thashNumPairings    stat\n\t\tlegacyTries        stat\n\t\tstateLookups       stat\n\t\taccountTries       stat\n\t\tstorageTries       stat\n\t\tcodes              stat\n\t\ttxLookups          stat\n\t\taccountSnaps       stat\n\t\tstorageSnaps       stat\n\t\tpreimages          stat\n\t\tcliqueSnaps        stat\n\t\tparliaSnaps        stat\n\t\tbloomBits          stat\n\t\tfilterMapRows      stat\n\t\tfilterMapLastBlock stat\n\t\tfilterMapBlockLV   stat\n\n\t\t// Path-mode archive data\n\t\tstateIndex stat\n\n\t\t// Verkle statistics\n\t\tverkleTries        stat\n\t\tverkleStateLookups stat\n\n\t\t// Meta- and unaccounted data\n\t\tmetadata    stat\n\t\tunaccounted stat\n\n\t\t// Totals\n\t\ttotal common.StorageSize\n\n\t\t// This map tracks example keys for unaccounted data.\n\t\t// For each unique two-byte prefix, the first unaccounted key encountered\n\t\t// by the iterator will be stored.\n\t\tunaccountedKeys = make(map[[2]byte][]byte)\n\t)\n\t// Inspect key-value database first.\n\tfor it.Next() {\n\t\tvar (\n\t\t\tkey  = it.Key()\n\t\t\tsize = common.StorageSize(len(key) + len(it.Value()))\n\t\t)\n\t\ttotal += size\n\t\tswitch {\n\t\tcase bytes.HasPrefix(key, headerPrefix) && len(key) == (len(headerPrefix)+8+common.HashLength):\n\t\t\theaders.Add(size)\n\t\tcase bytes.HasPrefix(key, blockBodyPrefix) && len(key) == (len(blockBodyPrefix)+8+common.HashLength):\n\t\t\tbodies.Add(size)\n\t\tcase bytes.HasPrefix(key, blockReceiptsPrefix) && len(key) == (len(blockReceiptsPrefix)+8+common.HashLength):\n\t\t\treceipts.Add(size)\n\t\tcase bytes.HasPrefix(key, headerPrefix) && bytes.HasSuffix(key, headerTDSuffix):\n\t\t\ttds.Add(size)\n\t\tcase bytes.HasPrefix(key, BlockBlobSidecarsPrefix):\n\t\t\tblobSidecars.Add(size)\n\t\tcase bytes.HasPrefix(key, headerPrefix) && bytes.HasSuffix(key, headerHashSuffix):\n\t\t\tnumHashPairings.Add(size)\n\t\tcase bytes.HasPrefix(key, headerNumberPrefix) && len(key) == (len(headerNumberPrefix)+common.HashLength):\n\t\t\thashNumPairings.Add(size)\n\t\tcase IsLegacyTrieNode(key, it.Value()):\n\t\t\tlegacyTries.Add(size)\n\t\tcase bytes.HasPrefix(key, stateIDPrefix) && len(key) == len(stateIDPrefix)+common.HashLength:\n\t\t\tstateLookups.Add(size)\n\t\tcase IsAccountTrieNode(key):\n\t\t\taccountTries.Add(size)\n\t\tcase IsStorageTrieNode(key):\n\t\t\tstorageTries.Add(size)\n\t\tcase bytes.HasPrefix(key, CodePrefix) && len(key) == len(CodePrefix)+common.HashLength:\n\t\t\tcodes.Add(size)\n\t\tcase bytes.HasPrefix(key, txLookupPrefix) && len(key) == (len(txLookupPrefix)+common.HashLength):\n\t\t\ttxLookups.Add(size)\n\t\tcase bytes.HasPrefix(key, SnapshotAccountPrefix) && len(key) == (len(SnapshotAccountPrefix)+common.HashLength):\n\t\t\taccountSnaps.Add(size)\n\t\tcase bytes.HasPrefix(key, SnapshotStoragePrefix) && len(key) == (len(SnapshotStoragePrefix)+2*common.HashLength):\n\t\t\tstorageSnaps.Add(size)\n\t\tcase bytes.HasPrefix(key, PreimagePrefix) && len(key) == (len(PreimagePrefix)+common.HashLength):\n\t\t\tpreimages.Add(size)\n\t\tcase bytes.HasPrefix(key, configPrefix) && len(key) == (len(configPrefix)+common.HashLength):\n\t\t\tmetadata.Add(size)\n\t\tcase bytes.HasPrefix(key, genesisPrefix) && len(key) == (len(genesisPrefix)+common.HashLength):\n\t\t\tmetadata.Add(size)\n\t\tcase bytes.HasPrefix(key, CliqueSnapshotPrefix) && len(key) == 7+common.HashLength:\n\t\t\tcliqueSnaps.Add(size)\n\t\tcase bytes.HasPrefix(key, ParliaSnapshotPrefix) && len(key) == 7+common.HashLength:\n\t\t\tparliaSnaps.Add(size)\n\n\t\t// new log index\n\t\tcase bytes.HasPrefix(key, filterMapRowPrefix) && len(key) <= len(filterMapRowPrefix)+9:\n\t\t\tfilterMapRows.Add(size)\n\t\tcase bytes.HasPrefix(key, filterMapLastBlockPrefix) && len(key) == len(filterMapLastBlockPrefix)+4:\n\t\t\tfilterMapLastBlock.Add(size)\n\t\tcase bytes.HasPrefix(key, filterMapBlockLVPrefix) && len(key) == len(filterMapBlockLVPrefix)+8:\n\t\t\tfilterMapBlockLV.Add(size)\n\n\t\t// old log index (deprecated)\n\t\tcase bytes.HasPrefix(key, bloomBitsPrefix) && len(key) == (len(bloomBitsPrefix)+10+common.HashLength):\n\t\t\tbloomBits.Add(size)\n\t\tcase bytes.HasPrefix(key, bloomBitsMetaPrefix) && len(key) < len(bloomBitsMetaPrefix)+8:\n\t\t\tbloomBits.Add(size)\n\n\t\t// Path-based historic state indexes\n\t\tcase bytes.HasPrefix(key, StateHistoryIndexPrefix) && len(key) >= len(StateHistoryIndexPrefix)+common.HashLength:\n\t\t\tstateIndex.Add(size)\n\n\t\t// Verkle trie data is detected, determine the sub-category\n\t\tcase bytes.HasPrefix(key, VerklePrefix):\n\t\t\tremain := key[len(VerklePrefix):]\n\t\t\tswitch {\n\t\t\tcase IsAccountTrieNode(remain):\n\t\t\t\tverkleTries.Add(size)\n\t\t\tcase bytes.HasPrefix(remain, stateIDPrefix) && len(remain) == len(stateIDPrefix)+common.HashLength:\n\t\t\t\tverkleStateLookups.Add(size)\n\t\t\tcase bytes.Equal(remain, persistentStateIDKey):\n\t\t\t\tmetadata.Add(size)\n\t\t\tcase bytes.Equal(remain, trieJournalKey):\n\t\t\t\tmetadata.Add(size)\n\t\t\tcase bytes.Equal(remain, snapSyncStatusFlagKey):\n\t\t\t\tmetadata.Add(size)\n\t\t\tdefault:\n\t\t\t\tunaccounted.Add(size)\n\t\t\t}\n\n\t\t// Metadata keys\n\t\tcase slices.ContainsFunc(knownMetadataKeys, func(x []byte) bool { return bytes.Equal(x, key) }):\n\t\t\tmetadata.Add(size)\n\n\t\tdefault:\n\t\t\tunaccounted.Add(size)\n\t\t\tif len(key) >= 2 {\n\t\t\t\tprefix := [2]byte(key[:2])\n\t\t\t\tif _, ok := unaccountedKeys[prefix]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/database.go",
          "line": 791,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\n\t\t\tswitch {\n\t\t\tcase IsLegacyTrieNode(key, value):\n\t\t\t\tlegacyTries.Add(size)\n\t\t\tcase bytes.HasPrefix(key, stateIDPrefix) && len(key) == len(stateIDPrefix)+common.HashLength:\n\t\t\t\tstateLookups.Add(size)\n\t\t\tcase IsAccountTrieNode(key):\n\t\t\t\taccountTries.Add(size)\n\t\t\tcase IsStorageTrieNode(key):\n\t\t\t\tstorageTries.Add(size)\n\t\t\tdefault:\n\t\t\t\tvar accounted bool\n\t\t\t\tfor _, meta := range [][]byte{\n\t\t\t\t\tfastTrieProgressKey, persistentStateIDKey, trieJournalKey, snapSyncStatusFlagKey} {\n\t\t\t\t\tif bytes.Equal(key, meta) {\n\t\t\t\t\t\tmetadata.Add(size)\n\t\t\t\t\t\taccounted = true\n\t\t\t\t\t\tbreak\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif !accounted {\n\t\t\t\t\tunaccounted.Add(size)\n\t\t\t\t}\n\t\t\t}\n\t\t\tcount++\n\t\t\tif count%1000 == 0 && time.Since(logged) > 8*time.Second {\n\t\t\t\tlog.Info(\"Inspecting separate state database\", \"count\", count, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\tlogged = time.Now()\n\t\t\t}\n\t\t}\n\t\tlog.Info(\"Inspecting separate state database\", \"count\", count, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t}\n\t// Display the database statistic of key-value store.\n\tstats := [][]string{\n\t\t{\"Key-Value store\", \"Headers\", headers.Size(), headers.Count()},\n\t\t{\"Key-Value store\", \"Bodies\", bodies.Size(), bodies.Count()},\n\t\t{\"Key-Value store\", \"Receipt lists\", receipts.Size(), receipts.Count()},\n\t\t{\"Key-Value store\", \"Difficulties\", tds.Size(), tds.Count()},\n\t\t{\"Key-Value store\", \"BlobSidecars\", blobSidecars.Size(), blobSidecars.Count()},\n\t\t{\"Key-Value store\", \"Block number->hash\", numHashPairings.Size(), numHashPairings.Count()},\n\t\t{\"Key-Value store\", \"Block hash->number\", hashNumPairings.Size(), hashNumPairings.Count()},\n\t\t{\"Key-Value store\", \"Transaction index\", txLookups.Size(), txLookups.Count()},\n\t\t{\"Key-Value store\", \"Log index filter-map rows\", filterMapRows.Size(), filterMapRows.Count()},\n\t\t{\"Key-Value store\", \"Log index last-block-of-map\", filterMapLastBlock.Size(), filterMapLastBlock.Count()},\n\t\t{\"Key-Value store\", \"Log index block-lv\", filterMapBlockLV.Size(), filterMapBlockLV.Count()},\n\t\t{\"Key-Value store\", \"Log bloombits (deprecated)\", bloomBits.Size(), bloomBits.Count()},\n\t\t{\"Key-Value store\", \"Contract codes\", codes.Size(), codes.Count()},\n\t\t{\"Key-Value store\", \"Hash trie nodes\", legacyTries.Size(), legacyTries.Count()},\n\t\t{\"Key-Value store\", \"Path trie state lookups\", stateLookups.Size(), stateLookups.Count()},\n\t\t{\"Key-Value store\", \"Path trie account nodes\", accountTries.Size(), accountTries.Count()},\n\t\t{\"Key-Value store\", \"Path trie storage nodes\", storageTries.Size(), storageTries.Count()},\n\t\t{\"Key-Value store\", \"Path state history indexes\", stateIndex.Size(), stateIndex.Count()},\n\t\t{\"Key-Value store\", \"Verkle trie nodes\", verkleTries.Size(), verkleTries.Count()},\n\t\t{\"Key-Value store\", \"Verkle trie state lookups\", verkleStateLookups.Size(), verkleStateLookups.Count()},\n\t\t{\"Key-Value store\", \"Trie preimages\", preimages.Size(), preimages.Count()},\n\t\t{\"Key-Value store\", \"Account snapshot\", accountSnaps.Size(), accountSnaps.Count()},\n\t\t{\"Key-Value store\", \"Storage snapshot\", storageSnaps.Size(), storageSnaps.Count()},\n\t\t{\"Key-Value store\", \"Clique snapshots\", cliqueSnaps.Size(), cliqueSnaps.Count()},\n\t\t{\"Key-Value store\", \"Parlia snapshots\", parliaSnaps.Size(), parliaSnaps.Count()},\n\t\t{\"Key-Value store\", \"Singleton metadata\", metadata.Size(), metadata.Count()},\n\t}\n\t// Inspect all registered append-only file store then.\n\tancients, err := inspectFreezers(db)\n\tif err != nil {\n\t\treturn err\n\t}\n\tfor _, ancient := range ancients {\n\t\tfor _, table := range ancient.sizes {\n\t\t\tstats = append(stats, []string{\n\t\t\t\tfmt.Sprintf(\"Ancient store (%s)\", strings.Title(ancient.name)),\n\t\t\t\tstrings.Title(table.name),\n\t\t\t\ttable.size.String(),\n\t\t\t\tfmt.Sprintf(\"%d\", ancient.count()),\n\t\t\t})\n\t\t}\n\t\ttotal += ancient.size()\n\t}\n\n\t// inspect ancient state in separate trie db if exist\n\tif trieIter != nil {\n\t\tstateAncients, err := inspectFreezers(db.GetStateStore())\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tfor _, ancient := range stateAncients {\n\t\t\tfor _, table := range ancient.sizes {\n\t\t\t\tif ancient.name == \"chain\" {\n\t\t\t\t\tbreak\n\t\t\t\t}\n\t\t\t\tstats = append(stats, []string{\n\t\t\t\t\tfmt.Sprintf(\"Ancient store (%s)\", strings.Title(ancient.name)),\n\t\t\t\t\tstrings.Title(table.name),\n\t\t\t\t\ttable.size.String(),\n\t\t\t\t\tfmt.Sprintf(\"%d\", ancient.count()),\n\t\t\t\t})\n\t\t\t}\n\t\t\ttotal += ancient.size()\n\t\t}\n\t}\n\ttable := tablewriter.NewWriter(os.Stdout)\n\ttable.SetHeader([]string{\"Database\", \"Category\", \"Size\", \"Items\"})\n\ttable.SetFooter([]string{\"\", \"Total\", total.String(), \" \"})\n\ttable.AppendBulk(stats)\n\ttable.Render()\n\n\tif unaccounted.size > 0 {\n\t\tlog.Error(\"Database contains unaccounted data\", \"size\", unaccounted.size, \"count\", unaccounted.count)\n\t\tfor _, e := range slices.SortedFunc(maps.Values(unaccountedKeys), bytes.Compare) {\n\t\t\tlog.Error(fmt.Sprintf(\"   example key: %x\", e))\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc DeleteTrieState(db ethdb.Database) error {\n\tvar (\n\t\tit     ethdb.Iterator\n\t\tbatch  = db.NewBatch()\n\t\tstart  = time.Now()\n\t\tlogged = time.Now()\n\t\tcount  int64\n\t\tkey    []byte\n\t)\n\n\tprefixKeys := map[string]func([]byte) bool{\n\t\tstring(TrieNodeAccountPrefix): IsAccountTrieNode,\n\t\tstring(TrieNodeStoragePrefix): IsStorageTrieNode,\n\t\tstring(stateIDPrefix):         func(key []byte) bool { return len(key) == len(stateIDPrefix)+common.HashLength },\n\t}\n\n\tfor prefix, isValid := range prefixKeys {\n\t\tit = db.NewIterator([]byte(prefix), nil)\n\n\t\tfor it.Next() {\n\t\t\tkey = it.Key()\n\t\t\tif !isValid(key) {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tbatch.Delete(it.Key())\n\t\t\tif batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/rawdb/database.go",
          "line": 994,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/rawdb/database.go",
          "line": 995,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 523,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= indexEntrySize {\n\t\tentry, err := read()\n\t\tif err != nil {\n\t\t\treturn 0, err\n\t\t}\n\t\tif offset == 0 {\n\t\t\thead = entry\n\t\t\tcontinue\n\t\t}\n\t\t// Ensure that the first non-head index refers to the earliest file,\n\t\t// or the next file if the earliest file has no space to place the\n\t\t// first item.\n\t\tif offset == indexEntrySize {\n\t\t\tif entry.filenum != head.filenum && entry.filenum != head.filenum+1 {\n\t\t\t\tlog.Error(\"Corrupted index item detected\", \"earliest\", head.filenum, \"filenumber\", entry.filenum)\n\t\t\t\treturn truncate(offset)\n\t\t\t}\n\t\t\tprev = entry\n\t\t\tcontinue\n\t\t}\n\t\t// ensure two consecutive index items are in order\n\t\tif err := t.checkIndexItems(prev, entry)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 955,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= indexEntrySize\n\t\tindices = append(indices, index)\n\t}\n\tif from == 0 {\n\t\t// Special case if we're reading the first item in the freezer. We assume that\n\t\t// the first item always start from zero(regarding the deletion, we\n\t\t// only support deletion by files, so that the assumption is held).\n\t\t// This means we can use the first item metadata to carry information about\n\t\t// the 'global' offset, for the deletion-case\n\t\tindices[0].offset = 0\n\t\tindices[0].filenum = indices[1].filenum\n\t}\n\treturn indices, nil\n}\n\n// Retrieve looks up the data offset of an item with the given number and retrieves\n// the raw binary blob from the data file.\nfunc (t *freezerTable) Retrieve(item uint64) ([]byte, error) {\n\titems, err := t.RetrieveItems(item, 1, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn items[0], nil\n}\n\n// RetrieveItems returns multiple items in sequence, starting from the index 'start'.\n// It will return at most 'max' items, but will abort earlier to respect the\n// 'maxBytes' argument. However, if the 'maxBytes' is smaller than the size of one\n// item, it _will_ return one element and possibly overflow the maxBytes.\nfunc (t *freezerTable) RetrieveItems(start, count, maxBytes uint64) ([][]byte, error) {\n\t// First we read the 'raw' data, which might be compressed.\n\tdiskData, sizes, err := t.retrieveItems(start, count, maxBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tvar (\n\t\toutput     = make([][]byte, 0, count)\n\t\toffset     int // offset for reading\n\t\toutputSize int // size of uncompressed data\n\t)\n\t// Now slice up the data and decompress.\n\tfor i, diskSize := range sizes {\n\t\titem := diskData[offset : offset+diskSize]\n\t\toffset += diskSize\n\t\tdecompressedSize := diskSize\n\t\tif !t.config.noSnappy {\n\t\t\tdecompressedSize, _ = snappy.DecodedLen(item)\n\t\t}\n\t\tif i > 0 && maxBytes != 0 && uint64(outputSize+decompressedSize) > maxBytes {\n\t\t\tbreak\n\t\t}\n\t\tif !t.config.noSnappy {\n\t\t\tdata, err := snappy.Decode(nil, item)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\toutput = append(output, data)\n\t\t} else {\n\t\t\toutput = append(output, item)\n\t\t}\n\t\toutputSize += decompressedSize\n\t}\n\treturn output, nil\n}\n\n// retrieveItems reads up to 'count' items from the table. It reads at least\n// one item, but otherwise avoids reading more than maxBytes bytes. Freezer\n// will ignore the size limitation and continuously allocate memory to store\n// data if maxBytes is 0. It returns the (potentially compressed) data, and\n// the sizes.\nfunc (t *freezerTable) retrieveItems(start, count, maxBytes uint64) ([]byte, []int, error) {\n\tt.lock.RLock()\n\tdefer t.lock.RUnlock()\n\n\t// Ensure the table and the item are accessible\n\tif t.index == nil || t.head == nil || t.metadata.file == nil {\n\t\treturn nil, nil, errClosed\n\t}\n\tvar (\n\t\titems  = t.items.Load()      // the total items(head + 1)\n\t\thidden = t.itemHidden.Load() // the number of hidden items\n\t)\n\t// Ensure the start is written, not deleted from the tail, and that the\n\t// caller actually wants something\n\tif items <= start || hidden > start || count == 0 {\n\t\treturn nil, nil, errOutOfBounds\n\t}\n\tif start+count > items {\n\t\tcount = items - start\n\t}\n\tvar output []byte // Buffer to read data into\n\tif maxBytes != 0 {\n\t\toutput = make([]byte, 0, maxBytes)\n\t} else {\n\t\toutput = make([]byte, 0, 1024) // initial buffer cap\n\t}\n\t// readData is a helper method to read a single data item from disk.\n\treadData := func(fileId, start uint32, length int) error {\n\t\toutput = grow(output, length)\n\t\tdataFile, exist := t.files[fileId]\n\t\tif !exist {\n\t\t\treturn fmt.Errorf(\"missing data file %d\", fileId)\n\t\t}\n\t\tif _, err := dataFile.ReadAt(output[len(output)-length:], int64(start))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 1102,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\t\ttotalSize += size\n\t\tsizes = append(sizes, size)\n\t\tif i == len(indices)-2 || (uint64(totalSize) > maxBytes && maxBytes != 0) {\n\t\t\t// Last item, need to do the read now\n\t\t\tif err := readData(secondIndex.filenum, readStart, unreadSize)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 1337,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= t.itemHidden.Load()\n\n\t// overwrite metadata file\n\tt.metadata.setVirtualTail(legacyOffset, true)\n\tif t.metadata.flushOffset < int64(legacyOffset) {\n\t\tt.metadata.setFlushOffset(int64(legacyOffset), true)\n\t}\n\n\t// overwrite first index\n\tvar firstIndex indexEntry\n\tbuffer := make([]byte, indexEntrySize)\n\tt.index.ReadAt(buffer, 0)\n\tfirstIndex.unmarshalBinary(buffer)\n\tfirstIndex.offset = uint32(legacyOffset)\n\tif _, err := t.index.WriteAt(firstIndex.append(nil), 0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 326,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= indexEntrySize\n\n\t\t\t// If the index file is truncated beyond the flush offset, move the flush\n\t\t\t// offset back to the new end of the file. A crash may occur before the\n\t\t\t// offset is updated, leaving a dangling reference that points to a position\n\t\t\t// outside the file. If so, the offset will be reset to the new end of the\n\t\t\t// file during the next run.\n\t\t\tif t.metadata.flushOffset > newOffset {\n\t\t\t\tif err := t.metadata.setFlushOffset(newOffset, true)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/rawdb/freezer_table.go",
          "line": 783,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1 {\n\t\tif _, err := t.index.ReadAt(buffer, int64((current-deleted+1)*indexEntrySize))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/ancient_utils.go",
          "line": 51,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= table.size\n\t}\n\treturn total\n}\n\nfunc inspect(name string, order map[string]freezerTableConfig, reader ethdb.AncientReader) (freezerInfo, error) {\n\tinfo := freezerInfo{name: name}\n\tfor t := range order {\n\t\tsize, err := reader.AncientSize(t)\n\t\tif err != nil {\n\t\t\treturn freezerInfo{}, err\n\t\t}\n\t\tinfo.sizes = append(info.sizes, tableSize{name: t, size: common.StorageSize(size)})\n\t}\n\t// Retrieve the number of last stored item\n\tancients, err := reader.Ancients()\n\tif err != nil {\n\t\treturn freezerInfo{}, err\n\t}\n\tinfo.head = ancients - 1\n\n\t// Retrieve the number of first stored item\n\ttail, err := reader.Tail()\n\tif err != nil {\n\t\treturn freezerInfo{}, err\n\t}\n\tinfo.tail = tail\n\treturn info, nil\n}\n\n// inspectFreezers inspects all freezers registered in the system.\nfunc inspectFreezers(db ethdb.Database) ([]freezerInfo, error) {\n\tvar infos []freezerInfo\n\tfor _, freezer := range freezers {\n\t\tswitch freezer {\n\t\tcase ChainFreezerName:\n\t\t\tinfo, err := inspect(ChainFreezerName, chainFreezerTableConfigs, db)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tinfos = append(infos, info)\n\n\t\tcase MerkleStateFreezerName, VerkleStateFreezerName:\n\t\t\tif db.HasSeparateStateStore() {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tdatadir, err := db.AncientDatadir()\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\n\t\t\tfile, err := os.Open(filepath.Join(datadir, MerkleStateFreezerName))\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tdefer file.Close()\n\t\t\t// if state freezer folder has been pruned, there is no need for inspection\n\t\t\t_, err = file.Readdirnames(1)\n\t\t\tif err == io.EOF {\n\t\t\t\tcontinue\n\t\t\t}\n\n\t\t\tf, err := NewStateFreezer(datadir, freezer == VerkleStateFreezerName, true)\n\t\t\tif err != nil {\n\t\t\t\tcontinue // might be possible the state freezer is not existent\n\t\t\t}\n\t\t\tdefer f.Close()\n\n\t\t\tinfo, err := inspect(freezer, stateFreezerTableConfigs, f)\n\t\t\tif err != nil {\n\t\t\t\treturn nil, err\n\t\t\t}\n\t\t\tinfos = append(infos, info)\n\n\t\tdefault:\n\t\t\treturn nil, fmt.Errorf(\"unknown freezer, supported ones: %v\", freezers)\n\t\t}\n\t}\n\treturn infos, nil\n}\n\n// InspectFreezerTable dumps out the index of a specific freezer table. The passed\n// ancient indicates the path of root ancient directory where the chain freezer can\n// be opened. Start and end specify the range for dumping out indexes.\n// Note this function can only be used for debugging purposes.\nfunc InspectFreezerTable(ancient string, freezerName string, tableName string, start, end int64, multiDatabase bool) error {\n\tvar (\n\t\tpath   string\n\t\ttables map[string]freezerTableConfig\n\t)\n\tswitch freezerName {\n\tcase ChainFreezerName:\n\t\tpath, tables = resolveChainFreezerDir(ancient), chainFreezerTableConfigs\n\n\tcase MerkleStateFreezerName, VerkleStateFreezerName:\n\t\tif multiDatabase {\n\t\t\tpath, tables = filepath.Join(filepath.Dir(ancient)+\"/state/ancient\", freezerName), stateFreezerTableConfigs\n\t\t} else {\n\t\t\tpath, tables = filepath.Join(ancient, freezerName), stateFreezerTableConfigs\n\t\t}\n\tdefault:\n\t\treturn fmt.Errorf(\"unknown freezer, supported ones: %v\", freezers)\n\t}\n\tnoSnappy, exist := tables[tableName]\n\tif !exist {\n\t\tvar names []string\n\t\tfor name := range tables {\n\t\t\tnames = append(names, name)\n\t\t}\n\t\treturn fmt.Errorf(\"unknown table, supported ones: %v\", names)\n\t}\n\ttable, err := newFreezerTable(path, tableName, noSnappy, true)\n\tif err != nil {\n\t\treturn err\n\t}\n\ttable.dumpIndexStdout(start, end)\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/accessors_metadata.go",
          "line": 132,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(numDel)\n\t}\n\t// And save it again\n\tdata, _ := rlp.EncodeToBytes(uncleanShutdowns)\n\tif err := db.Put(uncleanShutdownKey, data)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/schema.go",
          "line": 230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(buf[n:], accountHash.Bytes())\n\tcopy(buf[n:], storageHash.Bytes())\n\treturn buf\n}\n\n// storageSnapshotsKey = SnapshotStoragePrefix + account hash + storage hash\nfunc storageSnapshotsKey(accountHash common.Hash) []byte {\n\treturn append(SnapshotStoragePrefix, accountHash.Bytes()...)\n}\n\n// preimageKey = PreimagePrefix + hash\nfunc preimageKey(hash common.Hash) []byte {\n\treturn append(PreimagePrefix, hash.Bytes()...)\n}\n\n// codeKey = CodePrefix + hash\nfunc codeKey(hash common.Hash) []byte {\n\treturn append(CodePrefix, hash.Bytes()...)\n}\n\n// IsCodeKey reports whether the given byte slice is the key of contract code,\n// if so return the raw code hash as well.\nfunc IsCodeKey(key []byte) (bool, []byte) {\n\tif bytes.HasPrefix(key, CodePrefix) && len(key) == common.HashLength+len(CodePrefix) {\n\t\treturn true, key[len(CodePrefix):]\n\t}\n\treturn false, nil\n}\n\n// configKey = configPrefix + hash\nfunc configKey(hash common.Hash) []byte {\n\treturn append(configPrefix, hash.Bytes()...)\n}\n\n// genesisStateSpecKey = genesisPrefix + hash\nfunc genesisStateSpecKey(hash common.Hash) []byte {\n\treturn append(genesisPrefix, hash.Bytes()...)\n}\n\n// stateIDKey = stateIDPrefix + root (32 bytes)\nfunc stateIDKey(root common.Hash) []byte {\n\treturn append(stateIDPrefix, root.Bytes()...)\n}\n\n// accountTrieNodeKey = TrieNodeAccountPrefix + nodePath.\nfunc accountTrieNodeKey(path []byte) []byte {\n\treturn append(TrieNodeAccountPrefix, path...)\n}\n\n// storageTrieNodeKey = TrieNodeStoragePrefix + accountHash + nodePath.\nfunc storageTrieNodeKey(accountHash common.Hash, path []byte) []byte {\n\tbuf := make([]byte, len(TrieNodeStoragePrefix)+common.HashLength+len(path))\n\tn := copy(buf, TrieNodeStoragePrefix)\n\tn += copy(buf[n:], accountHash.Bytes())\n\tcopy(buf[n:], path)\n\treturn buf\n}\n\n// IsLegacyTrieNode reports whether a provided database entry is a legacy trie\n// node. The characteristics of legacy trie node are:\n// - the key length is 32 bytes\n// - the key is the hash of val\nfunc IsLegacyTrieNode(key []byte, val []byte) bool {\n\tif len(key) != common.HashLength {\n\t\treturn false\n\t}\n\treturn bytes.Equal(key, crypto.Keccak256(val))\n}\n\n// ResolveAccountTrieNodeKey reports whether a provided database entry is an\n// account trie node in path-based state scheme, and returns the resolved\n// node path if so.\nfunc ResolveAccountTrieNodeKey(key []byte) (bool, []byte) {\n\tif !bytes.HasPrefix(key, TrieNodeAccountPrefix) {\n\t\treturn false, nil\n\t}\n\t// The remaining key should only consist a hex node path\n\t// whose length is in the range 0 to 64 (64 is excluded\n\t// since leaves are always wrapped with shortNode).\n\tif len(key) >= len(TrieNodeAccountPrefix)+common.HashLength*2 {\n\t\treturn false, nil\n\t}\n\treturn true, key[len(TrieNodeAccountPrefix):]\n}\n\n// IsAccountTrieNode reports whether a provided database entry is an account\n// trie node in path-based state scheme.\nfunc IsAccountTrieNode(key []byte) bool {\n\tok, _ := ResolveAccountTrieNodeKey(key)\n\treturn ok\n}\n\n// ResolveStorageTrieNode reports whether a provided database entry is a storage\n// trie node in path-based state scheme, and returns the resolved account hash\n// and node path if so.\nfunc ResolveStorageTrieNode(key []byte) (bool, common.Hash, []byte) {\n\tif !bytes.HasPrefix(key, TrieNodeStoragePrefix) {\n\t\treturn false, common.Hash{}, nil\n\t}\n\t// The remaining key consists of 2 parts:\n\t// - 32 bytes account hash\n\t// - hex node path whose length is in the range 0 to 64\n\tif len(key) < len(TrieNodeStoragePrefix)+common.HashLength {\n\t\treturn false, common.Hash{}, nil\n\t}\n\tif len(key) >= len(TrieNodeStoragePrefix)+common.HashLength+common.HashLength*2 {\n\t\treturn false, common.Hash{}, nil\n\t}\n\taccountHash := common.BytesToHash(key[len(TrieNodeStoragePrefix) : len(TrieNodeStoragePrefix)+common.HashLength])\n\treturn true, accountHash, key[len(TrieNodeStoragePrefix)+common.HashLength:]\n}\n\n// IsStorageTrieNode reports whether a provided database entry is a storage\n// trie node in path-based state scheme.\nfunc IsStorageTrieNode(key []byte) bool {\n\tok, _, _ := ResolveStorageTrieNode(key)\n\treturn ok\n}\n\n// filterMapRowKey = filterMapRowPrefix + mapRowIndex (uint64 big endian)\nfunc filterMapRowKey(mapRowIndex uint64, base bool) []byte {\n\textLen := 8\n\tif base {\n\t\textLen = 9\n\t}\n\tl := len(filterMapRowPrefix)\n\tkey := make([]byte, l+extLen)\n\tcopy(key[:l], filterMapRowPrefix)\n\tbinary.BigEndian.PutUint64(key[l:l+8], mapRowIndex)\n\treturn key\n}\n\n// filterMapLastBlockKey = filterMapLastBlockPrefix + mapIndex (uint32 big endian)\nfunc filterMapLastBlockKey(mapIndex uint32) []byte {\n\tl := len(filterMapLastBlockPrefix)\n\tkey := make([]byte, l+4)\n\tcopy(key[:l], filterMapLastBlockPrefix)\n\tbinary.BigEndian.PutUint32(key[l:], mapIndex)\n\treturn key\n}\n\n// filterMapBlockLVKey = filterMapBlockLVPrefix + num (uint64 big endian)\nfunc filterMapBlockLVKey(number uint64) []byte {\n\tl := len(filterMapBlockLVPrefix)\n\tkey := make([]byte, l+8)\n\tcopy(key[:l], filterMapBlockLVPrefix)\n\tbinary.BigEndian.PutUint64(key[l:], number)\n\treturn key\n}\n\n// accountHistoryIndexKey = StateHistoryAccountMetadataPrefix + addressHash\nfunc accountHistoryIndexKey(addressHash common.Hash) []byte {\n\treturn append(StateHistoryAccountMetadataPrefix, addressHash.Bytes()...)\n}\n\n// storageHistoryIndexKey = StateHistoryStorageMetadataPrefix + addressHash + storageHash\nfunc storageHistoryIndexKey(addressHash common.Hash, storageHash common.Hash) []byte {\n\treturn append(append(StateHistoryStorageMetadataPrefix, addressHash.Bytes()...), storageHash.Bytes()...)\n}\n\n// accountHistoryIndexBlockKey = StateHistoryAccountBlockPrefix + addressHash + blockID\nfunc accountHistoryIndexBlockKey(addressHash common.Hash, blockID uint32) []byte {\n\tvar buf [4]byte\n\tbinary.BigEndian.PutUint32(buf[:], blockID)\n\treturn append(append(StateHistoryAccountBlockPrefix, addressHash.Bytes()...), buf[:]...)\n}\n\n// storageHistoryIndexBlockKey = StateHistoryStorageBlockPrefix + addressHash + storageHash + blockID\nfunc storageHistoryIndexBlockKey(addressHash common.Hash, storageHash common.Hash, blockID uint32) []byte {\n\tvar buf [4]byte\n\tbinary.BigEndian.PutUint32(buf[:], blockID)\n\treturn append(append(append(StateHistoryStorageBlockPrefix, addressHash.Bytes()...), storageHash.Bytes()...), buf[:]...)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 188,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueReader)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 197,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueWriter, hash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 204,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueReader)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 213,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueWriter, hash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 220,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueReader)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 229,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db ethdb.KeyValueWriter, hash common.Hash)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/rawdb/accessors_chain.go",
          "line": 1129,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/freezer_table_test.go",
          "line": 1265,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(index)\n\t\t\t\tstep.target = deleted\n\t\t\t}\n\t\tcase opTruncateTailAll:\n\t\t\tstep.target = deleted + uint64(len(items))\n\t\t\titems = items[:0]\n\t\t\tdeleted = step.target\n\t\t}\n\t\tsteps = append(steps, step)\n\t}\n\treturn reflect.ValueOf(steps)\n}\n\nfunc runRandTest(rt randTest) bool {\n\tfname := fmt.Sprintf(\"randtest-%d\", rand.Uint64())\n\tf, err := newTable(os.TempDir(), fname, metrics.NewMeter(), metrics.NewMeter(), metrics.NewGauge(), 50, freezerTableConfig{noSnappy: true}, false)\n\tif err != nil {\n\t\tpanic(\"failed to initialize table\")\n\t}\n\tvar values [][]byte\n\tfor i, step := range rt {\n\t\tswitch step.op {\n\t\tcase opReload:\n\t\t\tf.Close()\n\t\t\tf, err = newTable(os.TempDir(), fname, metrics.NewMeter(), metrics.NewMeter(), metrics.NewGauge(), 50, freezerTableConfig{noSnappy: true}, false)\n\t\t\tif err != nil {\n\t\t\t\trt[i].err = fmt.Errorf(\"failed to reload table %v\", err)\n\t\t\t}\n\t\tcase opCheckAll:\n\t\t\ttail := f.itemHidden.Load()\n\t\t\thead := f.items.Load()\n\n\t\t\tif tail == head {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tgot, err := f.RetrieveItems(f.itemHidden.Load(), head-tail, 100000)\n\t\t\tif err != nil {\n\t\t\t\trt[i].err = err\n\t\t\t} else {\n\t\t\t\tif !reflect.DeepEqual(got, values) {\n\t\t\t\t\trt[i].err = fmt.Errorf(\"mismatch on retrieved values %v %v\", got, values)\n\t\t\t\t}\n\t\t\t}\n\n\t\tcase opAppend:\n\t\t\tbatch := f.newBatch()\n\t\t\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/accessors_indexes.go",
          "line": 304,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= logs\n\t\t}\n\t}\n\treturn nil, RawReceiptContext{}, fmt.Errorf(\"receipt not found, %d, %x, %d\", blockNumber, blockHash, txIndex)\n}\n\n// ReadFilterMapExtRow retrieves a filter map row at the given mapRowIndex\n// (see filtermaps.mapRowIndex for the storage index encoding).\n// Note that zero length rows are not stored in the database and therefore all\n// non-existent entries are interpreted as empty rows and return no error.\n// Also note that the mapRowIndex indexing scheme is the same as the one\n// proposed in EIP-7745 for tree-hashing the filter map structure and for the\n// same data proximity reasons it is also suitable for database representation.\n// See also:\n// https://eips.ethereum.org/EIPS/eip-7745#hash-tree-structure\nfunc ReadFilterMapExtRow(db ethdb.KeyValueReader, mapRowIndex uint64, bitLength uint) ([]uint32, error) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tkey := filterMapRowKey(mapRowIndex, false)\n\thas, err := db.Has(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif !has {\n\t\treturn nil, nil\n\t}\n\tencRow, err := db.Get(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tif len(encRow)%byteLength != 0 {\n\t\treturn nil, errors.New(\"invalid encoded extended filter row length\")\n\t}\n\trow := make([]uint32, len(encRow)/byteLength)\n\tvar b [4]byte\n\tfor i := range row {\n\t\tcopy(b[:byteLength], encRow[i*byteLength:(i+1)*byteLength])\n\t\trow[i] = binary.LittleEndian.Uint32(b[:])\n\t}\n\treturn row, nil\n}\n\nfunc ReadFilterMapBaseRows(db ethdb.KeyValueReader, mapRowIndex uint64, rowCount uint32, bitLength uint) ([][]uint32, error) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tkey := filterMapRowKey(mapRowIndex, true)\n\thas, err := db.Has(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\trows := make([][]uint32, rowCount)\n\tif !has {\n\t\treturn rows, nil\n\t}\n\tencRows, err := db.Get(key)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tencLen := len(encRows)\n\tvar (\n\t\tentryCount, entriesInRow, rowIndex, headerLen, headerBits int\n\t\theaderByte                                                byte\n\t)\n\tfor headerLen+byteLength*entryCount < encLen {\n\t\tif headerBits == 0 {\n\t\t\theaderByte = encRows[headerLen]\n\t\t\theaderLen++\n\t\t\theaderBits = 8\n\t\t}\n\t\tif headerByte&1 > 0 {\n\t\t\tentriesInRow++\n\t\t\tentryCount++\n\t\t} else {\n\t\t\tif entriesInRow > 0 {\n\t\t\t\trows[rowIndex] = make([]uint32, entriesInRow)\n\t\t\t\tentriesInRow = 0\n\t\t\t}\n\t\t\trowIndex++\n\t\t}\n\t\theaderByte >>= 1\n\t\theaderBits--\n\t}\n\tif headerLen+byteLength*entryCount > encLen {\n\t\treturn nil, errors.New(\"invalid encoded base filter rows length\")\n\t}\n\tif entriesInRow > 0 {\n\t\trows[rowIndex] = make([]uint32, entriesInRow)\n\t}\n\tnextEntry := headerLen\n\tfor _, row := range rows {\n\t\tfor i := range row {\n\t\t\tvar b [4]byte\n\t\t\tcopy(b[:byteLength], encRows[nextEntry:nextEntry+byteLength])\n\t\t\trow[i] = binary.LittleEndian.Uint32(b[:])\n\t\t\tnextEntry += byteLength\n\t\t}\n\t}\n\treturn rows, nil\n}\n\n// WriteFilterMapExtRow stores an extended filter map row at the given mapRowIndex\n// or deletes any existing entry if the row is empty.\nfunc WriteFilterMapExtRow(db ethdb.KeyValueWriter, mapRowIndex uint64, row []uint32, bitLength uint) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tvar err error\n\tif len(row) > 0 {\n\t\tencRow := make([]byte, len(row)*byteLength)\n\t\tfor i, c := range row {\n\t\t\tvar b [4]byte\n\t\t\tbinary.LittleEndian.PutUint32(b[:], c)\n\t\t\tcopy(encRow[i*byteLength:(i+1)*byteLength], b[:byteLength])\n\t\t}\n\t\terr = db.Put(filterMapRowKey(mapRowIndex, false), encRow)\n\t} else {\n\t\terr = db.Delete(filterMapRowKey(mapRowIndex, false))\n\t}\n\tif err != nil {\n\t\tlog.Crit(\"Failed to store extended filter map row\", \"err\", err)\n\t}\n}\n\nfunc WriteFilterMapBaseRows(db ethdb.KeyValueWriter, mapRowIndex uint64, rows [][]uint32, bitLength uint) {\n\tbyteLength := int(bitLength) / 8\n\tif int(bitLength) != byteLength*8 {\n\t\tpanic(\"invalid bit length\")\n\t}\n\tvar entryCount, zeroBits int\n\tfor i, row := range rows {\n\t\tif len(row) > 0 {\n\t\t\tentryCount += len(row)\n\t\t\tzeroBits = i\n\t\t}\n\t}\n\tvar err error\n\tif entryCount > 0 {\n\t\theaderLen := (zeroBits + entryCount + 7) / 8\n\t\tencRows := make([]byte, headerLen+entryCount*byteLength)\n\t\tnextEntry := headerLen\n\n\t\theaderPtr, headerByte := 0, byte(1)\n\t\taddHeaderBit := func(bit bool) {\n\t\t\tif bit {\n\t\t\t\tencRows[headerPtr] += headerByte\n\t\t\t}\n\t\t\tif headerByte += headerByte",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/accessors_indexes.go",
          "line": 466,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= byteLength\n\t\t\t\taddHeaderBit(true)\n\t\t\t}\n\t\t\tif zeroBits == 0 {\n\t\t\t\tbreak\n\t\t\t}\n\t\t\taddHeaderBit(false)\n\t\t\tzeroBits--\n\t\t}\n\t\terr = db.Put(filterMapRowKey(mapRowIndex, true), encRows)\n\t} else {\n\t\terr = db.Delete(filterMapRowKey(mapRowIndex, true))\n\t}\n\tif err != nil {\n\t\tlog.Crit(\"Failed to store base filter map rows\", \"err\", err)\n\t}\n}\n\nfunc DeleteFilterMapRows(db ethdb.KeyValueStore, mapRows common.Range[uint64], hashScheme bool, stopCallback func(bool) bool) error {\n\treturn SafeDeleteRange(db, filterMapRowKey(mapRows.First(), false), filterMapRowKey(mapRows.AfterLast(), false), hashScheme, stopCallback)\n}\n\n// ReadFilterMapLastBlock retrieves the number of the block that generated the\n// last log value entry of the given map.\nfunc ReadFilterMapLastBlock(db ethdb.KeyValueReader, mapIndex uint32) (uint64, common.Hash, error) {\n\tenc, err := db.Get(filterMapLastBlockKey(mapIndex))\n\tif err != nil {\n\t\treturn 0, common.Hash{}, err\n\t}\n\tif len(enc) != 40 {\n\t\treturn 0, common.Hash{}, errors.New(\"invalid block number and id encoding\")\n\t}\n\tvar id common.Hash\n\tcopy(id[:], enc[8:])\n\treturn binary.BigEndian.Uint64(enc[:8]), id, nil\n}\n\n// WriteFilterMapLastBlock stores the number of the block that generated the\n// last log value entry of the given map.\nfunc WriteFilterMapLastBlock(db ethdb.KeyValueWriter, mapIndex uint32, blockNumber uint64, id common.Hash) {\n\tvar enc [40]byte\n\tbinary.BigEndian.PutUint64(enc[:8], blockNumber)\n\tcopy(enc[8:], id[:])\n\tif err := db.Put(filterMapLastBlockKey(mapIndex), enc[:])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 739,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= batchSize {\n\t\tlength := batchSize\n\t\tif i+batchSize > b.N {\n\t\t\tlength = b.N - i\n\t\t}\n\n\t\tblocks := allBlocks[i : i+length]\n\t\treceipts := batchReceipts[:length]\n\t\tfor j := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 754,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= writeSize\n\t}\n\n\t// Enable MB/s reporting.\n\tb.SetBytes(totalSize / int64(b.N))\n}\n\n// makeTestBlocks creates fake blocks for the ancient write benchmark.\nfunc makeTestBlocks(nblock int, txsPerBlock int) []*types.Block {\n\tkey, _ := crypto.HexToECDSA(\"b71c71a67e1177ad4e901695e1b4b9ee17ae16c6668d313eac2f96dbcda3f291\")\n\tsigner := types.LatestSignerForChainID(big.NewInt(8))\n\n\t// Create transactions.\n\ttxs := make([]*types.Transaction, txsPerBlock)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 326,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 329,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 334,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, blockFull.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 335,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db, blockFast.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 341,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/rawdb/accessors_chain_test.go",
          "line": 344,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(db)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.83,
          "confidence": 0.9747,
          "ensemble_confidence": 0.8772300000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/freezer_batch.go",
          "line": 85,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= tb.totalBytes\n\t}\n\treturn item, writeSize, nil\n}\n\n// freezerTableBatch is a batch for a freezer table.\ntype freezerTableBatch struct {\n\tt *freezerTable\n\n\tsb          *snappyBuffer\n\tencBuffer   writeBuffer\n\tdataBuffer  []byte\n\tindexBuffer []byte\n\tcurItem     uint64 // expected index of next append\n\ttotalBytes  int64  // counts written bytes since reset\n}\n\n// newBatch creates a new batch for the freezer table.\nfunc (t *freezerTable) newBatch() *freezerTableBatch {\n\tbatch := &freezerTableBatch{t: t}\n\tif !t.config.noSnappy {\n\t\tbatch.sb = new(snappyBuffer)\n\t}\n\tbatch.reset()\n\treturn batch\n}\n\n// reset clears the batch for reuse.\nfunc (batch *freezerTableBatch) reset() {\n\tbatch.dataBuffer = batch.dataBuffer[:0]\n\tbatch.indexBuffer = batch.indexBuffer[:0]\n\tcurItem := batch.t.items.Load()\n\tbatch.curItem = atomic.LoadUint64(&curItem)\n\tbatch.totalBytes = 0\n}\n\n// Append rlp-encodes and adds data at the end of the freezer table. The item number is a\n// precautionary parameter to ensure data correctness, but the table will reject already\n// existing data.\nfunc (batch *freezerTableBatch) Append(item uint64, data interface{}) error {\n\tif item != batch.curItem {\n\t\treturn fmt.Errorf(\"%w: have %d want %d\", errOutOrderInsertion, item, batch.curItem)\n\t}\n\n\t// Encode the item.\n\tbatch.encBuffer.Reset()\n\tif err := rlp.Encode(&batch.encBuffer, data)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/freezer_batch.go",
          "line": 173,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= itemSize\n\n\t// Put index entry to buffer.\n\tentry := indexEntry{filenum: batch.t.headId, offset: uint32(itemOffset + itemSize)}\n\tbatch.indexBuffer = entry.append(batch.indexBuffer)\n\tbatch.curItem++\n\n\treturn batch.maybeCommit()\n}\n\n// maybeCommit writes the buffered data if the buffer is full enough.\nfunc (batch *freezerTableBatch) maybeCommit() error {\n\tif len(batch.dataBuffer) > freezerBatchBufferLimit {\n\t\treturn batch.commit()\n\t}\n\treturn nil\n}\n\n// commit writes the batched items to the backing freezerTable. Note index\n// file isn't fsync'd after the file write, the recent write can be lost\n// after the power failure.\nfunc (batch *freezerTableBatch) commit() error {\n\t_, err := batch.t.head.Write(batch.dataBuffer)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdataSize := int64(len(batch.dataBuffer))\n\tbatch.dataBuffer = batch.dataBuffer[:0]\n\n\t_, err = batch.t.index.Write(batch.indexBuffer)\n\tif err != nil {\n\t\treturn err\n\t}\n\tindexSize := int64(len(batch.indexBuffer))\n\tbatch.indexBuffer = batch.indexBuffer[:0]\n\n\t// Update headBytes of table.\n\tbatch.t.headBytes += dataSize\n\titems := batch.curItem\n\tbatch.t.items.Store(items)\n\n\t// Update metrics.\n\tbatch.t.sizeGauge.Inc(dataSize + indexSize)\n\tbatch.t.writeMeter.Mark(dataSize + indexSize)\n\n\t// Periodically sync the table, todo (rjl493456442) make it configurable?\n\tif time.Since(batch.t.lastSync) > 30*time.Second {\n\t\tbatch.t.lastSync = time.Now()\n\t\treturn batch.t.Sync()\n\t}\n\treturn nil\n}\n\n// snappyBuffer writes snappy in block format, and can be reused. It is\n// reset when WriteTo is called.\ntype snappyBuffer struct {\n\tdst []byte\n}\n\n// compress snappy-compresses the data.\nfunc (s *snappyBuffer) compress(data []byte) []byte {\n\t// The snappy library does not care what the capacity of the buffer is,\n\t// but only checks the length. If the length is too small, it will\n\t// allocate a brand new buffer.\n\t// To avoid that, we check the required size here, and grow the size of the\n\t// buffer to utilize the full capacity.\n\tif n := snappy.MaxEncodedLen(len(data))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/freezer_memory.go",
          "line": 78,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(t.data[index]))\n\t}\n\treturn batch, nil\n}\n\n// truncateHead discards any recent data above the provided threshold number.\nfunc (t *memoryTable) truncateHead(items uint64) error {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\t// Short circuit if nothing to delete.\n\tif t.items <= items {\n\t\treturn nil\n\t}\n\tif items < t.offset {\n\t\treturn errors.New(\"truncation below tail\")\n\t}\n\tt.data = t.data[:items-t.offset]\n\tt.items = items\n\treturn nil\n}\n\n// truncateTail discards any recent data before the provided threshold number.\nfunc (t *memoryTable) truncateTail(items uint64) error {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\t// Short circuit if nothing to delete.\n\tif t.offset >= items {\n\t\treturn nil\n\t}\n\tif t.items < items {\n\t\treturn errors.New(\"truncation above head\")\n\t}\n\tt.data = t.data[items-t.offset:]\n\tt.offset = items\n\treturn nil\n}\n\n// commit merges the given item batch into table. It's presumed that the\n// batch is ordered and continuous with table.\nfunc (t *memoryTable) commit(batch [][]byte) error {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tfor _, item := range batch {\n\t\tt.size += uint64(len(item))\n\t}\n\tt.data = append(t.data, batch...)\n\tt.items += uint64(len(batch))\n\treturn nil\n}\n\n// memoryBatch is the singleton batch used for ancient write.\ntype memoryBatch struct {\n\tdata map[string][][]byte\n\tnext map[string]uint64\n\tsize map[string]int64\n}\n\nfunc newMemoryBatch() *memoryBatch {\n\treturn &memoryBatch{\n\t\tdata: make(map[string][][]byte),\n\t\tnext: make(map[string]uint64),\n\t\tsize: make(map[string]int64),\n\t}\n}\n\nfunc (b *memoryBatch) reset(freezer *MemoryFreezer) {\n\tb.data = make(map[string][][]byte)\n\tb.next = make(map[string]uint64)\n\tb.size = make(map[string]int64)\n\n\tfor name, table := range freezer.tables {\n\t\tb.next[name] = table.items\n\t}\n}\n\n// Append adds an RLP-encoded item.\nfunc (b *memoryBatch) Append(kind string, number uint64, item interface{}) error {\n\tif b.next[kind] != number {\n\t\treturn errOutOrderInsertion\n\t}\n\tblob, err := rlp.EncodeToBytes(item)\n\tif err != nil {\n\t\treturn err\n\t}\n\tb.data[kind] = append(b.data[kind], blob)\n\tb.next[kind]++\n\tb.size[kind] += int64(len(blob))\n\treturn nil\n}\n\n// AppendRaw adds an item without RLP-encoding it.\nfunc (b *memoryBatch) AppendRaw(kind string, number uint64, blob []byte) error {\n\tif b.next[kind] != number {\n\t\treturn errOutOrderInsertion\n\t}\n\tb.data[kind] = append(b.data[kind], common.CopyBytes(blob))\n\tb.next[kind]++\n\tb.size[kind] += int64(len(blob))\n\treturn nil\n}\n\n// commit is called at the end of a write operation and writes all remaining\n// data to tables.\nfunc (b *memoryBatch) commit(freezer *MemoryFreezer) (items uint64, writeSize int64, err error) {\n\t// Check that count agrees on all batches.\n\titems = math.MaxUint64\n\tfor name, next := range b.next {\n\t\t// skip empty addition tables\n\t\tif slices.Contains(additionTables, name) && next == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tif items < math.MaxUint64 && next != items {\n\t\t\treturn 0, 0, fmt.Errorf(\"table %s is at item %d, want %d\", name, next, items)\n\t\t}\n\t\titems = next\n\t}\n\t// Commit all table batches.\n\tfor name, batch := range b.data {\n\t\ttable := freezer.tables[name]\n\t\tif err := table.commit(batch)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/rawdb/freezer_memory.go",
          "line": 203,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= b.size[name]\n\t}\n\treturn items, writeSize, nil\n}\n\n// MemoryFreezer is an ephemeral ancient store. It implements the ethdb.AncientStore\n// interface and can be used along with ephemeral key-value store.\ntype MemoryFreezer struct {\n\titems      uint64                  // Number of items stored\n\ttail       uint64                  // Number of the first stored item in the freezer\n\treadonly   bool                    // Flag if the freezer is only for reading\n\tlock       sync.RWMutex            // Lock to protect fields\n\ttables     map[string]*memoryTable // Tables for storing everything\n\twriteBatch *memoryBatch            // Pre-allocated write batch\n}\n\n// NewMemoryFreezer initializes an in-memory freezer instance.\nfunc NewMemoryFreezer(readonly bool, tableName map[string]freezerTableConfig) *MemoryFreezer {\n\ttables := make(map[string]*memoryTable)\n\tfor name, cfg := range tableName {\n\t\ttables[name] = newMemoryTable(name, cfg)\n\t}\n\treturn &MemoryFreezer{\n\t\twriteBatch: newMemoryBatch(),\n\t\treadonly:   readonly,\n\t\ttables:     tables,\n\t}\n}\n\n// Ancient retrieves an ancient binary blob from the in-memory freezer.\nfunc (f *MemoryFreezer) Ancient(kind string, number uint64) ([]byte, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\tt := f.tables[kind]\n\tif t == nil {\n\t\treturn nil, errUnknownTable\n\t}\n\tdata, err := t.retrieve(number, 1, 0)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn data[0], nil\n}\n\n// AncientRange retrieves multiple items in sequence, starting from the index 'start'.\n// It will return\n//   - at most 'count' items,\n//   - if maxBytes is specified: at least 1 item (even if exceeding the maxByteSize),\n//     but will otherwise return as many items as fit into maxByteSize.\n//   - if maxBytes is not specified, 'count' items will be returned if they are present\nfunc (f *MemoryFreezer) AncientRange(kind string, start, count, maxBytes uint64) ([][]byte, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\tt := f.tables[kind]\n\tif t == nil {\n\t\treturn nil, errUnknownTable\n\t}\n\treturn t.retrieve(start, count, maxBytes)\n}\n\n// Ancients returns the ancient item numbers in the freezer.\nfunc (f *MemoryFreezer) Ancients() (uint64, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\treturn f.items, nil\n}\n\n// Tail returns the number of first stored item in the freezer.\n// This number can also be interpreted as the total deleted item numbers.\nfunc (f *MemoryFreezer) Tail() (uint64, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\treturn f.tail, nil\n}\n\n// AncientSize returns the ancient size of the specified category.\nfunc (f *MemoryFreezer) AncientSize(kind string) (uint64, error) {\n\tf.lock.RLock()\n\tdefer f.lock.RUnlock()\n\n\tif table := f.tables[kind]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 152,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 154,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 208,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 210,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 357,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 359,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 378,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 380,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 399,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 401,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 420,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 422,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 456,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 458,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 487,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 489,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0016",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 592,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0017",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 594,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0018",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 658,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0019",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 660,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0020",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 835,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0021",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 837,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0022",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 926,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0023",
          "file": "bsc/core/systemcontracts/upgrade.go",
          "line": 928,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/const.go",
          "line": 14,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/txpool.go",
          "line": 436,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= run\n\t\tblocked += block\n\t}\n\treturn runnable, blocked\n}\n\n// Content retrieves the data content of the transaction pool, returning all the\n// pending as well as queued transactions, grouped by account and sorted by nonce.\nfunc (p *TxPool) Content() (map[common.Address][]*types.Transaction, map[common.Address][]*types.Transaction) {\n\tvar (\n\t\trunnable = make(map[common.Address][]*types.Transaction)\n\t\tblocked  = make(map[common.Address][]*types.Transaction)\n\t)\n\tfor _, subpool := range p.subpools {\n\t\trun, block := subpool.Content()\n\n\t\tmaps.Copy(runnable, run)\n\t\tmaps.Copy(blocked, block)\n\t}\n\treturn runnable, blocked\n}\n\n// ContentFrom retrieves the data content of the transaction pool, returning the\n// pending as well as queued transactions of this address, grouped by nonce.\nfunc (p *TxPool) ContentFrom(addr common.Address) ([]*types.Transaction, []*types.Transaction) {\n\tfor _, subpool := range p.subpools {\n\t\trun, block := subpool.ContentFrom(addr)\n\t\tif len(run) != 0 || len(block) != 0 {\n\t\t\treturn run, block\n\t\t}\n\t}\n\treturn []*types.Transaction{}, []*types.Transaction{}\n}\n\n// Status returns the known status (unknown/pending/queued) of a transaction\n// identified by its hash.\nfunc (p *TxPool) Status(hash common.Hash) TxStatus {\n\tfor _, subpool := range p.subpools {\n\t\tif status := subpool.Status(hash)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/statedb_fuzz_test.go",
          "line": 128,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \" \" + strings.Join(names, \", \")\n\treturn action\n}\n\n// Generate returns a new snapshot test of the given size. All randomness is\n// derived from r.\nfunc (*stateTest) Generate(r *rand.Rand, size int) reflect.Value {\n\taddrs := make([]common.Address, 5)\n\tfor i := range addrs {\n\t\taddrs[i][0] = byte(i)\n\t}\n\tactions := make([][]testAction, rand.Intn(5)+1)\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/statedb_test.go",
          "line": 476,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= strings.Join(nameargs, \", \")\n\treturn action\n}\n\n// Generate returns a new snapshot test of the given size. All randomness is\n// derived from r.\nfunc (*snapshotTest) Generate(r *rand.Rand, size int) reflect.Value {\n\t// Generate random actions.\n\taddrs := make([]common.Address, 50)\n\tfor i := range addrs {\n\t\taddrs[i][0] = byte(i)\n\t}\n\tactions := make([]testAction, size)\n\tfor i := range actions {\n\t\taddr := addrs[r.Intn(len(addrs))]\n\t\tactions[i] = newTestAction(addr, r)\n\t}\n\t// Generate snapshot indexes.\n\tnsnapshots := int(math.Sqrt(float64(size)))\n\tif size > 0 && nsnapshots == 0 {\n\t\tnsnapshots = 1\n\t}\n\tsnapshots := make([]int, nsnapshots)\n\tsnaplen := len(actions) / nsnapshots\n\tfor i := range snapshots {\n\t\t// Try to place the snapshots some number of actions apart from each other.\n\t\tsnapshots[i] = (i * snaplen) + r.Intn(snaplen)\n\t}\n\treturn reflect.ValueOf(&snapshotTest{addrs, actions, snapshots, nil})\n}\n\nfunc (test *snapshotTest) String() string {\n\tout := new(bytes.Buffer)\n\tsindex := 0\n\tfor i, action := range test.actions {\n\t\tif len(test.snapshots) > sindex && i == test.snapshots[sindex] {\n\t\t\tfmt.Fprintf(out, \"---- snapshot %d ----\\n\", sindex)\n\t\t\tsindex++\n\t\t}\n\t\tfmt.Fprintf(out, \"%4d: %s\\n\", i, action.name)\n\t}\n\treturn out.String()\n}\n\nfunc (test *snapshotTest) run() bool {\n\t// Run all actions and create snapshots.\n\tvar (\n\t\tstate, _     = New(types.EmptyRootHash, NewDatabaseForTesting())\n\t\tsnapshotRevs = make([]int, len(test.snapshots))\n\t\tsindex       = 0\n\t\tcheckstates  = make([]*StateDB, len(test.snapshots))\n\t)\n\tfor i, action := range test.actions {\n\t\tif len(test.snapshots) > sindex && i == test.snapshots[sindex] {\n\t\t\tsnapshotRevs[sindex] = state.Snapshot()\n\t\t\tcheckstates[sindex] = state.Copy()\n\t\t\tsindex++\n\t\t}\n\t\taction.fn(action, state)\n\t}\n\t// Revert all snapshots in reverse order. Each revert must yield a state\n\t// that is equivalent to fresh state with all actions up the snapshot applied.\n\tfor sindex--",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/trie_prefetcher.go",
          "line": 446,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= feedNum\n\t\t}\n\t}\n\t// Children did not consume all the keys, to create new subfetch to handle left keys.\n\tkeysLeft := keys[keyIndex:]\n\tkeysLeftSize := len(keysLeft)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/state_object.go",
          "line": 218,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t}\n\n\t// Schedule the resolved storage slots for prefetching if it's enabled.\n\tif s.db.prefetcher != nil && s.data.Root != types.EmptyRootHash {\n\t\tif err = s.db.prefetcher.prefetch(s.addrHash, s.origin.Root, s.address, nil, []common.Hash{key}, true)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/access_events.go",
          "line": 102,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= consumed\n\tconsumed, expected = ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, isWrite, availableGas-consumed)\n\tif consumed < expected {\n\t\treturn expected + gas\n\t}\n\tgas += expected\n\treturn gas\n}\n\n// MessageCallGas returns the gas to be charged for each of the currently\n// cold member fields of an account, that need to be touched when making a message\n// call to that account.\nfunc (ae *AccessEvents) MessageCallGas(destination common.Address, availableGas uint64) uint64 {\n\t_, expected := ae.touchAddressAndChargeGas(destination, zeroTreeIndex, utils.BasicDataLeafKey, false, availableGas)\n\tif expected == 0 {\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n\n// ValueTransferGas returns the gas to be charged for each of the currently\n// cold balance member fields of the caller and the callee accounts.\nfunc (ae *AccessEvents) ValueTransferGas(callerAddr, targetAddr common.Address, availableGas uint64) uint64 {\n\t_, expected1 := ae.touchAddressAndChargeGas(callerAddr, zeroTreeIndex, utils.BasicDataLeafKey, true, availableGas)\n\tif expected1 > availableGas {\n\t\treturn expected1\n\t}\n\t_, expected2 := ae.touchAddressAndChargeGas(targetAddr, zeroTreeIndex, utils.BasicDataLeafKey, true, availableGas-expected1)\n\tif expected1+expected2 == 0 {\n\t\treturn params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected1 + expected2\n}\n\n// ContractCreatePreCheckGas charges access costs before\n// a contract creation is initiated. It is just reads, because the\n// address collision is done before the transfer, and so no write\n// are guaranteed to happen at this point.\nfunc (ae *AccessEvents) ContractCreatePreCheckGas(addr common.Address, availableGas uint64) uint64 {\n\tconsumed, expected1 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, false, availableGas)\n\t_, expected2 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, false, availableGas-consumed)\n\treturn expected1 + expected2\n}\n\n// ContractCreateInitGas returns the access gas costs for the initialization of\n// a contract creation.\nfunc (ae *AccessEvents) ContractCreateInitGas(addr common.Address, availableGas uint64) (uint64, uint64) {\n\tvar gas uint64\n\tconsumed, expected1 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, true, availableGas)\n\tgas += consumed\n\tconsumed, expected2 := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, true, availableGas-consumed)\n\tgas += consumed\n\treturn gas, expected1 + expected2\n}\n\n// AddTxOrigin adds the member fields of the sender account to the access event list,\n// so that cold accesses are not charged, since they are covered by the 21000 gas.\nfunc (ae *AccessEvents) AddTxOrigin(originAddr common.Address) {\n\tae.touchAddressAndChargeGas(originAddr, zeroTreeIndex, utils.BasicDataLeafKey, true, gomath.MaxUint64)\n\tae.touchAddressAndChargeGas(originAddr, zeroTreeIndex, utils.CodeHashLeafKey, false, gomath.MaxUint64)\n}\n\n// AddTxDestination adds the member fields of the sender account to the access event list,\n// so that cold accesses are not charged, since they are covered by the 21000 gas.\nfunc (ae *AccessEvents) AddTxDestination(addr common.Address, sendsValue, doesntExist bool) {\n\tae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, sendsValue, gomath.MaxUint64)\n\tae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, doesntExist, gomath.MaxUint64)\n}\n\n// SlotGas returns the amount of gas to be charged for a cold storage access.\nfunc (ae *AccessEvents) SlotGas(addr common.Address, slot common.Hash, isWrite bool, availableGas uint64, chargeWarmCosts bool) uint64 {\n\ttreeIndex, subIndex := utils.StorageIndex(slot.Bytes())\n\t_, expected := ae.touchAddressAndChargeGas(addr, *treeIndex, subIndex, isWrite, availableGas)\n\tif expected == 0 && chargeWarmCosts {\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n\n// touchAddressAndChargeGas adds any missing access event to the access event list, and returns the\n// consumed and required gas.\nfunc (ae *AccessEvents) touchAddressAndChargeGas(addr common.Address, treeIndex uint256.Int, subIndex byte, isWrite bool, availableGas uint64) (uint64, uint64) {\n\tbranchKey := newBranchAccessKey(addr, treeIndex)\n\tchunkKey := newChunkAccessKey(branchKey, subIndex)\n\n\t// Read access.\n\tvar branchRead, chunkRead bool\n\tif _, hasStem := ae.branches[branchKey]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/access_events.go",
          "line": 211,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.WitnessBranchReadCost\n\t}\n\tif chunkRead {\n\t\tgas += params.WitnessChunkReadCost\n\t}\n\tif branchWrite {\n\t\tgas += params.WitnessBranchWriteCost\n\t}\n\tif chunkWrite {\n\t\tgas += params.WitnessChunkWriteCost\n\t}\n\tif chunkFill {\n\t\tgas += params.WitnessChunkFillCost\n\t}\n\n\tif availableGas < gas {\n\t\t// consumed != expected\n\t\treturn availableGas, gas\n\t}\n\n\tif branchRead {\n\t\tae.branches[branchKey] = AccessWitnessReadFlag\n\t}\n\tif branchWrite {\n\t\tae.branches[branchKey] |= AccessWitnessWriteFlag\n\t}\n\tif chunkRead {\n\t\tae.chunks[chunkKey] = AccessWitnessReadFlag\n\t}\n\tif chunkWrite {\n\t\tae.chunks[chunkKey] |= AccessWitnessWriteFlag\n\t}\n\n\t// consumed == expected\n\treturn gas, gas\n}\n\ntype branchAccessKey struct {\n\taddr      common.Address\n\ttreeIndex uint256.Int\n}\n\nfunc newBranchAccessKey(addr common.Address, treeIndex uint256.Int) branchAccessKey {\n\tvar sk branchAccessKey\n\tsk.addr = addr\n\tsk.treeIndex = treeIndex\n\treturn sk\n}\n\ntype chunkAccessKey struct {\n\tbranchAccessKey\n\tleafKey byte\n}\n\nfunc newChunkAccessKey(branchKey branchAccessKey, leafKey byte) chunkAccessKey {\n\tvar lk chunkAccessKey\n\tlk.branchAccessKey = branchKey\n\tlk.leafKey = leafKey\n\treturn lk\n}\n\n// CodeChunksRangeGas is a helper function to touch every chunk in a code range and charge witness gas costs\nfunc (ae *AccessEvents) CodeChunksRangeGas(contractAddr common.Address, startPC, size uint64, codeLen uint64, isWrite bool, availableGas uint64) (uint64, uint64) {\n\t// note that in the case where the copied code is outside the range of the\n\t// contract code but touches the last leaf with contract code in it,\n\t// we don't include the last leaf of code in the AccessWitness.  The\n\t// reason that we do not need the last leaf is the account's code size\n\t// is already in the AccessWitness so a stateless verifier can see that\n\t// the code from the last leaf is not needed.\n\tif (codeLen == 0 && size == 0) || startPC > codeLen {\n\t\treturn 0, 0\n\t}\n\n\tendPC := min(startPC+size, codeLen)\n\tif endPC > 0 {\n\t\tendPC -= 1 // endPC is the last bytecode that will be touched.\n\t}\n\n\tvar statelessGasCharged uint64\n\tfor chunkNumber := startPC / 31",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/state/access_events.go",
          "line": 286,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1 // endPC is the last bytecode that will be touched.\n\t}\n\n\tvar statelessGasCharged uint64\n\tfor chunkNumber := startPC / 31",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/state/access_events.go",
          "line": 303,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= consumed\n\t}\n\treturn statelessGasCharged, statelessGasCharged\n}\n\n// BasicDataGas adds the account's basic data to the accessed data, and returns the\n// amount of gas that it costs.\n// Note that an access in write mode implies an access in read mode, whereas an\n// access in read mode does not imply an access in write mode.\nfunc (ae *AccessEvents) BasicDataGas(addr common.Address, isWrite bool, availableGas uint64, chargeWarmCosts bool) uint64 {\n\t_, expected := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.BasicDataLeafKey, isWrite, availableGas)\n\tif expected == 0 && chargeWarmCosts {\n\t\tif availableGas < params.WarmStorageReadCostEIP2929 {\n\t\t\treturn availableGas\n\t\t}\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n\n// CodeHashGas adds the account's code hash to the accessed data, and returns the\n// amount of gas that it costs.\n// in write mode. If false, the charged gas corresponds to an access in read mode.\n// Note that an access in write mode implies an access in read mode, whereas an access in\n// read mode does not imply an access in write mode.\nfunc (ae *AccessEvents) CodeHashGas(addr common.Address, isWrite bool, availableGas uint64, chargeWarmCosts bool) uint64 {\n\t_, expected := ae.touchAddressAndChargeGas(addr, zeroTreeIndex, utils.CodeHashLeafKey, isWrite, availableGas)\n\tif expected == 0 && chargeWarmCosts {\n\t\tif availableGas < params.WarmStorageReadCostEIP2929 {\n\t\t\treturn availableGas\n\t\t}\n\t\texpected = params.WarmStorageReadCostEIP2929\n\t}\n\treturn expected\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/statedb.go",
          "line": 333,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gas\n}\n\n// SubRefund removes gas from the refund counter.\n// This method will panic if the refund counter goes below zero\nfunc (s *StateDB) SubRefund(gas uint64) {\n\ts.journal.refundChange(s.refund)\n\tif gas > s.refund {\n\t\tpanic(fmt.Sprintf(\"Refund counter below zero (gas: %d > refund: %d)\", gas, s.refund))\n\t}\n\ts.refund -= gas\n}\n\n// Exist reports whether the given account address exists in the state.\n// Notably this also returns true for self-destructed accounts within the current transaction.\nfunc (s *StateDB) Exist(addr common.Address) bool {\n\treturn s.getStateObject(addr) != nil\n}\n\n// Empty returns whether the state object is either non-existent\n// or empty according to the EIP161 specification (balance = nonce = code = 0)\nfunc (s *StateDB) Empty(addr common.Address) bool {\n\tso := s.getStateObject(addr)\n\treturn so == nil || so.empty()\n}\n\n// GetBalance retrieves the balance from the given address or 0 if object not found\nfunc (s *StateDB) GetBalance(addr common.Address) *uint256.Int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Balance()\n\t}\n\treturn common.U2560\n}\n\n// GetNonce retrieves the nonce from the given address or 0 if object not found\nfunc (s *StateDB) GetNonce(addr common.Address) uint64 {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Nonce()\n\t}\n\n\treturn 0\n}\n\n// GetStorageRoot retrieves the storage root from the given address or empty\n// if object not found.\nfunc (s *StateDB) GetStorageRoot(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Root()\n\t}\n\treturn common.Hash{}\n}\n\n// TxIndex returns the current transaction index set by SetTxContext.\nfunc (s *StateDB) TxIndex() int {\n\treturn s.txIndex\n}\n\nfunc (s *StateDB) GetCode(addr common.Address) []byte {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.Code()\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) GetCodeSize(addr common.Address) int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.CodeSize()\n\t}\n\treturn 0\n}\n\nfunc (s *StateDB) GetCodeHash(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn common.BytesToHash(stateObject.CodeHash())\n\t}\n\treturn common.Hash{}\n}\n\n// GetState retrieves the value associated with the specific key.\nfunc (s *StateDB) GetState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// GetCommittedState retrieves the value associated with the specific key\n// without any mutations caused in the current execution.\nfunc (s *StateDB) GetCommittedState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetCommittedState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// Database retrieves the low level database supporting the lower level trie ops.\nfunc (s *StateDB) Database() Database {\n\treturn s.db\n}\n\n// Reader retrieves the low level database reader supporting the\n// lower level operations.\nfunc (s *StateDB) Reader() Reader {\n\treturn s.reader\n}\n\nfunc (s *StateDB) HasSelfDestructed(addr common.Address) bool {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.selfDestructed\n\t}\n\treturn false\n}\n\n/*\n * SETTERS\n */\n\n// AddBalance adds amount to the account associated with addr.\nfunc (s *StateDB) AddBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\treturn stateObject.AddBalance(amount)\n}\n\n// SubBalance subtracts amount from the account associated with addr.\nfunc (s *StateDB) SubBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\tif amount.IsZero() {\n\t\treturn *(stateObject.Balance())\n\t}\n\treturn stateObject.SetBalance(new(uint256.Int).Sub(stateObject.Balance(), amount))\n}\n\nfunc (s *StateDB) SetBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetBalance(amount)\n\t}\n}\n\nfunc (s *StateDB) SetNonce(addr common.Address, nonce uint64, reason tracing.NonceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetNonce(nonce)\n\t}\n}\n\nfunc (s *StateDB) SetCode(addr common.Address, code []byte) (prev []byte) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.SetCode(crypto.Keccak256Hash(code), code)\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) SetState(addr common.Address, key, value common.Hash) common.Hash {\n\tif stateObject := s.getOrNewStateObject(addr)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/statedb.go",
          "line": 655,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t}\n\n\t// Short circuit if the account is not found\n\tif acct == nil {\n\t\treturn nil\n\t}\n\t// Schedule the resolved account for prefetching if it's enabled.\n\tif s.prefetcher != nil {\n\t\tif err = s.prefetcher.prefetch(common.Hash{}, s.originalRoot, common.Address{}, []common.Address{addr}, nil, true)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/state/statedb.go",
          "line": 958,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t}\n\n\t// Now we're about to start to write changes to the trie. The trie is so far\n\t// _untouched_. We can check with the prefetcher, if it can give us a trie\n\t// which has the same root, but also has some content loaded into it.\n\t//\n\t// Don't check prefetcher if verkle trie has been used. In the context of verkle,\n\t// only a single trie is used for state hashing. Replacing a non-nil verkle tree\n\t// here could result in losing uncommitted changes from storage.\n\tif metrics.EnabledExpensive() {\n\t\tstart = time.Now()\n\t}\n\tif s.prefetcher != nil {\n\t\tif trie := s.prefetcher.trie(common.Hash{}, s.originalRoot)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/state/statedb.go",
          "line": 1002,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t\tusedAddrs = append(usedAddrs, addr) // Copy needed for closure\n\t}\n\tfor _, deletedAddr := range deletedAddrs {\n\t\ts.deleteStateObject(deletedAddr)\n\t\ts.AccountDeleted += 1\n\t}\n\tif metrics.EnabledExpensive() {\n\t\ts.AccountUpdates += time.Since(start)\n\t}\n\n\tif s.prefetcher != nil && len(usedAddrs) > 0 {\n\t\ts.prefetcher.used(common.Hash{}, s.originalRoot, usedAddrs, nil)\n\t}\n\n\tif metrics.EnabledExpensive() {\n\t\t// Track the amount of time wasted on hashing the account trie\n\t\tdefer func(start time.Time) { s.AccountHashes += time.Since(start) }(time.Now())\n\t}\n\n\thash := s.trie.Hash()\n\n\t// If witness building is enabled, gather the account trie witness\n\tif s.witness != nil {\n\t\ts.witness.AddState(s.trie.Witness())\n\t}\n\n\tif s.db.NoTries() {\n\t\treturn s.expectedRoot\n\t} else {\n\t\treturn hash\n\t}\n}\n\n// SetTxContext sets the current transaction hash and index which are\n// used when the EVM emits new state logs. It should be invoked before\n// transaction execution.\nfunc (s *StateDB) SetTxContext(thash common.Hash, ti int) {\n\ts.thash = thash\n\ts.txIndex = ti\n}\n\n// StateDB.Prepare is not called before processing a system transaction, call ClearAccessList instead.\nfunc (s *StateDB) ClearAccessList() {\n\ts.accessList = newAccessList()\n}\n\nfunc (s *StateDB) clearJournalAndRefund() {\n\ts.journal.reset()\n\ts.refund = 0\n}\n\n// fastDeleteStorage is the function that efficiently deletes the storage trie\n// of a specific account. It leverages the associated state snapshot for fast\n// storage iteration and constructs trie node deletion markers by creating\n// stack trie with iterated slots.\nfunc (s *StateDB) fastDeleteStorage(snaps *snapshot.Tree, addrHash common.Hash, root common.Hash) (map[common.Hash][]byte, map[common.Hash][]byte, *trienode.NodeSet, error) {\n\titer, err := snaps.StorageIterator(s.originalRoot, addrHash, common.Hash{})\n\tif err != nil {\n\t\treturn nil, nil, nil, err\n\t}\n\tdefer iter.Release()\n\n\tvar (\n\t\tnodes          = trienode.NewNodeSet(addrHash) // the set for trie node mutations (value is nil)\n\t\tstorages       = make(map[common.Hash][]byte)  // the set for storage mutations (value is nil)\n\t\tstorageOrigins = make(map[common.Hash][]byte)  // the set for tracking the original value of slot\n\t)\n\tstack := trie.NewStackTrie(func(path []byte, hash common.Hash, blob []byte) {\n\t\tnodes.AddNode(path, trienode.NewDeleted())\n\t})\n\tfor iter.Next() {\n\t\tslot := common.CopyBytes(iter.Slot())\n\t\tif err := iter.Error()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/state/statedb.go",
          "line": 1274,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= updates\n\t\t\t\taccountTrieNodesDeleted += deletes\n\t\t\t} else {\n\t\t\t\tstorageTrieNodesUpdated += updates\n\t\t\t\tstorageTrieNodesDeleted += deletes\n\t\t\t}\n\t\t\tif s.NoTries() {\n\t\t\t\treturn nil\n\t\t\t}\n\t\t\treturn nodes.Merge(set)\n\t\t}\n\t)\n\t// Given that some accounts could be destroyed and then recreated within\n\t// the same block, account deletions must be processed first. This ensures\n\t// that the storage trie nodes deleted during destruction and recreated\n\t// during subsequent resurrection can be combined correctly.\n\tdeletes, delNodes, err := s.handleDestruction(noStorageWiping)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tfor _, set := range delNodes {\n\t\tif err := merge(set)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/state/statedb.go",
          "line": 1437,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\t}\n\t\t}\n\t\t// If trie database is enabled, commit the state update as a new layer\n\t\tif db := s.db.TrieDB()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/state/statedb.go",
          "line": 1447,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\t}\n\t\t}\n\t}\n\ts.reader, _ = s.db.Reader(s.originalRoot)\n\treturn ret, err\n}\n\n// Commit writes the state mutations into the configured data stores.\n//\n// Once the state is committed, tries cached in stateDB (including account\n// trie, storage tries) will no longer be functional. A new state instance\n// must be created with new root and updated database for accessing post-\n// commit states.\n//\n// The associated block number of the state transition is also provided\n// for more chain context.\n//\n// noStorageWiping is a flag indicating whether storage wiping is permitted.\n// Since self-destruction was deprecated with the Cancun fork and there are\n// no empty accounts left that could be deleted by EIP-158, storage wiping\n// should not occur.\nfunc (s *StateDB) Commit(block uint64, deleteEmptyObjects bool, noStorageWiping bool) (common.Hash, error) {\n\tret, err := s.commitAndFlush(block, deleteEmptyObjects, noStorageWiping)\n\tif err != nil {\n\t\treturn common.Hash{}, err\n\t}\n\treturn ret.root, nil\n}\n\n// Prepare handles the preparatory steps for executing a state transition with.\n// This method must be invoked before state transition.\n//\n// Berlin fork:\n// - Add sender to access list (2929)\n// - Add destination to access list (2929)\n// - Add precompiles to access list (2929)\n// - Add the contents of the optional tx access list (2930)\n//\n// Potential EIPs:\n// - Reset access list (Berlin)\n// - Add coinbase to access list (EIP-3651)\n// - Reset transient storage (EIP-1153)\nfunc (s *StateDB) Prepare(rules params.Rules, sender, coinbase common.Address, dst *common.Address, precompiles []common.Address, list types.AccessList) {\n\tif rules.IsEIP2929 && rules.IsEIP4762 {\n\t\tpanic(\"eip2929 and eip4762 are both activated\")\n\t}\n\tif rules.IsEIP2929 {\n\t\t// Clear out any leftover from previous executions\n\t\tal := newAccessList()\n\t\ts.accessList = al\n\n\t\tal.AddAddress(sender)\n\t\tif dst != nil {\n\t\t\tal.AddAddress(*dst)\n\t\t\t// If it's a create-tx, the destination will be added inside evm.create\n\t\t}\n\t\tfor _, addr := range precompiles {\n\t\t\tal.AddAddress(addr)\n\t\t}\n\t\tfor _, el := range list {\n\t\t\tal.AddAddress(el.Address)\n\t\t\tfor _, key := range el.StorageKeys {\n\t\t\t\tal.AddSlot(el.Address, key)\n\t\t\t}\n\t\t}\n\t\tif rules.IsShanghai { // EIP-3651: warm coinbase\n\t\t\tal.AddAddress(coinbase)\n\t\t}\n\t}\n\t// Reset transient storage at the beginning of transaction execution\n\ts.transientStorage = newTransientStorage()\n}\n\n// AddAddressToAccessList adds the given address to the access list\nfunc (s *StateDB) AddAddressToAccessList(addr common.Address) {\n\tif s.accessList.AddAddress(addr) {\n\t\ts.journal.accessListAddAccount(addr)\n\t}\n}\n\n// AddSlotToAccessList adds the given (address, slot)-tuple to the access list\nfunc (s *StateDB) AddSlotToAccessList(addr common.Address, slot common.Hash) {\n\taddrMod, slotMod := s.accessList.AddSlot(addr, slot)\n\tif addrMod {\n\t\t// In practice, this should not happen, since there is no way to enter the\n\t\t// scope of 'address' without having the 'address' become already added\n\t\t// to the access list (via call-variant, create, etc).\n\t\t// Better safe than sorry, though\n\t\ts.journal.accessListAddAccount(addr)\n\t}\n\tif slotMod {\n\t\ts.journal.accessListAddSlot(addr, slot)\n\t}\n}\n\n// AddressInAccessList returns true if the given address is in the access list.\nfunc (s *StateDB) AddressInAccessList(addr common.Address) bool {\n\treturn s.accessList.ContainsAddress(addr)\n}\n\n// SlotInAccessList returns true if the given (address, slot)-tuple is in the access list.\nfunc (s *StateDB) SlotInAccessList(addr common.Address, slot common.Hash) (addressPresent bool, slotPresent bool) {\n\treturn s.accessList.Contains(addr, slot)\n}\n\nfunc (s *StateDB) GetSnap() snapshot.Snapshot {\n\tsnaps := s.db.Snapshot()\n\tif snaps != nil {\n\t\treturn snaps.Snapshot(s.originalRoot)\n\t}\n\treturn nil\n}\n\n// markDelete is invoked when an account is deleted but the deletion is\n// not yet committed. The pending mutation is cached and will be applied\n// all together\nfunc (s *StateDB) markDelete(addr common.Address) {\n\tif _, ok := s.mutations[addr]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/state/statedb.go",
          "line": 343,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas\n}\n\n// Exist reports whether the given account address exists in the state.\n// Notably this also returns true for self-destructed accounts within the current transaction.\nfunc (s *StateDB) Exist(addr common.Address) bool {\n\treturn s.getStateObject(addr) != nil\n}\n\n// Empty returns whether the state object is either non-existent\n// or empty according to the EIP161 specification (balance = nonce = code = 0)\nfunc (s *StateDB) Empty(addr common.Address) bool {\n\tso := s.getStateObject(addr)\n\treturn so == nil || so.empty()\n}\n\n// GetBalance retrieves the balance from the given address or 0 if object not found\nfunc (s *StateDB) GetBalance(addr common.Address) *uint256.Int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Balance()\n\t}\n\treturn common.U2560\n}\n\n// GetNonce retrieves the nonce from the given address or 0 if object not found\nfunc (s *StateDB) GetNonce(addr common.Address) uint64 {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Nonce()\n\t}\n\n\treturn 0\n}\n\n// GetStorageRoot retrieves the storage root from the given address or empty\n// if object not found.\nfunc (s *StateDB) GetStorageRoot(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.Root()\n\t}\n\treturn common.Hash{}\n}\n\n// TxIndex returns the current transaction index set by SetTxContext.\nfunc (s *StateDB) TxIndex() int {\n\treturn s.txIndex\n}\n\nfunc (s *StateDB) GetCode(addr common.Address) []byte {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.Code()\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) GetCodeSize(addr common.Address) int {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\tif s.witness != nil {\n\t\t\ts.witness.AddCode(stateObject.Code())\n\t\t}\n\t\treturn stateObject.CodeSize()\n\t}\n\treturn 0\n}\n\nfunc (s *StateDB) GetCodeHash(addr common.Address) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn common.BytesToHash(stateObject.CodeHash())\n\t}\n\treturn common.Hash{}\n}\n\n// GetState retrieves the value associated with the specific key.\nfunc (s *StateDB) GetState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// GetCommittedState retrieves the value associated with the specific key\n// without any mutations caused in the current execution.\nfunc (s *StateDB) GetCommittedState(addr common.Address, hash common.Hash) common.Hash {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.GetCommittedState(hash)\n\t}\n\treturn common.Hash{}\n}\n\n// Database retrieves the low level database supporting the lower level trie ops.\nfunc (s *StateDB) Database() Database {\n\treturn s.db\n}\n\n// Reader retrieves the low level database reader supporting the\n// lower level operations.\nfunc (s *StateDB) Reader() Reader {\n\treturn s.reader\n}\n\nfunc (s *StateDB) HasSelfDestructed(addr common.Address) bool {\n\tstateObject := s.getStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.selfDestructed\n\t}\n\treturn false\n}\n\n/*\n * SETTERS\n */\n\n// AddBalance adds amount to the account associated with addr.\nfunc (s *StateDB) AddBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\treturn stateObject.AddBalance(amount)\n}\n\n// SubBalance subtracts amount from the account associated with addr.\nfunc (s *StateDB) SubBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) uint256.Int {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject == nil {\n\t\treturn uint256.Int{}\n\t}\n\tif amount.IsZero() {\n\t\treturn *(stateObject.Balance())\n\t}\n\treturn stateObject.SetBalance(new(uint256.Int).Sub(stateObject.Balance(), amount))\n}\n\nfunc (s *StateDB) SetBalance(addr common.Address, amount *uint256.Int, reason tracing.BalanceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetBalance(amount)\n\t}\n}\n\nfunc (s *StateDB) SetNonce(addr common.Address, nonce uint64, reason tracing.NonceChangeReason) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\tstateObject.SetNonce(nonce)\n\t}\n}\n\nfunc (s *StateDB) SetCode(addr common.Address, code []byte) (prev []byte) {\n\tstateObject := s.getOrNewStateObject(addr)\n\tif stateObject != nil {\n\t\treturn stateObject.SetCode(crypto.Keccak256Hash(code), code)\n\t}\n\treturn nil\n}\n\nfunc (s *StateDB) SetState(addr common.Address, key, value common.Hash) common.Hash {\n\tif stateObject := s.getOrNewStateObject(addr)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/gas_table_test.go",
          "line": 100,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/gas_table_test.go",
          "line": 156,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/gas_table.go",
          "line": 381,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CallNewAccountGas\n\t\t}\n\t} else if !evm.StateDB.Exist(address) {\n\t\tgas += params.CallNewAccountGas\n\t}\n\tif transfersValue && !evm.chainRules.IsEIP4762 {\n\t\tgas += params.CallValueTransferGas\n\t}\n\tmemoryGas, err := memoryGasCost(mem, memorySize)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tvar overflow bool\n\tif gas, overflow = math.SafeAdd(gas, memoryGas)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/gas_table.go",
          "line": 419,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CallValueTransferGas\n\t}\n\tif gas, overflow = math.SafeAdd(gas, memoryGas)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/gas_table.go",
          "line": 476,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CreateBySelfdestructGas\n\t\t\t}\n\t\t} else if !evm.StateDB.Exist(address) {\n\t\t\tgas += params.CreateBySelfdestructGas\n\t\t}\n\t}\n\n\tif !evm.StateDB.HasSelfDestructed(contract.Address()) {\n\t\tevm.StateDB.AddRefund(params.SelfdestructRefundGas)\n\t}\n\treturn gas, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/evm.go",
          "line": 223,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= wgas\n\t\t}\n\n\t\tif !isPrecompile && evm.chainRules.IsEIP158 && value.IsZero() {\n\t\t\t// Calling a non-existing account, don't do anything.\n\t\t\treturn nil, gas, nil\n\t\t}\n\t\tevm.StateDB.CreateAccount(addr)\n\t}\n\tevm.Context.Transfer(evm.StateDB, caller, addr, value)\n\n\tif isPrecompile {\n\t\tret, gas, err = RunPrecompiledContract(p, input, gas, evm.Config.Tracer)\n\t} else {\n\t\t// Initialise a new contract and set the code that is to be used by the EVM.\n\t\tcode := evm.resolveCode(addr)\n\t\tif len(code) == 0 {\n\t\t\tret, err = nil, nil // gas is unchanged\n\t\t} else {\n\t\t\t// If the account has no code, we can abort here\n\t\t\t// The depth-check is already done, and precompiles handled above\n\t\t\tcontract := GetContract(caller, addr, value, gas, evm.jumpDests)\n\t\t\tdefer ReturnContract(contract)\n\n\t\t\tcontract.IsSystemCall = isSystemCall(caller)\n\t\t\tcontract.SetCallCode(evm.resolveCodeHash(addr), code)\n\t\t\tret, err = evm.interpreter.Run(contract, input, false)\n\t\t\tgas = contract.Gas\n\t\t}\n\t}\n\t// When an error was returned by the EVM or when setting the creation code\n\t// above we revert to the snapshot and consume any gas remaining. Additionally,\n\t// when we're in homestead this also counts for code storage gas errors.\n\tif err != nil {\n\t\tevm.StateDB.RevertToSnapshot(snapshot)\n\t\tif err != ErrExecutionReverted {\n\t\t\tif evm.Config.Tracer != nil && evm.Config.Tracer.OnGasChange != nil {\n\t\t\t\tevm.Config.Tracer.OnGasChange(gas, 0, tracing.GasChangeCallFailedExecution)\n\t\t\t}\n\n\t\t\tgas = 0\n\t\t}\n\t\t// TODO: consider clearing up unused snapshots:\n\t\t// } else {\n\t\t//\tevm.StateDB.DiscardSnapshot(snapshot)\n\t}\n\treturn ret, gas, err\n}\n\n// CallCode executes the contract associated with the addr with the given input\n// as parameters. It also handles any necessary value transfer required and takes\n// the necessary steps to create accounts and reverses the state in case of an\n// execution error or failed value transfer.\n//\n// CallCode differs from Call in the sense that it executes the given address'\n// code with the caller as context.\nfunc (evm *EVM) CallCode(caller common.Address, addr common.Address, input []byte, gas uint64, value *uint256.Int) (ret []byte, leftOverGas uint64, err error) {\n\t// Invoke tracer hooks that signal entering/exiting a call frame\n\tif evm.Config.Tracer != nil {\n\t\tevm.captureBegin(evm.depth, CALLCODE, caller, addr, input, gas, value.ToBig())\n\t\tdefer func(startGas uint64) {\n\t\t\tevm.captureEnd(evm.depth, startGas, leftOverGas, ret, err)\n\t\t}(gas)\n\t}\n\t// Fail if we're trying to execute above the call depth limit\n\tif evm.depth > int(params.CallCreateDepth) {\n\t\treturn nil, gas, ErrDepth\n\t}\n\t// Fail if we're trying to transfer more than the available balance\n\t// Note although it's noop to transfer X ether to caller itself. But\n\t// if caller doesn't have enough balance, it would be an error to allow\n\t// over-charging itself. So the check here is necessary.\n\tif !evm.Context.CanTransfer(evm.StateDB, caller, value) {\n\t\treturn nil, gas, ErrInsufficientBalance\n\t}\n\tvar snapshot = evm.StateDB.Snapshot()\n\n\t// It is allowed to call precompiles, even via delegatecall\n\tif p, isPrecompile := evm.precompile(addr)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/operations_acl.go",
          "line": 185,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= coldCost\n\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, coldCost)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/operations_acl.go",
          "line": 237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CreateBySelfdestructGas\n\t\t}\n\t\tif refundsEnabled && !evm.StateDB.HasSelfDestructed(contract.Address()) {\n\t\t\tevm.StateDB.AddRefund(params.SelfdestructRefundGas)\n\t\t}\n\t\treturn gas, nil\n\t}\n\treturn gasFunc\n}\n\nvar (\n\tgasCallEIP7702         = makeCallVariantGasCallEIP7702(gasCall)\n\tgasDelegateCallEIP7702 = makeCallVariantGasCallEIP7702(gasDelegateCall)\n\tgasStaticCallEIP7702   = makeCallVariantGasCallEIP7702(gasStaticCall)\n\tgasCallCodeEIP7702     = makeCallVariantGasCallEIP7702(gasCallCode)\n)\n\nfunc makeCallVariantGasCallEIP7702(oldCalculator gasFunc) gasFunc {\n\treturn func(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n\t\tvar (\n\t\t\ttotal uint64 // total dynamic gas used\n\t\t\taddr  = common.Address(stack.Back(1).Bytes20())\n\t\t)\n\n\t\t// Check slot presence in the access list\n\t\tif !evm.StateDB.AddressInAccessList(addr) {\n\t\t\tevm.StateDB.AddAddressToAccessList(addr)\n\t\t\t// The WarmStorageReadCostEIP2929 (100) is already deducted in the form of a constant cost, so\n\t\t\t// the cost to charge for cold access, if any, is Cold - Warm\n\t\t\tcoldCost := params.ColdAccountAccessCostEIP2929 - params.WarmStorageReadCostEIP2929\n\t\t\t// Charge the remaining difference here already, to correctly calculate available\n\t\t\t// gas for call\n\t\t\tif !contract.UseGas(coldCost, evm.Config.Tracer, tracing.GasChangeCallStorageColdAccess) {\n\t\t\t\treturn 0, ErrOutOfGas\n\t\t\t}\n\t\t\ttotal += coldCost\n\t\t}\n\n\t\t// Check if code is a delegation and if so, charge for resolution.\n\t\tif target, ok := types.ParseDelegation(evm.StateDB.GetCode(addr))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/operations_acl.go",
          "line": 287,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= cost\n\t\t}\n\n\t\t// Now call the old calculator, which takes into account\n\t\t// - create new account\n\t\t// - transfer value\n\t\t// - memory expansion\n\t\t// - 63/64ths rule\n\t\told, err := oldCalculator(evm, contract, stack, mem, memorySize)\n\t\tif err != nil {\n\t\t\treturn old, err\n\t\t}\n\n\t\t// Temporarily add the gas charge back to the contract and return value. By\n\t\t// adding it to the return, it will be charged outside of this function, as\n\t\t// part of the dynamic gas. This will ensure it is correctly reported to\n\t\t// tracers.\n\t\tcontract.Gas += total\n\n\t\tvar overflow bool\n\t\tif total, overflow = math.SafeAdd(old, total)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/eips.go",
          "line": 360,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif *pc < codeLen {\n\t\tscope.Stack.push(integer.SetUint64(uint64(scope.Contract.Code[*pc])))\n\n\t\tif !scope.Contract.IsDeployment && !scope.Contract.IsSystemCall && *pc%31 == 0 {\n\t\t\t// touch next chunk if PUSH1 is at the boundary. if so, *pc has\n\t\t\t// advanced past this boundary.\n\t\t\tcontractAddr := scope.Contract.Address()\n\t\t\tconsumed, wanted := interpreter.evm.AccessEvents.CodeChunksRangeGas(contractAddr, *pc+1, uint64(1), uint64(len(scope.Contract.Code)), false, scope.Contract.Gas)\n\t\t\tscope.Contract.UseGas(wanted, interpreter.evm.Config.Tracer, tracing.GasChangeUnspecified)\n\t\t\tif consumed < wanted {\n\t\t\t\treturn nil, ErrOutOfGas\n\t\t\t}\n\t\t}\n\t} else {\n\t\tscope.Stack.push(integer.Clear())\n\t}\n\treturn nil, nil\n}\n\nfunc makePushEIP4762(size uint64, pushByteSize int) executionFunc {\n\treturn func(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t\tvar (\n\t\t\tcodeLen = len(scope.Contract.Code)\n\t\t\tstart   = min(codeLen, int(*pc+1))\n\t\t\tend     = min(codeLen, start+pushByteSize)\n\t\t)\n\t\tscope.Stack.push(new(uint256.Int).SetBytes(\n\t\t\tcommon.RightPadBytes(\n\t\t\t\tscope.Contract.Code[start:end],\n\t\t\t\tpushByteSize,\n\t\t\t)),\n\t\t)\n\n\t\tif !scope.Contract.IsDeployment && !scope.Contract.IsSystemCall {\n\t\t\tcontractAddr := scope.Contract.Address()\n\t\t\tconsumed, wanted := interpreter.evm.AccessEvents.CodeChunksRangeGas(contractAddr, uint64(start), uint64(pushByteSize), uint64(len(scope.Contract.Code)), false, scope.Contract.Gas)\n\t\t\tscope.Contract.UseGas(consumed, interpreter.evm.Config.Tracer, tracing.GasChangeUnspecified)\n\t\t\tif consumed < wanted {\n\t\t\t\treturn nil, ErrOutOfGas\n\t\t\t}\n\t\t}\n\n\t\t*pc += size\n\t\treturn nil, nil\n\t}\n}\n\nfunc enable4762(jt *JumpTable) {\n\tjt[SSTORE] = &operation{\n\t\tdynamicGas: gasSStore4762,\n\t\texecute:    opSstore,\n\t\tminStack:   minStack(2, 0),\n\t\tmaxStack:   maxStack(2, 0),\n\t}\n\tjt[SLOAD] = &operation{\n\t\tdynamicGas: gasSLoad4762,\n\t\texecute:    opSload,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[BALANCE] = &operation{\n\t\texecute:    opBalance,\n\t\tdynamicGas: gasBalance4762,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[EXTCODESIZE] = &operation{\n\t\texecute:    opExtCodeSize,\n\t\tdynamicGas: gasExtCodeSize4762,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[EXTCODEHASH] = &operation{\n\t\texecute:    opExtCodeHash,\n\t\tdynamicGas: gasExtCodeHash4762,\n\t\tminStack:   minStack(1, 1),\n\t\tmaxStack:   maxStack(1, 1),\n\t}\n\n\tjt[EXTCODECOPY] = &operation{\n\t\texecute:    opExtCodeCopyEIP4762,\n\t\tdynamicGas: gasExtCodeCopyEIP4762,\n\t\tminStack:   minStack(4, 0),\n\t\tmaxStack:   maxStack(4, 0),\n\t\tmemorySize: memoryExtCodeCopy,\n\t}\n\n\tjt[CODECOPY] = &operation{\n\t\texecute:     opCodeCopy,\n\t\tconstantGas: GasFastestStep,\n\t\tdynamicGas:  gasCodeCopyEip4762,\n\t\tminStack:    minStack(3, 0),\n\t\tmaxStack:    maxStack(3, 0),\n\t\tmemorySize:  memoryCodeCopy,\n\t}\n\n\tjt[SELFDESTRUCT] = &operation{\n\t\texecute:     opSelfdestruct6780,\n\t\tdynamicGas:  gasSelfdestructEIP4762,\n\t\tconstantGas: params.SelfdestructGasEIP150,\n\t\tminStack:    minStack(1, 0),\n\t\tmaxStack:    maxStack(1, 0),\n\t}\n\n\tjt[CREATE] = &operation{\n\t\texecute:     opCreate,\n\t\tconstantGas: params.CreateNGasEip4762,\n\t\tdynamicGas:  gasCreateEip3860,\n\t\tminStack:    minStack(3, 1),\n\t\tmaxStack:    maxStack(3, 1),\n\t\tmemorySize:  memoryCreate,\n\t}\n\n\tjt[CREATE2] = &operation{\n\t\texecute:     opCreate2,\n\t\tconstantGas: params.CreateNGasEip4762,\n\t\tdynamicGas:  gasCreate2Eip3860,\n\t\tminStack:    minStack(4, 1),\n\t\tmaxStack:    maxStack(4, 1),\n\t\tmemorySize:  memoryCreate2,\n\t}\n\n\tjt[CALL] = &operation{\n\t\texecute:    opCall,\n\t\tdynamicGas: gasCallEIP4762,\n\t\tminStack:   minStack(7, 1),\n\t\tmaxStack:   maxStack(7, 1),\n\t\tmemorySize: memoryCall,\n\t}\n\n\tjt[CALLCODE] = &operation{\n\t\texecute:    opCallCode,\n\t\tdynamicGas: gasCallCodeEIP4762,\n\t\tminStack:   minStack(7, 1),\n\t\tmaxStack:   maxStack(7, 1),\n\t\tmemorySize: memoryCall,\n\t}\n\n\tjt[STATICCALL] = &operation{\n\t\texecute:    opStaticCall,\n\t\tdynamicGas: gasStaticCallEIP4762,\n\t\tminStack:   minStack(6, 1),\n\t\tmaxStack:   maxStack(6, 1),\n\t\tmemorySize: memoryStaticCall,\n\t}\n\n\tjt[DELEGATECALL] = &operation{\n\t\texecute:    opDelegateCall,\n\t\tdynamicGas: gasDelegateCallEIP4762,\n\t\tminStack:   minStack(6, 1),\n\t\tmaxStack:   maxStack(6, 1),\n\t\tmemorySize: memoryDelegateCall,\n\t}\n\n\tjt[PUSH1] = &operation{\n\t\texecute:     opPush1EIP4762,\n\t\tconstantGas: GasFastestStep,\n\t\tminStack:    minStack(0, 1),\n\t\tmaxStack:    maxStack(0, 1),\n\t}\n\tfor i := 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/instructions.go",
          "line": 751,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= params.CallStipend\n\t}\n\tret, returnGas, err := interpreter.evm.Call(scope.Contract.Address(), toAddr, args, gas, &value)\n\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opCallCode(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, value, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tif !value.IsZero() {\n\t\tgas += params.CallStipend\n\t}\n\n\tret, returnGas, err := interpreter.evm.CallCode(scope.Contract.Address(), toAddr, args, gas, &value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opDelegateCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tstack := scope.Stack\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.DelegateCall(scope.Contract.Caller(), scope.Contract.Address(), toAddr, args, gas, scope.Contract.value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opStaticCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.StaticCall(scope.Contract.Address(), toAddr, args, gas)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opReturn(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\treturn ret, errStopToken\n}\n\nfunc opRevert(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\tinterpreter.returnData = ret\n\treturn ret, ErrExecutionReverted\n}\n\nfunc opUndefined(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, &ErrInvalidOpCode{opcode: OpCode(scope.Contract.Code[*pc])}\n}\n\nfunc opStop(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, errStopToken\n}\n\nfunc opSelfdestruct(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tif interpreter.readOnly {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tbeneficiary := scope.Stack.pop()\n\tbalance := interpreter.evm.StateDB.GetBalance(scope.Contract.Address())\n\tinterpreter.evm.StateDB.AddBalance(beneficiary.Bytes20(), balance, tracing.BalanceIncreaseSelfdestruct)\n\tinterpreter.evm.StateDB.SelfDestruct(scope.Contract.Address())\n\tif tracer := interpreter.evm.Config.Tracer",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/instructions.go",
          "line": 960,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif *pc < codeLen {\n\t\tscope.Stack.push(integer.SetUint64(uint64(scope.Contract.Code[*pc])))\n\t} else {\n\t\tscope.Stack.push(integer.Clear())\n\t}\n\treturn nil, nil\n}\n\n// opPush2 is a specialized version of pushN\nfunc opPush2(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tvar (\n\t\tcodeLen = uint64(len(scope.Contract.Code))\n\t\tinteger = new(uint256.Int)\n\t)\n\tif *pc+2 < codeLen {\n\t\tscope.Stack.push(integer.SetBytes2(scope.Contract.Code[*pc+1 : *pc+3]))\n\t} else if *pc+1 < codeLen {\n\t\tscope.Stack.push(integer.SetUint64(uint64(scope.Contract.Code[*pc+1]) << 8))\n\t} else {\n\t\tscope.Stack.push(integer.Clear())\n\t}\n\t*pc += 2\n\treturn nil, nil\n}\n\n// make push instruction function\nfunc makePush(size uint64, pushByteSize int) executionFunc {\n\treturn func(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t\tvar (\n\t\t\tcodeLen = len(scope.Contract.Code)\n\t\t\tstart   = min(codeLen, int(*pc+1))\n\t\t\tend     = min(codeLen, start+pushByteSize)\n\t\t)\n\t\ta := new(uint256.Int).SetBytes(scope.Contract.Code[start:end])\n\n\t\t// Missing bytes: pushByteSize - len(pushData)\n\t\tif missing := pushByteSize - (end - start)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/instructions.go",
          "line": 1001,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\t\treturn nil, nil\n\t}\n}\n\n// make dup instruction function\nfunc makeDup(size int64) executionFunc {\n\treturn func(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t\tscope.Stack.dup(int(size))\n\t\treturn nil, nil\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vm/instructions.go",
          "line": 667,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas / 64\n\t}\n\n\t// reuse size int for stackvalue\n\tstackvalue := size\n\n\tscope.Contract.UseGas(gas, interpreter.evm.Config.Tracer, tracing.GasChangeCallContractCreation)\n\n\tres, addr, returnGas, suberr := interpreter.evm.Create(scope.Contract.Address(), input, gas, &value)\n\t// Push item on the stack based on the returned error. If the ruleset is\n\t// homestead we must check for CodeStoreOutOfGasError (homestead only\n\t// rule) and treat as an error, if the ruleset is frontier we must\n\t// ignore this error and pretend the operation was successful.\n\tif interpreter.evm.chainRules.IsHomestead && suberr == ErrCodeStoreOutOfGas {\n\t\tstackvalue.Clear()\n\t} else if suberr != nil && suberr != ErrCodeStoreOutOfGas {\n\t\tstackvalue.Clear()\n\t} else {\n\t\tstackvalue.SetBytes(addr.Bytes())\n\t}\n\tscope.Stack.push(&stackvalue)\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tif suberr == ErrExecutionReverted {\n\t\tinterpreter.returnData = res // set REVERT data to return data buffer\n\t\treturn res, nil\n\t}\n\tinterpreter.returnData = nil // clear dirty return data buffer\n\treturn nil, nil\n}\n\nfunc opCreate2(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tif interpreter.readOnly {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tvar (\n\t\tendowment    = scope.Stack.pop()\n\t\toffset, size = scope.Stack.pop(), scope.Stack.pop()\n\t\tsalt         = scope.Stack.pop()\n\t\tinput        = scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\t\tgas          = scope.Contract.Gas\n\t)\n\n\t// Apply EIP150\n\tgas -= gas / 64\n\tscope.Contract.UseGas(gas, interpreter.evm.Config.Tracer, tracing.GasChangeCallContractCreation2)\n\t// reuse size int for stackvalue\n\tstackvalue := size\n\tres, addr, returnGas, suberr := interpreter.evm.Create2(scope.Contract.Address(), input, gas,\n\t\t&endowment, &salt)\n\t// Push item on the stack based on the returned error.\n\tif suberr != nil {\n\t\tstackvalue.Clear()\n\t} else {\n\t\tstackvalue.SetBytes(addr.Bytes())\n\t}\n\tscope.Stack.push(&stackvalue)\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tif suberr == ErrExecutionReverted {\n\t\tinterpreter.returnData = res // set REVERT data to return data buffer\n\t\treturn res, nil\n\t}\n\tinterpreter.returnData = nil // clear dirty return data buffer\n\treturn nil, nil\n}\n\nfunc opCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tstack := scope.Stack\n\t// Pop gas. The actual gas in interpreter.evm.callGasTemp.\n\t// We can use this as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, value, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get the arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tif interpreter.readOnly && !value.IsZero() {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tif !value.IsZero() {\n\t\tgas += params.CallStipend\n\t}\n\tret, returnGas, err := interpreter.evm.Call(scope.Contract.Address(), toAddr, args, gas, &value)\n\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opCallCode(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, value, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tif !value.IsZero() {\n\t\tgas += params.CallStipend\n\t}\n\n\tret, returnGas, err := interpreter.evm.CallCode(scope.Contract.Address(), toAddr, args, gas, &value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opDelegateCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tstack := scope.Stack\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.DelegateCall(scope.Contract.Caller(), scope.Contract.Address(), toAddr, args, gas, scope.Contract.value)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opStaticCall(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\t// Pop gas. The actual gas is in interpreter.evm.callGasTemp.\n\tstack := scope.Stack\n\t// We use it as a temporary value\n\ttemp := stack.pop()\n\tgas := interpreter.evm.callGasTemp\n\t// Pop other call parameters.\n\taddr, inOffset, inSize, retOffset, retSize := stack.pop(), stack.pop(), stack.pop(), stack.pop(), stack.pop()\n\ttoAddr := common.Address(addr.Bytes20())\n\t// Get arguments from the memory.\n\targs := scope.Memory.GetPtr(inOffset.Uint64(), inSize.Uint64())\n\n\tret, returnGas, err := interpreter.evm.StaticCall(scope.Contract.Address(), toAddr, args, gas)\n\tif err != nil {\n\t\ttemp.Clear()\n\t} else {\n\t\ttemp.SetOne()\n\t}\n\tstack.push(&temp)\n\tif err == nil || err == ErrExecutionReverted {\n\t\tscope.Memory.Set(retOffset.Uint64(), retSize.Uint64(), ret)\n\t}\n\n\tscope.Contract.RefundGas(returnGas, interpreter.evm.Config.Tracer, tracing.GasChangeCallLeftOverRefunded)\n\n\tinterpreter.returnData = ret\n\treturn ret, nil\n}\n\nfunc opReturn(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\treturn ret, errStopToken\n}\n\nfunc opRevert(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\toffset, size := scope.Stack.pop(), scope.Stack.pop()\n\tret := scope.Memory.GetCopy(offset.Uint64(), size.Uint64())\n\n\tinterpreter.returnData = ret\n\treturn ret, ErrExecutionReverted\n}\n\nfunc opUndefined(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, &ErrInvalidOpCode{opcode: OpCode(scope.Contract.Code[*pc])}\n}\n\nfunc opStop(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\treturn nil, errStopToken\n}\n\nfunc opSelfdestruct(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext) ([]byte, error) {\n\tif interpreter.readOnly {\n\t\treturn nil, ErrWriteProtection\n\t}\n\tbeneficiary := scope.Stack.pop()\n\tbalance := interpreter.evm.StateDB.GetBalance(scope.Contract.Address())\n\tinterpreter.evm.StateDB.AddBalance(beneficiary.Bytes20(), balance, tracing.BalanceIncreaseSelfdestruct)\n\tinterpreter.evm.StateDB.SelfDestruct(scope.Contract.Address())\n\tif tracer := interpreter.evm.Config.Tracer",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/vm/instructions.go",
          "line": 753,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/vm/instructions.go",
          "line": 816,
          "category": "unchecked_calls",
          "pattern": "\\.delegatecall\\s*\\(",
          "match": ".DelegateCall(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/vm/instructions.go",
          "line": 426,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "Blockhash(pc *uint64, interpreter *EVMInterpreter, scope *ScopeContext)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/vm/instructions.go",
          "line": 444,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(num64)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.86,
          "confidence": 0.9774,
          "ensemble_confidence": 0.8796600000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/operations_verkle.go",
          "line": 94,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= witnessGas // restore witness gas so that it can be charged at the callsite\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, witnessGas)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/operations_verkle.go",
          "line": 137,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= wanted\n\t}\n\t// Charge write costs if it transfers value\n\tif !balanceIsZero {\n\t\twanted := evm.AccessEvents.BasicDataGas(contractAddr, true, contract.Gas-statelessGas, false)\n\t\tif wanted > contract.Gas-statelessGas {\n\t\t\treturn statelessGas + wanted, nil\n\t\t}\n\t\tstatelessGas += wanted\n\n\t\tif contractAddr != beneficiaryAddr {\n\t\t\tif evm.StateDB.Exist(beneficiaryAddr) {\n\t\t\t\twanted = evm.AccessEvents.BasicDataGas(beneficiaryAddr, true, contract.Gas-statelessGas, false)\n\t\t\t} else {\n\t\t\t\twanted = evm.AccessEvents.AddAccount(beneficiaryAddr, true, contract.Gas-statelessGas)\n\t\t\t}\n\t\t\tif wanted > contract.Gas-statelessGas {\n\t\t\t\treturn statelessGas + wanted, nil\n\t\t\t}\n\t\t\tstatelessGas += wanted\n\t\t}\n\t}\n\treturn statelessGas, nil\n}\n\nfunc gasCodeCopyEip4762(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n\tgas, err := gasCodeCopy(evm, contract, stack, mem, memorySize)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\tif !contract.IsDeployment && !contract.IsSystemCall {\n\t\tvar (\n\t\t\tcodeOffset = stack.Back(1)\n\t\t\tlength     = stack.Back(2)\n\t\t)\n\t\tuint64CodeOffset, overflow := codeOffset.Uint64WithOverflow()\n\t\tif overflow {\n\t\t\tuint64CodeOffset = gomath.MaxUint64\n\t\t}\n\n\t\t_, copyOffset, nonPaddedCopyLength := getDataAndAdjustedBounds(contract.Code, uint64CodeOffset, length.Uint64())\n\t\t_, wanted := evm.AccessEvents.CodeChunksRangeGas(contract.Address(), copyOffset, nonPaddedCopyLength, uint64(len(contract.Code)), false, contract.Gas-gas)\n\t\tgas += wanted\n\t}\n\treturn gas, nil\n}\n\nfunc gasExtCodeCopyEIP4762(evm *EVM, contract *Contract, stack *Stack, mem *Memory, memorySize uint64) (uint64, error) {\n\t// memory expansion first (dynamic part of pre-2929 implementation)\n\tgas, err := gasExtCodeCopy(evm, contract, stack, mem, memorySize)\n\tif err != nil {\n\t\treturn 0, err\n\t}\n\taddr := common.Address(stack.peek().Bytes20())\n\t_, isPrecompile := evm.precompile(addr)\n\tif isPrecompile || addr == params.HistoryStorageAddress {\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, params.WarmStorageReadCostEIP2929)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/operations_verkle.go",
          "line": 91,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= witnessGas\n\t\t// if the operation fails, adds witness gas to the gas before returning the error\n\t\tgas, err := oldCalculator(evm, contract, stack, mem, memorySize)\n\t\tcontract.Gas += witnessGas // restore witness gas so that it can be charged at the callsite\n\t\tvar overflow bool\n\t\tif gas, overflow = math.SafeAdd(gas, witnessGas)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/contracts.go",
          "line": 892,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 192 {\n\t\tc, err := newCurvePoint(input[i : i+64])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tt, err := newTwistPoint(input[i+64 : i+192])\n\t\tif err != nil {\n\t\t\treturn nil, err\n\t\t}\n\t\tcs = append(cs, c)\n\t\tts = append(ts, t)\n\t}\n\t// Execute the pairing checks and return the results\n\tif bn256.PairingCheck(cs, ts) {\n\t\treturn true32Byte, nil\n\t}\n\treturn false32Byte, nil\n}\n\n// bn256PairingIstanbul implements a pairing pre-compile for the bn256 curve\n// conforming to Istanbul consensus rules.\ntype bn256PairingIstanbul struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\nfunc (c *bn256PairingIstanbul) RequiredGas(input []byte) uint64 {\n\treturn params.Bn256PairingBaseGasIstanbul + uint64(len(input)/192)*params.Bn256PairingPerPointGasIstanbul\n}\n\nfunc (c *bn256PairingIstanbul) Run(input []byte) ([]byte, error) {\n\treturn runBn256Pairing(input)\n}\n\n// bn256PairingByzantium implements a pairing pre-compile for the bn256 curve\n// conforming to Byzantium consensus rules.\ntype bn256PairingByzantium struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\nfunc (c *bn256PairingByzantium) RequiredGas(input []byte) uint64 {\n\treturn params.Bn256PairingBaseGasByzantium + uint64(len(input)/192)*params.Bn256PairingPerPointGasByzantium\n}\n\nfunc (c *bn256PairingByzantium) Run(input []byte) ([]byte, error) {\n\treturn runBn256Pairing(input)\n}\n\ntype blake2F struct{}\n\nfunc (c *blake2F) RequiredGas(input []byte) uint64 {\n\t// If the input is malformed, we can't calculate the gas, return 0 and let the\n\t// actual call choke and fault.\n\tif len(input) != blake2FInputLength {\n\t\treturn 0\n\t}\n\treturn uint64(binary.BigEndian.Uint32(input[0:4]))\n}\n\nconst (\n\tblake2FInputLength        = 213\n\tblake2FFinalBlockBytes    = byte(1)\n\tblake2FNonFinalBlockBytes = byte(0)\n)\n\nvar (\n\terrBlake2FInvalidInputLength = errors.New(\"invalid input length\")\n\terrBlake2FInvalidFinalFlag   = errors.New(\"invalid final flag\")\n)\n\nfunc (c *blake2F) Run(input []byte) ([]byte, error) {\n\t// Make sure the input is valid (correct length and final flag)\n\tif len(input) != blake2FInputLength {\n\t\treturn nil, errBlake2FInvalidInputLength\n\t}\n\tif input[212] != blake2FNonFinalBlockBytes && input[212] != blake2FFinalBlockBytes {\n\t\treturn nil, errBlake2FInvalidFinalFlag\n\t}\n\t// Parse the input into the Blake2b call parameters\n\tvar (\n\t\trounds = binary.BigEndian.Uint32(input[0:4])\n\t\tfinal  = input[212] == blake2FFinalBlockBytes\n\n\t\th [8]uint64\n\t\tm [16]uint64\n\t\tt [2]uint64\n\t)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/contracts.go",
          "line": 496,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gasCost\n\toutput, err := p.Run(input)\n\treturn output, suppliedGas, err\n}\n\n// ecrecover implemented as a native contract.\ntype ecrecover struct{}\n\nfunc (c *ecrecover) RequiredGas(input []byte) uint64 {\n\treturn params.EcrecoverGas\n}\n\nfunc (c *ecrecover) Run(input []byte) ([]byte, error) {\n\tconst ecRecoverInputLength = 128\n\n\tinput = common.RightPadBytes(input, ecRecoverInputLength)\n\t// \"input\" is (hash, v, r, s), each 32 bytes\n\t// but for ecrecover we want (r, s, v)\n\n\tr := new(big.Int).SetBytes(input[64:96])\n\ts := new(big.Int).SetBytes(input[96:128])\n\tv := input[63] - 27\n\n\t// tighter sig s values input homestead only apply to tx sigs\n\tif !allZero(input[32:63]) || !crypto.ValidateSignatureValues(v, r, s, false) {\n\t\treturn nil, nil\n\t}\n\t// We must make sure not to modify the 'input', so placing the 'v' along with\n\t// the signature needs to be done on a new allocation\n\tsig := make([]byte, 65)\n\tcopy(sig, input[64:128])\n\tsig[64] = v\n\t// v needs to be at the end for libsecp256k1\n\tpubKey, err := crypto.Ecrecover(input[:32], sig)\n\t// make sure the public key is a valid one\n\tif err != nil {\n\t\treturn nil, nil\n\t}\n\n\t// the first byte of pubkey is bitcoin heritage\n\treturn common.LeftPadBytes(crypto.Keccak256(pubKey[1:])[12:], 32), nil\n}\n\n// SHA256 implemented as a native contract.\ntype sha256hash struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\n//\n// This method does not require any overflow checking as the input size gas costs\n// required for anything significant is so high it's impossible to pay for.\nfunc (c *sha256hash) RequiredGas(input []byte) uint64 {\n\treturn uint64(len(input)+31)/32*params.Sha256PerWordGas + params.Sha256BaseGas\n}\n\nfunc (c *sha256hash) Run(input []byte) ([]byte, error) {\n\th := sha256.Sum256(input)\n\treturn h[:], nil\n}\n\n// RIPEMD160 implemented as a native contract.\ntype ripemd160hash struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\n//\n// This method does not require any overflow checking as the input size gas costs\n// required for anything significant is so high it's impossible to pay for.\nfunc (c *ripemd160hash) RequiredGas(input []byte) uint64 {\n\treturn uint64(len(input)+31)/32*params.Ripemd160PerWordGas + params.Ripemd160BaseGas\n}\n\nfunc (c *ripemd160hash) Run(input []byte) ([]byte, error) {\n\tripemd := ripemd160.New()\n\tripemd.Write(input)\n\treturn common.LeftPadBytes(ripemd.Sum(nil), 32), nil\n}\n\n// data copy implemented as a native contract.\ntype dataCopy struct{}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\n//\n// This method does not require any overflow checking as the input size gas costs\n// required for anything significant is so high it's impossible to pay for.\nfunc (c *dataCopy) RequiredGas(input []byte) uint64 {\n\treturn uint64(len(input)+31)/32*params.IdentityPerWordGas + params.IdentityBaseGas\n}\n\nfunc (c *dataCopy) Run(in []byte) ([]byte, error) {\n\treturn common.CopyBytes(in), nil\n}\n\n// bigModExp implements a native big integer exponential modular operation.\ntype bigModExp struct {\n\teip2565 bool\n\teip7823 bool\n\teip7883 bool\n}\n\nvar (\n\tbig1      = big.NewInt(1)\n\tbig3      = big.NewInt(3)\n\tbig7      = big.NewInt(7)\n\tbig20     = big.NewInt(20)\n\tbig32     = big.NewInt(32)\n\tbig64     = big.NewInt(64)\n\tbig96     = big.NewInt(96)\n\tbig480    = big.NewInt(480)\n\tbig1024   = big.NewInt(1024)\n\tbig3072   = big.NewInt(3072)\n\tbig199680 = big.NewInt(199680)\n)\n\n// modexpMultComplexity implements bigModexp multComplexity formula, as defined in EIP-198\n//\n//\tdef mult_complexity(x):\n//\t\tif x <= 64: return x ** 2\n//\t\telif x <= 1024: return x ** 2 // 4 + 96 * x - 3072\n//\t\telse: return x ** 2 // 16 + 480 * x - 199680\n//\n// where is x is max(length_of_MODULUS, length_of_BASE)\nfunc modexpMultComplexity(x *big.Int) *big.Int {\n\tswitch {\n\tcase x.Cmp(big64) <= 0:\n\t\tx.Mul(x, x) // x ** 2\n\tcase x.Cmp(big1024) <= 0:\n\t\t// (x ** 2 // 4 ) + ( 96 * x - 3072)\n\t\tx = new(big.Int).Add(\n\t\t\tnew(big.Int).Rsh(new(big.Int).Mul(x, x), 2),\n\t\t\tnew(big.Int).Sub(new(big.Int).Mul(big96, x), big3072),\n\t\t)\n\tdefault:\n\t\t// (x ** 2 // 16) + (480 * x - 199680)\n\t\tx = new(big.Int).Add(\n\t\t\tnew(big.Int).Rsh(new(big.Int).Mul(x, x), 4),\n\t\t\tnew(big.Int).Sub(new(big.Int).Mul(big480, x), big199680),\n\t\t)\n\t}\n\treturn x\n}\n\n// RequiredGas returns the gas required to execute the pre-compiled contract.\nfunc (c *bigModExp) RequiredGas(input []byte) uint64 {\n\tvar (\n\t\tbaseLen = new(big.Int).SetBytes(getData(input, 0, 32))\n\t\texpLen  = new(big.Int).SetBytes(getData(input, 32, 32))\n\t\tmodLen  = new(big.Int).SetBytes(getData(input, 64, 32))\n\t)\n\tif len(input) > 96 {\n\t\tinput = input[96:]\n\t} else {\n\t\tinput = input[:0]\n\t}\n\t// Retrieve the head 32 bytes of exp for the adjusted exponent length\n\tvar expHead *big.Int\n\tif big.NewInt(int64(len(input))).Cmp(baseLen) <= 0 {\n\t\texpHead = new(big.Int)\n\t} else {\n\t\tif expLen.Cmp(big32) > 0 {\n\t\t\texpHead = new(big.Int).SetBytes(getData(input, baseLen.Uint64(), 32))\n\t\t} else {\n\t\t\texpHead = new(big.Int).SetBytes(getData(input, baseLen.Uint64(), expLen.Uint64()))\n\t\t}\n\t}\n\t// Calculate the adjusted exponent length\n\tvar msb int\n\tif bitlen := expHead.BitLen()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/interpreter.go",
          "line": 289,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= dynamicCost // for tracing\n\t\t\tif err != nil {\n\t\t\t\treturn nil, fmt.Errorf(\"%w: %v\", ErrOutOfGas, err)\n\t\t\t}\n\t\t\t// for tracing: this gas consumption event is emitted below in the debug section.\n\t\t\tif contract.Gas < dynamicCost {\n\t\t\t\treturn nil, ErrOutOfGas\n\t\t\t} else {\n\t\t\t\tcontract.Gas -= dynamicCost\n\t\t\t}\n\t\t}\n\n\t\t// Do tracing before potential memory expansion\n\t\tif debug {\n\t\t\tif in.evm.Config.Tracer.OnGasChange != nil {\n\t\t\t\tin.evm.Config.Tracer.OnGasChange(gasCopy, gasCopy-cost, tracing.GasChangeCallOpCode)\n\t\t\t}\n\t\t\tif in.evm.Config.Tracer.OnOpcode != nil {\n\t\t\t\tin.evm.Config.Tracer.OnOpcode(pc, byte(op), gasCopy, cost, callContext, in.returnData, in.evm.depth, VMErrorFromErr(err))\n\t\t\t\tlogged = true\n\t\t\t}\n\t\t}\n\t\tif memorySize > 0 {\n\t\t\tmem.Resize(memorySize)\n\t\t}\n\n\t\t// execute the operation\n\t\tres, err = operation.execute(&pc, in, callContext)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tpc++\n\t}\n\n\tif err == errStopToken {\n\t\terr = nil // clear stop token error\n\t}\n\n\treturn res, err\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/interpreter.go",
          "line": 263,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= cost\n\t\t}\n\n\t\t// All ops with a dynamic memory usage also has a dynamic gas cost.\n\t\tvar memorySize uint64\n\t\tif operation.dynamicGas != nil {\n\t\t\t// calculate the new memory size and expand the memory to fit\n\t\t\t// the operation\n\t\t\t// Memory check needs to be done prior to evaluating the dynamic gas portion,\n\t\t\t// to detect calculation overflows\n\t\t\tif operation.memorySize != nil {\n\t\t\t\tmemSize, overflow := operation.memorySize(stack)\n\t\t\t\tif overflow {\n\t\t\t\t\treturn nil, ErrGasUintOverflow\n\t\t\t\t}\n\t\t\t\t// memory is expanded in words of 32 bytes. Gas\n\t\t\t\t// is also calculated in words.\n\t\t\t\tif memorySize, overflow = math.SafeMul(toWordSize(memSize), 32)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/interpreter.go",
          "line": 297,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= dynamicCost\n\t\t\t}\n\t\t}\n\n\t\t// Do tracing before potential memory expansion\n\t\tif debug {\n\t\t\tif in.evm.Config.Tracer.OnGasChange != nil {\n\t\t\t\tin.evm.Config.Tracer.OnGasChange(gasCopy, gasCopy-cost, tracing.GasChangeCallOpCode)\n\t\t\t}\n\t\t\tif in.evm.Config.Tracer.OnOpcode != nil {\n\t\t\t\tin.evm.Config.Tracer.OnOpcode(pc, byte(op), gasCopy, cost, callContext, in.returnData, in.evm.depth, VMErrorFromErr(err))\n\t\t\t\tlogged = true\n\t\t\t}\n\t\t}\n\t\tif memorySize > 0 {\n\t\t\tmem.Resize(memorySize)\n\t\t}\n\n\t\t// execute the operation\n\t\tres, err = operation.execute(&pc, in, callContext)\n\t\tif err != nil {\n\t\t\tbreak\n\t\t}\n\t\tpc++\n\t}\n\n\tif err == errStopToken {\n\t\terr = nil // clear stop token error\n\t}\n\n\treturn res, err\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/interpreter_test.go",
          "line": 56,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/contract.go",
          "line": 151,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gas\n}\n\n// Address returns the contracts address\nfunc (c *Contract) Address() common.Address {\n\treturn c.address\n}\n\n// Value returns the contract's value (sent to it from it's caller)\nfunc (c *Contract) Value() *uint256.Int {\n\treturn c.value\n}\n\n// SetCallCode sets the code of the contract,\nfunc (c *Contract) SetCallCode(hash common.Hash, code []byte) {\n\tc.Code = code\n\tc.CodeHash = hash\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/contract.go",
          "line": 139,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= gas\n\treturn true\n}\n\n// RefundGas refunds gas to the contract\nfunc (c *Contract) RefundGas(gas uint64, logger *tracing.Hooks, reason tracing.GasChangeReason) {\n\tif gas == 0 {\n\t\treturn\n\t}\n\tif logger != nil && logger.OnGasChange != nil && reason != tracing.GasChangeIgnored {\n\t\tlogger.OnGasChange(c.Gas, c.Gas+gas, reason)\n\t}\n\tc.Gas += gas\n}\n\n// Address returns the contracts address\nfunc (c *Contract) Address() common.Address {\n\treturn c.address\n}\n\n// Value returns the contract's value (sent to it from it's caller)\nfunc (c *Contract) Value() *uint256.Int {\n\treturn c.value\n}\n\n// SetCallCode sets the code of the contract,\nfunc (c *Contract) SetCallCode(hash common.Hash, code []byte) {\n\tc.Code = code\n\tc.CodeHash = hash\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 86,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 16\n\t\t\t}\n\t\t\tfor ",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 90,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 8\n\t\t\t}\n\t\t}\n\t\tswitch numbits {\n\t\tcase 1:\n\t\t\tbits.set1(pc)\n\t\t\tpc += 1\n\t\tcase 2:\n\t\t\tbits.setN(set2BitsMask, pc)\n\t\t\tpc += 2\n\t\tcase 3:\n\t\t\tbits.setN(set3BitsMask, pc)\n\t\t\tpc += 3\n\t\tcase 4:\n\t\t\tbits.setN(set4BitsMask, pc)\n\t\t\tpc += 4\n\t\tcase 5:\n\t\t\tbits.setN(set5BitsMask, pc)\n\t\t\tpc += 5\n\t\tcase 6:\n\t\t\tbits.setN(set6BitsMask, pc)\n\t\t\tpc += 6\n\t\tcase 7:\n\t\t\tbits.setN(set7BitsMask, pc)\n\t\t\tpc += 7\n\t\t}\n\t}\n\treturn bits\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 84,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 16 {\n\t\t\t\tbits.set16(pc)\n\t\t\t\tpc += 16\n\t\t\t}\n\t\t\tfor ",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vm/analysis_legacy.go",
          "line": 88,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 8 {\n\t\t\t\tbits.set8(pc)\n\t\t\t\tpc += 8\n\t\t\t}\n\t\t}\n\t\tswitch numbits {\n\t\tcase 1:\n\t\t\tbits.set1(pc)\n\t\t\tpc += 1\n\t\tcase 2:\n\t\t\tbits.setN(set2BitsMask, pc)\n\t\t\tpc += 2\n\t\tcase 3:\n\t\t\tbits.setN(set3BitsMask, pc)\n\t\t\tpc += 3\n\t\tcase 4:\n\t\t\tbits.setN(set4BitsMask, pc)\n\t\t\tpc += 4\n\t\tcase 5:\n\t\t\tbits.setN(set5BitsMask, pc)\n\t\t\tpc += 5\n\t\tcase 6:\n\t\t\tbits.setN(set6BitsMask, pc)\n\t\t\tpc += 6\n\t\tcase 7:\n\t\t\tbits.setN(set7BitsMask, pc)\n\t\t\tpc += 7\n\t\t}\n\t}\n\treturn bits\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/stateless/witness.go",
          "line": 74,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/filtermaps/matcher.go",
          "line": 417,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= groupLength\n\t}\n\tm.cleanMapIndices()\n\tm.stats.setState(&st, stNone)\n\treturn results, nil\n}\n\n// dropIndices implements matcherInstance.\nfunc (m *singleMatcherInstance) dropIndices(dropIndices []uint32) {\n\tfor _, mapIndex := range dropIndices {\n\t\tdelete(m.filterRows, mapIndex)\n\t}\n\tm.cleanMapIndices()\n}\n\n// cleanMapIndices removes map indices from the list if there is no matching\n// filterRows entry because a result has been returned or the index has been\n// dropped.\nfunc (m *singleMatcherInstance) cleanMapIndices() {\n\tvar j int\n\tfor i, mapIndex := range m.mapIndices {\n\t\tif _, ok := m.filterRows[mapIndex]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/filtermaps/matcher.go",
          "line": 555,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(res)\n\t}\n\tmerged := make(potentialMatches, 0, sumLen)\n\tfor {\n\t\tbest := -1\n\t\tfor i, res := range results {\n\t\t\tif len(res) == 0 {\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tif best < 0 || res[0] < results[best][0] {\n\t\t\t\tbest = i\n\t\t\t}\n\t\t}\n\t\tif best < 0 {\n\t\t\treturn merged\n\t\t}\n\t\tif len(merged) == 0 || results[best][0] > merged[len(merged)-1] {\n\t\t\tmerged = append(merged, results[best][0])\n\t\t}\n\t\tresults[best] = results[best][1:]\n\t}\n}\n\n// matchSequence combines two matchers, a \"base\" and a \"next\" matcher with a\n// positive integer offset so that the resulting matcher signals a match at log\n// value index X when the base matcher returns a match at X and the next matcher\n// gives a match at X+offset. Note that matchSequence can be used recursively to\n// detect any log value sequence.\ntype matchSequence struct {\n\tparams               *Params\n\tbase, next           matcher\n\toffset               uint64\n\tstatsLock            sync.Mutex\n\tbaseStats, nextStats matchOrderStats\n}\n\n// newInstance creates a new instance of matchSequence.\nfunc (m *matchSequence) newInstance(mapIndices []uint32) matcherInstance {\n\t// determine set of indices to request from next matcher\n\tneedMatched := make(map[uint32]struct{})\n\tbaseRequested := make(map[uint32]struct{})\n\tnextRequested := make(map[uint32]struct{})\n\tfor _, mapIndex := range mapIndices {\n\t\tneedMatched[mapIndex] = struct{}{}\n\t\tbaseRequested[mapIndex] = struct{}{}\n\t\tnextRequested[mapIndex] = struct{}{}\n\t}\n\treturn &matchSequenceInstance{\n\t\tmatchSequence: m,\n\t\tbaseInstance:  m.base.newInstance(mapIndices),\n\t\tnextInstance:  m.next.newInstance(mapIndices),\n\t\tneedMatched:   needMatched,\n\t\tbaseRequested: baseRequested,\n\t\tnextRequested: nextRequested,\n\t\tbaseResults:   make(map[uint32]potentialMatches),\n\t\tnextResults:   make(map[uint32]potentialMatches),\n\t}\n}\n\n// matchOrderStats collects statistics about the evaluating cost and the\n// occurrence of empty result sets from both base and next child matchers.\n// This allows the optimization of the evaluation order by evaluating the\n// child first that is cheaper and/or gives empty results more often and not\n// evaluating the other child in most cases.\n// Note that matchOrderStats is specific to matchSequence and the results are\n// carried over to future instances as the results are mostly useful when\n// evaluating layer zero of each instance. For this reason it should be used\n// in a thread safe way as is may be accessed from multiple worker goroutines.\ntype matchOrderStats struct {\n\ttotalCount, nonEmptyCount, totalCost uint64\n}\n\n// add collects statistics after a child has been evaluated for a certain layer.\nfunc (ms *matchOrderStats) add(empty bool, layerIndex uint32) {\n\tif empty && layerIndex != 0 {\n\t\t// matchers may be evaluated for higher layers after all results have\n\t\t// been returned. Also, empty results are not relevant when previous\n\t\t// layers yielded matches already, so these cases can be ignored.\n\t\treturn\n\t}\n\tms.totalCount++\n\tif !empty {\n\t\tms.nonEmptyCount++\n\t}\n\tms.totalCost += uint64(layerIndex + 1)\n}\n\n// mergeStats merges two sets of matchOrderStats.\nfunc (ms *matchOrderStats) mergeStats(add matchOrderStats) {\n\tms.totalCount += add.totalCount\n\tms.nonEmptyCount += add.nonEmptyCount\n\tms.totalCost += add.totalCost\n}\n\n// baseFirst returns true if the base child matcher should be evaluated first.\nfunc (m *matchSequence) baseFirst() bool {\n\tm.statsLock.Lock()\n\tbf := float64(m.baseStats.totalCost)*float64(m.nextStats.totalCount)+\n\t\tfloat64(m.baseStats.nonEmptyCount)*float64(m.nextStats.totalCost) <\n\t\tfloat64(m.baseStats.totalCost)*float64(m.nextStats.nonEmptyCount)+\n\t\t\tfloat64(m.nextStats.totalCost)*float64(m.baseStats.totalCount)\n\tm.statsLock.Unlock()\n\treturn bf\n}\n\n// mergeBaseStats merges a set of matchOrderStats into the base matcher stats.\nfunc (m *matchSequence) mergeBaseStats(stats matchOrderStats) {\n\tm.statsLock.Lock()\n\tm.baseStats.mergeStats(stats)\n\tm.statsLock.Unlock()\n}\n\n// mergeNextStats merges a set of matchOrderStats into the next matcher stats.\nfunc (m *matchSequence) mergeNextStats(stats matchOrderStats) {\n\tm.statsLock.Lock()\n\tm.nextStats.mergeStats(stats)\n\tm.statsLock.Unlock()\n}\n\n// newMatchSequence creates a recursive sequence matcher from a list of underlying\n// matchers. The resulting matcher signals a match at log value index X when each\n// underlying matcher matchers[i] returns a match at X+i.\nfunc newMatchSequence(params *Params, matchers []matcher) matcher {\n\tif len(matchers) == 0 {\n\t\tpanic(\"zero length sequence matchers are not allowed\")\n\t}\n\tif len(matchers) == 1 {\n\t\treturn matchers[0]\n\t}\n\treturn &matchSequence{\n\t\tparams: params,\n\t\tbase:   newMatchSequence(params, matchers[:len(matchers)-1]),\n\t\tnext:   matchers[len(matchers)-1],\n\t\toffset: uint64(len(matchers) - 1),\n\t}\n}\n\n// matchSequenceInstance is an instance of matchSequence.\ntype matchSequenceInstance struct {\n\t*matchSequence\n\tbaseInstance, nextInstance                matcherInstance\n\tbaseRequested, nextRequested, needMatched map[uint32]struct{}\n\tbaseResults, nextResults                  map[uint32]potentialMatches\n}\n\n// getMatchesForLayer implements matcherInstance.\nfunc (m *matchSequenceInstance) getMatchesForLayer(ctx context.Context, layerIndex uint32) (matchedResults []matcherResult, err error) {\n\t// decide whether to evaluate base or next matcher first\n\tbaseFirst := m.baseFirst()\n\tif baseFirst {\n\t\tif err := m.evalBase(ctx, layerIndex)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/filtermaps/indexer_test.go",
          "line": 114,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= rand.Intn(1000-head) + 1\n\t\t\t\tts.fm.testSnapshotUsed = false\n\t\t\t\tts.chain.setCanonicalChain(forks[fork][:head+1])\n\t\t\t}\n\t\t}\n\t\tts.fm.WaitIdle()\n\t\tif checkSnapshot {\n\t\t\tif ts.fm.testSnapshotUsed == ts.fm.testDisableSnapshots {\n\t\t\t\tts.t.Fatalf(\"Invalid snapshot used state after head extension (used: %v, disabled: %v)\", ts.fm.testSnapshotUsed, ts.fm.testDisableSnapshots)\n\t\t\t}\n\t\t\tcheckSnapshot = false\n\t\t}\n\t\tif noHistory {\n\t\t\tif ts.fm.indexedRange.initialized {\n\t\t\t\tt.Fatalf(\"filterMapsRange initialized while indexing is disabled\")\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\tif !ts.fm.indexedRange.initialized {\n\t\t\tt.Fatalf(\"filterMapsRange not initialized while indexing is enabled\")\n\t\t}\n\t\tvar tailBlock uint64\n\t\tif history > 0 && history <= head {\n\t\t\ttailBlock = uint64(head + 1 - history)\n\t\t}\n\t\tvar tailEpoch uint32\n\t\tif tailBlock > 0 {\n\t\t\ttailLvPtr := expspos(tailBlock) - 1\n\t\t\ttailEpoch = uint32(tailLvPtr >> (testParams.logValuesPerMap + testParams.logMapsPerEpoch))\n\t\t}\n\t\ttailLvPtr := uint64(tailEpoch) << (testParams.logValuesPerMap + testParams.logMapsPerEpoch) // first available lv ptr\n\t\tvar expTailBlock uint64\n\t\tif tailEpoch > 0 {\n\t\t\tfor expspos(expTailBlock) <= tailLvPtr {\n\t\t\t\texpTailBlock++\n\t\t\t}\n\t\t}\n\t\tif ts.fm.indexedRange.blocks.Last() != uint64(head) {\n\t\t\tts.t.Fatalf(\"Invalid index head (expected #%d, got #%d)\", head, ts.fm.indexedRange.blocks.Last())\n\t\t}\n\t\texpHeadDelimiter := expdpos(uint64(head))\n\t\tif ts.fm.indexedRange.headDelimiter != expHeadDelimiter {\n\t\t\tts.t.Fatalf(\"Invalid index head delimiter pointer (expected %d, got %d)\", expHeadDelimiter, ts.fm.indexedRange.headDelimiter)\n\t\t}\n\n\t\tif ts.fm.indexedRange.blocks.First() != expTailBlock {\n\t\t\tts.t.Fatalf(\"Invalid index tail block (expected #%d, got #%d)\", expTailBlock, ts.fm.indexedRange.blocks.First())\n\t\t}\n\t}\n}\n\nfunc TestIndexerMatcherView(t *testing.T) {\n\ttestIndexerMatcherView(t, false)\n}\n\nfunc TestIndexerMatcherViewWithConcurrentRead(t *testing.T) {\n\ttestIndexerMatcherView(t, true)\n}\n\nfunc testIndexerMatcherView(t *testing.T, concurrentRead bool) {\n\tts := newTestSetup(t)\n\tdefer ts.close()\n\n\tforks := make([][]common.Hash, 20)\n\thashes := make([]common.Hash, 20)\n\tts.chain.addBlocks(100, 5, 2, 4, true)\n\tts.setHistory(0, false)\n\tfor i := range forks {\n\t\tif i != 0 {\n\t\t\tts.chain.setHead(100 - i)\n\t\t\tts.chain.addBlocks(i, 5, 2, 4, true)\n\t\t}\n\t\tts.fm.WaitIdle()\n\t\tforks[i] = ts.chain.getCanonicalChain()\n\t\thashes[i] = ts.matcherViewHash()\n\t}\n\tfork := len(forks) - 1\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 72,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 79,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(number)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 94,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(number)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 99,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 105,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 117,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.87,
          "confidence": 0.9783000000000001,
          "ensemble_confidence": 0.8804700000000001
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 135,
          "category": "ensemble_mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(n)",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.6816599999999999,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 166,
          "category": "ensemble_mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(number)",
          "severity": "CRITICAL",
          "model": "ensemble_fusion",
          "classical_detected": false,
          "omega_detected": true,
          "ensemble_confidence": 0.68481,
          "fusion_score": 1.0
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/core/filtermaps/chain_view.go",
          "line": 198,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockHash(number uint64)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/filtermaps/filtermaps.go",
          "line": 557,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= r // skip to map boundary\n\t\t\t}\n\t\t\tif lvPointer > lvIndex {\n\t\t\t\t// lvIndex does not point to the first log value (address value)\n\t\t\t\t// generated by a log as true matches should always do, so it\n\t\t\t\t// is considered a false positive (no log and no error returned).\n\t\t\t\treturn nil, nil\n\t\t\t}\n\t\t\tif lvPointer == lvIndex {\n\t\t\t\treturn log, nil // potential match\n\t\t\t}\n\t\t\tlvPointer += l\n\t\t}\n\t}\n\treturn nil, nil\n}\n\n// getFilterMap fetches an entire filter map from the database.\nfunc (f *FilterMaps) getFilterMap(mapIndex uint32) (filterMap, error) {\n\tif fm, ok := f.filterMapCache.Get(mapIndex)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/filtermaps/filtermaps.go",
          "line": 608,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= groupLength\n\t}\n\treturn rows, nil\n}\n\n// getFilterMapRowsOfGroup fetches a set of filter map rows at map indices\n// belonging to the same base row group.\nfunc (f *FilterMaps) getFilterMapRowsOfGroup(target []FilterRow, mapIndices []uint32, rowIndex uint32, baseLayerOnly bool) error {\n\tvar (\n\t\tgroupIndex  = f.mapGroupIndex(mapIndices[0])\n\t\tmapRowIndex = f.mapRowIndex(groupIndex, rowIndex)\n\t)\n\tbaseRows, err := rawdb.ReadFilterMapBaseRows(f.db, mapRowIndex, f.baseRowGroupSize, f.logMapWidth)\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to retrieve base row group %d of row %d: %v\", groupIndex, rowIndex, err)\n\t}\n\tfor i, mapIndex := range mapIndices {\n\t\tif f.mapGroupIndex(mapIndex) != groupIndex {\n\t\t\treturn fmt.Errorf(\"maps are not in the same base row group, index: %d, group: %d\", mapIndex, groupIndex)\n\t\t}\n\t\trow := baseRows[f.mapGroupOffset(mapIndex)]\n\t\tif !baseLayerOnly {\n\t\t\textRow, err := rawdb.ReadFilterMapExtRow(f.db, f.mapRowIndex(mapIndex, rowIndex), f.logMapWidth)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to retrieve filter map %d extended row %d: %v\", mapIndex, rowIndex, err)\n\t\t\t}\n\t\t\trow = append(row, extRow...)\n\t\t}\n\t\ttarget[i] = row\n\t}\n\treturn nil\n}\n\n// storeFilterMapRows stores a set of filter map rows at the corresponding map\n// indices and a shared row index.\nfunc (f *FilterMaps) storeFilterMapRows(batch ethdb.Batch, mapIndices []uint32, rowIndex uint32, rows []FilterRow) error {\n\tfor len(mapIndices) > 0 {\n\t\tvar (\n\t\t\tpos        = 1\n\t\t\tgroupIndex = f.mapGroupIndex(mapIndices[0])\n\t\t)\n\t\tfor pos < len(mapIndices) && f.mapGroupIndex(mapIndices[pos]) == groupIndex {\n\t\t\tpos++\n\t\t}\n\t\tif err := f.storeFilterMapRowsOfGroup(batch, mapIndices[:pos], rowIndex, rows[:pos])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/filtermaps/math_test.go",
          "line": 130,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(matches) - testPmLen\n\t\t\t}\n\t\t}\n\t}\n\t// Whenever looking for a certain log value hash, each entry in the row that\n\t// was generated by another log value hash (a \"foreign entry\") has a\n\t// valuesPerMap // 2^32 chance of yielding a false positive if the reverse\n\t// transformed 32 bit integer is by random chance less than valuesPerMap and\n\t// is therefore considered a potentially valid match.\n\t// We have testPmLen unique hash entries and a testPmLen long series of entries\n\t// for the same hash. For each of the testPmLen unique hash entries there are\n\t// testPmLen*2-1 foreign entries while for the long series there are testPmLen\n\t// foreign entries. This means that after performing all these filtering runs,\n\t// we have processed 2*testPmLen^2 foreign entries, which given us an estimate\n\t// of how many false positives to expect.\n\texpFalse := int(uint64(testPmCount*testPmLen*testPmLen*2) * params.valuesPerMap >> params.logMapWidth)\n\tif falsePositives < expFalse/2 || falsePositives > expFalse*3/2 {\n\t\tt.Fatalf(\"False positive rate out of expected range (got %d, expected %d +-50%%)\", falsePositives, expFalse)\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 329,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\tif stopCb() {\n\t\t\t\treturn false, nil\n\t\t\t}\n\t\t\tstart = time.Now()\n\t\t\tif !r.iterator.updateChainView(r.f.targetView) {\n\t\t\t\treturn false, errChainUpdate\n\t\t\t}\n\t\t\twaitCnt = 0\n\t\t}\n\t\tif logValue := r.iterator.getValueHash()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 375,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\tmapRenderTimer.Update(totalTime)\n\tmapLogValueMeter.Mark(logValuesProcessed)\n\tmapBlockMeter.Mark(blocksProcessed)\n\treturn true, nil\n}\n\n// writeFinishedMaps writes rendered maps to the database and updates\n// filterMapsRange and indexedView accordingly.\nfunc (r *mapRenderer) writeFinishedMaps(pauseCb func() bool) error {\n\tvar totalTime time.Duration\n\tstart := time.Now()\n\tif len(r.finishedMaps) == 0 {\n\t\treturn nil\n\t}\n\tr.f.indexLock.Lock()\n\tdefer r.f.indexLock.Unlock()\n\n\toldRange := r.f.indexedRange\n\ttempRange, err := r.getTempRange()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get temporary rendered range: %v\", err)\n\t}\n\tnewRange, err := r.getUpdatedRange()\n\tif err != nil {\n\t\treturn fmt.Errorf(\"failed to get updated rendered range: %v\", err)\n\t}\n\trenderedView := r.f.targetView // pauseCb callback might still change targetView while writing finished maps\n\n\tbatch := r.f.db.NewBatch()\n\tvar writeCnt int\n\tcheckWriteCnt := func() {\n\t\twriteCnt++\n\t\tif writeCnt == rowsPerBatch {\n\t\t\twriteCnt = 0\n\t\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 416,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\t\t\tpauseCb()\n\t\t\tstart = time.Now()\n\t\t\tr.f.indexLock.Lock()\n\t\t\tbatch = r.f.db.NewBatch()\n\t\t}\n\t}\n\n\tif tempRange != r.f.indexedRange {\n\t\tr.f.setRange(batch, r.f.indexedView, tempRange, true)\n\t}\n\t// add or update filter rows\n\tfor rowIndex := uint32(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 512,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(start)\n\tmapWriteTimer.Update(totalTime)\n\treturn nil\n}\n\n// getTempRange returns a temporary filterMapsRange that is committed to the\n// database while the newly rendered maps are partially written. Writing all\n// processed maps in a single database batch would be a serious hit on db\n// performance so instead safety is ensured by first reverting the valid map\n// range to the unchanged region until all new map data is committed.\nfunc (r *mapRenderer) getTempRange() (filterMapsRange, error) {\n\ttempRange := r.f.indexedRange\n\tif err := tempRange.addRenderedRange(r.finished.First(), r.finished.First(), r.renderBefore, r.f.mapsPerEpoch)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/filtermaps/map_renderer.go",
          "line": 626,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= e.d\n\t\tif i < len(endpoints)-1 && endpoints[i+1].m == e.m {\n\t\t\tcontinue\n\t\t}\n\t\tif (sum > 0) != last {\n\t\t\tmerged = append(merged, e.m)\n\t\t\tlast = !last\n\t\t}\n\t}\n\n\tswitch len(merged) {\n\tcase 0:\n\t\t// Initialized database, but no finished maps yet.\n\t\tfmr.tailPartialEpoch = 0\n\t\tfmr.maps = common.NewRange(firstRendered, 0)\n\n\tcase 2:\n\t\t// One rendered section (no partial tail epoch).\n\t\tfmr.tailPartialEpoch = 0\n\t\tfmr.maps = common.NewRange(merged[0], merged[1]-merged[0])\n\n\tcase 4:\n\t\t// Two rendered sections (with a gap).\n\t\t// First section (merged[0]-merged[1]) is for the partial tail epoch,\n\t\t// and it has to start exactly one epoch before the main section.\n\t\tif merged[2] != merged[0]+mapsPerEpoch {\n\t\t\treturn fmt.Errorf(\"invalid tail partial epoch: %v\", merged)\n\t\t}\n\t\tfmr.tailPartialEpoch = merged[1] - merged[0]\n\t\tfmr.maps = common.NewRange(merged[2], merged[3]-merged[2])\n\n\tdefault:\n\t\treturn fmt.Errorf(\"invalid number of rendered sections: %v\", merged)\n\t}\n\treturn nil\n}\n\n// logIterator iterates on the linear log value index range.\ntype logIterator struct {\n\tparams                                          *Params\n\tchainView                                       *ChainView\n\tblockNumber                                     uint64\n\treceipts                                        types.Receipts\n\tblockStart, delimiter, skipToBoundary, finished bool\n\ttxIndex, logIndex, topicIndex                   int\n\tlvIndex                                         uint64\n}\n\nvar errUnindexedRange = errors.New(\"unindexed range\")\n\n// newLogIteratorFromBlockDelimiter creates a logIterator starting at the\n// given block's first log value entry (the block delimiter), according to the\n// current targetView.\nfunc (f *FilterMaps) newLogIteratorFromBlockDelimiter(blockNumber, lvIndex uint64) (*logIterator, error) {\n\tif blockNumber > f.targetView.HeadNumber() {\n\t\treturn nil, fmt.Errorf(\"iterator entry point %d after target chain head block %d\", blockNumber, f.targetView.HeadNumber())\n\t}\n\tif !f.indexedRange.blocks.Includes(blockNumber) {\n\t\treturn nil, errUnindexedRange\n\t}\n\tfinished := blockNumber == f.targetView.HeadNumber()\n\tl := &logIterator{\n\t\tchainView:   f.targetView,\n\t\tparams:      &f.Params,\n\t\tblockNumber: blockNumber,\n\t\tfinished:    finished,\n\t\tdelimiter:   !finished,\n\t\tlvIndex:     lvIndex,\n\t}\n\tl.enforceValidState()\n\treturn l, nil\n}\n\n// newLogIteratorFromMapBoundary creates a logIterator starting at the given\n// map boundary, according to the current targetView.\nfunc (f *FilterMaps) newLogIteratorFromMapBoundary(mapIndex uint32, startBlock, startLvPtr uint64) (*logIterator, error) {\n\tif startBlock > f.targetView.HeadNumber() {\n\t\treturn nil, fmt.Errorf(\"iterator entry point %d after target chain head block %d\", startBlock, f.targetView.HeadNumber())\n\t}\n\t// get block receipts\n\treceipts := f.targetView.RawReceipts(startBlock)\n\tif receipts == nil {\n\t\treturn nil, fmt.Errorf(\"receipts not found for start block %d\", startBlock)\n\t}\n\t// initialize iterator at block start\n\tl := &logIterator{\n\t\tchainView:   f.targetView,\n\t\tparams:      &f.Params,\n\t\tblockNumber: startBlock,\n\t\treceipts:    receipts,\n\t\tblockStart:  true,\n\t\tlvIndex:     startLvPtr,\n\t}\n\tl.enforceValidState()\n\ttargetIndex := uint64(mapIndex) << f.logValuesPerMap\n\tif l.lvIndex > targetIndex {\n\t\treturn nil, fmt.Errorf(\"log value pointer %d of last block of map is after map boundary %d\", l.lvIndex, targetIndex)\n\t}\n\t// iterate to map boundary\n\tfor l.lvIndex < targetIndex {\n\t\tif l.finished {\n\t\t\treturn nil, fmt.Errorf(\"iterator already finished at %d before map boundary target %d\", l.lvIndex, targetIndex)\n\t\t}\n\t\tif err := l.next()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/runtime/runtime.go",
          "line": 144,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/runtime/runtime.go",
          "line": 213,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 742,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 758,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 754,
          "category": "unchecked_calls",
          "pattern": "\\.delegatecall\\s*\\(",
          "match": ".DelegateCall(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 322,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "Blockhash(t *testing.T)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 349,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(x)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.82,
          "confidence": 0.9738,
          "ensemble_confidence": 0.87642
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 350,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(x-1)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.84,
          "confidence": 0.9756,
          "ensemble_confidence": 0.87804
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/vm/runtime/runtime_test.go",
          "line": 352,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "blockhash(x - i)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.86,
          "confidence": 0.9774,
          "ensemble_confidence": 0.8796600000000001
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 63,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 70,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 263,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vm/program/program_test.go",
          "line": 303,
          "category": "unchecked_calls",
          "pattern": "\\.delegatecall\\s*\\(",
          "match": ".DelegateCall(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/program/program.go",
          "line": 271,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32 {\n\t\tchunk := data[idx : idx+32]\n\t\t// push the value\n\t\tp.Push(chunk)\n\t\t// push the memory index\n\t\tp.Push(uint32(idx) + memStart)\n\t\tp.Op(vm.MSTORE)\n\t}\n\t// Remainders become stored using MSTORE8\n\tfor ",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/program/program.go",
          "line": 322,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 32 {\n\t\tdataStart := idx\n\t\t// Mload the chunk\n\t\tp.Push(dataStart)\n\t\tp.Op(vm.MLOAD)\n\t\t// Value is now on stack,\n\t\tp.Push(startSlot)\n\t\tp.Op(vm.SSTORE)\n\t\tstartSlot++\n\t}\n\treturn p\n}\n\n// ReturnViaCodeCopy utilises CODECOPY to place the given data in the bytecode of\n// p, loads into memory (offset 0) and returns the code.\n// This is a typical \"constructor\".\n// Note: since all indexing is calculated immediately, the preceding bytecode\n// must not be expanded or shortened.\nfunc (p *Program) ReturnViaCodeCopy(data []byte) *Program {\n\tp.Push(len(data))\n\t// For convenience, we'll use PUSH2 for the offset. Then we know we can always\n\t// fit, since code is limited to 0xc000\n\tp.Op(vm.PUSH2)\n\toffsetPos := p.Size()  // Need to update this position later on\n\tp.Append([]byte{0, 0}) // Offset of the code to be copied\n\tp.Push(0)              // Offset in memory (destination)\n\tp.Op(vm.CODECOPY)      // Copy from code[offset:offset+len] to memory[0:]\n\tp.Return(0, len(data)) // Return memory[0:len]\n\toffset := p.Size()\n\tp.Append(data) // And add the data\n\n\t// Now, go back and fix the offset\n\tp.code[offsetPos] = byte(offset >> 8)\n\tp.code[offsetPos+1] = byte(offset)\n\treturn p\n}\n\n// Sstore stores the given byte array to the given slot.\n// OBS! Does not verify that the value indeed fits into 32 bytes.\n// If it does not, it will panic later on via doPush.\nfunc (p *Program) Sstore(slot any, value any) *Program {\n\tp.Push(value)\n\tp.Push(slot)\n\treturn p.Op(vm.SSTORE)\n}\n\n// Tstore stores the given byte array to the given t-slot.\n// OBS! Does not verify that the value indeed fits into 32 bytes.\n// If it does not, it will panic later on via doPush.\nfunc (p *Program) Tstore(slot any, value any) *Program {\n\tp.Push(value)\n\tp.Push(slot)\n\treturn p.Op(vm.TSTORE)\n}\n\n// Return implements RETURN\nfunc (p *Program) Return(offset, len int) *Program {\n\tp.Push(len)\n\tp.Push(offset)\n\treturn p.Op(vm.RETURN)\n}\n\n// ReturnData loads the given data into memory, and does a return with it\nfunc (p *Program) ReturnData(data []byte) *Program {\n\tp.Mstore(data, 0)\n\treturn p.Return(0, len(data))\n}\n\n// Create2 uses create2 to construct a contract with the given bytecode.\n// This operation leaves either '0' or address on the stack.\nfunc (p *Program) Create2(code []byte, salt any) *Program {\n\tvar (\n\t\tvalue  = 0\n\t\toffset = 0\n\t\tsize   = len(code)\n\t)\n\t// Load the code into mem\n\tp.Mstore(code, 0)\n\t// Create it\n\treturn p.Push(salt).\n\t\tPush(size).\n\t\tPush(offset).\n\t\tPush(value).\n\t\tOp(vm.CREATE2)\n\t// On the stack now, is either\n\t// - zero: in case of failure, OR\n\t// - address: in case of success\n}\n\n// Create2ThenCall calls create2 with the given initcode and salt, and then calls\n// into the created contract (or calls into zero, if the creation failed).\nfunc (p *Program) Create2ThenCall(code []byte, salt any) *Program {\n\tp.Create2(code, salt)\n\t// If there happen to be a zero on the stack, it doesn't matter, we're\n\t// not sending any value anyway\n\tp.Push(0).Push(0) // mem out\n\tp.Push(0).Push(0) // mem in\n\tp.Push(0)         // value\n\tp.Op(vm.DUP6)     // address\n\tp.Op(vm.GAS)\n\tp.Op(vm.CALL)\n\tp.Op(vm.POP)        // pop the retval\n\treturn p.Op(vm.POP) // pop the address\n}\n\n// Selfdestruct pushes beneficiary and invokes selfdestruct.\nfunc (p *Program) Selfdestruct(beneficiary any) *Program {\n\tp.Push(beneficiary)\n\treturn p.Op(vm.SELFDESTRUCT)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 50,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\theight := binary.BigEndian.Uint64(input[pos : pos+heightLength])\n\tpos += heightLength\n\n\tappHash := input[pos : pos+appHashLength]\n\tpos += appHashLength\n\n\tcurValidatorSetHash := input[pos : pos+validatorSetHashLength]\n\tpos += validatorSetHashLength\n\n\tnextValidatorSetLength := (inputLen - minimumLength) / singleValidatorBytesLength\n\tvalidatorSetBytes := input[pos:]\n\tvar validatorSet []*tmtypes.Validator\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 104,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+heightLength], cs.Height)\n\tpos += heightLength\n\n\tcopy(encodingBytes[pos:pos+appHashLength], cs.AppHash)\n\tpos += appHashLength\n\n\tcopy(encodingBytes[pos:pos+validatorSetHashLength], cs.CurValidatorSetHash)\n\tpos += validatorSetHashLength\n\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 123,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= validatorPubkeyLength\n\n\t\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+validatorVotingPowerLength], uint64(validator.VotingPower))\n\t\tpos += validatorVotingPowerLength\n\t}\n\n\treturn encodingBytes, nil\n}\n\nfunc (cs *ConsensusState) ApplyHeader(header *Header) (bool, error) {\n\tif uint64(header.Height) <= cs.Height {\n\t\treturn false, fmt.Errorf(\"header height <= consensus height (%d <= %d)\", header.Height, cs.Height)\n\t}\n\n\tif err := header.Validate(cs.ChainID)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vm/lightclient/v1/types.go",
          "line": 278,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= storeNameLengthBytesLength\n\n\tkeyLength := binary.BigEndian.Uint64(input[pos+keyLengthBytesLength-8 : pos+keyLengthBytesLength])\n\tpos += keyLengthBytesLength\n\n\tfixedSize = storeNameLengthBytesLength + keyLengthBytesLength + valueLengthBytesLength\n\tif inputLength <= fixedSize+keyLength || fixedSize+keyLength < fixedSize {\n\t\treturn nil, fmt.Errorf(\"invalid input, keyLength %d is too long\", keyLength)\n\t}\n\tkey := input[pos : pos+keyLength]\n\tpos += keyLength\n\n\tvalueLength := binary.BigEndian.Uint64(input[pos+valueLengthBytesLength-8 : pos+valueLengthBytesLength])\n\tpos += valueLengthBytesLength\n\n\tfixedSize = storeNameLengthBytesLength + keyLengthBytesLength + valueLengthBytesLength + appHashLength\n\tif inputLength <= fixedSize+keyLength+valueLength ||\n\t\tfixedSize+keyLength < fixedSize ||\n\t\tfixedSize+keyLength+valueLength < valueLength {\n\t\treturn nil, fmt.Errorf(\"invalid input, valueLength %d is too long\", valueLength)\n\t}\n\tvalue := input[pos : pos+valueLength]\n\tpos += valueLength\n\n\tappHash := input[pos : pos+appHashLength]\n\tpos += appHashLength\n\n\tproofBytes := input[pos:]\n\tvar merkleProof merkle.Proof\n\terr := merkleProof.Unmarshal(proofBytes)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\n\tkeyValueMerkleProof := &KeyValueMerkleProof{\n\t\tKey:       key,\n\t\tValue:     value,\n\t\tStoreName: storeName,\n\t\tAppHash:   appHash,\n\t\tProof:     &merkleProof,\n\t}\n\n\treturn keyValueMerkleProof, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 56,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+heightLength], cs.Height)\n\tpos += heightLength\n\n\tcopy(encodingBytes[pos:pos+validatorSetHashLength], cs.NextValidatorSetHash)\n\tpos += validatorSetHashLength\n\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 72,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= validatorPubkeyLength\n\n\t\tbinary.BigEndian.PutUint64(encodingBytes[pos:pos+validatorVotingPowerLength], uint64(validator.VotingPower))\n\t\tpos += validatorVotingPowerLength\n\n\t\tcopy(encodingBytes[pos:pos+relayerAddressLength], validator.RelayerAddress)\n\t\tpos += relayerAddressLength\n\n\t\tcopy(encodingBytes[pos:pos+relayerBlsKeyLength], validator.BlsKey)\n\t\tpos += relayerBlsKeyLength\n\t}\n\n\treturn encodingBytes, nil\n}\n\nfunc (cs *ConsensusState) ApplyLightBlock(block *types.LightBlock, isHertz bool) (bool, error) {\n\tif uint64(block.Height) <= cs.Height {\n\t\treturn false, fmt.Errorf(\"block height <= consensus height (%d < %d)\", block.Height, cs.Height)\n\t}\n\n\tif err := block.ValidateBasic(cs.ChainID)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 149,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= chainIDLength\n\n\theight := binary.BigEndian.Uint64(input[pos : pos+heightLength])\n\tpos += heightLength\n\n\tnextValidatorSetHash := input[pos : pos+validatorSetHashLength]\n\tpos += validatorSetHashLength\n\n\tvalidatorSetLength := (inputLen - minimumLength) / singleValidatorBytesLength\n\tvalidatorSetBytes := input[pos:]\n\tvalidatorSet := make([]*types.Validator, 0, validatorSetLength)\n\tfor index := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/vm/lightclient/v2/lightclient.go",
          "line": 166,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= validatorPubkeyLength\n\n\t\tvotingPower := int64(binary.BigEndian.Uint64(validatorBytes[pos : pos+validatorVotingPowerLength]))\n\t\tpos += validatorVotingPowerLength\n\n\t\trelayerAddress := make([]byte, relayerAddressLength)\n\t\tcopy(relayerAddress[:], validatorBytes[pos:pos+relayerAddressLength])\n\t\tpos += relayerAddressLength\n\n\t\trelayerBlsKey := make([]byte, relayerBlsKeyLength)\n\t\tcopy(relayerBlsKey[:], validatorBytes[pos:])\n\n\t\tvalidator := types.NewValidator(pubkey, votingPower)\n\t\tvalidator.SetRelayerAddress(relayerAddress)\n\t\tvalidator.SetBlsKey(relayerBlsKey)\n\t\tvalidatorSet = append(validatorSet, validator)\n\t}\n\n\tconsensusState := ConsensusState{\n\t\tChainID:              chainID,\n\t\tHeight:               height,\n\t\tNextValidatorSetHash: nextValidatorSetHash,\n\t\tValidatorSet: &types.ValidatorSet{\n\t\t\tValidators: validatorSet,\n\t\t},\n\t}\n\n\treturn consensusState, nil\n}\n\n// input:\n// consensus state length | consensus state | light block |\n// 32 bytes               |                 |             |\nfunc DecodeLightBlockValidationInput(input []byte) (*ConsensusState, *types.LightBlock, error) {\n\tif uint64(len(input)) <= consensusStateLengthBytesLength {\n\t\treturn nil, nil, errors.New(\"invalid input\")\n\t}\n\n\tcsLen := binary.BigEndian.Uint64(input[consensusStateLengthBytesLength-uint64TypeLength : consensusStateLengthBytesLength])\n\n\tif consensusStateLengthBytesLength+csLen < consensusStateLengthBytesLength {\n\t\treturn nil, nil, fmt.Errorf(\"integer overflow, csLen: %d\", csLen)\n\t}\n\n\tif uint64(len(input)) <= consensusStateLengthBytesLength+csLen {\n\t\treturn nil, nil, fmt.Errorf(\"expected payload size %d, actual size: %d\", consensusStateLengthBytesLength+csLen, len(input))\n\t}\n\n\tcs, err := DecodeConsensusState(input[consensusStateLengthBytesLength : consensusStateLengthBytesLength+csLen])\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\tvar lbpb tmproto.LightBlock\n\terr = lbpb.Unmarshal(input[consensusStateLengthBytesLength+csLen:])\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\tblock, err := types.LightBlockFromProto(&lbpb)\n\tif err != nil {\n\t\treturn nil, nil, err\n\t}\n\n\treturn &cs, block, nil\n}\n\n// output:\n// | validatorSetChanged | empty      | consensusStateBytesLength |  new consensusState |\n// | 1 byte              | 23 bytes   | 8 bytes                   |                     |\nfunc EncodeLightBlockValidationResult(validatorSetChanged bool, consensusStateBytes []byte) []byte {\n\tlengthBytes := make([]byte, validateResultMetaDataLength)\n\tif validatorSetChanged {\n\t\tcopy(lengthBytes[:1], []byte{0x01})\n\t}\n\n\tconsensusStateBytesLength := uint64(len(consensusStateBytes))\n\tbinary.BigEndian.PutUint64(lengthBytes[validateResultMetaDataLength-uint64TypeLength:], consensusStateBytesLength)\n\n\tresult := append(lengthBytes, consensusStateBytes...)\n\treturn result\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 415,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t\t\t\tcontinue\n\t\t\t} else if cmp == 0 {\n\t\t\t\t// the snapshot key can be overwritten\n\t\t\t\tcreated--\n\t\t\t\tif write = !bytes.Equal(kvvals[0], iter.Value)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 434,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Since(istart)\n\t}\n\tif iter.Err != nil {\n\t\t// Trie errors should never happen. Still, in case of a bug, expose the\n\t\t// error here, as the outer code will presume errors are interrupts, not\n\t\t// some deeper issues.\n\t\tlog.Error(\"State snapshotter failed to iterate trie\", \"err\", iter.Err)\n\t\treturn false, nil, iter.Err\n\t}\n\t// Delete all stale snapshot states remaining\n\tistart := time.Now()\n\tfor _, key := range kvkeys {\n\t\tif err := onState(key, nil, false, true)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 449,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\tinternal += time.Since(istart)\n\n\t// Update metrics for counting trie iteration\n\tif kind == snapStorage {\n\t\tsnapStorageTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t} else {\n\t\tsnapAccountTrieReadCounter.Inc((time.Since(start) - internal).Nanoseconds())\n\t}\n\tlogger.Debug(\"Regenerated state range\", \"root\", trieId.Root, \"last\", hexutil.Encode(last),\n\t\t\"count\", count, \"created\", created, \"updated\", updated, \"untouched\", untouched, \"deleted\", deleted)\n\n\t// If there are either more trie items, or there are more snap items\n\t// (in the next segment), then we need to keep working\n\treturn !trieMore && !result.diskMore, last, nil\n}\n\n// checkAndFlush checks if an interruption signal is received or the\n// batch size has exceeded the allowance.\nfunc (dl *diskLayer) checkAndFlush(ctx *generatorContext, current []byte) error {\n\tvar abort chan *generatorStats\n\tselect {\n\tcase abort = <-dl.genAbort:\n\tdefault:\n\t}\n\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize || abort != nil {\n\t\tif bytes.Compare(current, dl.genMarker) < 0 {\n\t\t\tlog.Error(\"Snapshot generator went backwards\", \"current\", fmt.Sprintf(\"%x\", current), \"genMarker\", fmt.Sprintf(\"%x\", dl.genMarker))\n\t\t}\n\t\t// Flush out the batch anyway no matter it's empty or not.\n\t\t// It's possible that all the states are recovered and the\n\t\t// generation indeed makes progress.\n\t\tjournalProgress(ctx.batch, current, ctx.stats)\n\n\t\tif err := ctx.batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 527,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + 2*common.HashLength + len(val))\n\t\tctx.stats.slots++\n\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := dl.checkAndFlush(ctx, append(account[:], key...))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 595,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tctx.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(dl.genMarker) > common.HashLength {\n\t\t\tmarker = dl.genMarker[:]\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := dl.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/state/snapshot/generate.go",
          "line": 583,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 32\n\t\t\t\t}\n\t\t\t\tif acc.Root == types.EmptyRootHash {\n\t\t\t\t\tdataLen -= 32\n\t\t\t\t}\n\t\t\t\tsnapRecoveredAccountMeter.Mark(1)\n\t\t\t} else {\n\t\t\t\tdata := types.SlimAccountRLP(acc)\n\t\t\t\tdataLen = len(data)\n\t\t\t\trawdb.WriteAccountSnapshot(ctx.batch, account, data)\n\t\t\t\tsnapGeneratedAccountMeter.Mark(1)\n\t\t\t}\n\t\t\tctx.stats.storage += common.StorageSize(1 + common.HashLength + dataLen)\n\t\t\tctx.stats.accounts++\n\t\t}\n\t\t// If the snap generation goes here after interrupted, genMarker may go backward\n\t\t// when last genMarker is consisted of accountHash and storageHash\n\t\tmarker := account[:]\n\t\tif accMarker != nil && bytes.Equal(marker, accMarker) && len(dl.genMarker) > common.HashLength {\n\t\t\tmarker = dl.genMarker[:]\n\t\t}\n\t\t// If we've exceeded our batch allowance or termination was requested, flush to disk\n\t\tif err := dl.checkAndFlush(ctx, marker)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/difflayer.go",
          "line": 146,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(common.HashLength + len(blob))\n\t\tsnapshotDirtyAccountWriteMeter.Mark(int64(len(blob)))\n\t}\n\tfor accountHash, slots := range storage {\n\t\tif slots == nil {\n\t\t\tpanic(fmt.Sprintf(\"storage %#x nil\", accountHash))\n\t\t}\n\t\t// Determine memory size and track the dirty writes\n\t\tfor _, data := range slots {\n\t\t\tdl.memory += uint64(common.HashLength + len(data))\n\t\t\tsnapshotDirtyStorageWriteMeter.Mark(int64(len(data)))\n\t\t}\n\t}\n\treturn dl\n}\n\n// rebloom discards the layer's current bloom and rebuilds it from scratch based\n// on the parent's and the local diffs.\nfunc (dl *diffLayer) rebloom(origin *diskLayer) {\n\tdl.lock.Lock()\n\tdefer dl.lock.Unlock()\n\n\tdefer func(start time.Time) {\n\t\tsnapshotBloomIndexTimer.Update(time.Since(start))\n\t}(time.Now())\n\n\t// Inject the new origin that triggered the rebloom\n\tdl.origin = origin\n\n\t// Retrieve the parent bloom or create a fresh empty one\n\tif parent, ok := dl.parent.(*diffLayer)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/snapshot/difflayer.go",
          "line": 453,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(dl.accountList) * common.HashLength)\n\treturn dl.accountList\n}\n\n// StorageList returns a sorted list of all storage slot hashes in this diffLayer\n// for the given account. If the whole storage is destructed in this layer, then\n// an additional flag *destructed = true* will be returned, otherwise the flag is\n// false. Besides, the returned list will include the hash of deleted storage slot.\n// Note a special case is an account is deleted in a prior tx but is recreated in\n// the following tx with some storage slots set. In this case the returned list is\n// not empty but the flag is true.\n//\n// Note, the returned slice is not a copy, so do not modify it.\nfunc (dl *diffLayer) StorageList(accountHash common.Hash) []common.Hash {\n\tdl.lock.RLock()\n\tif _, ok := dl.storageData[accountHash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/state/snapshot/difflayer.go",
          "line": 486,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(dl.storageList)*common.HashLength + common.HashLength)\n\treturn storageList\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/conversion.go",
          "line": 128,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tstat.head = account\n}\n\n// finishAccounts updates the generator stats for the finished account range.\nfunc (stat *generateStats) finishAccounts(done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.accounts += done\n}\n\n// progressContract updates the generator stats for a specific in-progress contract.\nfunc (stat *generateStats) progressContract(account common.Hash, slot common.Hash, done uint64) {\n\tstat.lock.Lock()\n\tdefer stat.lock.Unlock()\n\n\tstat.slots += done\n\tstat.slotsHead[account] = slot\n\tif _, ok := stat.slotsStart[account]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/snapshot/conversion.go",
          "line": 157,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= done\n\tdelete(stat.slotsHead, account)\n\tdelete(stat.slotsStart, account)\n}\n\n// report prints the cumulative progress statistic smartly.\nfunc (stat *generateStats) report() {\n\tstat.lock.RLock()\n\tdefer stat.lock.RUnlock()\n\n\tctx := []interface{}{\n\t\t\"accounts\", stat.accounts,\n\t\t\"slots\", stat.slots,\n\t\t\"elapsed\", common.PrettyDuration(time.Since(stat.start)),\n\t}\n\tif stat.accounts > 0 {\n\t\t// If there's progress on the account trie, estimate the time to finish crawling it\n\t\tif done := binary.BigEndian.Uint64(stat.head[:8]) / stat.accounts",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/iterator_test.go",
          "line": 87,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t}\n\t\t}\n\t\tstorage[h] = accStorage\n\t\tnilStorage[h] = nilstorage\n\t}\n\t// Add some (identical) layers on top\n\tdiffLayer := newDiffLayer(emptyLayer(), common.Hash{}, copyAccounts(accounts), copyStorage(storage))\n\tfor account := range accounts {\n\t\tit := diffLayer.StorageIterator(account, common.Hash{})\n\t\tverifyIterator(t, 100, it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n\n\tdiskLayer := diffToDisk(diffLayer)\n\tfor account := range accounts {\n\t\tit := diskLayer.StorageIterator(account, common.Hash{})\n\t\tverifyIterator(t, 100-nilStorage[account], it, verifyNothing) // Nil is allowed for single layer iterator\n\t}\n}\n\ntype testIterator struct {\n\tvalues []byte\n}\n\nfunc newTestIterator(values ...byte) *testIterator {\n\treturn &testIterator{values}\n}\n\nfunc (ti *testIterator) Seek(common.Hash) {\n\tpanic(\"implement me\")\n}\n\nfunc (ti *testIterator) Next() bool {\n\tti.values = ti.values[1:]\n\treturn len(ti.values) > 0\n}\n\nfunc (ti *testIterator) Error() error {\n\treturn nil\n}\n\nfunc (ti *testIterator) Hash() common.Hash {\n\treturn common.BytesToHash([]byte{ti.values[0]})\n}\n\nfunc (ti *testIterator) Account() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Slot() []byte {\n\treturn nil\n}\n\nfunc (ti *testIterator) Release() {}\n\nfunc TestFastIteratorBasics(t *testing.T) {\n\ttype testCase struct {\n\t\tlists   [][]byte\n\t\texpKeys []byte\n\t}\n\tfor i, tc := range []testCase{\n\t\t{lists: [][]byte{{0, 1, 8}, {1, 2, 8}, {2, 9}, {4},\n\t\t\t{7, 14, 15}, {9, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 4, 7, 8, 9, 13, 14, 15, 16}},\n\t\t{lists: [][]byte{{0, 8}, {1, 2, 8}, {7, 14, 15}, {8, 9},\n\t\t\t{9, 10}, {10, 13, 15, 16}},\n\t\t\texpKeys: []byte{0, 1, 2, 7, 8, 9, 10, 13, 14, 15, 16}},\n\t} {\n\t\tvar iterators []*weightedIterator\n\t\tfor i, data := range tc.lists {\n\t\t\tit := newTestIterator(data...)\n\t\t\titerators = append(iterators, &weightedIterator{it, i})\n\t\t}\n\t\tfi := &fastIterator{\n\t\t\titerators: iterators,\n\t\t\tinitiated: false,\n\t\t}\n\t\tcount := 0\n\t\tfor fi.Next() {\n\t\t\tif got, exp := fi.Hash()[31], tc.expKeys[count]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/context.go",
          "line": 186,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= count\n\tsnapStorageCleanCounter.Inc(time.Since(start).Nanoseconds())\n}\n\n// removeStorageAt deletes all storage entries which are located in the specified\n// account. When the iterator touches the storage entry which is outside the given\n// account, it stops and holds the current iterated element locally. An error will\n// be returned if the initial position of iterator is not in the given account.\nfunc (ctx *generatorContext) removeStorageAt(account common.Hash) error {\n\tvar (\n\t\tcount int64\n\t\tstart = time.Now()\n\t\titer  = ctx.storage\n\t)\n\tfor iter.Next() {\n\t\tkey := iter.Key()\n\t\tcmp := bytes.Compare(key[1:1+common.HashLength], account.Bytes())\n\t\tif cmp < 0 {\n\t\t\treturn errors.New(\"invalid iterator position\")\n\t\t}\n\t\tif cmp > 0 {\n\t\t\titer.Hold()\n\t\t\tbreak\n\t\t}\n\t\tcount++\n\t\tctx.batch.Delete(key)\n\t\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\tctx.batch.Write()\n\t\t\tctx.batch.Reset()\n\t\t}\n\t}\n\tsnapWipedStorageMeter.Mark(count)\n\tsnapStorageCleanCounter.Inc(time.Since(start).Nanoseconds())\n\treturn nil\n}\n\n// removeStorageLeft deletes all storage entries which are located after\n// the current iterator position.\nfunc (ctx *generatorContext) removeStorageLeft() {\n\tvar (\n\t\tcount uint64\n\t\tstart = time.Now()\n\t\titer  = ctx.storage\n\t)\n\tfor iter.Next() {\n\t\tcount++\n\t\tctx.batch.Delete(iter.Key())\n\t\tif ctx.batch.ValueSize() > ethdb.IdealBatchSize {\n\t\t\tctx.batch.Write()\n\t\t\tctx.batch.Reset()\n\t\t}\n\t}\n\tctx.stats.dangling += count\n\tsnapDanglingStorageMeter.Mark(int64(count))\n\tsnapStorageCleanCounter.Inc(time.Since(start).Nanoseconds())\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/disklayer_test.go",
          "line": 535,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tacc := common.Hash{byte(i)}\n\t\trawdb.WriteAccountSnapshot(db, acc, acc[:])\n\t}\n\t// Add an 'higher' key, with incorrect (higher) prefix\n\thighKey := []byte{rawdb.SnapshotAccountPrefix[0] + 1}\n\tdb.Put(highKey, []byte{0xff, 0xff})\n\n\tbaseRoot := randomHash()\n\trawdb.WriteSnapshotRoot(db, baseRoot)\n\n\tsnaps := &Tree{\n\t\tlayers: map[common.Hash]snapshot{\n\t\t\tbaseRoot: &diskLayer{\n\t\t\t\tdiskdb: db,\n\t\t\t\tcache:  fastcache.New(500 * 1024),\n\t\t\t\troot:   baseRoot,\n\t\t\t},\n\t\t},\n\t}\n\t// Test some different seek positions\n\ttype testcase struct {\n\t\tpos    byte\n\t\texpkey byte\n\t}\n\tvar cases = []testcase{\n\t\t{0xff, 0x55}, // this should exit immediately without checking key\n\t\t{0x01, 0x02},\n\t\t{0xfe, 0xfe},\n\t\t{0xfd, 0xfe},\n\t\t{0x00, 0x00},\n\t}\n\tfor i, tc := range cases {\n\t\tit, err := snaps.AccountIterator(baseRoot, common.Hash{tc.pos})\n\t\tif err != nil {\n\t\t\tt.Fatalf(\"case %d, error: %v\", i, err)\n\t\t}\n\t\tcount := 0\n\t\tfor it.Next() {\n\t\t\tk, v, err := it.Hash()[0], it.Account()[0], it.Error()\n\t\t\tif err != nil {\n\t\t\t\tt.Fatalf(\"test %d, item %d, error: %v\", i, count, err)\n\t\t\t}\n\t\t\t// First item in iterator should have the expected key\n\t\t\tif count == 0 && k != tc.expkey {\n\t\t\t\tt.Fatalf(\"test %d, item %d, got %v exp %v\", i, count, k, tc.expkey)\n\t\t\t}\n\t\t\tcount++\n\t\t\tif v != k {\n\t\t\t\tt.Fatalf(\"test %d, item %d, value wrong, got %v exp %v\", i, count, v, k)\n\t\t\t}\n\t\t}\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/snapshot/snapshot.go",
          "line": 868,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= common.StorageSize(layer.memory)\n\t\t}\n\t}\n\treturn size, 0, 0\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/snapshot/snapshot.go",
          "line": 330,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\tif limits == 0 {\n\t\t\tbreak\n\t\t}\n\t\tparent := layer.Parent()\n\t\tif parent == nil {\n\t\t\tbreak\n\t\t}\n\t\tlayer = parent\n\t}\n\treturn ret\n}\n\n// Update adds a new snapshot into the tree, if that can be linked to an existing\n// old parent. It is disallowed to insert a disk layer (the origin of all).\nfunc (t *Tree) Update(blockRoot common.Hash, parentRoot common.Hash, accounts map[common.Hash][]byte, storage map[common.Hash]map[common.Hash][]byte) error {\n\t// Reject noop updates to avoid self-loops in the snapshot tree. This is a\n\t// special case that can only happen for Clique networks where empty blocks\n\t// don't modify the state (0 block subsidy).\n\t//\n\t// Although we could silently ignore this internally, it should be the caller's\n\t// responsibility to avoid even attempting to insert such a snapshot.\n\tif blockRoot == parentRoot {\n\t\treturn errSnapshotCycle\n\t}\n\t// Generate a new snapshot on top of the parent\n\tparent := t.Snapshot(parentRoot)\n\tif parent == nil {\n\t\treturn fmt.Errorf(\"parent [%#x] snapshot missing\", parentRoot)\n\t}\n\tsnap := parent.(snapshot).Update(blockRoot, accounts, storage)\n\n\t// Save the new snapshot for later\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tt.layers[snap.root] = snap\n\tlog.Debug(\"Snapshot updated\", \"blockRoot\", blockRoot)\n\treturn nil\n}\n\nfunc (t *Tree) CapLimit() int {\n\treturn t.capLimit\n}\n\n// Cap traverses downwards the snapshot tree from a head block hash until the\n// number of allowed layers are crossed. All layers beyond the permitted number\n// are flattened downwards.\n//\n// Note, the final diff layer count in general will be one more than the amount\n// requested. This happens because the bottom-most diff layer is the accumulator\n// which may or may not overflow and cascade to disk. Since this last layer's\n// survival is only known *after* capping, we need to omit it from the count if\n// we want to ensure that *at least* the requested number of diff layers remain.\nfunc (t *Tree) Cap(root common.Hash, layers int) error {\n\t// Retrieve the head snapshot to cap from\n\tsnap := t.Snapshot(root)\n\tif snap == nil {\n\t\treturn fmt.Errorf(\"snapshot [%#x] missing\", root)\n\t}\n\tdiff, ok := snap.(*diffLayer)\n\tif !ok {\n\t\treturn fmt.Errorf(\"snapshot [%#x] is disk layer\", root)\n\t}\n\t// If the generator is still running, use a more aggressive cap\n\tdiff.origin.lock.RLock()\n\tif diff.origin.genMarker != nil && layers > 8 {\n\t\tlayers = 8\n\t}\n\tdiff.origin.lock.RUnlock()\n\n\t// Run the internal capping and discard all stale layers\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\t// Flattening the bottom-most diff layer requires special casing since there's\n\t// no child to rewire to the grandparent. In that case we can fake a temporary\n\t// child for the capping and then remove it.\n\tif layers == 0 {\n\t\t// If full commit was requested, flatten the diffs and merge onto disk\n\t\tdiff.lock.RLock()\n\t\tbase := diffToDisk(diff.flatten().(*diffLayer))\n\t\tdiff.lock.RUnlock()\n\n\t\t// Replace the entire snapshot tree with the flat base\n\t\tt.layers = map[common.Hash]snapshot{base.root: base}\n\t\treturn nil\n\t}\n\tpersisted := t.cap(diff, layers)\n\n\t// Remove any layer that is stale or links into a stale layer\n\tchildren := make(map[common.Hash][]common.Hash)\n\tfor root, snap := range t.layers {\n\t\tif diff, ok := snap.(*diffLayer)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/state/pruner/pruner.go",
          "line": 161,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t}\n\t\t\tcount += 1\n\t\t\tsize += common.StorageSize(len(key) + len(iter.Value()))\n\t\t\tbatch.Delete(key)\n\n\t\t\tvar eta time.Duration // Realistically will never remain uninited\n\t\t\tif done := binary.BigEndian.Uint64(key[:8])",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/state/pruner/pruner.go",
          "line": 225,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 0x10 {\n\t\t\tvar (\n\t\t\t\tstart = []byte{byte(b)}\n\t\t\t\tend   = []byte{byte(b + 0x10)}\n\t\t\t)\n\t\t\tif b == 0xf0 {\n\t\t\t\tend = nil\n\t\t\t}\n\t\t\tlog.Info(\"Compacting database\", \"range\", fmt.Sprintf(\"%#x-%#x\", start, end), \"elapsed\", common.PrettyDuration(time.Since(cstart)))\n\t\t\tif err := pruneDB.Compact(start, end)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/blobpool/slotter.go",
          "line": 33,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= blobSize\n\t\tfinished := slotsize > uint32(maxBlobsPerTransaction)*blobSize+txMaxSize\n\n\t\treturn slotsize, finished\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/txpool/blobpool/slotter.go",
          "line": 30,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint32(blobSize) // underflows, it's ok, will overflow back in the first return\n\n\treturn func() (size uint32, done bool) {\n\t\tslotsize += blobSize\n\t\tfinished := slotsize > uint32(maxBlobsPerTransaction)*blobSize+txMaxSize\n\n\t\treturn slotsize, finished\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 523,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize)\n\treturn nil\n}\n\n// recheck verifies the pool's content for a specific account and drops anything\n// that does not fit anymore (dangling or filled nonce, overdraft).\nfunc (p *BlobPool) recheck(addr common.Address, inclusions map[common.Hash]uint64) {\n\t// Sort the transactions belonging to the account so reinjects can be simpler\n\ttxs := p.index[addr]\n\tif inclusions != nil && txs == nil { // during reorgs, we might find new accounts\n\t\treturn\n\t}\n\tsort.Slice(txs, func(i, j int) bool {\n\t\treturn txs[i].nonce < txs[j].nonce\n\t})\n\t// If there is a gap between the chain state and the blob pool, drop\n\t// all the transactions as they are non-executable. Similarly, if the\n\t// entire tx range was included, drop all.\n\tvar (\n\t\tnext   = p.state.GetNonce(addr)\n\t\tgapped = txs[0].nonce > next\n\t\tfilled = txs[len(txs)-1].nonce < next\n\t)\n\tif gapped || filled {\n\t\tvar (\n\t\t\tids    []uint64\n\t\t\tnonces []uint64\n\t\t)\n\t\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1032,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize)\n\treturn nil\n}\n\n// SetGasTip implements txpool.SubPool, allowing the blob pool's gas requirements\n// to be kept in sync with the main transaction pool's gas requirements.\nfunc (p *BlobPool) SetGasTip(tip *big.Int) {\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\t// Store the new minimum gas tip\n\told := p.gasTip\n\tp.gasTip = uint256.MustFromBig(tip)\n\n\t// If the min miner fee increased, remove transactions below the new threshold\n\tif old == nil || p.gasTip.Cmp(old) > 0 {\n\t\tfor addr, txs := range p.index {\n\t\t\tfor i, tx := range txs {\n\t\t\t\tif tx.execTipCap.Cmp(p.gasTip) < 0 {\n\t\t\t\t\t// Drop the offending transaction\n\t\t\t\t\tvar (\n\t\t\t\t\t\tids    = []uint64{tx.id}\n\t\t\t\t\t\tnonces = []uint64{tx.nonce}\n\t\t\t\t\t)\n\t\t\t\t\tp.spent[addr] = new(uint256.Int).Sub(p.spent[addr], txs[i].costCap)\n\t\t\t\t\tp.stored -= uint64(tx.storageSize)\n\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\ttxs[i] = nil\n\n\t\t\t\t\t// Drop everything afterwards, no gaps allowed\n\t\t\t\t\tfor j, tx := range txs[i+1:] {\n\t\t\t\t\t\tids = append(ids, tx.id)\n\t\t\t\t\t\tnonces = append(nonces, tx.nonce)\n\n\t\t\t\t\t\tp.spent[addr] = new(uint256.Int).Sub(p.spent[addr], tx.costCap)\n\t\t\t\t\t\tp.stored -= uint64(tx.storageSize)\n\t\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\t\ttxs[i+1+j] = nil\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the dropped transactions from the index\n\t\t\t\t\tif i > 0 {\n\t\t\t\t\t\tp.index[addr] = txs[:i]\n\t\t\t\t\t\theap.Fix(p.evict, p.evict.index[addr])\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdelete(p.index, addr)\n\t\t\t\t\t\tdelete(p.spent, addr)\n\n\t\t\t\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t\t\t\t\tp.reserver.Release(addr)\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the transactions from the data store\n\t\t\t\t\tlog.Warn(\"Dropping underpriced blob transaction\", \"from\", addr, \"rejected\", tx.nonce, \"tip\", tx.execTipCap, \"want\", tip, \"drop\", nonces, \"ids\", ids)\n\t\t\t\t\tdropUnderpricedMeter.Mark(int64(len(ids)))\n\n\t\t\t\t\tfor _, id := range ids {\n\t\t\t\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1486,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize) - uint64(prev.storageSize)\n\t} else {\n\t\t// Transaction extends previously scheduled ones\n\t\tp.index[from] = append(p.index[from], meta)\n\t\tif _, ok := p.spent[from]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1496,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(meta.storageSize)\n\t}\n\t// Recompute the rolling eviction fields. In case of a replacement, this will\n\t// recompute all subsequent fields. In case of an append, this will only do\n\t// the fresh calculation.\n\ttxs := p.index[from]\n\n\tfor i := offset",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1694,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= slotDataused\n\t\tdatareal += slotDataused + slotDatagaps\n\t\tslotused += shelf.FilledSlots\n\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfDatausedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDataused))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfDatagapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDatagaps))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfSlotusedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.FilledSlots))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(shelfSlotgapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.GappedSlots))\n\n\t\tmaxBlobs := eip4844.LatestMaxBlobsPerBlock(p.chain.Config())\n\t\tif shelf.SlotSize/blobSize > uint32(maxBlobs) {\n\t\t\toversizedDataused += slotDataused\n\t\t\toversizedDatagaps += slotDatagaps\n\t\t\toversizedSlotused += shelf.FilledSlots\n\t\t\toversizedSlotgaps += shelf.GappedSlots\n\t\t}\n\t}\n\tdatausedGauge.Update(int64(dataused))\n\tdatarealGauge.Update(int64(datareal))\n\tslotusedGauge.Update(int64(slotused))\n\n\toversizedDatausedGauge.Update(int64(oversizedDataused))\n\toversizedDatagapsGauge.Update(int64(oversizedDatagaps))\n\toversizedSlotusedGauge.Update(int64(oversizedSlotused))\n\toversizedSlotgapsGauge.Update(int64(oversizedSlotgaps))\n\n\tp.updateLimboMetrics()\n}\n\n// updateLimboMetrics retrieves a bunch of stats from the limbo store and pushes\n// them out as metrics.\nfunc (p *BlobPool) updateLimboMetrics() {\n\tstats := p.limbo.store.Infos()\n\n\tvar (\n\t\tdataused uint64\n\t\tdatareal uint64\n\t\tslotused uint64\n\t)\n\tfor _, shelf := range stats.Shelves {\n\t\tslotDataused := shelf.FilledSlots * uint64(shelf.SlotSize)\n\t\tslotDatagaps := shelf.GappedSlots * uint64(shelf.SlotSize)\n\n\t\tdataused += slotDataused\n\t\tdatareal += slotDataused + slotDatagaps\n\t\tslotused += shelf.FilledSlots\n\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfDatausedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDataused))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfDatagapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(slotDatagaps))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfSlotusedGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.FilledSlots))\n\t\tmetrics.GetOrRegisterGauge(fmt.Sprintf(limboShelfSlotgapsGaugeName, shelf.SlotSize/blobSize), nil).Update(int64(shelf.GappedSlots))\n\t}\n\tlimboDatausedGauge.Update(int64(dataused))\n\tlimboDatarealGauge.Update(int64(datareal))\n\tlimboSlotusedGauge.Update(int64(slotused))\n}\n\n// SubscribeTransactions registers a subscription for new transaction events,\n// supporting feeding only newly seen or also resurrected transactions.\nfunc (p *BlobPool) SubscribeTransactions(ch chan<- core.NewTxsEvent, reorgs bool) event.Subscription {\n\tif reorgs {\n\t\treturn p.insertFeed.Subscribe(ch)\n\t} else {\n\t\treturn p.discoverFeed.Subscribe(ch)\n\t}\n}\n\n// SubscribeReannoTxsEvent registers a subscription of ReannoTxsEvent and\n// starts sending event to the given channel.\nfunc (p *BlobPool) SubscribeReannoTxsEvent(ch chan<- core.ReannoTxsEvent) event.Subscription {\n\treturn p.scope.Track(p.reannoTxFeed.Subscribe(ch))\n}\n\n// Nonce returns the next nonce of an account, with all transactions executable\n// by the pool already applied on top.\nfunc (p *BlobPool) Nonce(addr common.Address) uint64 {\n\t// We need a write lock here, since state.GetNonce might write the cache.\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\tif txs, ok := p.index[addr]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1788,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(txs)\n\t}\n\treturn pending, 0 // No non-executable txs in the blob pool\n}\n\n// Content retrieves the data content of the transaction pool, returning all the\n// pending as well as queued transactions, grouped by account and sorted by nonce.\n//\n// For the blob pool, this method will return nothing for now.\n// TODO(karalabe): Abstract out the returned metadata.\nfunc (p *BlobPool) Content() (map[common.Address][]*types.Transaction, map[common.Address][]*types.Transaction) {\n\treturn make(map[common.Address][]*types.Transaction), make(map[common.Address][]*types.Transaction)\n}\n\n// ContentFrom retrieves the data content of the transaction pool, returning the\n// pending as well as queued transactions of this address, grouped by nonce.\n//\n// For the blob pool, this method will return nothing for now.\n// TODO(karalabe): Abstract out the returned metadata.\nfunc (p *BlobPool) ContentFrom(addr common.Address) ([]*types.Transaction, []*types.Transaction) {\n\treturn []*types.Transaction{}, []*types.Transaction{}\n}\n\n// Status returns the known status (unknown/pending/queued) of a transaction\n// identified by their hashes.\nfunc (p *BlobPool) Status(hash common.Hash) txpool.TxStatus {\n\tif p.Has(hash) {\n\t\treturn txpool.TxStatusPending\n\t}\n\treturn txpool.TxStatusUnknown\n}\n\nfunc (p *BlobPool) SetMaxGas(maxGas uint64) {\n\tp.maxGas.Store(maxGas)\n}\n\nfunc (p *BlobPool) GetMaxGas() uint64 {\n\treturn p.maxGas.Load()\n}\n\n// Clear implements txpool.SubPool, removing all tracked transactions\n// from the blob pool and persistent store.\n//\n// Note, do not use this in production / live code. In live code, the pool is\n// meant to reset on a separate thread to avoid DoS vectors.\nfunc (p *BlobPool) Clear() {\n\tp.lock.Lock()\n\tdefer p.lock.Unlock()\n\n\t// manually iterating and deleting every entry is super sub-optimal\n\t// However, Clear is not currently used in production so\n\t// performance is not critical at the moment.\n\tfor hash := range p.lookup.txIndex {\n\t\tid, _ := p.lookup.storeidOfTx(hash)\n\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 555,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[i].storageSize)\n\t\t\tp.lookup.untrack(txs[i])\n\n\t\t\t// Included transactions blobs need to be moved to the limbo\n\t\t\tif filled && inclusions != nil {\n\t\t\t\tp.offload(addr, txs[i].nonce, txs[i].id, inclusions)\n\t\t\t}\n\t\t}\n\t\tdelete(p.index, addr)\n\t\tdelete(p.spent, addr)\n\t\tif inclusions != nil { // only during reorgs will the heap be initialized\n\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t}\n\t\tp.reserver.Release(addr)\n\n\t\tif gapped {\n\t\t\tlog.Warn(\"Dropping dangling blob transactions\", \"from\", addr, \"missing\", next, \"drop\", nonces, \"ids\", ids)\n\t\t\tdropDanglingMeter.Mark(int64(len(ids)))\n\t\t} else {\n\t\t\tlog.Trace(\"Dropping filled blob transactions\", \"from\", addr, \"filled\", nonces, \"ids\", ids)\n\t\t\tdropFilledMeter.Mark(int64(len(ids)))\n\t\t}\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 596,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[0].storageSize)\n\t\t\tp.lookup.untrack(txs[0])\n\n\t\t\t// Included transactions blobs need to be moved to the limbo\n\t\t\tif inclusions != nil {\n\t\t\t\tp.offload(addr, txs[0].nonce, txs[0].id, inclusions)\n\t\t\t}\n\t\t\ttxs = txs[1:]\n\t\t}\n\t\tlog.Trace(\"Dropping overlapped blob transactions\", \"from\", addr, \"overlapped\", nonces, \"ids\", ids, \"left\", len(txs))\n\t\tdropOverlappedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 652,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[i].storageSize)\n\t\t\tp.lookup.untrack(txs[i])\n\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 674,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(txs[j].storageSize)\n\t\t\tp.lookup.untrack(txs[j])\n\t\t}\n\t\ttxs = txs[:i]\n\n\t\tlog.Error(\"Dropping gapped blob transactions\", \"from\", addr, \"missing\", txs[i-1].nonce+1, \"drop\", nonces, \"ids\", ids)\n\t\tdropGappedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 712,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(last.storageSize)\n\t\t\tp.lookup.untrack(last)\n\t\t}\n\t\tif len(txs) == 0 {\n\t\t\tdelete(p.index, addr)\n\t\t\tdelete(p.spent, addr)\n\t\t\tif inclusions != nil { // only during reorgs will the heap be initialized\n\t\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t\t}\n\t\t\tp.reserver.Release(addr)\n\t\t} else {\n\t\t\tp.index[addr] = txs\n\t\t}\n\t\tlog.Warn(\"Dropping overdrafted blob transactions\", \"from\", addr, \"balance\", balance, \"spent\", spent, \"drop\", nonces, \"ids\", ids)\n\t\tdropOverdraftedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 752,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(last.storageSize)\n\t\t\tp.lookup.untrack(last)\n\t\t}\n\t\tp.index[addr] = txs\n\n\t\tlog.Warn(\"Dropping overcapped blob transactions\", \"from\", addr, \"kept\", len(txs), \"drop\", nonces, \"ids\", ids)\n\t\tdropOvercappedMeter.Mark(int64(len(ids)))\n\n\t\tfor _, id := range ids {\n\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1057,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(tx.storageSize)\n\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\ttxs[i] = nil\n\n\t\t\t\t\t// Drop everything afterwards, no gaps allowed\n\t\t\t\t\tfor j, tx := range txs[i+1:] {\n\t\t\t\t\t\tids = append(ids, tx.id)\n\t\t\t\t\t\tnonces = append(nonces, tx.nonce)\n\n\t\t\t\t\t\tp.spent[addr] = new(uint256.Int).Sub(p.spent[addr], tx.costCap)\n\t\t\t\t\t\tp.stored -= uint64(tx.storageSize)\n\t\t\t\t\t\tp.lookup.untrack(tx)\n\t\t\t\t\t\ttxs[i+1+j] = nil\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the dropped transactions from the index\n\t\t\t\t\tif i > 0 {\n\t\t\t\t\t\tp.index[addr] = txs[:i]\n\t\t\t\t\t\theap.Fix(p.evict, p.evict.index[addr])\n\t\t\t\t\t} else {\n\t\t\t\t\t\tdelete(p.index, addr)\n\t\t\t\t\t\tdelete(p.spent, addr)\n\n\t\t\t\t\t\theap.Remove(p.evict, p.evict.index[addr])\n\t\t\t\t\t\tp.reserver.Release(addr)\n\t\t\t\t\t}\n\t\t\t\t\t// Clear out the transactions from the data store\n\t\t\t\t\tlog.Warn(\"Dropping underpriced blob transaction\", \"from\", addr, \"rejected\", tx.nonce, \"tip\", tx.execTipCap, \"want\", tip, \"drop\", nonces, \"ids\", ids)\n\t\t\t\t\tdropUnderpricedMeter.Mark(int64(len(ids)))\n\n\t\t\t\t\tfor _, id := range ids {\n\t\t\t\t\t\tif err := p.store.Delete(id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1583,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(drop.storageSize)\n\tp.lookup.untrack(drop)\n\n\t// Remove the transaction from the pool's eviction heap:\n\t//   - If the entire account was dropped, pop off the address\n\t//   - Otherwise, if the new tail has better eviction caps, fix the heap\n\tif last {\n\t\theap.Pop(p.evict)\n\t} else {\n\t\ttail := txs[len(txs)-1] // new tail, surely exists\n\n\t\tevictionExecFeeDiff := tail.evictionExecFeeJumps - drop.evictionExecFeeJumps\n\t\tevictionBlobFeeDiff := tail.evictionBlobFeeJumps - drop.evictionBlobFeeJumps\n\n\t\tif evictionExecFeeDiff > 0.001 || evictionBlobFeeDiff > 0.001 { // no need for math.Abs, monotonic decreasing\n\t\t\theap.Fix(p.evict, 0)\n\t\t}\n\t}\n\t// Remove the transaction from the data store\n\tlog.Debug(\"Evicting overflown blob transaction\", \"from\", from, \"evicted\", drop.nonce, \"id\", drop.id)\n\tdropOverflownMeter.Mark(1)\n\n\tif err := p.store.Delete(drop.id)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 838,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1383,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0016",
          "file": "bsc/core/txpool/blobpool/blobpool.go",
          "line": 1384,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/blobpool/blobpool_test.go",
          "line": 390,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(tx.storageSize)\n\t\t}\n\t}\n\tif pool.stored != stored {\n\t\tt.Errorf(\"pool storage mismatch: have %d, want %d\", pool.stored, stored)\n\t}\n\t// Verify the price heap internals\n\tverifyHeapInternals(t, pool.evict)\n\n\t// Verify that all the blobs can be retrieved\n\tverifyBlobRetrievals(t, pool)\n}\n\n// verifyBlobRetrievals attempts to retrieve all testing blobs and checks that\n// whatever is in the pool, it can be retrieved correctly.\nfunc verifyBlobRetrievals(t *testing.T, pool *BlobPool) {\n\t// Collect all the blobs tracked by the pool\n\tknown := make(map[common.Hash]struct{})\n\tfor _, txs := range pool.index {\n\t\tfor _, tx := range txs {\n\t\t\tfor _, vhash := range tx.vhashes {\n\t\t\t\tknown[vhash] = struct{}{}\n\t\t\t}\n\t\t}\n\t}\n\t// Attempt to retrieve all test blobs\n\thashes := make([]common.Hash, len(testBlobVHashes))\n\tfor i := range testBlobVHashes {\n\t\tcopy(hashes[i][:], testBlobVHashes[i][:])\n\t}\n\tsidecars := pool.GetBlobs(hashes)\n\tvar blobs []*kzg4844.Blob\n\tvar proofs []*kzg4844.Proof\n\tfor idx, sidecar := range sidecars {\n\t\tif sidecar == nil {\n\t\t\tblobs = append(blobs, nil)\n\t\t\tproofs = append(proofs, nil)\n\t\t\tcontinue\n\t\t}\n\t\tblobHashes := sidecar.BlobHashes()\n\t\tfor i, hash := range blobHashes {\n\t\t\tif hash == hashes[idx] {\n\t\t\t\tblobs = append(blobs, &sidecar.Blobs[i])\n\t\t\t\tproofs = append(proofs, &sidecar.Proofs[i])\n\t\t\t}\n\t\t}\n\t}\n\t// Cross validate what we received vs what we wanted\n\tif len(blobs) != len(hashes) || len(proofs) != len(hashes) {\n\t\tt.Errorf(\"retrieved blobs/proofs size mismatch: have %d/%d, want %d\", len(blobs), len(proofs), len(hashes))\n\t\treturn\n\t}\n\tfor i, hash := range hashes {\n\t\t// If an item is missing, but shouldn't, error\n\t\tif blobs[i] == nil || proofs[i] == nil {\n\t\t\tif _, ok := known[hash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/legacypool/list.go",
          "line": 663,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= numSlots(tx)\n\t\t}\n\t}\n\t// If we still can't make enough room for the new transaction\n\tif slots > 0 {\n\t\tfor _, tx := range drop {\n\t\t\theap.Push(&l.urgent, tx)\n\t\t}\n\t\treturn nil, false\n\t}\n\treturn drop, true\n}\n\n// Reheap forcibly rebuilds the heap based on the current remote transaction set.\nfunc (l *pricedList) Reheap() {\n\tl.reheapMu.Lock()\n\tdefer l.reheapMu.Unlock()\n\tstart := time.Now()\n\tl.stales.Store(0)\n\tl.urgent.list = make([]*types.Transaction, 0, l.all.Count())\n\tl.all.Range(func(hash common.Hash, tx *types.Transaction) bool {\n\t\tl.urgent.list = append(l.urgent.list, tx)\n\t\treturn true\n\t})\n\theap.Init(&l.urgent)\n\n\t// balance out the two heaps by moving the worse half of transactions into the\n\t// floating heap\n\t// Note: Discard would also do this before the first eviction but Reheap can do\n\t// is more efficiently. Also, Underpriced would work suboptimally the first time\n\t// if the floating queue was empty.\n\tfloatingCount := len(l.urgent.list) * floatingRatio / (urgentRatio + floatingRatio)\n\tl.floating.list = make([]*types.Transaction, floatingCount)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 123,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= txSlots\n\tOverflowPoolGauge.Inc(1)\n\n\treturn true\n}\n\nfunc (tp *TxOverflowPool) Get(hash common.Hash) (*types.Transaction, bool) {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\tif item, ok := tp.index[hash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 112,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(numSlots(oldestItem.tx))\n\t\tOverflowPoolGauge.Dec(1)\n\t}\n\n\t// Add the new transaction\n\titem := &txHeapItem{\n\t\ttx:        tx,\n\t\ttimestamp: time.Now().UnixNano(),\n\t}\n\theap.Push(&tp.txHeap, item)\n\ttp.index[tx.Hash()] = item\n\ttp.totalSize += txSlots\n\tOverflowPoolGauge.Inc(1)\n\n\treturn true\n}\n\nfunc (tp *TxOverflowPool) Get(hash common.Hash) (*types.Transaction, bool) {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\tif item, ok := tp.index[hash]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 144,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(numSlots(item.tx))\n\t\tOverflowPoolGauge.Dec(1)\n\t}\n}\n\nfunc (tp *TxOverflowPool) Flush(n int) []*types.Transaction {\n\ttp.mu.Lock()\n\tdefer tp.mu.Unlock()\n\tif n > tp.txHeap.Len() {\n\t\tn = tp.txHeap.Len()\n\t}\n\ttxs := make([]*types.Transaction, n)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/txpool/legacypool/tx_overflowpool.go",
          "line": 163,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(numSlots(item.tx))\n\t}\n\n\tOverflowPoolGauge.Dec(int64(n))\n\treturn txs\n}\n\nfunc (tp *TxOverflowPool) Len() int {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\treturn tp.txHeap.Len()\n}\n\nfunc (tp *TxOverflowPool) Size() uint64 {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\treturn tp.totalSize\n}\n\nfunc (tp *TxOverflowPool) PrintTxStats() {\n\ttp.mu.RLock()\n\tdefer tp.mu.RUnlock()\n\tfor _, item := range tp.txHeap {\n\t\ttx := item.tx\n\t\tfmt.Printf(\"Hash: %s, Timestamp: %d, GasFeeCap: %s, GasTipCap: %s\\n\",\n\t\t\ttx.Hash().String(), item.timestamp, tx.GasFeeCap().String(), tx.GasTipCap().String())\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1028,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= list.Len()\n\t}\n\tif queued > int(config.GlobalQueue) {\n\t\tt.Fatalf(\"total transactions overflow allowance: %d > %d\", queued, config.GlobalQueue)\n\t}\n}\n\n// Tests that if an account remains idle for a prolonged amount of time, any\n// non-executable transactions queued up are dropped to prevent wasting resources\n// on shuffling them around.\nfunc TestQueueTimeLimiting(t *testing.T) {\n\t// Reduce the eviction interval to a testable amount\n\tdefer func(old time.Duration) { evictionInterval = old }(evictionInterval)\n\tevictionInterval = time.Millisecond * 100\n\n\t// Create the pool to test the non-expiration enforcement\n\tstatedb, _ := state.New(types.EmptyRootHash, state.NewDatabaseForTesting())\n\tblockchain := newTestBlockChain(params.TestChainConfig, 1000000, statedb, new(event.Feed))\n\n\tconfig := testTxPoolConfig\n\tconfig.Lifetime = time.Second\n\n\tpool := New(config, blockchain)\n\tpool.Init(config.PriceLimit, blockchain.CurrentBlock(), newReserver())\n\tdefer pool.Close()\n\n\t// Create a test account to ensure remotes expire\n\tremote, _ := crypto.GenerateKey()\n\n\ttestAddBalance(pool, crypto.PubkeyToAddress(remote.PublicKey), big.NewInt(1000000000))\n\n\t// Add the transaction and ensure it is queued up\n\tif err := pool.addRemote(pricedTransaction(1, 100000, big.NewInt(1), remote))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= list.Len()\n\t}\n\tif pending > int(config.GlobalSlots) {\n\t\tt.Fatalf(\"total pending transactions overflow allowance: %d > %d\", pending, config.GlobalSlots)\n\t}\n\tif err := validatePoolInternals(pool)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1926,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 100 {\n\t\tpool.priced.SetBaseFee(big.NewInt(int64(baseFee)))\n\t\tadd(true)\n\t\tcheck(highCap, \"fee cap\")\n\t\tadd(false)\n\t\tcheck(highTip, \"effective tip\")\n\t}\n\n\tif err := validatePoolInternals(pool)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 1961,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tfirsts = append(firsts, txs[i])\n\t}\n\terrs := pool.addRemotesSync(firsts)\n\tif len(errs) != len(firsts) {\n\t\tt.Fatalf(\"first add mismatching result count: have %d, want %d\", len(errs), len(firsts))\n\t}\n\tfor i, err := range errs {\n\t\tif err != nil {\n\t\t\tt.Errorf(\"add %d failed: %v\", i, err)\n\t\t}\n\t}\n\tpending, queued := pool.Stats()\n\tif pending != 1 {\n\t\tt.Fatalf(\"pending transactions mismatched: have %d, want %d\", pending, 1)\n\t}\n\tif queued != len(txs)/2-1 {\n\t\tt.Fatalf(\"queued transactions mismatched: have %d, want %d\", queued, len(txs)/2-1)\n\t}\n\t// Try to add all of them now and ensure previous ones error out as knowns\n\terrs = pool.addRemotesSync(txs)\n\tif len(errs) != len(txs) {\n\t\tt.Fatalf(\"all add mismatching result count: have %d, want %d\", len(errs), len(txs))\n\t}\n\tfor i, err := range errs {\n\t\tif i%2 == 0 && err == nil {\n\t\t\tt.Errorf(\"add %d succeeded, should have failed as known\", i)\n\t\t}\n\t\tif i%2 == 1 && err != nil {\n\t\t\tt.Errorf(\"add %d failed: %v\", i, err)\n\t\t}\n\t}\n\tpending, queued = pool.Stats()\n\tif pending != len(txs) {\n\t\tt.Fatalf(\"pending transactions mismatched: have %d, want %d\", pending, len(txs))\n\t}\n\tif queued != 0 {\n\t\tt.Fatalf(\"queued transactions mismatched: have %d, want %d\", queued, 0)\n\t}\n\tif err := validatePoolInternals(pool)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/txpool/legacypool/legacypool_test.go",
          "line": 2628,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 3 {\n\t\t\t\t\tif err := pool.addRemoteSync(pricedSetCodeTx(0, 250000, uint256.NewInt(10), uint256.NewInt(3), keys[i], []unsignedAuth{{0, keys[i]}, {0, keys[i+1]}, {0, keys[i+2]}}))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 522,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= list.Len()\n\t}\n\tqueued := 0\n\tfor _, list := range pool.queue {\n\t\tqueued += list.Len()\n\t}\n\treturn pending, queued\n}\n\n// Content retrieves the data content of the transaction pool, returning all the\n// pending as well as queued transactions, grouped by account and sorted by nonce.\nfunc (pool *LegacyPool) Content() (map[common.Address][]*types.Transaction, map[common.Address][]*types.Transaction) {\n\tpool.mu.Lock()\n\tdefer pool.mu.Unlock()\n\n\tpending := make(map[common.Address][]*types.Transaction, len(pool.pending))\n\tfor addr, list := range pool.pending {\n\t\tpending[addr] = list.Flatten()\n\t}\n\tqueued := make(map[common.Address][]*types.Transaction, len(pool.queue))\n\tfor addr, list := range pool.queue {\n\t\tqueued[addr] = list.Flatten()\n\t}\n\treturn pending, queued\n}\n\n// ContentFrom retrieves the data content of the transaction pool, returning the\n// pending as well as queued transactions of this address, grouped by nonce.\nfunc (pool *LegacyPool) ContentFrom(addr common.Address) ([]*types.Transaction, []*types.Transaction) {\n\tpool.mu.RLock()\n\tdefer pool.mu.RUnlock()\n\n\tvar pending []*types.Transaction\n\tif list, ok := pool.pending[addr]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 733,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= pending.Len()\n\t\t\t}\n\t\t\tif queue := pool.queue[auth]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 736,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= queue.Len()\n\t\t\t}\n\t\t\tif count > 1 {\n\t\t\t\treturn ErrAuthorityReserved\n\t\t\t}\n\t\t\t// Because there is no exclusive lock held between different subpools\n\t\t\t// when processing transactions, the SetCode transaction may be accepted\n\t\t\t// while other transactions with the same sender address are also\n\t\t\t// accepted simultaneously in the other pools.\n\t\t\t//\n\t\t\t// This scenario is considered acceptable, as the rule primarily ensures\n\t\t\t// that attackers cannot easily stack a SetCode transaction when the sender\n\t\t\t// is reserved by other pools.\n\t\t\tif pool.reserver.Has(auth) {\n\t\t\t\treturn ErrAuthorityReserved\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\n// add validates a transaction and inserts it into the non-executable queue for later\n// pending promotion and execution. If the transaction is a replacement for an already\n// pending or queued one, it overwrites the previous transaction if its price is higher.\nfunc (pool *LegacyPool) add(tx *types.Transaction) (replaced bool, err error) {\n\t// If the transaction is already known, discard it\n\thash := tx.Hash()\n\tif pool.all.Get(hash) != nil {\n\t\tlog.Trace(\"Discarding already known transaction\", \"hash\", hash)\n\t\tknownTxMeter.Mark(1)\n\t\treturn false, txpool.ErrAlreadyKnown\n\t}\n\n\t// If the transaction fails basic validation, discard it\n\tif err := pool.validateTx(tx)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 859,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= dropped\n\t\t}\n\t}\n\n\t// Try to replace an existing transaction in the pending pool\n\tif list := pool.pending[from]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1598,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= length\n\t\tif length > pool.config.AccountSlots {\n\t\t\tspammers.Push(addr, length)\n\t\t}\n\t}\n\tif pending <= pool.config.GlobalSlots {\n\t\treturn\n\t}\n\tpendingBeforeCap := pending\n\n\t// Gradually drop transactions from offenders\n\toffenders := []common.Address{}\n\tfor pending > pool.config.GlobalSlots && !spammers.Empty() {\n\t\t// Retrieve the next offender\n\t\toffender, _ := spammers.Pop()\n\t\toffenders = append(offenders, offender)\n\n\t\t// Equalize balances until all the same or below threshold\n\t\tif len(offenders) > 1 {\n\t\t\t// Calculate the equalization threshold for all current offenders\n\t\t\tthreshold := pool.pending[offender].Len()\n\n\t\t\t// Iteratively reduce all offenders until below limit or threshold reached\n\t\t\tfor pending > pool.config.GlobalSlots && pool.pending[offenders[len(offenders)-2]].Len() > threshold {\n\t\t\t\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1673,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(list.Len())\n\t}\n\tif queued <= pool.config.GlobalQueue {\n\t\treturn\n\t}\n\n\t// Sort all accounts with queued transactions by heartbeat\n\taddresses := make(addressesByHeartbeat, 0, len(pool.queue))\n\tfor addr := range pool.queue {\n\t\taddresses = append(addresses, addressByHeartbeat{addr, pool.beats[addr]})\n\t}\n\tsort.Sort(sort.Reverse(addresses))\n\n\t// Drop transactions until the total is below the limit\n\tfor drop := queued - pool.config.GlobalQueue",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1909,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= numSlots(tx)\n\tslotsGauge.Update(int64(t.slots))\n\n\tt.txs[tx.Hash()] = tx\n\tt.addAuthorities(tx)\n}\n\n// Remove removes a transaction from the lookup.\nfunc (t *lookup) Remove(hash common.Hash) {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\ttx, ok := t.txs[hash]\n\tif !ok {\n\t\tlog.Error(\"No transaction found to be deleted\", \"hash\", hash)\n\t\treturn\n\t}\n\tt.removeAuthorities(tx)\n\tt.slots -= numSlots(tx)\n\tslotsGauge.Update(int64(t.slots))\n\n\tdelete(t.txs, hash)\n}\n\n// Clear resets the lookup structure, removing all stored entries.\nfunc (t *lookup) Clear() {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tt.slots = 0\n\tt.txs = make(map[common.Hash]*types.Transaction)\n\tt.auths = make(map[common.Address][]common.Hash)\n}\n\n// TxsBelowTip finds all remote transactions below the given tip threshold.\nfunc (t *lookup) TxsBelowTip(threshold *big.Int) types.Transactions {\n\tfound := make(types.Transactions, 0, 128)\n\tt.Range(func(hash common.Hash, tx *types.Transaction) bool {\n\t\tif tx.GasTipCapIntCmp(threshold) < 0 {\n\t\t\tfound = append(found, tx)\n\t\t}\n\t\treturn true\n\t})\n\treturn found\n}\n\n// addAuthorities tracks the supplied tx in relation to each authority it\n// specifies.\nfunc (t *lookup) addAuthorities(tx *types.Transaction) {\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist, ok := t.auths[addr]\n\t\tif !ok {\n\t\t\tlist = []common.Hash{}\n\t\t}\n\t\tif slices.Contains(list, tx.Hash()) {\n\t\t\t// Don't add duplicates.\n\t\t\tcontinue\n\t\t}\n\t\tlist = append(list, tx.Hash())\n\t\tt.auths[addr] = list\n\t}\n}\n\n// removeAuthorities stops tracking the supplied tx in relation to its\n// authorities.\nfunc (t *lookup) removeAuthorities(tx *types.Transaction) {\n\thash := tx.Hash()\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist := t.auths[addr]\n\t\t// Remove tx from tracker.\n\t\tif i := slices.Index(list, hash)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1698,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= size\n\t\t\tqueuedRateLimitMeter.Mark(int64(size))\n\t\t\tcontinue\n\t\t}\n\t\t// Otherwise drop only last few transactions\n\t\ttxs := list.Flatten()\n\t\tfor i := len(txs) - 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1927,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= numSlots(tx)\n\tslotsGauge.Update(int64(t.slots))\n\n\tdelete(t.txs, hash)\n}\n\n// Clear resets the lookup structure, removing all stored entries.\nfunc (t *lookup) Clear() {\n\tt.lock.Lock()\n\tdefer t.lock.Unlock()\n\n\tt.slots = 0\n\tt.txs = make(map[common.Hash]*types.Transaction)\n\tt.auths = make(map[common.Address][]common.Hash)\n}\n\n// TxsBelowTip finds all remote transactions below the given tip threshold.\nfunc (t *lookup) TxsBelowTip(threshold *big.Int) types.Transactions {\n\tfound := make(types.Transactions, 0, 128)\n\tt.Range(func(hash common.Hash, tx *types.Transaction) bool {\n\t\tif tx.GasTipCapIntCmp(threshold) < 0 {\n\t\t\tfound = append(found, tx)\n\t\t}\n\t\treturn true\n\t})\n\treturn found\n}\n\n// addAuthorities tracks the supplied tx in relation to each authority it\n// specifies.\nfunc (t *lookup) addAuthorities(tx *types.Transaction) {\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist, ok := t.auths[addr]\n\t\tif !ok {\n\t\t\tlist = []common.Hash{}\n\t\t}\n\t\tif slices.Contains(list, tx.Hash()) {\n\t\t\t// Don't add duplicates.\n\t\t\tcontinue\n\t\t}\n\t\tlist = append(list, tx.Hash())\n\t\tt.auths[addr] = list\n\t}\n}\n\n// removeAuthorities stops tracking the supplied tx in relation to its\n// authorities.\nfunc (t *lookup) removeAuthorities(tx *types.Transaction) {\n\thash := tx.Hash()\n\tfor _, addr := range tx.SetCodeAuthorities() {\n\t\tlist := t.auths[addr]\n\t\t// Remove tx from tracker.\n\t\tif i := slices.Index(list, hash)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 426,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/core/txpool/legacypool/legacypool.go",
          "line": 1420,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".Send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/locals/journal.go",
          "line": 154,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(txs)\n\t}\n\treplacement.Close()\n\n\t// Replace the live journal with the newly generated one\n\tif err = os.Rename(journal.path+\".new\", journal.path)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/txpool/locals/tx_tracker.go",
          "line": 131,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(stales)\n\n\t\t// Check the non-stale\n\t\tfor _, tx := range txs.Flatten() {\n\t\t\tif tracker.pool.Has(tx.Hash()) {\n\t\t\t\tnumOk++\n\t\t\t\tcontinue\n\t\t\t}\n\t\t\tresubmits = append(resubmits, tx)\n\t\t}\n\t}\n\n\tif journalCheck { // rejournal\n\t\trejournal = make(map[common.Address]types.Transactions)\n\t\tfor _, tx := range tracker.all {\n\t\t\taddr, _ := types.Sender(tracker.signer, tx)\n\t\t\trejournal[addr] = append(rejournal[addr], tx)\n\t\t}\n\t\t// Sort them\n\t\tfor _, list := range rejournal {\n\t\t\t// cmp(a, b) should return a negative number when a < b,\n\t\t\tslices.SortFunc(list, func(a, b *types.Transaction) int {\n\t\t\t\treturn int(a.Nonce() - b.Nonce())\n\t\t\t})\n\t\t}\n\t}\n\tlocalGauge.Update(int64(len(tracker.all)))\n\tlog.Debug(\"Tx tracker status\", \"need-resubmit\", len(resubmits), \"stale\", numStales, \"ok\", numOk)\n\treturn resubmits, rejournal\n}\n\n// Start implements node.Lifecycle interface\n// Start is called after all services have been constructed and the networking\n// layer was also initialized to spawn any goroutines required by the service.\nfunc (tracker *TxTracker) Start() error {\n\ttracker.wg.Add(1)\n\tgo tracker.loop()\n\treturn nil\n}\n\n// Stop implements node.Lifecycle interface\n// Stop terminates all goroutines belonging to the service, blocking until they\n// are all terminated.\nfunc (tracker *TxTracker) Stop() error {\n\tclose(tracker.shutdownCh)\n\ttracker.wg.Wait()\n\treturn nil\n}\n\nfunc (tracker *TxTracker) loop() {\n\tdefer tracker.wg.Done()\n\n\tif tracker.journal != nil {\n\t\ttracker.journal.load(func(transactions []*types.Transaction) []error {\n\t\t\ttracker.TrackAll(transactions)\n\t\t\treturn nil\n\t\t})\n\t\tdefer tracker.journal.close()\n\t}\n\tvar (\n\t\tlastJournal = time.Now()\n\t\ttimer       = time.NewTimer(10 * time.Second) // Do initial check after 10 seconds, do rechecks more seldom.\n\t)\n\tfor {\n\t\tselect {\n\t\tcase <-tracker.shutdownCh:\n\t\t\treturn\n\t\tcase <-timer.C:\n\t\t\tcheckJournal := tracker.journal != nil && time.Since(lastJournal) > tracker.rejournal\n\t\t\tresubmits, rejournal := tracker.recheck(checkJournal)\n\t\t\tif len(resubmits) > 0 {\n\t\t\t\ttracker.pool.Add(resubmits, false)\n\t\t\t}\n\t\t\tif checkJournal {\n\t\t\t\t// Lock to prevent journal.rotate <-> journal.insert (via TrackAll) conflicts\n\t\t\t\ttracker.mu.Lock()\n\t\t\t\tlastJournal = time.Now()\n\t\t\t\tif err := tracker.journal.rotate(rejournal)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 15,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 16,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 29,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/systemcontracts/luban/types.go",
          "line": 30,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 11,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 12,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 21,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/systemcontracts/planck/types.go",
          "line": 22,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 35,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 36,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 90,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/systemcontracts/pascal/types.go",
          "line": 91,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/niels/types.go",
          "line": 25,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/niels/types.go",
          "line": 26,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 11,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 12,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 21,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/systemcontracts/moran/types.go",
          "line": 22,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 15,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 16,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 43,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/core/systemcontracts/feynman/types.go",
          "line": 44,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/systemcontracts/ramanujan/types.go",
          "line": 25,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/core/systemcontracts/ramanujan/types.go",
          "line": 26,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/core/rawdb/ancienttest/testsuite.go",
          "line": 300,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1 {\n\t\tvals = append(vals, testrand.Bytes(value))\n\t}\n\treturn vals\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/blake2b/blake2x.go",
          "line": 132,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(p, x.block[x.offset:])\n\t\t\tx.remaining -= uint64(n)\n\t\t\treturn\n\t\t}\n\t\tcopy(p, x.block[x.offset:])\n\t\tp = p[blockRemaining:]\n\t\tx.offset = 0\n\t\tx.remaining -= uint64(blockRemaining)\n\t}\n\n\tfor len(p) >= Size {\n\t\tbinary.LittleEndian.PutUint32(x.cfg[8:], x.nodeOffset)\n\t\tx.nodeOffset++\n\n\t\tx.d.initConfig(&x.cfg)\n\t\tx.d.Write(x.root[:])\n\t\tx.d.finalize(&x.block)\n\n\t\tcopy(p, x.block[:])\n\t\tp = p[Size:]\n\t\tx.remaining -= uint64(Size)\n\t}\n\n\tif todo := len(p)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/crypto/blake2b/blake2x.go",
          "line": 133,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(n)\n\t\t\treturn\n\t\t}\n\t\tcopy(p, x.block[x.offset:])\n\t\tp = p[blockRemaining:]\n\t\tx.offset = 0\n\t\tx.remaining -= uint64(blockRemaining)\n\t}\n\n\tfor len(p) >= Size {\n\t\tbinary.LittleEndian.PutUint32(x.cfg[8:], x.nodeOffset)\n\t\tx.nodeOffset++\n\n\t\tx.d.initConfig(&x.cfg)\n\t\tx.d.Write(x.root[:])\n\t\tx.d.finalize(&x.block)\n\n\t\tcopy(p, x.block[:])\n\t\tp = p[Size:]\n\t\tx.remaining -= uint64(Size)\n\t}\n\n\tif todo := len(p)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/crypto/blake2b/blake2x.go",
          "line": 167,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= uint64(todo)\n\t}\n\treturn\n}\n\nfunc (d *digest) initConfig(cfg *[Size]byte) {\n\td.offset, d.c[0], d.c[1] = 0, 0, 0\n\tfor i := range d.h {\n\t\td.h[i] = iv[i] ^ binary.LittleEndian.Uint64(cfg[i*8:])\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/blake2b/blake2b_generic.go",
          "line": 34,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= BlockSize\n\t\tif c0 < BlockSize {\n\t\t\tc1++\n\t\t}\n\t\tfor j := range m {\n\t\t\tm[j] = binary.LittleEndian.Uint64(blocks[i:])\n\t\t\ti += 8\n\t\t}\n\t\tfGeneric(h, &m, c0, c1, flag, 12)\n\t}\n\tc[0], c[1] = c0, c1\n}\n\nfunc fGeneric(h *[8]uint64, m *[16]uint64, c0, c1 uint64, flag uint64, rounds uint64) {\n\tv0, v1, v2, v3, v4, v5, v6, v7 := h[0], h[1], h[2], h[3], h[4], h[5], h[6], h[7]\n\tv8, v9, v10, v11, v12, v13, v14, v15 := iv[0], iv[1], iv[2], iv[3], iv[4], iv[5], iv[6], iv[7]\n\tv12 ^= c0\n\tv13 ^= c1\n\tv14 ^= flag\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/crypto/blake2b/blake2b_generic.go",
          "line": 57,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= m[s[0]]\n\t\tv0 += v4\n\t\tv12 ^= v0\n\t\tv12 = bits.RotateLeft64(v12, -32)\n\t\tv8 += v12\n\t\tv4 ^= v8\n\t\tv4 = bits.RotateLeft64(v4, -24)\n\t\tv1 += m[s[1]]\n\t\tv1 += v5\n\t\tv13 ^= v1\n\t\tv13 = bits.RotateLeft64(v13, -32)\n\t\tv9 += v13\n\t\tv5 ^= v9\n\t\tv5 = bits.RotateLeft64(v5, -24)\n\t\tv2 += m[s[2]]\n\t\tv2 += v6\n\t\tv14 ^= v2\n\t\tv14 = bits.RotateLeft64(v14, -32)\n\t\tv10 += v14\n\t\tv6 ^= v10\n\t\tv6 = bits.RotateLeft64(v6, -24)\n\t\tv3 += m[s[3]]\n\t\tv3 += v7\n\t\tv15 ^= v3\n\t\tv15 = bits.RotateLeft64(v15, -32)\n\t\tv11 += v15\n\t\tv7 ^= v11\n\t\tv7 = bits.RotateLeft64(v7, -24)\n\n\t\tv0 += m[s[4]]\n\t\tv0 += v4\n\t\tv12 ^= v0\n\t\tv12 = bits.RotateLeft64(v12, -16)\n\t\tv8 += v12\n\t\tv4 ^= v8\n\t\tv4 = bits.RotateLeft64(v4, -63)\n\t\tv1 += m[s[5]]\n\t\tv1 += v5\n\t\tv13 ^= v1\n\t\tv13 = bits.RotateLeft64(v13, -16)\n\t\tv9 += v13\n\t\tv5 ^= v9\n\t\tv5 = bits.RotateLeft64(v5, -63)\n\t\tv2 += m[s[6]]\n\t\tv2 += v6\n\t\tv14 ^= v2\n\t\tv14 = bits.RotateLeft64(v14, -16)\n\t\tv10 += v14\n\t\tv6 ^= v10\n\t\tv6 = bits.RotateLeft64(v6, -63)\n\t\tv3 += m[s[7]]\n\t\tv3 += v7\n\t\tv15 ^= v3\n\t\tv15 = bits.RotateLeft64(v15, -16)\n\t\tv11 += v15\n\t\tv7 ^= v11\n\t\tv7 = bits.RotateLeft64(v7, -63)\n\n\t\tv0 += m[s[8]]\n\t\tv0 += v5\n\t\tv15 ^= v0\n\t\tv15 = bits.RotateLeft64(v15, -32)\n\t\tv10 += v15\n\t\tv5 ^= v10\n\t\tv5 = bits.RotateLeft64(v5, -24)\n\t\tv1 += m[s[9]]\n\t\tv1 += v6\n\t\tv12 ^= v1\n\t\tv12 = bits.RotateLeft64(v12, -32)\n\t\tv11 += v12\n\t\tv6 ^= v11\n\t\tv6 = bits.RotateLeft64(v6, -24)\n\t\tv2 += m[s[10]]\n\t\tv2 += v7\n\t\tv13 ^= v2\n\t\tv13 = bits.RotateLeft64(v13, -32)\n\t\tv8 += v13\n\t\tv7 ^= v8\n\t\tv7 = bits.RotateLeft64(v7, -24)\n\t\tv3 += m[s[11]]\n\t\tv3 += v4\n\t\tv14 ^= v3\n\t\tv14 = bits.RotateLeft64(v14, -32)\n\t\tv9 += v14\n\t\tv4 ^= v9\n\t\tv4 = bits.RotateLeft64(v4, -24)\n\n\t\tv0 += m[s[12]]\n\t\tv0 += v5\n\t\tv15 ^= v0\n\t\tv15 = bits.RotateLeft64(v15, -16)\n\t\tv10 += v15\n\t\tv5 ^= v10\n\t\tv5 = bits.RotateLeft64(v5, -63)\n\t\tv1 += m[s[13]]\n\t\tv1 += v6\n\t\tv12 ^= v1\n\t\tv12 = bits.RotateLeft64(v12, -16)\n\t\tv11 += v12\n\t\tv6 ^= v11\n\t\tv6 = bits.RotateLeft64(v6, -63)\n\t\tv2 += m[s[14]]\n\t\tv2 += v7\n\t\tv13 ^= v2\n\t\tv13 = bits.RotateLeft64(v13, -16)\n\t\tv8 += v13\n\t\tv7 ^= v8\n\t\tv7 = bits.RotateLeft64(v7, -63)\n\t\tv3 += m[s[15]]\n\t\tv3 += v4\n\t\tv14 ^= v3\n\t\tv14 = bits.RotateLeft64(v14, -16)\n\t\tv9 += v14\n\t\tv4 ^= v9\n\t\tv4 = bits.RotateLeft64(v4, -63)\n\t}\n\th[0] ^= v0 ^ v8\n\th[1] ^= v1 ^ v9\n\th[2] ^= v2 ^ v10\n\th[3] ^= v3 ^ v11\n\th[4] ^= v4 ^ v12\n\th[5] ^= v5 ^ v13\n\th[6] ^= v6 ^ v14\n\th[7] ^= v7 ^ v15\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 161,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= BlockSize\n\t\tif c0 < BlockSize {\n\t\t\tc1++\n\t\t}\n\t\tfor j := range m {\n\t\t\tm[j] = binary.LittleEndian.Uint64(blocks[i:])\n\t\t\ti += 8\n\t\t}\n\t\tf(h, &m, c0, c1, flag, 12)\n\t}\n\tc[0], c[1] = c0, c1\n}\n\ntype digest struct {\n\th      [8]uint64\n\tc      [2]uint64\n\tsize   int\n\tblock  [BlockSize]byte\n\toffset int\n\n\tkey    [BlockSize]byte\n\tkeyLen int\n}\n\nconst (\n\tmagic         = \"b2b\"\n\tmarshaledSize = len(magic) + 8*8 + 2*8 + 1 + BlockSize + 1\n)\n\nfunc (d *digest) MarshalBinary() ([]byte, error) {\n\tif d.keyLen != 0 {\n\t\treturn nil, errors.New(\"crypto/blake2b: cannot marshal MACs\")\n\t}\n\tb := make([]byte, 0, marshaledSize)\n\tb = append(b, magic...)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 249,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(d.block[d.offset:], p)\n\t\t\treturn\n\t\t}\n\t\tcopy(d.block[d.offset:], p[:remaining])\n\t\thashBlocks(&d.h, &d.c, 0, d.block[:])\n\t\td.offset = 0\n\t\tp = p[remaining:]\n\t}\n\n\tif length := len(p)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 268,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= copy(d.block[:], p)\n\t}\n\n\treturn\n}\n\nfunc (d *digest) Sum(sum []byte) []byte {\n\tvar hash [Size]byte\n\td.finalize(&hash)\n\treturn append(sum, hash[:d.size]...)\n}\n\nfunc (d *digest) finalize(hash *[Size]byte) {\n\tvar block [BlockSize]byte\n\tcopy(block[:], d.block[:d.offset])\n\tremaining := uint64(BlockSize - d.offset)\n\n\tc := d.c\n\tif c[0] < remaining {\n\t\tc[1]--\n\t}\n\tc[0] -= remaining\n\n\th := d.h\n\thashBlocks(&h, &c, 0xFFFFFFFFFFFFFFFF, block[:])\n\n\tfor i, v := range h {\n\t\tbinary.LittleEndian.PutUint64(hash[8*i:], v)\n\t}\n}\n\nfunc appendUint64(b []byte, x uint64) []byte {\n\tvar a [8]byte\n\tbinary.BigEndian.PutUint64(a[:], x)\n\treturn append(b, a[:]...)\n}\n\n//nolint:unused,deadcode\nfunc appendUint32(b []byte, x uint32) []byte {\n\tvar a [4]byte\n\tbinary.BigEndian.PutUint32(a[:], x)\n\treturn append(b, a[:]...)\n}\n\nfunc consumeUint64(b []byte) ([]byte, uint64) {\n\tx := binary.BigEndian.Uint64(b)\n\treturn b[8:], x\n}\n\n//nolint:unused,deadcode\nfunc consumeUint32(b []byte) ([]byte, uint32) {\n\tx := binary.BigEndian.Uint32(b)\n\treturn b[4:], x\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 135,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= BlockSize\n\t\t}\n\t\thashBlocks(&h, &c, 0, data[:n])\n\t\tdata = data[n:]\n\t}\n\n\tvar block [BlockSize]byte\n\toffset := copy(block[:], data)\n\tremaining := uint64(BlockSize - offset)\n\tif c[0] < remaining {\n\t\tc[1]--\n\t}\n\tc[0] -= remaining\n\n\thashBlocks(&h, &c, 0xFFFFFFFFFFFFFFFF, block[:])\n\n\tfor i, v := range h[:(hashSize+7)/8] {\n\t\tbinary.LittleEndian.PutUint64(sum[8*i:], v)\n\t}\n}\n\nfunc hashBlocks(h *[8]uint64, c *[2]uint64, flag uint64, blocks []byte) {\n\tvar m [16]uint64\n\tc0, c1 := c[0], c[1]\n\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/crypto/blake2b/blake2b.go",
          "line": 261,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= BlockSize\n\t\t}\n\t\thashBlocks(&d.h, &d.c, 0, p[:nn])\n\t\tp = p[nn:]\n\t}\n\n\tif len(p) > 0 {\n\t\td.offset += copy(d.block[:], p)\n\t}\n\n\treturn\n}\n\nfunc (d *digest) Sum(sum []byte) []byte {\n\tvar hash [Size]byte\n\td.finalize(&hash)\n\treturn append(sum, hash[:d.size]...)\n}\n\nfunc (d *digest) finalize(hash *[Size]byte) {\n\tvar block [BlockSize]byte\n\tcopy(block[:], d.block[:d.offset])\n\tremaining := uint64(BlockSize - d.offset)\n\n\tc := d.c\n\tif c[0] < remaining {\n\t\tc[1]--\n\t}\n\tc[0] -= remaining\n\n\th := d.h\n\thashBlocks(&h, &c, 0xFFFFFFFFFFFFFFFF, block[:])\n\n\tfor i, v := range h {\n\t\tbinary.LittleEndian.PutUint64(hash[8*i:], v)\n\t}\n}\n\nfunc appendUint64(b []byte, x uint64) []byte {\n\tvar a [8]byte\n\tbinary.BigEndian.PutUint64(a[:], x)\n\treturn append(b, a[:]...)\n}\n\n//nolint:unused,deadcode\nfunc appendUint32(b []byte, x uint32) []byte {\n\tvar a [4]byte\n\tbinary.BigEndian.PutUint32(a[:], x)\n\treturn append(b, a[:]...)\n}\n\nfunc consumeUint64(b []byte) ([]byte, uint64) {\n\tx := binary.BigEndian.Uint64(b)\n\treturn b[8:], x\n}\n\n//nolint:unused,deadcode\nfunc consumeUint32(b []byte) ([]byte, uint32) {\n\tx := binary.BigEndian.Uint32(b)\n\treturn b[4:], x\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/kzg4844/kzg4844_test.go",
          "line": 41,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gokzg4844.SerializedScalarSize {\n\t\tfieldElementBytes := randFieldElement()\n\t\tcopy(blob[i:i+gokzg4844.SerializedScalarSize], fieldElementBytes[:])\n\t}\n\treturn &blob\n}\n\nfunc TestCKZGWithPoint(t *testing.T)  { testKZGWithPoint(t, true) }\nfunc TestGoKZGWithPoint(t *testing.T) { testKZGWithPoint(t, false) }\nfunc testKZGWithPoint(t *testing.T, ckzg bool) {\n\tif ckzg && !ckzgAvailable {\n\t\tt.Skip(\"CKZG unavailable in this test build\")\n\t}\n\tdefer func(old bool) { useCKZG.Store(old) }(useCKZG.Load())\n\tuseCKZG.Store(ckzg)\n\n\tblob := randBlob()\n\n\tcommitment, err := BlobToCommitment(blob)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create KZG commitment from blob: %v\", err)\n\t}\n\tpoint := randFieldElement()\n\tproof, claim, err := ComputeProof(blob, point)\n\tif err != nil {\n\t\tt.Fatalf(\"failed to create KZG proof at point: %v\", err)\n\t}\n\tif err := VerifyProof(commitment, point, claim, proof)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/bn256/cloudflare/gfp_generic.go",
          "line": 84,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a0 * b0\n\t\t\tbuff[off+1] += a1 * b0\n\t\t\tbuff[off+2] += a2*b0 + a0*b2\n\t\t\tbuff[off+3] += a3*b0 + a1*b2\n\t\t\tbuff[off+4] += a2 * b2\n\t\t\tbuff[off+5] += a3 * b2\n\t\t}\n\t}\n\n\tfor i := uint(1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/crypto/bn256/cloudflare/gfp_generic.go",
          "line": 130,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= a0 * b0\n\t\t\tbuff[off+1] += a1 * b0\n\t\t\tbuff[off+2] += a2*b0 + a0*b2\n\t\t\tbuff[off+3] += a3*b0 + a1*b2\n\t\t\tbuff[off+4] += a2 * b2\n\t\t\tbuff[off+5] += a3 * b2\n\t\t}\n\t}\n\n\tfor i := uint(1)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/bn256/cloudflare/lattice.go",
          "line": 100,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint8(x.Bit(i)) << uint(j)\n\t\t}\n\t}\n\n\treturn out\n}\n\n// round sets num to num/denom rounded to the nearest integer.\nfunc round(num, denom *big.Int) {\n\tr := new(big.Int)\n\tnum.DivMod(num, denom, r)\n\n\tif r.Cmp(half) == 1 {\n\t\tnum.Add(num, big.NewInt(1))\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/crypto/bn256/cloudflare/gfp.go",
          "line": 66,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(in[8*w+b]) << (56 - 8*b)\n\t\t}\n\t}\n\t// Ensure the point respects the curve modulus\n\tfor i := 3",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/misc/gaslimit.go",
          "line": 35,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= %d\", headerGasLimit, parentGasLimit, limit-1)\n\t}\n\tif headerGasLimit < params.MinGasLimit {\n\t\treturn fmt.Errorf(\"invalid gas limit below %d\", params.MinGasLimit)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/consensus/misc/gaslimit.go",
          "line": 31,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= -1\n\t}\n\tlimit := parentGasLimit / params.GasLimitBoundDivisor\n\tif uint64(diff) >= limit {\n\t\treturn fmt.Errorf(\"invalid gas limit: have %d, want %d +-= %d\", headerGasLimit, parentGasLimit, limit-1)\n\t}\n\tif headerGasLimit < params.MinGasLimit {\n\t\treturn fmt.Errorf(\"invalid gas limit below %d\", params.MinGasLimit)\n\t}\n\treturn nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/lubanFork.go",
          "line": 37,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 397,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= turnLengthSize\n\t}\n\tif num == 0 || len(header.Extra) < extraMinLen {\n\t\treturn nil\n\t}\n\treturn header.Extra[start:end]\n}\n\n// getVoteAttestationFromHeader returns the vote attestation extracted from the header's extra field if exists.\nfunc getVoteAttestationFromHeader(header *types.Header, chainConfig *params.ChainConfig, epochLength uint64) (*types.VoteAttestation, error) {\n\tif len(header.Extra) <= extraVanity+extraSeal {\n\t\treturn nil, nil\n\t}\n\n\tif !chainConfig.IsLuban(header.Number) {\n\t\treturn nil, nil\n\t}\n\n\tvar attestationBytes []byte\n\tif header.Number.Uint64()%epochLength != 0 {\n\t\tattestationBytes = header.Extra[extraVanity : len(header.Extra)-extraSeal]\n\t} else {\n\t\tnum := int(header.Extra[extraVanity])\n\t\tstart := extraVanity + validatorNumberSize + num*validatorBytesLength\n\t\tif chainConfig.IsBohr(header.Number, header.Time) {\n\t\t\tstart += turnLengthSize\n\t\t}\n\t\tend := len(header.Extra) - extraSeal\n\t\tif end <= start {\n\t\t\treturn nil, nil\n\t\t}\n\t\tattestationBytes = header.Extra[start:end]\n\t}\n\n\tvar attestation types.VoteAttestation\n\tif err := rlp.Decode(bytes.NewReader(attestationBytes), &attestation)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 719,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= %d\", header.GasLimit, parent.GasLimit, limit-1)\n\t}\n\n\t// Verify vote attestation for fast finality.\n\tif err := p.verifyVoteAttestation(chain, header, parents)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1286,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\tvalidVoteCount += 1\n\t\t\t}\n\t\t}\n\t\tquorum := cmath.CeilDiv(len(snap.Validators)*2, 3)\n\t\tif validVoteCount > quorum {\n\t\t\taccumulatedWeights[head.Coinbase] += uint64((validVoteCount - quorum) * collectAdditionalVotesRewardRatio / 100)\n\t\t}\n\t}\n\n\tvalidators := make([]common.Address, 0, len(accumulatedWeights))\n\tweights := make([]*big.Int, 0, len(accumulatedWeights))\n\tfor val := range accumulatedWeights {\n\t\tvalidators = append(validators, val)\n\t}\n\tsort.Sort(validatorsAscending(validators))\n\tfor _, val := range validators {\n\t\tweights = append(weights, big.NewInt(int64(accumulatedWeights[val])))\n\t}\n\n\t// generate system transaction\n\tmethod := \"distributeFinalityReward\"\n\tdata, err := p.validatorSetABI.Pack(method, validators, weights)\n\tif err != nil {\n\t\tlog.Error(\"Unable to pack tx for distributeFinalityReward\", \"error\", err)\n\t\treturn err\n\t}\n\tmsg := p.getSystemMessage(header.Coinbase, common.HexToAddress(systemcontracts.ValidatorContract), data, common.Big0)\n\treturn p.applyTransaction(msg, state, header, cx, txs, receipts, systemTxs, usedGas, mining, tracer)\n}\n\nfunc (p *Parlia) EstimateGasReservedForSystemTxs(chain consensus.ChainHeaderReader, header *types.Header) uint64 {\n\tparent := chain.GetHeaderByHash(header.ParentHash)\n\tif parent != nil {\n\t\t// Mainnet and Chapel have both passed Feynman. Now, simplify the logic before and during the Feynman hard fork.\n\t\tif p.chainConfig.IsFeynman(header.Number, header.Time) &&\n\t\t\t!p.chainConfig.IsOnFeynman(header.Number, parent.Time, header.Time) {\n\t\t\t// const (\n\t\t\t// \tthe following values represent the maximum values found in the most recent blocks on the mainnet\n\t\t\t// \tdepositTxGas         = uint64(60_000)\n\t\t\t// \tslashTxGas           = uint64(140_000)\n\t\t\t// \tfinalityRewardTxGas  = uint64(350_000)\n\t\t\t// \tupdateValidatorTxGas = uint64(12_160_000)\n\t\t\t// )\n\t\t\t// suggestReservedGas := depositTxGas\n\t\t\t// if header.Difficulty.Cmp(diffInTurn) != 0 {\n\t\t\t// \tsnap, err := p.snapshot(chain, header.Number.Uint64()-1, header.ParentHash, nil)\n\t\t\t// \tif err != nil || !snap.SignRecently(snap.inturnValidator()) {\n\t\t\t// \t\tsuggestReservedGas += slashTxGas\n\t\t\t// \t}\n\t\t\t// }\n\t\t\t// if header.Number.Uint64()%p.config.Epoch == 0 {\n\t\t\t// \tsuggestReservedGas += finalityRewardTxGas\n\t\t\t// }\n\t\t\t// if isBreatheBlock(parent.Time, header.Time) {\n\t\t\t// \tsuggestReservedGas += updateValidatorTxGas\n\t\t\t// }\n\t\t\t// return suggestReservedGas * 150 / 100\n\t\t\tif !isBreatheBlock(parent.Time, header.Time) {\n\t\t\t\t// params.SystemTxsGasSoftLimit > (depositTxGas+slashTxGas+finalityRewardTxGas)*150/100\n\t\t\t\treturn params.SystemTxsGasSoftLimit\n\t\t\t}\n\t\t}\n\t}\n\n\t// params.SystemTxsGasHardLimit > (depositTxGas+slashTxGas+finalityRewardTxGas+updateValidatorTxGas)*150/100\n\treturn params.SystemTxsGasHardLimit\n}\n\n// Finalize implements consensus.Engine, ensuring no uncles are set, nor block\n// rewards given.\nfunc (p *Parlia) Finalize(chain consensus.ChainHeaderReader, header *types.Header, state vm.StateDB, txs *[]*types.Transaction,\n\tuncles []*types.Header, _ []*types.Withdrawal, receipts *[]*types.Receipt, systemTxs *[]*types.Transaction, usedGas *uint64, tracer *tracing.Hooks) error {\n\t// warn if not in majority fork\n\tp.detectNewVersionWithFork(chain, header, state)\n\n\t// If the block is an epoch end block, verify the validator list\n\t// The verification can only be done when the state is ready, it can't be done in VerifyHeader.\n\tif err := p.verifyValidators(chain, header)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 2142,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= gasUsed\n\ttracingReceipt = types.NewReceipt(root, false, *usedGas)\n\ttracingReceipt.TxHash = expectedTx.Hash()\n\ttracingReceipt.GasUsed = gasUsed\n\n\t// Set the receipt logs and create a bloom for filtering\n\ttracingReceipt.Logs = state.GetLogs(expectedTx.Hash(), header.Number.Uint64(), header.Hash(), header.Time)\n\ttracingReceipt.Bloom = types.CreateBloom(tracingReceipt)\n\ttracingReceipt.BlockHash = header.Hash()\n\ttracingReceipt.BlockNumber = header.Number\n\ttracingReceipt.TransactionIndex = uint(state.TxIndex())\n\t*receipts = append(*receipts, tracingReceipt)\n\treturn nil\n}\n\n// GetJustifiedNumberAndHash retrieves the number and hash of the highest justified block\n// within the branch including `headers` and utilizing the latest element as the head.\nfunc (p *Parlia) GetJustifiedNumberAndHash(chain consensus.ChainHeaderReader, headers []*types.Header) (uint64, common.Hash, error) {\n\tif chain == nil || len(headers) == 0 || headers[len(headers)-1] == nil {\n\t\treturn 0, common.Hash{}, errors.New(\"illegal chain or header\")\n\t}\n\thead := headers[len(headers)-1]\n\tsnap, err := p.snapshot(chain, head.Number.Uint64(), head.Hash(), headers)\n\tif err != nil {\n\t\tlog.Error(\"Unexpected error when getting snapshot\",\n\t\t\t\"error\", err, \"blockNumber\", head.Number.Uint64(), \"blockHash\", head.Hash())\n\t\treturn 0, common.Hash{}, err\n\t}\n\n\tif snap.Attestation == nil {\n\t\tif p.chainConfig.IsLuban(head.Number) {\n\t\t\tlog.Debug(\"once one attestation generated, attestation of snap would not be nil forever basically\")\n\t\t}\n\t\treturn 0, chain.GetHeaderByNumber(0).Hash(), nil\n\t}\n\treturn snap.Attestation.TargetNumber, snap.Attestation.TargetHash, nil\n}\n\n// GetFinalizedHeader returns highest finalized block header.\nfunc (p *Parlia) GetFinalizedHeader(chain consensus.ChainHeaderReader, header *types.Header) *types.Header {\n\tif chain == nil || header == nil {\n\t\treturn nil\n\t}\n\tif !chain.Config().IsPlato(header.Number) {\n\t\treturn chain.GetHeaderByNumber(0)\n\t}\n\n\tsnap, err := p.snapshot(chain, header.Number.Uint64(), header.Hash(), nil)\n\tif err != nil {\n\t\tlog.Error(\"Unexpected error when getting snapshot\",\n\t\t\t\"error\", err, \"blockNumber\", header.Number.Uint64(), \"blockHash\", header.Hash())\n\t\treturn nil\n\t}\n\n\tif snap.Attestation == nil {\n\t\treturn chain.GetHeaderByNumber(0) // keep consistent with GetJustifiedNumberAndHash\n\t}\n\n\treturn chain.GetHeader(snap.Attestation.SourceHash, snap.Attestation.SourceNumber)\n}\n\n// ===========================     utility function        ==========================\nfunc (p *Parlia) backOffTime(snap *Snapshot, parent, header *types.Header, val common.Address) uint64 {\n\tif snap.inturn(val) {\n\t\tlog.Debug(\"backOffTime\", \"blockNumber\", header.Number, \"in turn validator\", val)\n\t\treturn 0\n\t} else {\n\t\tdelay := defaultInitialBackOffTime\n\t\t// When mining blocks, `header.Time` is temporarily set to time.Now() + 1.\n\t\t// Therefore, using `header.Time` to determine whether a hard fork has occurred is incorrect.\n\t\t// As a result, during the Bohr and Lorentz hard forks, the network may experience some instability,\n\t\t// So use `parent.Time` instead.\n\t\tisParerntLorentz := p.chainConfig.IsLorentz(parent.Number, parent.Time)\n\t\tif isParerntLorentz {\n\t\t\t// If the in-turn validator has not signed recently, the expected backoff times are [2, 3, 4, ...].\n\t\t\tdelay = lorentzInitialBackOffTime\n\t\t}\n\t\tvalidators := snap.validators()\n\t\tif p.chainConfig.IsPlanck(header.Number) {\n\t\t\tcounts := snap.countRecents()\n\t\t\tfor addr, seenTimes := range counts {\n\t\t\t\tlog.Trace(\"backOffTime\", \"blockNumber\", header.Number, \"validator\", addr, \"seenTimes\", seenTimes)\n\t\t\t}\n\n\t\t\t// The backOffTime does not matter when a validator has signed recently.\n\t\t\tif snap.signRecentlyByCounts(val, counts) {\n\t\t\t\treturn 0\n\t\t\t}\n\n\t\t\tinTurnAddr := snap.inturnValidator()\n\t\t\tif snap.signRecentlyByCounts(inTurnAddr, counts) {\n\t\t\t\tlog.Debug(\"in turn validator has recently signed, skip initialBackOffTime\",\n\t\t\t\t\t\"inTurnAddr\", inTurnAddr)\n\t\t\t\tdelay = 0\n\t\t\t}\n\n\t\t\t// Exclude the recently signed validators and the in turn validator\n\t\t\ttemp := make([]common.Address, 0, len(validators))\n\t\t\tfor _, addr := range validators {\n\t\t\t\tif snap.signRecentlyByCounts(addr, counts) {\n\t\t\t\t\tcontinue\n\t\t\t\t}\n\t\t\t\tif p.chainConfig.IsBohr(header.Number, header.Time) {\n\t\t\t\t\tif addr == inTurnAddr {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\ttemp = append(temp, addr)\n\t\t\t}\n\t\t\tvalidators = temp\n\t\t}\n\n\t\t// get the index of current validator and its shuffled backoff time.\n\t\tidx := -1\n\t\tfor index, itemAddr := range validators {\n\t\t\tif val == itemAddr {\n\t\t\t\tidx = index\n\t\t\t}\n\t\t}\n\t\tif idx < 0 {\n\t\t\tlog.Debug(\"The validator is not authorized\", \"addr\", val)\n\t\t\treturn 0\n\t\t}\n\n\t\trandSeed := snap.Number\n\t\tif p.chainConfig.IsBohr(header.Number, header.Time) {\n\t\t\trandSeed = header.Number.Uint64() / uint64(snap.TurnLength)\n\t\t}\n\t\ts := rand.NewSource(int64(randSeed))\n\t\tr := rand.New(s)\n\t\tn := len(validators)\n\t\tbackOffSteps := make([]uint64, 0, n)\n\n\t\tfor i := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 2290,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= backOffSteps[idx] * wiggleTime\n\t\treturn delay\n\t}\n}\n\n// BlockInterval returns number of blocks in one epoch for the given header\nfunc (p *Parlia) epochLength(chain consensus.ChainHeaderReader, header *types.Header, parents []*types.Header) (uint64, error) {\n\tif header == nil {\n\t\treturn defaultEpochLength, errUnknownBlock\n\t}\n\tif header.Number.Uint64() == 0 {\n\t\treturn defaultEpochLength, nil\n\t}\n\tsnap, err := p.snapshot(chain, header.Number.Uint64()-1, header.ParentHash, parents)\n\tif err != nil {\n\t\treturn defaultEpochLength, err\n\t}\n\treturn snap.EpochLength, nil\n}\n\n// BlockInterval returns the block interval in milliseconds for the given header\nfunc (p *Parlia) BlockInterval(chain consensus.ChainHeaderReader, header *types.Header) (uint64, error) {\n\tif header == nil {\n\t\treturn defaultBlockInterval, errUnknownBlock\n\t}\n\tif header.Number.Uint64() == 0 {\n\t\treturn defaultBlockInterval, nil\n\t}\n\tsnap, err := p.snapshot(chain, header.Number.Uint64()-1, header.ParentHash, nil)\n\tif err != nil {\n\t\treturn defaultBlockInterval, err\n\t}\n\treturn snap.BlockInterval, nil\n}\n\nfunc (p *Parlia) NextProposalBlock(chain consensus.ChainHeaderReader, header *types.Header, proposer common.Address) (uint64, uint64, error) {\n\tsnap, err := p.snapshot(chain, header.Number.Uint64(), header.Hash(), nil)\n\tif err != nil {\n\t\treturn 0, 0, err\n\t}\n\n\treturn snap.nextProposalBlock(proposer)\n}\n\nfunc (p *Parlia) checkNanoBlackList(state vm.StateDB, header *types.Header) error {\n\tif p.chainConfig.IsNano(header.Number) {\n\t\tfor _, blackListAddr := range types.NanoBlackList {\n\t\t\tif state.IsAddressInMutations(blackListAddr) {\n\t\t\t\tlog.Error(\"blacklisted address found\", \"address\", blackListAddr)\n\t\t\t\treturn fmt.Errorf(\"block contains blacklisted address: %s\", blackListAddr.Hex())\n\t\t\t}\n\t\t}\n\t}\n\treturn nil\n}\n\nfunc (p *Parlia) detectNewVersionWithFork(chain consensus.ChainHeaderReader, header *types.Header, state vm.StateDB) {\n\t// Ignore blocks that are considered too old\n\tconst maxBlockReceiveDelay = 10 * time.Second\n\tblockTime := time.UnixMilli(int64(header.MilliTimestamp()))\n\tif time.Since(blockTime) > maxBlockReceiveDelay {\n\t\treturn\n\t}\n\n\t// If the fork is not a majority, log a warning or debug message\n\tnumber := header.Number.Uint64()\n\tsnap, err := p.snapshot(chain, number-1, header.ParentHash, nil)\n\tif err != nil {\n\t\treturn\n\t}\n\tnextForkHash := forkid.NextForkHash(p.chainConfig, p.genesisHash, chain.GenesisHeader().Time, number, header.Time)\n\tforkHashHex := hex.EncodeToString(nextForkHash[:])\n\tif !snap.isMajorityFork(forkHashHex) {\n\t\tlogFn := log.Debug\n\t\tif state.NoTries() {\n\t\t\tlogFn = log.Warn\n\t\t}\n\t\tlogFn(\"possible fork detected: client is not in majority\", \"nextForkHash\", forkHashHex)\n\t}\n}\n\n// chain context\ntype chainContext struct {\n\tChain  consensus.ChainHeaderReader\n\tparlia consensus.Engine\n}\n\nfunc (c chainContext) Engine() consensus.Engine {\n\treturn c.parlia\n}\n\nfunc (c chainContext) GetHeader(hash common.Hash, number uint64) *types.Header {\n\treturn c.Chain.GetHeader(hash, number)\n}\n\nfunc (c chainContext) Config() *params.ChainConfig {\n\treturn c.Chain.Config()\n}\n\n// apply message\nfunc applyMessage(\n\tmsg *core.Message,\n\tevm *vm.EVM,\n\tstate vm.StateDB,\n\theader *types.Header,\n\tchainConfig *params.ChainConfig,\n\tchainContext core.ChainContext,\n) (uint64, error) {\n\t// Apply the transaction to the current state (included in the env)\n\tif chainConfig.IsCancun(header.Number, header.Time) {\n\t\trules := evm.ChainConfig().Rules(evm.Context.BlockNumber, evm.Context.Random != nil, evm.Context.Time)\n\t\tstate.Prepare(rules, msg.From, evm.Context.Coinbase, msg.To, vm.ActivePrecompiles(rules), msg.AccessList)\n\t} else {\n\t\tstate.ClearAccessList()\n\t}\n\t// Increment the nonce for the next transaction\n\tstate.SetNonce(msg.From, state.GetNonce(msg.From)+1, tracing.NonceChangeEoACall)\n\n\tret, returnGas, err := evm.Call(\n\t\tmsg.From,\n\t\t*msg.To,\n\t\tmsg.Data,\n\t\tmsg.GasLimit,\n\t\tuint256.MustFromBig(msg.Value),\n\t)\n\tif err != nil {\n\t\tlog.Error(\"apply message failed\", \"msg\", string(ret), \"err\", err)\n\t}\n\treturn msg.GasLimit - returnGas, err\n}\n\n// proposalKey build a key which is a combination of the block number and the proposer address.\nfunc proposalKey(header types.Header) string {\n\treturn header.ParentHash.String() + header.Coinbase.String()\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 710,
          "category": "overflow",
          "pattern": "\\*\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "*= -1\n\t}\n\tgasLimitBoundDivisor := gasLimitBoundDivisorBeforeLorentz\n\tif p.chainConfig.IsLorentz(header.Number, header.Time) {\n\t\tgasLimitBoundDivisor = params.GasLimitBoundDivisor\n\t}\n\tlimit := parent.GasLimit / gasLimitBoundDivisor\n\n\tif uint64(diff) >= limit || header.GasLimit < params.MinGasLimit {\n\t\treturn fmt.Errorf(\"invalid gas limit: have %d, want %d += %d\", header.GasLimit, parent.GasLimit, limit-1)\n\t}\n\n\t// Verify vote attestation for fast finality.\n\tif err := p.verifyVoteAttestation(chain, header, parents)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1896,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 2408,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1043,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(parent.Hash()",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 109,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/consensus/parlia/parlia.go",
          "line": 1999,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "CrossChainContract",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/abi.go",
          "line": 1067,
          "category": "quantum_entanglement_risks",
          "pattern": "crossChain\\w*",
          "match": "crossChain",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/snapshot_test.go",
          "line": 17,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/stakehub.go",
          "line": 39,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/consensus/parlia/stakehub.go",
          "line": 81,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 251,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\treturn counts\n}\n\nfunc (s *Snapshot) signRecentlyByCounts(validator common.Address, counts map[common.Address]uint8) bool {\n\tif seenTimes, ok := counts[validator]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 432,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(headers))\n\tsnap.Hash = headers[len(headers)-1].Hash()\n\treturn snap, nil\n}\n\n// validators retrieves the list of validators in ascending order.\nfunc (s *Snapshot) validators() []common.Address {\n\tvalidators := make([]common.Address, 0, len(s.Validators))\n\tfor v := range s.Validators {\n\t\tvalidators = append(validators, v)\n\t}\n\tsort.Sort(validatorsAscending(validators))\n\treturn validators\n}\n\n// lastBlockInOneTurn returns if the block at height `blockNumber` is the last block in current turn.\nfunc (s *Snapshot) lastBlockInOneTurn(blockNumber uint64) bool {\n\treturn (blockNumber+1)%uint64(s.TurnLength) == 0\n}\n\n// inturn returns if a validator at a given block height is in-turn or not.\nfunc (s *Snapshot) inturn(validator common.Address) bool {\n\treturn s.inturnValidator() == validator\n}\n\n// inturnValidator returns the validator for the following block height.\nfunc (s *Snapshot) inturnValidator() common.Address {\n\tvalidators := s.validators()\n\toffset := (s.Number + 1) / uint64(s.TurnLength) % uint64(len(validators))\n\treturn validators[offset]\n}\n\nfunc (s *Snapshot) nexValidatorsChangeBlock() uint64 {\n\tepochLength := s.EpochLength\n\tcurrentEpoch := s.Number - s.Number%epochLength\n\tcheckLen := s.minerHistoryCheckLen()\n\tif s.Number%epochLength < checkLen {\n\t\treturn currentEpoch + checkLen\n\t}\n\treturn currentEpoch + epochLength + checkLen\n}\n\n// nextProposalBlock returns the validator next proposal block.\nfunc (s *Snapshot) nextProposalBlock(proposer common.Address) (uint64, uint64, error) {\n\tvalidators := s.validators()\n\tcurrentIndex := int(s.Number / uint64(s.TurnLength) % uint64(len(validators)))\n\texpectIndex := s.indexOfVal(proposer)\n\tif expectIndex < 0 {\n\t\treturn 0, 0, errors.New(\"proposer not in validator set\")\n\t}\n\tstartBlock := s.Number + uint64(((expectIndex+len(validators)-currentIndex)%len(validators))*int(s.TurnLength))\n\tstartBlock = startBlock - startBlock%uint64(s.TurnLength)\n\tendBlock := startBlock + uint64(s.TurnLength) - 1\n\n\tchangeValidatorsBlock := s.nexValidatorsChangeBlock()\n\tif startBlock >= changeValidatorsBlock {\n\t\treturn 0, 0, errors.New(\"next proposal block is out of current epoch\")\n\t}\n\tif endBlock >= changeValidatorsBlock {\n\t\tendBlock = changeValidatorsBlock\n\t}\n\treturn startBlock, endBlock, nil\n}\n\nfunc (s *Snapshot) enoughDistance(validator common.Address, header *types.Header) bool {\n\tidx := s.indexOfVal(validator)\n\tif idx < 0 {\n\t\treturn true\n\t}\n\tvalidatorNum := int64(len(s.validators()))\n\tif validatorNum == 1 {\n\t\treturn true\n\t}\n\tif validator == header.Coinbase {\n\t\treturn false\n\t}\n\toffset := (int64(s.Number) + 1) % validatorNum\n\tif int64(idx) >= offset {\n\t\treturn int64(idx)-offset >= validatorNum-2\n\t} else {\n\t\treturn validatorNum+int64(idx)-offset >= validatorNum-2\n\t}\n}\n\nfunc (s *Snapshot) indexOfVal(validator common.Address) int {\n\tif validator, ok := s.Validators[validator]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 88,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[v] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 92,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[v] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/consensus/parlia/snapshot.go",
          "line": 166,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[v] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/bohrFork.go",
          "line": 69,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 63,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t}\n\t}\n\treturn recentSignTimes >= turnLength\n}\n\n// refer Snapshot.minerHistoryCheckLen\nfunc minerHistoryCheckLen(totalValidators int, turnLength int) uint64 {\n\treturn uint64(totalValidators/2+1)*uint64(turnLength) - 1\n}\n\n// refer Snapshot.inturnValidator\nfunc inturnValidator(totalValidators int, turnLength int, height int) int {\n\treturn height / turnLength % totalValidators\n}\n\nfunc simulateValidatorOutOfService(totalValidators int, downValidators int, turnLength int) {\n\tdownBlocks := 10000\n\trecoverBlocks := 10000\n\trecents := make(map[uint64]int)\n\n\tvalidators := make(map[int]bool, totalValidators)\n\tdown := make([]int, totalValidators)\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 87,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 130,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[down[i]] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/consensus/parlia/parlia_test.go",
          "line": 429,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validators[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 145,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 192,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 167,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "validatorItems[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/consensus/parlia/feynmanfork.go",
          "line": 228,
          "category": "spectral_anomalies",
          "pattern": "validator\\w*\\[.*\\]\\s*=",
          "match": "Validators[i] =",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 1.0,
          "confidence": 0.99,
          "ensemble_confidence": 0.891
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/parlia/ramanujanfork.go",
          "line": 26,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fixedBackOffTimeBeforeFork + time.Duration(rand.Int63n(int64(wiggle)))\n\t}\n\treturn delay\n}\n\nfunc (p *Parlia) blockTimeForRamanujanFork(snap *Snapshot, header, parent *types.Header) uint64 {\n\tblockTime := parent.MilliTimestamp() + snap.BlockInterval\n\tif p.chainConfig.IsRamanujan(header.Number) {\n\t\tblockTime = blockTime + p.backOffTime(snap, parent, header, p.val)\n\t}\n\tif now := uint64(time.Now().UnixMilli())",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/clique/clique.go",
          "line": 671,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= time.Duration(rand.Int63n(int64(wiggle)))\n\n\t\tlog.Trace(\"Out-of-turn signing requested\", \"wiggle\", common.PrettyDuration(wiggle))\n\t}\n\t// Sign all the things!\n\tsighash, err := signFn(accounts.Account{Address: signer}, accounts.MimetypeClique, CliqueRLP(header))\n\tif err != nil {\n\t\treturn err\n\t}\n\tcopy(header.Extra[len(header.Extra)-extraSeal:], sighash)\n\t// Wait until sealing is terminated or delay timeout.\n\tlog.Trace(\"Waiting for slot to sign and propagate\", \"delay\", common.PrettyDuration(delay))\n\tgopool.Submit(func() {\n\t\tselect {\n\t\tcase <-stop:\n\t\t\treturn\n\t\tcase <-time.After(delay):\n\t\t}\n\n\t\tselect {\n\t\tcase results <- block.WithSeal(header):\n\t\tdefault:\n\t\t\tlog.Warn(\"Sealing result is not read by miner\", \"sealhash\", SealHash(header))\n\t\t}\n\t})\n\n\treturn nil\n}\n\n// CalcDifficulty is the difficulty adjustment algorithm. It returns the difficulty\n// that a new block should have:\n// * DIFF_NOTURN(2) if BLOCK_NUMBER % SIGNER_COUNT != SIGNER_INDEX\n// * DIFF_INTURN(1) if BLOCK_NUMBER % SIGNER_COUNT == SIGNER_INDEX\nfunc (c *Clique) CalcDifficulty(chain consensus.ChainHeaderReader, time uint64, parent *types.Header) *big.Int {\n\tsnap, err := c.snapshot(chain, parent.Number.Uint64(), parent.Hash(), nil)\n\tif err != nil {\n\t\treturn nil\n\t}\n\tc.lock.RLock()\n\tsigner := c.signer\n\tc.lock.RUnlock()\n\treturn calcDifficulty(snap, signer)\n}\n\nfunc calcDifficulty(snap *Snapshot, signer common.Address) *big.Int {\n\tif snap.inturn(snap.Number+1, signer) {\n\t\treturn new(big.Int).Set(diffInTurn)\n\t}\n\treturn new(big.Int).Set(diffNoTurn)\n}\n\n// SealHash returns the hash of a block prior to it being sealed.\nfunc (c *Clique) SealHash(header *types.Header) common.Hash {\n\treturn SealHash(header)\n}\n\n// Close implements consensus.Engine. It's a noop for clique as there are no background threads.\nfunc (c *Clique) Close() error {\n\treturn nil\n}\n\n// SealHash returns the hash of a block prior to it being sealed.\nfunc SealHash(header *types.Header) (hash common.Hash) {\n\thasher := sha3.NewLegacyKeccak256()\n\tencodeSigHeader(hasher, header)\n\thasher.(crypto.KeccakState).Read(hash[:])\n\treturn hash\n}\n\n// CliqueRLP returns the rlp bytes which needs to be signed for the proof-of-authority\n// sealing. The RLP to sign consists of the entire header apart from the 65 byte signature\n// contained at the end of the extra data.\n//\n// Note, the method requires the extra data to be at least 65 bytes, otherwise it\n// panics. This is done to avoid accidentally using both forms (signature present\n// or not), which could be abused to produce different hashes for the same header.\nfunc CliqueRLP(header *types.Header) []byte {\n\tb := new(bytes.Buffer)\n\tencodeSigHeader(b, header)\n\treturn b.Bytes()\n}\n\nfunc encodeSigHeader(w io.Writer, header *types.Header) {\n\tenc := []interface{}{\n\t\theader.ParentHash,\n\t\theader.UncleHash,\n\t\theader.Coinbase,\n\t\theader.Root,\n\t\theader.TxHash,\n\t\theader.ReceiptHash,\n\t\theader.Bloom,\n\t\theader.Difficulty,\n\t\theader.Number,\n\t\theader.GasLimit,\n\t\theader.GasUsed,\n\t\theader.Time,\n\t\theader.Extra[:len(header.Extra)-crypto.SignatureLength], // Yes, this will panic if extra is too short\n\t\theader.MixDigest,\n\t\theader.Nonce,\n\t}\n\tif header.BaseFee != nil {\n\t\tenc = append(enc, header.BaseFee)\n\t}\n\tif header.WithdrawalsHash != nil {\n\t\tpanic(\"unexpected withdrawal hash value in clique\")\n\t}\n\tif header.ExcessBlobGas != nil {\n\t\tpanic(\"unexpected excess blob gas value in clique\")\n\t}\n\tif header.BlobGasUsed != nil {\n\t\tpanic(\"unexpected blob gas used value in clique\")\n\t}\n\tif header.ParentBeaconRoot != nil {\n\t\tpanic(\"unexpected parent beacon root value in clique\")\n\t}\n\tif err := rlp.Encode(w, enc)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/consensus/clique/snapshot.go",
          "line": 288,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= uint64(len(headers))\n\tsnap.Hash = headers[len(headers)-1].Hash()\n\n\treturn snap, nil\n}\n\n// signers retrieves the list of authorized signers in ascending order.\nfunc (s *Snapshot) signers() []common.Address {\n\tsigs := make([]common.Address, 0, len(s.Signers))\n\tfor sig := range s.Signers {\n\t\tsigs = append(sigs, sig)\n\t}\n\tslices.SortFunc(sigs, common.Address.Cmp)\n\treturn sigs\n}\n\n// inturn returns if a signer at a given block height is in-turn or not.\nfunc (s *Snapshot) inturn(number uint64, signer common.Address) bool {\n\tsigners, offset := s.signers(), 0\n\tfor offset < len(signers) && signers[offset] != signer {\n\t\toffset++\n\t}\n\treturn (number % uint64(len(signers))) == uint64(offset)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/signer/core/stdioui.go",
          "line": 46,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/signer/core/signed_data.go",
          "line": 70,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 27 // Transform V from 0/1 to 27/28 according to the yellow paper\n\t}\n\treturn signature, nil\n}\n\n// SignData signs the hash of the provided data, but does so differently\n// depending on the content-type specified.\n//\n// Different types of validation occur.\nfunc (api *SignerAPI) SignData(ctx context.Context, contentType string, addr common.MixedcaseAddress, data interface{}) (hexutil.Bytes, error) {\n\tvar req, transformV, err = api.determineSignatureFormat(ctx, contentType, addr, data)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tsignature, err := api.sign(req, transformV)\n\tif err != nil {\n\t\tapi.UI.ShowError(err.Error())\n\t\treturn nil, err\n\t}\n\treturn signature, nil\n}\n\n// determineSignatureFormat determines which signature method should be used based upon the mime type\n// In the cases where it matters ensure that the charset is handled. The charset\n// resides in the 'params' returned as the second returnvalue from mime.ParseMediaType\n// charset, ok := params[\"charset\"]\n// As it is now, we accept any charset and just treat it as 'raw'.\n// This method returns the mimetype for signing along with the request\nfunc (api *SignerAPI) determineSignatureFormat(ctx context.Context, contentType string, addr common.MixedcaseAddress, data interface{}) (*SignDataRequest, bool, error) {\n\tvar (\n\t\treq          *SignDataRequest\n\t\tuseEthereumV = true // Default to use V = 27 or 28, the legacy Ethereum format\n\t)\n\tmediaType, _, err := mime.ParseMediaType(contentType)\n\tif err != nil {\n\t\treturn nil, useEthereumV, err\n\t}\n\n\tswitch mediaType {\n\tcase apitypes.IntendedValidator.Mime:\n\t\t// Data with an intended validator\n\t\tvalidatorData, err := UnmarshalValidatorData(data)\n\t\tif err != nil {\n\t\t\treturn nil, useEthereumV, err\n\t\t}\n\t\tsighash, msg := SignTextValidator(validatorData)\n\t\tmessages := []*apitypes.NameValueType{\n\t\t\t{\n\t\t\t\tName:  \"This is a request to sign data intended for a particular validator (see EIP 191 version 0)\",\n\t\t\t\tTyp:   \"description\",\n\t\t\t\tValue: \"\",\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"Intended validator address\",\n\t\t\t\tTyp:   \"address\",\n\t\t\t\tValue: validatorData.Address.String(),\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"Application-specific data\",\n\t\t\t\tTyp:   \"hexdata\",\n\t\t\t\tValue: validatorData.Message,\n\t\t\t},\n\t\t\t{\n\t\t\t\tName:  \"Full message for signing\",\n\t\t\t\tTyp:   \"hexdata\",\n\t\t\t\tValue: fmt.Sprintf(\"%#x\", msg),\n\t\t\t},\n\t\t}\n\t\treq = &SignDataRequest{ContentType: mediaType, Rawdata: []byte(msg), Messages: messages, Hash: sighash}\n\tcase apitypes.ApplicationClique.Mime:\n\t\t// Clique is the Ethereum PoA standard\n\t\tcliqueData, err := fromHex(data)\n\t\tif err != nil {\n\t\t\treturn nil, useEthereumV, err\n\t\t}\n\t\theader := &types.Header{}\n\t\tif err := rlp.DecodeBytes(cliqueData, header)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/signer/core/signed_data.go",
          "line": 359,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 27 // Transform yellow paper V from 27/28 to 0/1\n\thash := accounts.TextHash(data)\n\trpk, err := crypto.SigToPub(hash, sig)\n\tif err != nil {\n\t\treturn common.Address{}, err\n\t}\n\treturn crypto.PubkeyToAddress(*rpk), nil\n}\n\n// UnmarshalValidatorData converts the bytes input to typed data\nfunc UnmarshalValidatorData(data interface{}) (apitypes.ValidatorData, error) {\n\traw, ok := data.(map[string]interface{})\n\tif !ok {\n\t\treturn apitypes.ValidatorData{}, errors.New(\"validator input is not a map[string]interface{}\")\n\t}\n\taddrBytes, err := fromHex(raw[\"address\"])\n\tif err != nil {\n\t\treturn apitypes.ValidatorData{}, fmt.Errorf(\"validator address error: %w\", err)\n\t}\n\tif len(addrBytes) == 0 {\n\t\treturn apitypes.ValidatorData{}, errors.New(\"validator address is undefined\")\n\t}\n\tmessageBytes, err := fromHex(raw[\"message\"])\n\tif err != nil {\n\t\treturn apitypes.ValidatorData{}, fmt.Errorf(\"message error: %w\", err)\n\t}\n\tif len(messageBytes) == 0 {\n\t\treturn apitypes.ValidatorData{}, errors.New(\"message is undefined\")\n\t}\n\treturn apitypes.ValidatorData{\n\t\tAddress: common.BytesToAddress(addrBytes),\n\t\tMessage: messageBytes,\n\t}, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/signer/rules/rules_test.go",
          "line": 307,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= storage.get(\"mykey\")\n\n\n\t\tstorage.put(\"mykey\", {\"an\": \"object\"}) \t// Should result in \"[object Object]\"\n\t\ta += storage.get(\"mykey\")\n\n\n\t\tstorage.put(\"mykey\", JSON.stringify({\"an\": \"object\"})) // Should result in '{\"an\":\"object\"}'\n\t\ta += storage.get(\"mykey\")\n\n\t\ta += storage.get(\"missingkey\")\t\t//Missing keys should result in empty string\n\t\tstorage.put(\"\",\"missing key==noop\") // Can't store with 0-length key\n\t\ta += storage.get(\"\")\t\t\t\t// Should result in ''\n\n\t\tvar b = new BigNumber(2)\n\t\tvar c = new BigNumber(16)//\"0xf0\",16)\n\t\tvar d = b.plus(c)\n\t\tconsole.log(d)\n\t\treturn a\n\t}\n`\n\tr, err := initRuleEngine(js)\n\tif err != nil {\n\t\tt.Errorf(\"Couldn't create evaluator %v\", err)\n\t\treturn\n\t}\n\n\tv, err := r.execute(\"testStorage\", nil)\n\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error %v\", err)\n\t}\n\tretval := v.ToString().String()\n\n\tif err != nil {\n\t\tt.Errorf(\"Unexpected error %v\", err)\n\t}\n\texp := `myvaluea,list[object Object]{\"an\":\"object\"}`\n\tif retval != exp {\n\t\tt.Errorf(\"Unexpected data, expected '%v', got '%v'\", exp, retval)\n\t}\n\tt.Logf(\"Err %v\", err)\n}\n\nconst ExampleTxWindow = `\n\tfunction big(str){\n\t\tif(str.slice(0,2) == \"0x\"){ return new BigNumber(str.slice(2),16)}\n\t\treturn new BigNumber(str)\n\t}\n\n\t// Time window: 1 week\n\tvar window = 1000* 3600*24*7",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/signer/core/apitypes/types.go",
          "line": 893,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 8 {\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"int%d\", n)] = struct{}{}\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"int%d[]\", n)] = struct{}{}\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"uint%d\", n)] = struct{}{}\n\t\tvalidPrimitiveTypes[fmt.Sprintf(\"uint%d[]\", n)] = struct{}{}\n\t}\n}\n\n// Checks if the primitive value is valid\nfunc isPrimitiveTypeValid(primitiveType string) bool {\n\tinput := strings.Split(primitiveType, \"[\")[0]\n\t_, ok := validPrimitiveTypes[input]\n\treturn ok\n}\n\n// validate checks if the given domain is valid, i.e. contains at least\n// the minimum viable keys and values\nfunc (domain *TypedDataDomain) validate() error {\n\tif domain.ChainId == nil && len(domain.Name) == 0 && len(domain.Version) == 0 && len(domain.VerifyingContract) == 0 && len(domain.Salt) == 0 {\n\t\treturn errors.New(\"domain is undefined\")\n\t}\n\n\treturn nil\n}\n\n// Map is a helper function to generate a map version of the domain\nfunc (domain *TypedDataDomain) Map() map[string]interface{} {\n\tdataMap := map[string]interface{}{}\n\n\tif domain.ChainId != nil {\n\t\tdataMap[\"chainId\"] = domain.ChainId\n\t}\n\n\tif len(domain.Name) > 0 {\n\t\tdataMap[\"name\"] = domain.Name\n\t}\n\n\tif len(domain.Version) > 0 {\n\t\tdataMap[\"version\"] = domain.Version\n\t}\n\n\tif len(domain.VerifyingContract) > 0 {\n\t\tdataMap[\"verifyingContract\"] = domain.VerifyingContract\n\t}\n\n\tif len(domain.Salt) > 0 {\n\t\tdataMap[\"salt\"] = domain.Salt\n\t}\n\treturn dataMap\n}\n\n// NameValueType is a very simple struct with Name, Value and Type. It's meant for simple\n// json structures used to communicate signing-info about typed data with the UI\ntype NameValueType struct {\n\tName  string      `json:\"name\"`\n\tValue interface{} `json:\"value\"`\n\tTyp   string      `json:\"type\"`\n}\n\n// Pprint returns a pretty-printed version of nvt\nfunc (nvt *NameValueType) Pprint(depth int) string {\n\toutput := bytes.Buffer{}\n\toutput.WriteString(strings.Repeat(\"\\u00a0\", depth*2))\n\toutput.WriteString(fmt.Sprintf(\"%s [%s]: \", nvt.Name, nvt.Typ))\n\tif nvts, ok := nvt.Value.([]*NameValueType)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/enrcmd.go",
          "line": 105,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey := kv[i].(string)\n\t\tif len(key) > longestKey {\n\t\t\tlongestKey = len(key)\n\t\t}\n\t}\n\t// Print the keys, invoking formatters for known keys.\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/devp2p/enrcmd.go",
          "line": 112,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\tkey := kv[i].(string)\n\t\tval := kv[i+1].(rlp.RawValue)\n\t\tpad := longestKey - len(key)\n\t\tout += strings.Repeat(\" \", indent) + strconv.Quote(key) + strings.Repeat(\" \", pad+1)\n\t\tformatter := attrFormatters[key]\n\t\tif formatter == nil {\n\t\t\tformatter = formatAttrRaw\n\t\t}\n\t\tfmtval, ok := formatter(val)\n\t\tif ok {\n\t\t\tout += fmtval + \"\\n\"\n\t\t} else {\n\t\t\tout += hex.EncodeToString(val) + \" (!)\\n\"\n\t\t}\n\t}\n\treturn out\n}\n\n// parseNode parses a node record and verifies its signature.\nfunc parseNode(source string) (*enode.Node, error) {\n\tif strings.HasPrefix(source, \"enode://\") {\n\t\treturn enode.ParseV4(source)\n\t}\n\tr, err := parseRecord(source)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\treturn enode.New(enode.ValidSchemes, r)\n}\n\n// parseRecord parses a node record from hex, base64, or raw binary input.\nfunc parseRecord(source string) (*enr.Record, error) {\n\tbin := []byte(source)\n\tif d, ok := decodeRecordHex(bytes.TrimSpace(bin))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/nodesetcmd.go",
          "line": 79,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 2 {\n\t\t\tkey := attrlist[i].(string)\n\t\t\tattrcount[key]++\n\t\t}\n\t}\n\n\tvar keys []string\n\tvar maxlength int\n\tfor key := range attrcount {\n\t\tkeys = append(keys, key)\n\t\tif len(key) > maxlength {\n\t\t\tmaxlength = len(key)\n\t\t}\n\t}\n\tsort.Strings(keys)\n\tfmt.Println(\"ENR attribute counts:\")\n\tfor _, key := range keys {\n\t\tfmt.Printf(\"%s%s: %d\\n\", strings.Repeat(\" \", maxlength-len(key)+1), key, attrcount[key])\n\t}\n}\n\nfunc nodesetFilter(ctx *cli.Context) error {\n\tif ctx.NArg() < 1 {\n\t\treturn errors.New(\"need nodes file as argument\")\n\t}\n\t// Parse -limit.\n\tlimit, err := parseFilterLimit(ctx.Args().Tail())\n\tif err != nil {\n\t\treturn err\n\t}\n\t// Parse the filters.\n\tfilter, err := andFilter(ctx.Args().Tail())\n\tif err != nil {\n\t\treturn err\n\t}\n\n\t// Load nodes and apply filters.\n\tns := loadNodesJSON(ctx.Args().First())\n\tresult := make(nodeSet)\n\tfor id, n := range ns {\n\t\tif filter(n) {\n\t\t\tresult[id] = n\n\t\t}\n\t}\n\tif limit >= 0 {\n\t\tresult = result.topN(limit)\n\t}\n\twriteNodesJSON(\"-\", result)\n\treturn nil\n}\n\ntype nodeFilter func(nodeJSON) bool\n\ntype nodeFilterC struct {\n\tnarg int\n\tfn   func([]string) (nodeFilter, error)\n}\n\nvar filterFlags = map[string]nodeFilterC{\n\t\"-limit\":       {1, trueFilter}, // needed to skip over -limit\n\t\"-ip\":          {1, ipFilter},\n\t\"-min-age\":     {1, minAgeFilter},\n\t\"-eth-network\": {1, ethFilter},\n\t\"-les-server\":  {0, lesFilter},\n\t\"-snap\":        {0, snapFilter},\n}\n\n// parseFilters parses nodeFilters from args.\nfunc parseFilters(args []string) ([]nodeFilter, error) {\n\tvar filters []nodeFilter\n\tfor len(args) > 0 {\n\t\tfc, ok := filterFlags[args[0]]\n\t\tif !ok {\n\t\t\treturn nil, fmt.Errorf(\"invalid filter %q\", args[0])\n\t\t}\n\t\tif len(args)-1 < fc.narg {\n\t\t\treturn nil, fmt.Errorf(\"filter %q wants %d arguments, have %d\", args[0], fc.narg, len(args)-1)\n\t\t}\n\t\tfilter, err := fc.fn(args[1 : 1+fc.narg])\n\t\tif err != nil {\n\t\t\treturn nil, fmt.Errorf(\"%s: %v\", args[0], err)\n\t\t}\n\t\tfilters = append(filters, filter)\n\t\targs = args[1+fc.narg:]\n\t}\n\treturn filters, nil\n}\n\n// parseFilterLimit parses the -limit option in args. It returns -1 if there is no limit.\nfunc parseFilterLimit(args []string) (int, error) {\n\tlimit := -1\n\tfor i, arg := range args {\n\t\tif arg == \"-limit\" {\n\t\t\tif i == len(args)-1 {\n\t\t\t\treturn -1, errors.New(\"-limit requires an argument\")\n\t\t\t}\n\t\t\tn, err := strconv.Atoi(args[i+1])\n\t\t\tif err != nil {\n\t\t\t\treturn -1, fmt.Errorf(\"invalid -limit %q\", args[i+1])\n\t\t\t}\n\t\t\tlimit = n\n\t\t}\n\t}\n\treturn limit, nil\n}\n\n// andFilter parses node filters in args and returns a single filter that requires all\n// of them to match.\nfunc andFilter(args []string) (nodeFilter, error) {\n\tchecks, err := parseFilters(args)\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := func(n nodeJSON) bool {\n\t\tfor _, filter := range checks {\n\t\t\tif !filter(n) {\n\t\t\t\treturn false\n\t\t\t}\n\t\t}\n\t\treturn true\n\t}\n\treturn f, nil\n}\n\nfunc trueFilter(args []string) (nodeFilter, error) {\n\treturn func(n nodeJSON) bool { return true }, nil\n}\n\nfunc ipFilter(args []string) (nodeFilter, error) {\n\tprefix, err := netip.ParsePrefix(args[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := func(n nodeJSON) bool { return prefix.Contains(n.N.IPAddr()) }\n\treturn f, nil\n}\n\nfunc minAgeFilter(args []string) (nodeFilter, error) {\n\tminage, err := time.ParseDuration(args[0])\n\tif err != nil {\n\t\treturn nil, err\n\t}\n\tf := func(n nodeJSON) bool {\n\t\tage := n.LastResponse.Sub(n.FirstResponse)\n\t\treturn age >= minage\n\t}\n\treturn f, nil\n}\n\nfunc ethFilter(args []string) (nodeFilter, error) {\n\tvar filter forkid.Filter\n\tswitch args[0] {\n\tcase \"mainnet\":\n\t\tfilter = forkid.NewStaticFilter(params.MainnetChainConfig, core.DefaultGenesisBlock().ToBlock())\n\tdefault:\n\t\treturn nil, fmt.Errorf(\"unknown network %q\", args[0])\n\t}\n\n\tf := func(n nodeJSON) bool {\n\t\tvar eth struct {\n\t\t\tForkID forkid.ID\n\t\t\tTail   []rlp.RawValue `rlp:\"tail\"`\n\t\t}\n\t\tif n.N.Load(enr.WithEntry(\"eth\", &eth)) != nil {\n\t\t\treturn false\n\t\t}\n\t\treturn filter(eth.ForkID) == nil\n\t}\n\treturn f, nil\n}\n\nfunc lesFilter(args []string) (nodeFilter, error) {\n\tf := func(n nodeJSON) bool {\n\t\tvar les struct {\n\t\t\tTail []rlp.RawValue `rlp:\"tail\"`\n\t\t}\n\t\treturn n.N.Load(enr.WithEntry(\"les\", &les)) == nil\n\t}\n\treturn f, nil\n}\n\nfunc snapFilter(args []string) (nodeFilter, error) {\n\tf := func(n nodeJSON) bool {\n\t\tvar snap struct {\n\t\t\tTail []rlp.RawValue `rlp:\"tail\"`\n\t\t}\n\t\treturn n.N.Load(enr.WithEntry(\"snap\", &snap)) == nil\n\t}\n\treturn f, nil\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/dns_route53.go",
          "line": 320,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= size\n\t\tbatchCount += count\n\t}\n\treturn batches\n}\n\n// changeSize returns the RDATA size of a DNS change.\nfunc changeSize(ch types.Change) int {\n\tsize := 0\n\tfor _, rr := range ch.ResourceRecordSet.ResourceRecords {\n\t\tif rr.Value != nil {\n\t\t\tsize += len(*rr.Value)\n\t\t}\n\t}\n\treturn size\n}\n\nfunc changeCount(ch types.Change) int {\n\tif ch.Action == types.ChangeActionUpsert {\n\t\treturn 2\n\t}\n\treturn 1\n}\n\n// collectRecords collects all TXT records below the given name.\nfunc (c *route53Client) collectRecords(name string) (map[string]recordSet, error) {\n\tvar req route53.ListResourceRecordSetsInput\n\treq.HostedZoneId = &c.zoneID\n\texisting := make(map[string]recordSet)\n\tlog.Info(\"Loading existing TXT records\", \"name\", name, \"zone\", c.zoneID)\n\tfor page := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/geth/config.go",
          "line": 229,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(`\n\n Running in ephemeral mode.  The following account has been prefunded in the genesis:\n\n       Account\n       ------------------\n       0x%x (10^49 ETH)\n`, cfg.Eth.Miner.Etherbase)\n\t\tif cfg.Eth.Miner.Etherbase == utils.DeveloperAddr {\n\t\t\tdevModeBanner += fmt.Sprintf(` \n       Private Key\n       ------------------\n       0x%x\n`, crypto.FromECDSA(utils.DeveloperKey))\n\t\t}\n\t}\n\n\treturn devModeBanner\n}\n\n// makeFullNode loads geth configuration and creates the Ethereum backend.\nfunc makeFullNode(ctx *cli.Context) (*node.Node, ethapi.Backend) {\n\tstack, cfg := makeConfigNode(ctx)\n\tif ctx.IsSet(utils.RialtoHash.Name) {\n\t\tv := ctx.String(utils.RialtoHash.Name)\n\t\tparams.RialtoGenesisHash = common.HexToHash(v)\n\t}\n\n\tif ctx.IsSet(utils.OverridePassedForkTime.Name) {\n\t\tv := ctx.Uint64(utils.OverridePassedForkTime.Name)\n\t\tcfg.Eth.OverridePassedForkTime = &v\n\t}\n\tif ctx.IsSet(utils.OverrideLorentz.Name) {\n\t\tv := ctx.Uint64(utils.OverrideLorentz.Name)\n\t\tcfg.Eth.OverrideLorentz = &v\n\t}\n\tif ctx.IsSet(utils.OverrideMaxwell.Name) {\n\t\tv := ctx.Uint64(utils.OverrideMaxwell.Name)\n\t\tcfg.Eth.OverrideMaxwell = &v\n\t}\n\tif ctx.IsSet(utils.OverrideFermi.Name) {\n\t\tv := ctx.Uint64(utils.OverrideFermi.Name)\n\t\tcfg.Eth.OverrideFermi = &v\n\t}\n\tif ctx.IsSet(utils.OverrideVerkle.Name) {\n\t\tv := ctx.Uint64(utils.OverrideVerkle.Name)\n\t\tcfg.Eth.OverrideVerkle = &v\n\t}\n\tif ctx.IsSet(utils.OverrideFullImmutabilityThreshold.Name) {\n\t\tparams.FullImmutabilityThreshold = ctx.Uint64(utils.OverrideFullImmutabilityThreshold.Name)\n\t\tdownloader.FullMaxForkAncestry = ctx.Uint64(utils.OverrideFullImmutabilityThreshold.Name)\n\t}\n\tif ctx.IsSet(utils.OverrideMinBlocksForBlobRequests.Name) {\n\t\tparams.MinBlocksForBlobRequests = ctx.Uint64(utils.OverrideMinBlocksForBlobRequests.Name)\n\t\tparams.MinTimeDurationForBlobRequests = uint64(float64(params.MinBlocksForBlobRequests) * 0.75 /*maxwellBlockInterval*/)\n\t}\n\tif ctx.IsSet(utils.OverrideDefaultExtraReserveForBlobRequests.Name) {\n\t\tparams.DefaultExtraReserveForBlobRequests = ctx.Uint64(utils.OverrideDefaultExtraReserveForBlobRequests.Name)\n\t}\n\tif ctx.IsSet(utils.OverrideBreatheBlockInterval.Name) {\n\t\tparams.BreatheBlockInterval = ctx.Uint64(utils.OverrideBreatheBlockInterval.Name)\n\t}\n\tif ctx.IsSet(utils.OverrideFixedTurnLength.Name) {\n\t\tparams.FixedTurnLength = ctx.Uint64(utils.OverrideFixedTurnLength.Name)\n\t}\n\n\tbackend, eth := utils.RegisterEthService(stack, &cfg.Eth)\n\n\t// Create gauge with geth system and build information\n\tif eth != nil { // The 'eth' backend may be nil in light mode\n\t\tvar protos []string\n\t\tfor _, p := range eth.Protocols() {\n\t\t\tprotos = append(protos, fmt.Sprintf(\"%v/%d\", p.Name, p.Version))\n\t\t}\n\t\tmetrics.NewRegisteredGaugeInfo(\"geth/info\", nil).Update(metrics.GaugeInfoValue{\n\t\t\t\"arch\":      runtime.GOARCH,\n\t\t\t\"os\":        runtime.GOOS,\n\t\t\t\"version\":   cfg.Node.Version,\n\t\t\t\"protocols\": strings.Join(protos, \",\"),\n\t\t})\n\t}\n\n\t// Configure log filter RPC API.\n\tfilterSystem := utils.RegisterFilterAPI(stack, backend, &cfg.Eth)\n\n\t// Configure GraphQL if requested.\n\tif ctx.IsSet(utils.GraphQLEnabledFlag.Name) {\n\t\tutils.RegisterGraphQLService(stack, backend, filterSystem, &cfg.Node)\n\t}\n\t// Add the Ethereum Stats daemon if requested.\n\tif cfg.Ethstats.URL != \"\" {\n\t\tutils.RegisterEthStatsService(stack, backend, cfg.Ethstats.URL)\n\t}\n\n\tif ctx.IsSet(utils.DeveloperFlag.Name) {\n\t\t// Start dev mode.\n\t\tsimBeacon, err := catalyst.NewSimulatedBeacon(ctx.Uint64(utils.DeveloperPeriodFlag.Name), cfg.Eth.Miner.Etherbase, eth)\n\t\tif err != nil {\n\t\t\tutils.Fatalf(\"failed to register dev mode catalyst service: %v\", err)\n\t\t}\n\t\tcatalyst.RegisterSimulatedBeaconAPIs(stack, simBeacon)\n\t\tstack.RegisterLifecycle(simBeacon)\n\n\t\tbanner := constructDevModeBanner(ctx, cfg)\n\t\tfor _, line := range strings.Split(banner, \"\\n\") {\n\t\t\tlog.Warn(line)\n\t\t}\n\t}\n\n\tif ctx.IsSet(utils.FakeBeaconAddrFlag.Name) {\n\t\tcfg.FakeBeacon.Addr = ctx.String(utils.FakeBeaconAddrFlag.Name)\n\t}\n\tif ctx.IsSet(utils.FakeBeaconPortFlag.Name) {\n\t\tcfg.FakeBeacon.Port = ctx.Int(utils.FakeBeaconPortFlag.Name)\n\t}\n\tif cfg.FakeBeacon.Enable || ctx.IsSet(utils.FakeBeaconEnabledFlag.Name) {\n\t\tgo fakebeacon.NewService(&cfg.FakeBeacon, backend).Run()\n\t}\n\n\tgit, _ := version.VCS()\n\tutils.SetupMetrics(&cfg.Metrics,\n\t\tutils.EnableBuildInfo(git.Commit, git.Date),\n\t\tutils.EnableMinerInfo(ctx, &cfg.Eth.Miner),\n\t\tutils.EnableNodeInfo(&cfg.Eth.TxPool, stack.Server().NodeInfo()),\n\t\tutils.EnableNodeTrack(ctx, &cfg.Eth, stack),\n\t)\n\treturn stack, backend\n}\n\n// dumpConfig is the dumpconfig command.\nfunc dumpConfig(ctx *cli.Context) error {\n\t_, cfg := makeConfigNode(ctx)\n\tcomment := \"\"\n\n\tif cfg.Eth.Genesis != nil {\n\t\tcfg.Eth.Genesis = nil\n\t\tcomment += \"# Note: this config doesn't contain the genesis block.\\n\\n\"\n\t}\n\n\tout, err := tomlSettings.Marshal(&cfg)\n\tif err != nil {\n\t\treturn err\n\t}\n\n\tdump := os.Stdout\n\tif ctx.NArg() > 0 {\n\t\tdump, err = os.OpenFile(ctx.Args().Get(0), os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0644)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdefer dump.Close()\n\t}\n\tdump.WriteString(comment)\n\tdump.Write(out)\n\n\treturn nil\n}\n\nfunc applyMetricConfig(ctx *cli.Context, cfg *gethConfig) {\n\tif ctx.IsSet(utils.MetricsEnabledFlag.Name) {\n\t\tcfg.Metrics.Enabled = ctx.Bool(utils.MetricsEnabledFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsEnabledExpensiveFlag.Name) {\n\t\tlog.Warn(\"Expensive metrics will remain in BSC and may be removed in the future\", \"flag\", utils.MetricsEnabledExpensiveFlag.Name)\n\t\tcfg.Metrics.EnabledExpensive = ctx.Bool(utils.MetricsEnabledExpensiveFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsHTTPFlag.Name) {\n\t\tcfg.Metrics.HTTP = ctx.String(utils.MetricsHTTPFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsPortFlag.Name) {\n\t\tcfg.Metrics.Port = ctx.Int(utils.MetricsPortFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsEnableInfluxDBFlag.Name) {\n\t\tcfg.Metrics.EnableInfluxDB = ctx.Bool(utils.MetricsEnableInfluxDBFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBEndpointFlag.Name) {\n\t\tcfg.Metrics.InfluxDBEndpoint = ctx.String(utils.MetricsInfluxDBEndpointFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBDatabaseFlag.Name) {\n\t\tcfg.Metrics.InfluxDBDatabase = ctx.String(utils.MetricsInfluxDBDatabaseFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBUsernameFlag.Name) {\n\t\tcfg.Metrics.InfluxDBUsername = ctx.String(utils.MetricsInfluxDBUsernameFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBPasswordFlag.Name) {\n\t\tcfg.Metrics.InfluxDBPassword = ctx.String(utils.MetricsInfluxDBPasswordFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBTagsFlag.Name) {\n\t\tcfg.Metrics.InfluxDBTags = ctx.String(utils.MetricsInfluxDBTagsFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsEnableInfluxDBV2Flag.Name) {\n\t\tcfg.Metrics.EnableInfluxDBV2 = ctx.Bool(utils.MetricsEnableInfluxDBV2Flag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBTokenFlag.Name) {\n\t\tcfg.Metrics.InfluxDBToken = ctx.String(utils.MetricsInfluxDBTokenFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBBucketFlag.Name) {\n\t\tcfg.Metrics.InfluxDBBucket = ctx.String(utils.MetricsInfluxDBBucketFlag.Name)\n\t}\n\tif ctx.IsSet(utils.MetricsInfluxDBOrganizationFlag.Name) {\n\t\tcfg.Metrics.InfluxDBOrganization = ctx.String(utils.MetricsInfluxDBOrganizationFlag.Name)\n\t}\n\t// Sanity-check the commandline flags. It is fine if some unused fields is part\n\t// of the toml-config, but we expect the commandline to only contain relevant\n\t// arguments, otherwise it indicates an error.\n\tvar (\n\t\tenableExport   = ctx.Bool(utils.MetricsEnableInfluxDBFlag.Name)\n\t\tenableExportV2 = ctx.Bool(utils.MetricsEnableInfluxDBV2Flag.Name)\n\t)\n\tif enableExport || enableExportV2 {\n\t\tv1FlagIsSet := ctx.IsSet(utils.MetricsInfluxDBUsernameFlag.Name) ||\n\t\t\tctx.IsSet(utils.MetricsInfluxDBPasswordFlag.Name)\n\n\t\tv2FlagIsSet := ctx.IsSet(utils.MetricsInfluxDBTokenFlag.Name) ||\n\t\t\tctx.IsSet(utils.MetricsInfluxDBOrganizationFlag.Name) ||\n\t\t\tctx.IsSet(utils.MetricsInfluxDBBucketFlag.Name)\n\n\t\tif enableExport && v2FlagIsSet {\n\t\t\tutils.Fatalf(\"Flags --influxdb.metrics.organization, --influxdb.metrics.token, --influxdb.metrics.bucket are only available for influxdb-v2\")\n\t\t} else if enableExportV2 && v1FlagIsSet {\n\t\t\tutils.Fatalf(\"Flags --influxdb.metrics.username, --influxdb.metrics.password are only available for influxdb-v1\")\n\t\t}\n\t}\n}\n\nfunc setAccountManagerBackends(conf *node.Config, am *accounts.Manager, keydir string) error {\n\tscryptN := keystore.StandardScryptN\n\tscryptP := keystore.StandardScryptP\n\tif conf.UseLightweightKDF {\n\t\tscryptN = keystore.LightScryptN\n\t\tscryptP = keystore.LightScryptP\n\t}\n\n\t// Assemble the supported backends\n\tif len(conf.ExternalSigner) > 0 {\n\t\tlog.Info(\"Using external signer\", \"url\", conf.ExternalSigner)\n\t\tif extBackend, err := external.NewExternalBackend(conf.ExternalSigner)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/geth/snapshot.go",
          "line": 334,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\tvar acc types.StateAccount\n\t\tif err := rlp.DecodeBytes(accIter.Value, &acc)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/geth/snapshot.go",
          "line": 354,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\n\t\t\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\t\t\tlog.Info(\"Traversing state\", \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\tlastReport = time.Now()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif storageIter.Err != nil {\n\t\t\t\tlog.Error(\"Failed to traverse storage trie\", \"root\", acc.Root, \"err\", storageIter.Err)\n\t\t\t\treturn storageIter.Err\n\t\t\t}\n\t\t}\n\t\tif !bytes.Equal(acc.CodeHash, types.EmptyCodeHash.Bytes()) {\n\t\t\tif !rawdb.HasCode(chaindb, common.BytesToHash(acc.CodeHash)) {\n\t\t\t\tlog.Error(\"Code is missing\", \"hash\", common.BytesToHash(acc.CodeHash))\n\t\t\t\treturn errors.New(\"missing code\")\n\t\t\t}\n\t\t\tcodes += 1\n\t\t}\n\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\tlog.Info(\"Traversing state\", \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\tlastReport = time.Now()\n\t\t}\n\t}\n\tif accIter.Err != nil {\n\t\tlog.Error(\"Failed to traverse state trie\", \"root\", root, \"err\", accIter.Err)\n\t\treturn accIter.Err\n\t}\n\tlog.Info(\"State is complete\", \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\treturn nil\n}\n\n// traverseRawState is a helper function used for pruning verification.\n// Basically it just iterates the trie, ensure all nodes and associated\n// contract codes are present. It's basically identical to traverseState\n// but it will check each trie node.\nfunc traverseRawState(ctx *cli.Context) error {\n\tstack, _ := makeConfigNode(ctx)\n\tdefer stack.Close()\n\n\tchaindb := utils.MakeChainDatabase(ctx, stack, true)\n\tdefer chaindb.Close()\n\n\ttriedb := utils.MakeTrieDatabase(ctx, stack, chaindb, false, true, false)\n\tdefer triedb.Close()\n\n\theadBlock := rawdb.ReadHeadBlock(chaindb)\n\tif headBlock == nil {\n\t\tlog.Error(\"Failed to load head block\")\n\t\treturn errors.New(\"no head block\")\n\t}\n\tif ctx.NArg() > 1 {\n\t\tlog.Error(\"Too many arguments given\")\n\t\treturn errors.New(\"too many arguments\")\n\t}\n\tvar (\n\t\troot common.Hash\n\t\terr  error\n\t)\n\tif ctx.NArg() == 1 {\n\t\troot, err = parseRoot(ctx.Args().First())\n\t\tif err != nil {\n\t\t\tlog.Error(\"Failed to resolve state root\", \"err\", err)\n\t\t\treturn err\n\t\t}\n\t\tlog.Info(\"Start traversing the state\", \"root\", root)\n\t} else {\n\t\troot = headBlock.Root()\n\t\tlog.Info(\"Start traversing the state\", \"root\", root, \"number\", headBlock.NumberU64())\n\t}\n\tt, err := trie.NewStateTrie(trie.StateTrieID(root), triedb)\n\tif err != nil {\n\t\tlog.Error(\"Failed to open trie\", \"root\", root, \"err\", err)\n\t\treturn err\n\t}\n\tvar (\n\t\tnodes      int\n\t\taccounts   int\n\t\tslots      int\n\t\tcodes      int\n\t\tlastReport time.Time\n\t\tstart      = time.Now()\n\t\thasher     = crypto.NewKeccakState()\n\t\tgot        = make([]byte, 32)\n\t)\n\taccIter, err := t.NodeIterator(nil)\n\tif err != nil {\n\t\tlog.Error(\"Failed to open iterator\", \"root\", root, \"err\", err)\n\t\treturn err\n\t}\n\treader, err := triedb.NodeReader(root)\n\tif err != nil {\n\t\tlog.Error(\"State is non-existent\", \"root\", root)\n\t\treturn nil\n\t}\n\tfor accIter.Next(true) {\n\t\tnodes += 1\n\t\tnode := accIter.Hash()\n\n\t\t// Check the present for non-empty hash node(embedded node doesn't\n\t\t// have their own hash).\n\t\tif node != (common.Hash{}) {\n\t\t\tblob, _ := reader.Node(common.Hash{}, accIter.Path(), node)\n\t\t\tif len(blob) == 0 {\n\t\t\t\tlog.Error(\"Missing trie node(account)\", \"hash\", node)\n\t\t\t\treturn errors.New(\"missing account\")\n\t\t\t}\n\t\t\thasher.Reset()\n\t\t\thasher.Write(blob)\n\t\t\thasher.Read(got)\n\t\t\tif !bytes.Equal(got, node.Bytes()) {\n\t\t\t\tlog.Error(\"Invalid trie node(account)\", \"hash\", node.Hex(), \"value\", blob)\n\t\t\t\treturn errors.New(\"invalid account node\")\n\t\t\t}\n\t\t}\n\t\t// If it's a leaf node, yes we are touching an account,\n\t\t// dig into the storage trie further.\n\t\tif accIter.Leaf() {\n\t\t\taccounts += 1\n\t\t\tvar acc types.StateAccount\n\t\t\tif err := rlp.DecodeBytes(accIter.LeafBlob(), &acc)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/cmd/geth/snapshot.go",
          "line": 491,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\t\tnode := storageIter.Hash()\n\n\t\t\t\t\t// Check the presence for non-empty hash node(embedded node doesn't\n\t\t\t\t\t// have their own hash).\n\t\t\t\t\tif node != (common.Hash{}) {\n\t\t\t\t\t\tblob, _ := reader.Node(common.BytesToHash(accIter.LeafKey()), storageIter.Path(), node)\n\t\t\t\t\t\tif len(blob) == 0 {\n\t\t\t\t\t\t\tlog.Error(\"Missing trie node(storage)\", \"hash\", node)\n\t\t\t\t\t\t\treturn errors.New(\"missing storage\")\n\t\t\t\t\t\t}\n\t\t\t\t\t\thasher.Reset()\n\t\t\t\t\t\thasher.Write(blob)\n\t\t\t\t\t\thasher.Read(got)\n\t\t\t\t\t\tif !bytes.Equal(got, node.Bytes()) {\n\t\t\t\t\t\t\tlog.Error(\"Invalid trie node(storage)\", \"hash\", node.Hex(), \"value\", blob)\n\t\t\t\t\t\t\treturn errors.New(\"invalid storage node\")\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\t// Bump the counter if it's leaf node.\n\t\t\t\t\tif storageIter.Leaf() {\n\t\t\t\t\t\tslots += 1\n\t\t\t\t\t}\n\t\t\t\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\t\t\t\tlog.Info(\"Traversing state\", \"nodes\", nodes, \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\t\tlastReport = time.Now()\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif storageIter.Error() != nil {\n\t\t\t\t\tlog.Error(\"Failed to traverse storage trie\", \"root\", acc.Root, \"err\", storageIter.Error())\n\t\t\t\t\treturn storageIter.Error()\n\t\t\t\t}\n\t\t\t}\n\t\t\tif !bytes.Equal(acc.CodeHash, types.EmptyCodeHash.Bytes()) {\n\t\t\t\tif !rawdb.HasCode(chaindb, common.BytesToHash(acc.CodeHash)) {\n\t\t\t\t\tlog.Error(\"Code is missing\", \"account\", common.BytesToHash(accIter.LeafKey()))\n\t\t\t\t\treturn errors.New(\"missing code\")\n\t\t\t\t}\n\t\t\t\tcodes += 1\n\t\t\t}\n\t\t\tif time.Since(lastReport) > time.Second*8 {\n\t\t\t\tlog.Info(\"Traversing state\", \"nodes\", nodes, \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\tlastReport = time.Now()\n\t\t\t}\n\t\t}\n\t}\n\tif accIter.Error() != nil {\n\t\tlog.Error(\"Failed to traverse state trie\", \"root\", root, \"err\", accIter.Error())\n\t\treturn accIter.Error()\n\t}\n\tlog.Info(\"State is complete\", \"nodes\", nodes, \"accounts\", accounts, \"slots\", slots, \"codes\", codes, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\treturn nil\n}\n\nfunc parseRoot(input string) (common.Hash, error) {\n\tvar h common.Hash\n\tif err := h.UnmarshalText([]byte(input))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/geth/dbcmd.go",
          "line": 337,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= fmt.Sprintf(\"\\t- %s\\n\", path)\n\t}\n\tfmt.Println(msg)\n\tif ctx.IsSet(removeFlagName) {\n\t\tconfirm = ctx.Bool(removeFlagName)\n\t\tif confirm {\n\t\t\tfmt.Printf(\"Remove '%s'? [y/n] y\\n\", kind)\n\t\t} else {\n\t\t\tfmt.Printf(\"Remove '%s'? [y/n] n\\n\", kind)\n\t\t}\n\t} else {\n\t\tconfirm, err = prompt.Stdin.PromptConfirm(fmt.Sprintf(\"Remove '%s'?\", kind))\n\t}\n\tswitch {\n\tcase err != nil:\n\t\tutils.Fatalf(\"%v\", err)\n\tcase !confirm:\n\t\tlog.Info(\"Database deletion skipped\", \"kind\", kind, \"paths\", paths)\n\tdefault:\n\t\tvar (\n\t\t\tdeleted []string\n\t\t\tstart   = time.Now()\n\t\t)\n\t\tfor _, path := range paths {\n\t\t\tif common.FileExist(path) {\n\t\t\t\tremoveFolder(path)\n\t\t\t\tdeleted = append(deleted, path)\n\t\t\t} else {\n\t\t\t\tlog.Info(\"Folder is not existent\", \"path\", path)\n\t\t\t}\n\t\t}\n\t\tlog.Info(\"Database successfully deleted\", \"kind\", kind, \"paths\", deleted, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t}\n}\n\nfunc inspectTrie(ctx *cli.Context) error {\n\tif ctx.NArg() < 1 {\n\t\treturn fmt.Errorf(\"required arguments: %v\", ctx.Command.ArgsUsage)\n\t}\n\n\tif ctx.NArg() > 3 {\n\t\treturn fmt.Errorf(\"Max 3 arguments: %v\", ctx.Command.ArgsUsage)\n\t}\n\n\tvar (\n\t\tblockNumber  uint64\n\t\ttrieRootHash common.Hash\n\t\tjobnum       uint64\n\t\ttopN         uint64\n\t)\n\n\tstack, _ := makeConfigNode(ctx)\n\tdefer stack.Close()\n\n\tdb := utils.MakeChainDatabase(ctx, stack, true)\n\tdefer db.Close()\n\tvar headerBlockHash common.Hash\n\tif ctx.NArg() >= 1 {\n\t\tif ctx.Args().Get(0) == \"latest\" {\n\t\t\theaderHash := rawdb.ReadHeadHeaderHash(db)\n\t\t\tblockNumber = *(rawdb.ReadHeaderNumber(db, headerHash))\n\t\t} else if ctx.Args().Get(0) == \"snapshot\" {\n\t\t\ttrieRootHash = rawdb.ReadSnapshotRoot(db)\n\t\t\tblockNumber = math.MaxUint64\n\t\t} else {\n\t\t\tvar err error\n\t\t\tblockNumber, err = strconv.ParseUint(ctx.Args().Get(0), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse blocknum, Args[0]: %v, err: %v\", ctx.Args().Get(0), err)\n\t\t\t}\n\t\t}\n\n\t\tif ctx.NArg() == 1 {\n\t\t\tjobnum = 1000\n\t\t\ttopN = 10\n\t\t} else if ctx.NArg() == 2 {\n\t\t\tvar err error\n\t\t\tjobnum, err = strconv.ParseUint(ctx.Args().Get(1), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse jobnum, Args[1]: %v, err: %v\", ctx.Args().Get(1), err)\n\t\t\t}\n\t\t\ttopN = 10\n\t\t} else {\n\t\t\tvar err error\n\t\t\tjobnum, err = strconv.ParseUint(ctx.Args().Get(1), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse jobnum, Args[1]: %v, err: %v\", ctx.Args().Get(1), err)\n\t\t\t}\n\n\t\t\ttopN, err = strconv.ParseUint(ctx.Args().Get(2), 10, 64)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"failed to parse topn, Args[1]: %v, err: %v\", ctx.Args().Get(1), err)\n\t\t\t}\n\t\t}\n\n\t\tif blockNumber != math.MaxUint64 {\n\t\t\theaderBlockHash = rawdb.ReadCanonicalHash(db, blockNumber)\n\t\t\tif headerBlockHash == (common.Hash{}) {\n\t\t\t\treturn errors.New(\"ReadHeadBlockHash empty hash\")\n\t\t\t}\n\t\t\tblockHeader := rawdb.ReadHeader(db, headerBlockHash, blockNumber)\n\t\t\ttrieRootHash = blockHeader.Root\n\t\t}\n\t\tif (trieRootHash == common.Hash{}) {\n\t\t\tlog.Error(\"Empty root hash\")\n\t\t}\n\t\tfmt.Printf(\"ReadBlockHeader, root: %v, blocknum: %v\\n\", trieRootHash, blockNumber)\n\n\t\tdbScheme := rawdb.ReadStateScheme(db)\n\t\tvar config *triedb.Config\n\t\tif dbScheme == rawdb.PathScheme {\n\t\t\tconfig = &triedb.Config{\n\t\t\t\tPathDB: utils.PathDBConfigAddJournalFilePath(stack, pathdb.ReadOnly),\n\t\t\t}\n\t\t} else if dbScheme == rawdb.HashScheme {\n\t\t\tconfig = triedb.HashDefaults\n\t\t}\n\n\t\ttriedb := triedb.NewDatabase(db, config)\n\t\ttheTrie, err := trie.New(trie.TrieID(trieRootHash), triedb)\n\t\tif err != nil {\n\t\t\tfmt.Printf(\"fail to new trie tree, err: %v, rootHash: %v\\n\", err, trieRootHash.String())\n\t\t\treturn err\n\t\t}\n\t\ttheInspect, err := trie.NewInspector(theTrie, triedb, trieRootHash, blockNumber, jobnum, int(topN))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttheInspect.Run()\n\t\ttheInspect.DisplayResult()\n\t}\n\treturn nil\n}\n\nfunc inspect(ctx *cli.Context) error {\n\tvar (\n\t\tprefix []byte\n\t\tstart  []byte\n\t)\n\tif ctx.NArg() > 2 {\n\t\treturn fmt.Errorf(\"max 2 arguments: %v\", ctx.Command.ArgsUsage)\n\t}\n\tif ctx.NArg() >= 1 {\n\t\tif d, err := hexutil.Decode(ctx.Args().Get(0))",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/rlpdump/main.go",
          "line": 237,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= n\n\treturn n, err\n}\n\nfunc (rc *inStream) ReadByte() (byte, error) {\n\tb, err := rc.br.ReadByte()\n\tif err == nil {\n\t\trc.pos++\n\t}\n\treturn b, err\n}\n\nfunc (rc *inStream) posLabel() string {\n\tl := strconv.FormatInt(int64(rc.pos), 10)\n\tif len(l) < rc.columns {\n\t\tl = strings.Repeat(\" \", rc.columns-len(l)) + l\n\t}\n\treturn l\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 315,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\n\t\t\t\t// Give the user some feedback that something is happening.\n\t\t\t\tif time.Since(reported) >= 8*time.Second {\n\t\t\t\t\tlog.Info(\"Importing Era files\", \"head\", it.Number(), \"imported\", imported, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\timported = 0\n\t\t\t\t\treported = time.Now()\n\t\t\t\t}\n\t\t\t}\n\t\t\treturn nil\n\t\t}()\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t}\n\n\treturn nil\n}\n\nfunc missingBlocks(chain *core.BlockChain, blocks []*types.Block) []*types.Block {\n\thead := chain.CurrentBlock()\n\tfor i, block := range blocks {\n\t\t// If we're behind the chain head, only check block, state is available at head\n\t\tif head.Number.Uint64() > block.NumberU64() {\n\t\t\tif !chain.HasBlock(block.Hash(), block.NumberU64()) {\n\t\t\t\treturn blocks[i:]\n\t\t\t}\n\t\t\tcontinue\n\t\t}\n\t\t// If we're above the chain head, state availability is a must\n\t\tif !chain.HasBlockAndState(block.Hash(), block.NumberU64()) {\n\t\t\treturn blocks[i:]\n\t\t}\n\t}\n\treturn nil\n}\n\n// ExportChain exports a blockchain into the specified file, truncating any data\n// already present in the file.\nfunc ExportChain(blockchain *core.BlockChain, fn string) error {\n\tlog.Info(\"Exporting blockchain\", \"file\", fn)\n\n\t// Open the file handle and potentially wrap with a gzip stream\n\tfh, err := os.OpenFile(fn, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.ModePerm)\n\tif err != nil {\n\t\treturn err\n\t}\n\tdefer fh.Close()\n\n\tvar writer io.Writer = fh\n\tif strings.HasSuffix(fn, \".gz\") {\n\t\twriter = gzip.NewWriter(writer)\n\t\tdefer writer.(*gzip.Writer).Close()\n\t}\n\t// Iterate over the blocks and export them\n\tif err := blockchain.Export(writer)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 425,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= step {\n\t\terr := func() error {\n\t\t\tfilename := filepath.Join(dir, era.Filename(network, int(i/step), common.Hash{}))\n\t\t\tf, err := os.Create(filename)\n\t\t\tif err != nil {\n\t\t\t\treturn fmt.Errorf(\"could not create era file: %w\", err)\n\t\t\t}\n\t\t\tdefer f.Close()\n\n\t\t\tw := era.NewBuilder(f)\n\t\t\tfor j := uint64(0)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 616,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\thashCh <- hashAndPreimageSize{Hash: accIt.Hash(), Size: common.AddressLength}\n\n\t\t\tif acc.Root != (common.Hash{}) && acc.Root != types.EmptyRootHash {\n\t\t\t\tstIt, err := snaptree.StorageIterator(root, accIt.Hash(), common.Hash{})\n\t\t\t\tif err != nil {\n\t\t\t\t\tlog.Error(\"Failed to create storage iterator\", \"error\", err)\n\t\t\t\t\treturn\n\t\t\t\t}\n\t\t\t\tfor stIt.Next() {\n\t\t\t\t\tpreimages += 1\n\t\t\t\t\thashCh <- hashAndPreimageSize{Hash: stIt.Hash(), Size: common.HashLength}\n\n\t\t\t\t\tif time.Since(logged) > time.Second*8 {\n\t\t\t\t\t\tlogged = time.Now()\n\t\t\t\t\t\tlog.Info(\"Exporting preimages\", \"count\", preimages, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tstIt.Release()\n\t\t\t}\n\t\t\tif time.Since(logged) > time.Second*8 {\n\t\t\t\tlogged = time.Now()\n\t\t\t\tlog.Info(\"Exporting preimages\", \"count\", preimages, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor item := range hashCh {\n\t\tpreimage := rawdb.ReadPreimage(chaindb, item.Hash)\n\t\tif len(preimage) == 0 {\n\t\t\treturn fmt.Errorf(\"missing preimage for %v\", item.Hash)\n\t\t}\n\t\tif len(preimage) != item.Size {\n\t\t\treturn fmt.Errorf(\"invalid preimage size, have %d\", len(preimage))\n\t\t}\n\t\trlpenc, err := rlp.EncodeToBytes(preimage)\n\t\tif err != nil {\n\t\t\treturn fmt.Errorf(\"error encoding preimage: %w\", err)\n\t\t}\n\t\tif _, err := writer.Write(rlpenc)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/cmd/utils/cmd.go",
          "line": 773,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t}\n\t// Flush the last batch snapshot data\n\tif batch.ValueSize() > 0 {\n\t\tif err := batch.Write()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/utils/flags.go",
          "line": 2144,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= cfg.SnapshotCache\n\t\t\tcfg.SnapshotCache = 0 // Disabled\n\t\t}\n\t}\n\tif ctx.IsSet(VMEnableDebugFlag.Name) {\n\t\tcfg.EnablePreimageRecording = ctx.Bool(VMEnableDebugFlag.Name)\n\t}\n\n\tif ctx.IsSet(RPCGlobalGasCapFlag.Name) {\n\t\tcfg.RPCGasCap = ctx.Uint64(RPCGlobalGasCapFlag.Name)\n\t}\n\tif cfg.RPCGasCap != 0 {\n\t\tlog.Info(\"Set global gas cap\", \"cap\", cfg.RPCGasCap)\n\t} else {\n\t\tlog.Info(\"Global gas cap disabled\")\n\t}\n\tif ctx.IsSet(RPCGlobalEVMTimeoutFlag.Name) {\n\t\tcfg.RPCEVMTimeout = ctx.Duration(RPCGlobalEVMTimeoutFlag.Name)\n\t}\n\tif ctx.IsSet(RPCGlobalTxFeeCapFlag.Name) {\n\t\tcfg.RPCTxFeeCap = ctx.Float64(RPCGlobalTxFeeCapFlag.Name)\n\t}\n\tif ctx.IsSet(NoDiscoverFlag.Name) {\n\t\tcfg.EthDiscoveryURLs, cfg.SnapDiscoveryURLs, cfg.BscDiscoveryURLs = []string{}, []string{}, []string{}\n\t} else if ctx.IsSet(DNSDiscoveryFlag.Name) {\n\t\turls := ctx.String(DNSDiscoveryFlag.Name)\n\t\tif urls == \"\" {\n\t\t\tcfg.EthDiscoveryURLs = []string{}\n\t\t} else {\n\t\t\tcfg.EthDiscoveryURLs = SplitAndTrim(urls)\n\t\t}\n\t}\n\t// Override any default configs for hard coded networks.\n\tswitch {\n\tcase ctx.Bool(BSCMainnetFlag.Name):\n\t\tif !ctx.IsSet(NetworkIdFlag.Name) {\n\t\t\tcfg.NetworkId = 56\n\t\t}\n\t\tcfg.Genesis = core.DefaultBSCGenesisBlock()\n\t\tSetDNSDiscoveryDefaults(cfg, params.BSCGenesisHash)\n\tcase ctx.Bool(ChapelFlag.Name) || cfg.NetworkId == 97:\n\t\tif !ctx.IsSet(NetworkIdFlag.Name) {\n\t\t\tcfg.NetworkId = 97\n\t\t}\n\t\tcfg.Genesis = core.DefaultChapelGenesisBlock()\n\t\tSetDNSDiscoveryDefaults(cfg, params.ChapelGenesisHash)\n\tcase ctx.Bool(DeveloperFlag.Name):\n\t\tcfg.NetworkId = 1337\n\t\tcfg.SyncMode = ethconfig.FullSync\n\t\tcfg.EnablePreimageRecording = true\n\t\t// Create new developer account or reuse existing one\n\t\tvar (\n\t\t\tdeveloper  accounts.Account\n\t\t\tpassphrase string\n\t\t\terr        error\n\t\t)\n\t\tif list := MakePasswordList(ctx)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/utils/export_test.go",
          "line": 53,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif iter.index == 42 {\n\t\titer.index += 1\n\t}\n\treturn OpBatchAdd, fmt.Appendf(nil, \"key-%04d\", iter.index),\n\t\tfmt.Appendf(nil, \"value %d\", iter.index), true\n}\n\nfunc (iter *testIterator) Release() {}\n\nfunc testExport(t *testing.T, f string) {\n\terr := ExportChaindata(f, \"testdata\", newTestIterator(), make(chan struct{}))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdb := rawdb.NewMemoryDatabase()\n\terr = ImportLDBData(db, f, 5, make(chan struct{}))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\t// verify\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/utils/export_test.go",
          "line": 119,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\tif iter.index == 42 {\n\t\titer.index += 1\n\t}\n\treturn OpBatchDel, fmt.Appendf(nil, \"key-%04d\", iter.index), nil, true\n}\n\nfunc (iter *deletionIterator) Release() {}\n\nfunc testDeletion(t *testing.T, f string) {\n\terr := ExportChaindata(f, \"testdata\", newDeletionIterator(), make(chan struct{}))\n\tif err != nil {\n\t\tt.Fatal(err)\n\t}\n\tdb := rawdb.NewMemoryDatabase()\n\tfor i := 0",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/maliciousvote-submit/slash_indicator.go",
          "line": 165,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/maliciousvote-submit/slash_indicator.go",
          "line": 184,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/evm/runner.go",
          "line": 330,
          "category": "unchecked_calls",
          "pattern": "\\.call\\s*\\(",
          "match": ".Call(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/evm/reporter.go",
          "line": 64,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= \"\\n\" + string(state)\n\t}\n\treturn out\n}\n\n// report prints the after-test summary.\nfunc report(ctx *cli.Context, results []testResult) {\n\tif ctx.Bool(HumanReadableFlag.Name) {\n\t\tpass := 0\n\t\tfor _, r := range results {\n\t\t\tif r.Pass {\n\t\t\t\tpass++\n\t\t\t}\n\t\t}\n\t\tfor _, r := range results {\n\t\t\tfmt.Println(r)\n\t\t}\n\t\tfmt.Println(\"--\")\n\t\tfmt.Printf(\"%d tests passed, %d tests failed.\\n\", pass, len(results)-pass)\n\t\treturn\n\t}\n\tout, _ := json.MarshalIndent(results, \"\", \"  \")\n\tfmt.Println(string(out))\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/workload/filtertestgen.go",
          "line": 172,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= d\n\t\t} else {\n\t\t\textBefore = q.FromBlock\n\t\t}\n\t}\n\treturn &filterQuery{\n\t\tFromBlock: q.FromBlock - extBefore,\n\t\tToBlock:   q.ToBlock + extAfter,\n\t\tAddress:   q.Address,\n\t\tTopics:    q.Topics,\n\t}\n}\n\n// newQuery generates a new filter query.\nfunc (s *filterTestGen) newQuery() *filterQuery {\n\tfor {\n\t\tt := rand.Intn(100)\n\t\tif t < filterSeedChance {\n\t\t\treturn s.newSeedQuery()\n\t\t}\n\t\tif t < filterSeedChance+filterMergeChance {\n\t\t\tif query := s.newMergedQuery()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/workload/filtertestgen.go",
          "line": 170,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= d\n\t\tif extBefore+d <= q.FromBlock {\n\t\t\textBefore += d\n\t\t} else {\n\t\t\textBefore = q.FromBlock\n\t\t}\n\t}\n\treturn &filterQuery{\n\t\tFromBlock: q.FromBlock - extBefore,\n\t\tToBlock:   q.ToBlock + extAfter,\n\t\tAddress:   q.Address,\n\t\tTopics:    q.Topics,\n\t}\n}\n\n// newQuery generates a new filter query.\nfunc (s *filterTestGen) newQuery() *filterQuery {\n\tfor {\n\t\tt := rand.Intn(100)\n\t\tif t < filterSeedChance {\n\t\t\treturn s.newSeedQuery()\n\t\t}\n\t\tif t < filterSeedChance+filterMergeChance {\n\t\t\tif query := s.newMergedQuery()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/workload/historytestgen.go",
          "line": 82,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= stride {\n\t\ttest.BlockNumbers = append(test.BlockNumbers, b)\n\t}\n\n\t// Get blocks and assign block info into the test\n\tfmt.Println(\"Fetching blocks\")\n\tblocks := make([]*types.Block, len(test.BlockNumbers))\n\tfor i, blocknum := range test.BlockNumbers {\n\t\tb, err := client.Eth.BlockByNumber(ctx, new(big.Int).SetUint64(blocknum))\n\t\tif err != nil {\n\t\t\texit(fmt.Errorf(\"error fetching block %d: %v\", blocknum, err))\n\t\t}\n\t\tblocks[i] = b\n\t}\n\ttest.BlockHashes = make([]common.Hash, len(blocks))\n\ttest.TxCounts = make([]int, len(blocks))\n\tfor i, block := range blocks {\n\t\ttest.BlockHashes[i] = block.Hash()\n\t\ttest.TxCounts[i] = len(block.Transactions())\n\t}\n\n\t// Fill tx index.\n\ttest.TxHashIndex = make([]int, len(blocks))\n\ttest.TxHashes = make([]*common.Hash, len(blocks))\n\tfor i, block := range blocks {\n\t\ttxs := block.Transactions()\n\t\tif len(txs) == 0 {\n\t\t\tcontinue\n\t\t}\n\t\tindex := len(txs) / 2\n\t\ttxhash := txs[index].Hash()\n\t\ttest.TxHashIndex[i] = index\n\t\ttest.TxHashes[i] = &txhash\n\t}\n\n\t// Get receipts.\n\tfmt.Println(\"Fetching receipts\")\n\ttest.ReceiptsHashes = make([]common.Hash, len(blocks))\n\tfor i, blockHash := range test.BlockHashes {\n\t\treceipts, err := client.getBlockReceipts(ctx, blockHash)\n\t\tif err != nil {\n\t\t\texit(fmt.Errorf(\"error fetching block %v receipts: %v\", blockHash, err))\n\t\t}\n\t\ttest.ReceiptsHashes[i] = calcReceiptsHash(receipts)\n\t}\n\n\t// Write output file.\n\twriteJSON(outputFile, test)\n\treturn nil\n}\n\nfunc calcReceiptsHash(rcpt []*types.Receipt) common.Hash {\n\th := crypto.NewKeccakState()\n\trlp.Encode(h, rcpt)\n\treturn common.Hash(h.Sum(nil))\n}\n\nfunc writeJSON(fileName string, value any) {\n\tfile, err := os.Create(fileName)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"error creating %s: %v\", fileName, err))\n\t\treturn\n\t}\n\tdefer file.Close()\n\tjson.NewEncoder(file).Encode(value)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/workload/tracetestgen.go",
          "line": 119,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= 1\n\t\t\t\tbreak\n\t\t\t}\n\t\t\tblob, err := json.Marshal(result)\n\t\t\tif err != nil {\n\t\t\t\tfailed += 1\n\t\t\t\tbreak\n\t\t\t}\n\t\t\ttest.TxHashes = append(test.TxHashes, tx.Hash())\n\t\t\ttest.TraceConfigs = append(test.TraceConfigs, *config)\n\t\t\ttest.ResultHashes = append(test.ResultHashes, crypto.Keccak256Hash(blob))\n\t\t\twriteTraceResult(outputDir, tx.Hash(), result, configName)\n\t\t}\n\t\tif time.Since(logged) > time.Second*8 {\n\t\t\tlogged = time.Now()\n\t\t\tlog.Info(\"Tracing transactions\", \"executed\", len(test.TxHashes), \"failed\", failed, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\t\t}\n\t}\n\tlog.Info(\"Traced transactions\", \"executed\", len(test.TxHashes), \"failed\", failed, \"elapsed\", common.PrettyDuration(time.Since(start)))\n\n\t// Write output file.\n\twriteJSON(outputFile, test)\n\treturn nil\n}\n\nfunc randomTraceOption() (*tracers.TraceConfig, string) {\n\tx := rand.Intn(11)\n\tif x == 0 {\n\t\t// struct-logger, with all fields enabled, very heavy\n\t\treturn &tracers.TraceConfig{\n\t\t\tConfig: &logger.Config{\n\t\t\t\tEnableMemory:     true,\n\t\t\t\tEnableReturnData: true,\n\t\t\t},\n\t\t}, \"structAll\"\n\t}\n\tif x == 1 {\n\t\t// default options for struct-logger, with stack and storage capture\n\t\t// enabled\n\t\treturn &tracers.TraceConfig{\n\t\t\tConfig: &logger.Config{},\n\t\t}, \"structDefault\"\n\t}\n\tif x == 2 || x == 3 || x == 4 {\n\t\t// struct-logger with storage capture enabled\n\t\treturn &tracers.TraceConfig{\n\t\t\tConfig: &logger.Config{\n\t\t\t\tDisableStack: true,\n\t\t\t},\n\t\t}, \"structStorage\"\n\t}\n\t// Native tracer\n\tloggers := []string{\"callTracer\", \"4byteTracer\", \"flatCallTracer\", \"muxTracer\", \"noopTracer\", \"prestateTracer\"}\n\treturn &tracers.TraceConfig{\n\t\tTracer: &loggers[x-5],\n\t}, loggers[x-5]\n}\n\nfunc writeTraceResult(dir string, hash common.Hash, result any, configName string) {\n\tif dir == \"\" {\n\t\treturn\n\t}\n\tname := filepath.Join(dir, configName+\"_\"+hash.String())\n\tfile, err := os.Create(name)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"error creating %s: %v\", name, err))\n\t\treturn\n\t}\n\tdefer file.Close()\n\n\tdata, _ := json.MarshalIndent(result, \"\", \"    \")\n\t_, err = file.Write(data)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"error writing %s: %v\", name, err))\n\t\treturn\n\t}\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/workload/filtertest.go",
          "line": 166,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= len(bucket)\n\t}\n\tif count == 0 {\n\t\treturn fmt.Errorf(\"filterQueryFile %s is empty\", s.cfg.filterQueryFile)\n\t}\n\ts.queries = queries\n\treturn nil\n}\n\n// filterQuery is a single query for testing.\ntype filterQuery struct {\n\tFromBlock  int64            `json:\"fromBlock\"`\n\tToBlock    int64            `json:\"toBlock\"`\n\tAddress    []common.Address `json:\"address\"`\n\tTopics     [][]common.Hash  `json:\"topics\"`\n\tResultHash *common.Hash     `json:\"resultHash,omitempty\"`\n\tresults    []types.Log\n\tErr        error `json:\"error,omitempty\"`\n}\n\nfunc (fq *filterQuery) isWildcard() bool {\n\tif len(fq.Address) != 0 {\n\t\treturn false\n\t}\n\tfor _, topics := range fq.Topics {\n\t\tif len(topics) != 0 {\n\t\t\treturn false\n\t\t}\n\t}\n\treturn true\n}\n\nfunc (fq *filterQuery) calculateHash() common.Hash {\n\tenc, err := rlp.EncodeToBytes(&fq.results)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"Error encoding logs: %v\", err))\n\t}\n\treturn crypto.Keccak256Hash(enc)\n}\n\nfunc (fq *filterQuery) run(client *client, historyPruneBlock *uint64) {\n\tctx, cancel := context.WithTimeout(context.Background(), time.Second*30)\n\tdefer cancel()\n\tlogs, err := client.Eth.FilterLogs(ctx, ethereum.FilterQuery{\n\t\tFromBlock: big.NewInt(fq.FromBlock),\n\t\tToBlock:   big.NewInt(fq.ToBlock),\n\t\tAddresses: fq.Address,\n\t\tTopics:    fq.Topics,\n\t})\n\tfq.results = logs\n\tfq.Err = validateHistoryPruneErr(err, uint64(fq.FromBlock), historyPruneBlock)\n}\n\nfunc (fq *filterQuery) printError() {\n\tfmt.Printf(\"Filter query failed: fromBlock: %d toBlock: %d addresses: %v topics: %v error: %v\\n\",\n\t\tfq.FromBlock, fq.ToBlock, fq.Address, fq.Topics, fq.Err)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/workload/filtertestperf.go",
          "line": 115,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= qt.query.ToBlock + 1 - qt.query.FromBlock\n\t\tbs.count++\n\t\tbs.logs += len(qt.query.results)\n\t\tbs.runtime += qt.medianTime\n\t}\n\n\tfmt.Println()\n\tfor i := range stats {\n\t\tstats[i].print(fmt.Sprintf(\"bucket #%d\", i+1))\n\t}\n\twildcardStats.print(\"wild card queries\")\n\tfmt.Println()\n\tsort.Slice(queries, func(i, j int) bool {\n\t\treturn queries[i].medianTime > queries[j].medianTime\n\t})\n\tfor i, q := range queries {\n\t\tif i >= 10 {\n\t\t\tbreak\n\t\t}\n\t\tfmt.Printf(\"Most expensive query #%-2d   median runtime: %13v  max runtime: %13v  result count: %4d  fromBlock: %9d  toBlock: %9d  addresses: %v  topics: %v\\n\",\n\t\t\ti+1, q.medianTime, q.runtime[len(q.runtime)-1], len(q.query.results), q.query.FromBlock, q.query.ToBlock, q.query.Address, q.query.Topics)\n\t}\n\twriteErrors(ctx.String(filterErrorFileFlag.Name), errors)\n\treturn nil\n}\n\ntype bucketStats struct {\n\tblocks      int64\n\tcount, logs int\n\truntime     time.Duration\n}\n\nfunc (st *bucketStats) print(name string) {\n\tif st.count == 0 {\n\t\treturn\n\t}\n\tfmt.Printf(\"%-20s queries: %4d  average block length: %12.2f  average log count: %7.2f  average runtime: %13v\\n\",\n\t\tname, st.count, float64(st.blocks)/float64(st.count), float64(st.logs)/float64(st.count), st.runtime/time.Duration(st.count))\n}\n\n// writeQueries serializes the generated errors to the error file.\nfunc writeErrors(errorFile string, errors []*filterQuery) {\n\tfile, err := os.Create(errorFile)\n\tif err != nil {\n\t\texit(fmt.Errorf(\"Error creating filter error file %s: %v\", errorFile, err))\n\t\treturn\n\t}\n\tdefer file.Close()\n\tjson.NewEncoder(file).Encode(errors)\n}\n",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/evm/internal/t8ntool/execution.go",
          "line": 277,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= txBlobGas\n\t\tgasUsed += msgResult.UsedGas\n\n\t\t// Receipt:\n\t\t{\n\t\t\tvar root []byte\n\t\t\tif chainConfig.IsByzantium(vmContext.BlockNumber) {\n\t\t\t\tstatedb.Finalise(true)\n\t\t\t} else {\n\t\t\t\troot = statedb.IntermediateRoot(chainConfig.IsEIP158(vmContext.BlockNumber)).Bytes()\n\t\t\t}\n\n\t\t\t// Create a new receipt for the transaction, storing the intermediate root and\n\t\t\t// gas used by the tx.\n\t\t\treceipt := &types.Receipt{Type: tx.Type(), PostState: root, CumulativeGasUsed: gasUsed}\n\t\t\tif msgResult.Failed() {\n\t\t\t\treceipt.Status = types.ReceiptStatusFailed\n\t\t\t} else {\n\t\t\t\treceipt.Status = types.ReceiptStatusSuccessful\n\t\t\t}\n\t\t\treceipt.TxHash = tx.Hash()\n\t\t\treceipt.GasUsed = msgResult.UsedGas\n\n\t\t\t// If the transaction created a contract, store the creation address in the receipt.\n\t\t\tif msg.To == nil {\n\t\t\t\treceipt.ContractAddress = crypto.CreateAddress(evm.TxContext.Origin, tx.Nonce())\n\t\t\t}\n\n\t\t\t// Set the receipt logs and create the bloom filter.\n\t\t\treceipt.Logs = statedb.GetLogs(tx.Hash(), vmContext.BlockNumber.Uint64(), blockHash, vmContext.Time)\n\t\t\treceipt.Bloom = types.CreateBloom(receipt)\n\n\t\t\t// These three are non-consensus fields:\n\t\t\t//receipt.BlockHash\n\t\t\t//receipt.BlockNumber\n\t\t\treceipt.TransactionIndex = uint(txIndex)\n\t\t\treceipts = append(receipts, receipt)\n\t\t\tif evm.Config.Tracer != nil && evm.Config.Tracer.OnTxEnd != nil {\n\t\t\t\tevm.Config.Tracer.OnTxEnd(receipt, nil)\n\t\t\t}\n\t\t}\n\n\t\ttxIndex++\n\t}\n\tstatedb.IntermediateRoot(chainConfig.IsEIP158(vmContext.BlockNumber))\n\t// Add mining reward? (-1 means rewards are disabled)\n\tif miningReward >= 0 {\n\t\t// Add mining reward. The mining reward may be `0`, which only makes a difference in the cases\n\t\t// where\n\t\t// - the coinbase self-destructed, or\n\t\t// - there are only 'bad' transactions, which aren't executed. In those cases,\n\t\t//   the coinbase gets no txfee, so isn't created, and thus needs to be touched\n\t\tvar (\n\t\t\tblockReward = big.NewInt(miningReward)\n\t\t\tminerReward = new(big.Int).Set(blockReward)\n\t\t\tperOmmer    = new(big.Int).Rsh(blockReward, 5)\n\t\t)\n\t\tfor _, ommer := range pre.Env.Ommers {\n\t\t\t// Add 1/32th for each ommer included\n\t\t\tminerReward.Add(minerReward, perOmmer)\n\t\t\t// Add (8-delta)/8\n\t\t\treward := big.NewInt(8)\n\t\t\treward.Sub(reward, new(big.Int).SetUint64(ommer.Delta))\n\t\t\treward.Mul(reward, blockReward)\n\t\t\treward.Rsh(reward, 3)\n\t\t\tstatedb.AddBalance(ommer.Address, uint256.MustFromBig(reward), tracing.BalanceIncreaseRewardMineUncle)\n\t\t}\n\t\tstatedb.AddBalance(pre.Env.Coinbase, uint256.MustFromBig(minerReward), tracing.BalanceIncreaseRewardMineBlock)\n\t}\n\t// Apply withdrawals\n\tfor _, w := range pre.Env.Withdrawals {\n\t\t// Amount is in gwei, turn into wei\n\t\tamount := new(big.Int).Mul(new(big.Int).SetUint64(w.Amount), big.NewInt(params.GWei))\n\t\tstatedb.AddBalance(w.Address, uint256.MustFromBig(amount), tracing.BalanceIncreaseWithdrawal)\n\t}\n\n\t// Gather the execution-layer triggered requests.\n\tvar requests [][]byte\n\tif chainConfig.IsPrague(vmContext.BlockNumber, vmContext.Time) {\n\t\trequests = [][]byte{}\n\t\t// EIP-6110\n\t\tvar allLogs []*types.Log\n\t\tfor _, receipt := range receipts {\n\t\t\tallLogs = append(allLogs, receipt.Logs...)\n\t\t}\n\t\tif err := core.ParseDepositLogs(&requests, allLogs, chainConfig)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/evm/internal/t8ntool/execution.go",
          "line": 221,
          "category": "mathematical_inconsistencies",
          "pattern": "blockhash\\([^)]*\\)",
          "match": "BlockHash(prevHash, evm)",
          "severity": "CRITICAL",
          "model": "ensemble_single",
          "mathematical_score": 0.8999999999999999,
          "confidence": 0.981,
          "ensemble_confidence": 0.8829
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/internal/ethtest/suite.go",
          "line": 896,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= 1\n\t\t}\n\t\tinner := &types.BlobTx{\n\t\t\tChainID:    uint256.MustFromBig(s.chain.config.ChainID),\n\t\t\tNonce:      nonce + uint64(i),\n\t\t\tGasTipCap:  uint256.NewInt(1),\n\t\t\tGasFeeCap:  uint256.MustFromBig(s.chain.Head().BaseFee()),\n\t\t\tGas:        100000,\n\t\t\tBlobFeeCap: uint256.MustFromBig(eip4844.CalcBlobFee(s.chain.config, s.chain.Head().Header())),\n\t\t\tBlobHashes: makeSidecar(blobdata...).BlobHashes(),\n\t\t\tSidecar:    makeSidecar(blobdata...),\n\t\t}\n\t\ttx, err := s.chain.SignTx(from, types.NewTx(inner))\n\t\tif err != nil {\n\t\t\tpanic(\"blob tx signing failed\")\n\t\t}\n\t\ttxs = append(txs, tx)\n\t}\n\treturn txs\n}\n\nfunc (s *Suite) TestBlobViolations(t *utesting.T) {\n\tt.Log(`This test sends some invalid blob tx announcements and expects the node to disconnect.`)\n\n\tif err := s.engine.sendForkchoiceUpdated()",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/internal/ethtest/chain.go",
          "line": 180,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= amt\n}\n\n// Balance returns the balance of an account at the head of the chain.\nfunc (c *Chain) Balance(addr common.Address) *big.Int {\n\tbal := new(big.Int)\n\tif acc, ok := c.state[addr]",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/devp2p/internal/ethtest/chain.go",
          "line": 230,
          "category": "overflow",
          "pattern": "\\+\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "+= (1 + req.Skip)\n\t\theaders[i] = c.blocks[blockNumber].Header()\n\t}\n\treturn headers, nil\n}\n\n// Shorten returns a copy chain of a desired height from the imported\nfunc (c *Chain) Shorten(height int) *Chain {\n\tblocks := make([]*types.Block, height)\n\tcopy(blocks, c.blocks[:height])\n\n\tconfig := *c.config\n\treturn &Chain{\n\t\tblocks: blocks,\n\t\tconfig: &config,\n\t}\n}\n\nfunc loadGenesis(genesisFile string) (core.Genesis, error) {\n\tchainConfig, err := os.ReadFile(genesisFile)\n\tif err != nil {\n\t\treturn core.Genesis{}, err\n\t}\n\tvar gen core.Genesis\n\tif err := json.Unmarshal(chainConfig, &gen)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/cmd/devp2p/internal/ethtest/chain.go",
          "line": 224,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= (1 - req.Skip)\n\t\t\theaders[i] = c.blocks[blockNumber].Header()\n\t\t}\n\t\treturn headers, nil\n\t}\n\tfor i := 1",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/internal/ethtest/conn.go",
          "line": 154,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= baseProtoLen\n\n\t\tvar msg any\n\t\tswitch int(code) {\n\t\tcase eth.StatusMsg:\n\t\t\tmsg = new(eth.StatusPacket68)\n\t\tcase eth.GetBlockHeadersMsg:\n\t\t\tmsg = new(eth.GetBlockHeadersPacket)\n\t\tcase eth.BlockHeadersMsg:\n\t\t\tmsg = new(eth.BlockHeadersPacket)\n\t\tcase eth.GetBlockBodiesMsg:\n\t\t\tmsg = new(eth.GetBlockBodiesPacket)\n\t\tcase eth.BlockBodiesMsg:\n\t\t\tmsg = new(eth.BlockBodiesPacket)\n\t\tcase eth.NewBlockMsg:\n\t\t\tmsg = new(eth.NewBlockPacket)\n\t\tcase eth.NewBlockHashesMsg:\n\t\t\tmsg = new(eth.NewBlockHashesPacket)\n\t\tcase eth.TransactionsMsg:\n\t\t\tmsg = new(eth.TransactionsPacket)\n\t\tcase eth.NewPooledTransactionHashesMsg:\n\t\t\tmsg = new(eth.NewPooledTransactionHashesPacket)\n\t\tcase eth.GetPooledTransactionsMsg:\n\t\t\tmsg = new(eth.GetPooledTransactionsPacket)\n\t\tcase eth.PooledTransactionsMsg:\n\t\t\tmsg = new(eth.PooledTransactionsPacket)\n\t\tdefault:\n\t\t\tpanic(fmt.Sprintf(\"unhandled eth msg code %d\", code))\n\t\t}\n\t\tif err := rlp.DecodeBytes(data, msg)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/devp2p/internal/ethtest/conn.go",
          "line": 202,
          "category": "overflow",
          "pattern": "-\\s*=\\s*[^;]*(?!SafeMath)",
          "match": "-= baseProtoLen + ethProtoLen\n\n\t\tvar msg any\n\t\tswitch int(code) {\n\t\tcase snap.GetAccountRangeMsg:\n\t\t\tmsg = new(snap.GetAccountRangePacket)\n\t\tcase snap.AccountRangeMsg:\n\t\t\tmsg = new(snap.AccountRangePacket)\n\t\tcase snap.GetStorageRangesMsg:\n\t\t\tmsg = new(snap.GetStorageRangesPacket)\n\t\tcase snap.StorageRangesMsg:\n\t\t\tmsg = new(snap.StorageRangesPacket)\n\t\tcase snap.GetByteCodesMsg:\n\t\t\tmsg = new(snap.GetByteCodesPacket)\n\t\tcase snap.ByteCodesMsg:\n\t\t\tmsg = new(snap.ByteCodesPacket)\n\t\tcase snap.GetTrieNodesMsg:\n\t\t\tmsg = new(snap.GetTrieNodesPacket)\n\t\tcase snap.TrieNodesMsg:\n\t\t\tmsg = new(snap.TrieNodesPacket)\n\t\tdefault:\n\t\t\tpanic(fmt.Errorf(\"unhandled snap code: %d\", code))\n\t\t}\n\t\tif err := rlp.DecodeBytes(data, msg)",
          "severity": "HIGH",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 77,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 147,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0002",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 164,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0003",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 183,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0004",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 212,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0005",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 224,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0006",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 242,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0007",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 264,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0008",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 300,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0009",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 324,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0010",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 355,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0011",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 358,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0012",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 385,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0013",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 399,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0014",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 414,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0015",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 441,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0016",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 457,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0017",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 472,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0018",
          "file": "bsc/cmd/devp2p/internal/v4test/discv4tests.go",
          "line": 495,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/influxdb/influxdbv2.go",
          "line": 67,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/influxdb/influxdb_test.go",
          "line": 62,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/metrics/influxdb/influxdb_test.go",
          "line": 99,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0000",
          "file": "bsc/metrics/influxdb/influxdbv1.go",
          "line": 82,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        },
        {
          "id": "ENSEMBLE_0001",
          "file": "bsc/metrics/influxdb/influxdbv1.go",
          "line": 110,
          "category": "unchecked_calls",
          "pattern": "\\.send\\s*\\(",
          "match": ".send(",
          "severity": "MEDIUM",
          "model": "ensemble_single",
          "confidence": 0.85,
          "ensemble_confidence": 0.765
        }
      ],
      "stats": {
        "total_vulnerabilities": 1106,
        "confidence": 0.8708501808318264,
        "severity_distribution": {
          "HIGH": 554,
          "CRITICAL": 270,
          "MEDIUM": 282
        },
        "analysis_time": 2.4758639335632324,
        "files_processed": 1441
      }
    }
  },
  "comparative_analysis": {
    "detection_improvement": {
      "omega_vs_classical": "-68.1%",
      "ensemble_vs_classical": "+27.9%"
    },
    "confidence_comparison": {
      "classical": "0.850",
      "omega": "0.986",
      "ensemble": "0.792"
    },
    "mathematical_singularity_advantage": {
      "omega_unique_detections": -589,
      "mathematical_score_average": 0.9556884057971013
    }
  },
  "performance_metrics": {}
}