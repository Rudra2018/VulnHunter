#!/usr/bin/env python3
"""
Deploy VulnHunter Models to Vertex AI using existing Google Cloud account
This script detects your current GCP project and credentials automatically
"""

import os
import sys
import json
import subprocess
import logging
from datetime import datetime
from pathlib import Path

# Check for required libraries
try:
    from google.cloud import aiplatform as aip
    from google.cloud import storage
    from google.auth import default
    import joblib
    DEPENDENCIES_OK = True
except ImportError as e:
    DEPENDENCIES_OK = False
    print(f"Missing dependencies: {e}")
    print("Install with: pip install google-cloud-aiplatform google-cloud-storage joblib")

class ExistingAccountDeployer:
    """Deploy to Vertex AI using existing Google Cloud account"""

    def __init__(self):
        self.logger = self._setup_logging()
        self.project_id = None
        self.region = "us-central1"  # Default, will be configurable
        self.bucket_name = None

        self.logger.info("üöÄ VulnHunter Existing Account Deployer")

    def _setup_logging(self):
        """Setup logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.StreamHandler(sys.stdout),
                logging.FileHandler(f'existing_account_deploy_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
            ]
        )
        return logging.getLogger('ExistingAccountDeployer')

    def detect_gcp_environment(self):
        """Detect current GCP environment and credentials"""
        self.logger.info("üîç Detecting Google Cloud environment...")

        try:
            # Try to get default credentials
            credentials, project_id = default()
            if project_id:
                self.project_id = project_id
                self.logger.info(f"‚úÖ Found project: {self.project_id}")
            else:
                # Try to get project from gcloud config
                result = subprocess.run(['gcloud', 'config', 'get-value', 'project'],
                                      capture_output=True, text=True)
                if result.returncode == 0 and result.stdout.strip():
                    self.project_id = result.stdout.strip()
                    self.logger.info(f"‚úÖ Found project from gcloud: {self.project_id}")

        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è  Could not detect credentials automatically: {e}")

        if not self.project_id:
            # Ask user for project ID
            self.project_id = input("Enter your Google Cloud Project ID: ").strip()
            if not self.project_id:
                raise ValueError("Project ID is required")

        # Set bucket name based on project
        self.bucket_name = f"{self.project_id}-vulnhunter-models"

        self.logger.info(f"üìä Configuration:")
        self.logger.info(f"   Project ID: {self.project_id}")
        self.logger.info(f"   Region: {self.region}")
        self.logger.info(f"   Bucket: {self.bucket_name}")

        return True

    def check_required_apis(self):
        """Check if required APIs are enabled"""
        self.logger.info("üîß Checking required APIs...")

        required_apis = [
            'aiplatform.googleapis.com',
            'storage.googleapis.com'
        ]

        try:
            for api in required_apis:
                result = subprocess.run([
                    'gcloud', 'services', 'list', '--enabled',
                    '--filter', f'name:{api}',
                    '--format', 'value(name)'
                ], capture_output=True, text=True)

                if api in result.stdout:
                    self.logger.info(f"‚úÖ {api} is enabled")
                else:
                    self.logger.info(f"üîÑ Enabling {api}...")
                    enable_result = subprocess.run([
                        'gcloud', 'services', 'enable', api
                    ], capture_output=True, text=True)

                    if enable_result.returncode == 0:
                        self.logger.info(f"‚úÖ Enabled {api}")
                    else:
                        self.logger.error(f"‚ùå Failed to enable {api}: {enable_result.stderr}")
                        return False

        except FileNotFoundError:
            self.logger.warning("‚ö†Ô∏è  gcloud CLI not found - assuming APIs are enabled")
        except Exception as e:
            self.logger.error(f"‚ùå Error checking APIs: {e}")
            return False

        return True

    def create_storage_bucket(self):
        """Create GCS bucket for model artifacts"""
        self.logger.info(f"üì¶ Creating storage bucket: {self.bucket_name}")

        try:
            storage_client = storage.Client(project=self.project_id)

            # Check if bucket exists
            try:
                bucket = storage_client.bucket(self.bucket_name)
                bucket.reload()
                self.logger.info(f"‚úÖ Bucket {self.bucket_name} already exists")
                return True
            except:
                pass  # Bucket doesn't exist, create it

            # Create bucket
            bucket = storage_client.create_bucket(self.bucket_name, location=self.region)
            self.logger.info(f"‚úÖ Created bucket: {self.bucket_name}")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Failed to create bucket: {e}")
            return False

    def upload_models_to_gcs(self):
        """Upload trained models to Google Cloud Storage"""
        self.logger.info("üì§ Uploading models to Google Cloud Storage...")

        models_dir = Path('models')
        if not models_dir.exists():
            self.logger.error(f"‚ùå Models directory not found: {models_dir}")
            return False

        try:
            storage_client = storage.Client(project=self.project_id)
            bucket = storage_client.bucket(self.bucket_name)

            uploaded_models = {}

            for model_file in models_dir.glob('*.joblib'):
                model_name = model_file.stem.replace('_model', '')
                blob_name = f'models/{model_file.name}'

                self.logger.info(f"Uploading {model_file.name}...")

                blob = bucket.blob(blob_name)
                blob.upload_from_filename(str(model_file))

                uploaded_models[model_name] = f"gs://{self.bucket_name}/{blob_name}"
                self.logger.info(f"‚úÖ Uploaded {model_name}")

            self.logger.info(f"üìä Total models uploaded: {len(uploaded_models)}")
            return uploaded_models

        except Exception as e:
            self.logger.error(f"‚ùå Failed to upload models: {e}")
            return {}

    def deploy_models_to_vertex_ai(self, uploaded_models):
        """Deploy models to Vertex AI"""
        self.logger.info("üöÄ Deploying models to Vertex AI...")

        try:
            # Initialize Vertex AI
            aip.init(project=self.project_id, location=self.region)
            self.logger.info(f"‚úÖ Initialized Vertex AI for {self.project_id}")

            deployed_endpoints = {}

            for model_name, model_uri in uploaded_models.items():
                self.logger.info(f"Deploying {model_name} model...")

                try:
                    # Upload model to Vertex AI Model Registry
                    display_name = f"vulnhunter-{model_name}-v1"

                    self.logger.info(f"Uploading model to registry: {display_name}")
                    model = aip.Model.upload(
                        display_name=display_name,
                        artifact_uri=model_uri,
                        serving_container_image_uri="gcr.io/cloud-aiplatform/prediction/sklearn-cpu.1-0:latest",
                        description=f"VulnHunter {model_name} vulnerability detection model",
                    )

                    self.logger.info(f"‚úÖ Model uploaded: {model.resource_name}")

                    # Create endpoint
                    endpoint_display_name = f"{display_name}-endpoint"
                    self.logger.info(f"Creating endpoint: {endpoint_display_name}")

                    endpoint = aip.Endpoint.create(
                        display_name=endpoint_display_name,
                        description=f"Endpoint for {model_name} vulnerability detection"
                    )

                    self.logger.info(f"‚úÖ Endpoint created: {endpoint.resource_name}")

                    # Deploy model to endpoint
                    self.logger.info(f"Deploying model to endpoint...")

                    deployed_model = endpoint.deploy(
                        model=model,
                        deployed_model_display_name=f"{display_name}-deployment",
                        min_replica_count=1,
                        max_replica_count=5,
                        machine_type="n1-standard-4",
                        sync=True  # Wait for deployment to complete
                    )

                    self.logger.info(f"‚úÖ Model deployed successfully!")

                    deployed_endpoints[model_name] = {
                        "model_name": display_name,
                        "endpoint_name": endpoint_display_name,
                        "model_resource_name": model.resource_name,
                        "endpoint_resource_name": endpoint.resource_name,
                        "prediction_url": f"https://{self.region}-aiplatform.googleapis.com/v1/{endpoint.resource_name}:predict",
                        "console_url": f"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint.resource_name.split('/')[-1]}?project={self.project_id}",
                        "status": "DEPLOYED",
                        "deployment_time": datetime.now().isoformat()
                    }

                except Exception as e:
                    self.logger.error(f"‚ùå Failed to deploy {model_name}: {e}")
                    deployed_endpoints[model_name] = {
                        "status": "FAILED",
                        "error": str(e)
                    }

            return deployed_endpoints

        except Exception as e:
            self.logger.error(f"‚ùå Vertex AI deployment failed: {e}")
            return {}

    def test_deployed_endpoints(self, deployed_endpoints):
        """Test deployed endpoints with sample data"""
        self.logger.info("üß™ Testing deployed endpoints...")

        # Sample test data for each model type
        test_data = {
            "cve_nvd": [{"cvss_score": 8.5, "has_exploit": 1, "severity_level": 2}],
            "security_advisories": [{"severity_score": 7.2, "is_popular_package": 1, "weekly_downloads": 50000}],
            "vulnerability_db": [{"overall_score": 8.0, "has_public_exploit": 1, "estimated_affected_systems": 10000}],
            "exploit_db": [{"reliability_score": 0.85, "verified": 1, "payload_size": 1024}]
        }

        test_results = {}

        for model_name, endpoint_info in deployed_endpoints.items():
            if endpoint_info.get("status") != "DEPLOYED":
                continue

            try:
                self.logger.info(f"Testing {model_name} endpoint...")

                # Get endpoint
                endpoint = aip.Endpoint(endpoint_info["endpoint_resource_name"])

                # Make prediction
                sample_data = test_data.get(model_name, [{"test": 1}])
                response = endpoint.predict(instances=sample_data)

                test_results[model_name] = {
                    "status": "SUCCESS",
                    "predictions": response.predictions[:5],  # Limit output
                    "response_time_ms": "< 100"  # Placeholder
                }

                self.logger.info(f"‚úÖ {model_name} test successful")

            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è  {model_name} test failed: {e}")
                test_results[model_name] = {
                    "status": "FAILED",
                    "error": str(e)
                }

        return test_results

    def generate_console_links(self, deployed_endpoints):
        """Generate Google Cloud Console links"""
        self.logger.info("üåê Generating Google Cloud Console links...")

        console_links = {
            "main_dashboard": f"https://console.cloud.google.com/vertex-ai/dashboard?project={self.project_id}",
            "models": f"https://console.cloud.google.com/vertex-ai/models?project={self.project_id}",
            "endpoints": f"https://console.cloud.google.com/vertex-ai/endpoints?project={self.project_id}",
            "storage_bucket": f"https://console.cloud.google.com/storage/browser/{self.bucket_name}?project={self.project_id}",
            "monitoring": f"https://console.cloud.google.com/monitoring/dashboards?project={self.project_id}",
            "model_specific": {}
        }

        for model_name, endpoint_info in deployed_endpoints.items():
            if endpoint_info.get("status") == "DEPLOYED":
                endpoint_id = endpoint_info["endpoint_resource_name"].split('/')[-1]
                console_links["model_specific"][model_name] = {
                    "endpoint": f"https://console.cloud.google.com/vertex-ai/endpoints/{endpoint_id}?project={self.project_id}",
                    "model": f"https://console.cloud.google.com/vertex-ai/models/{endpoint_info['model_resource_name'].split('/')[-1]}?project={self.project_id}"
                }

        # Save links to file
        links_path = Path('deployment') / 'console_links.json'
        links_path.parent.mkdir(exist_ok=True)

        with open(links_path, 'w') as f:
            json.dump(console_links, f, indent=2)

        self.logger.info(f"üîó Console links saved to: {links_path}")
        return console_links

    def run_deployment(self):
        """Run complete deployment process"""
        self.logger.info("üöÄ Starting VulnHunter deployment to existing Google Cloud account")
        self.logger.info("=" * 80)

        try:
            # Step 1: Detect environment
            if not self.detect_gcp_environment():
                return False

            # Step 2: Check dependencies
            if not DEPENDENCIES_OK:
                self.logger.error("‚ùå Missing required dependencies")
                return False

            # Step 3: Check APIs
            if not self.check_required_apis():
                self.logger.warning("‚ö†Ô∏è  API check failed, continuing anyway...")

            # Step 4: Create storage bucket
            if not self.create_storage_bucket():
                return False

            # Step 5: Upload models
            uploaded_models = self.upload_models_to_gcs()
            if not uploaded_models:
                return False

            # Step 6: Deploy to Vertex AI
            deployed_endpoints = self.deploy_models_to_vertex_ai(uploaded_models)
            if not deployed_endpoints:
                return False

            # Step 7: Test endpoints
            test_results = self.test_deployed_endpoints(deployed_endpoints)

            # Step 8: Generate console links
            console_links = self.generate_console_links(deployed_endpoints)

            # Success summary
            self.logger.info("\n" + "=" * 80)
            self.logger.info("üéâ DEPLOYMENT COMPLETED SUCCESSFULLY!")
            self.logger.info("=" * 80)

            successful_deployments = sum(1 for info in deployed_endpoints.values() if info.get("status") == "DEPLOYED")
            self.logger.info(f"üìä Successfully deployed: {successful_deployments}/{len(deployed_endpoints)} models")

            self.logger.info(f"\nüåê View your models in Google Cloud Console:")
            self.logger.info(f"   Main Dashboard: {console_links['main_dashboard']}")
            self.logger.info(f"   Models: {console_links['models']}")
            self.logger.info(f"   Endpoints: {console_links['endpoints']}")

            self.logger.info(f"\nüéØ Individual Model Endpoints:")
            for model_name, endpoint_info in deployed_endpoints.items():
                if endpoint_info.get("status") == "DEPLOYED":
                    self.logger.info(f"   {model_name}: {endpoint_info['console_url']}")

            # Save deployment summary
            summary = {
                "deployment_timestamp": datetime.now().isoformat(),
                "project_id": self.project_id,
                "region": self.region,
                "bucket_name": self.bucket_name,
                "deployed_endpoints": deployed_endpoints,
                "test_results": test_results,
                "console_links": console_links
            }

            summary_path = Path('deployment') / 'real_deployment_summary.json'
            with open(summary_path, 'w') as f:
                json.dump(summary, f, indent=2)

            self.logger.info(f"\nüìã Deployment summary saved: {summary_path}")

            return True

        except Exception as e:
            self.logger.error(f"‚ùå Deployment failed: {str(e)}")
            import traceback
            self.logger.error(traceback.format_exc())
            return False

def main():
    """Main execution"""
    print("VulnHunter Deployment to Existing Google Cloud Account")
    print("=" * 80)

    if not DEPENDENCIES_OK:
        print("\n‚ùå Missing required dependencies!")
        print("Install with:")
        print("pip install google-cloud-aiplatform google-cloud-storage joblib pandas scikit-learn numpy")
        return 1

    deployer = ExistingAccountDeployer()
    success = deployer.run_deployment()

    if success:
        print("\n‚úÖ Deployment completed successfully!")
        print("üåê Check Google Cloud Console to see your models!")
        return 0
    else:
        print("\n‚ùå Deployment failed - check logs for details")
        return 1

if __name__ == "__main__":
    sys.exit(main())