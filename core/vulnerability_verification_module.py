#!/usr/bin/env python3
"""
Vulnerability Verification Module
Validates ML model predictions with multi-stage verification
"""

import re
import ast
import hashlib
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from enum import Enum


class VerificationStatus(Enum):
    """Verification status levels"""
    VERIFIED = "verified"
    LIKELY = "likely"
    POSSIBLE = "possible"
    FALSE_POSITIVE = "false_positive"
    NEEDS_MANUAL_REVIEW = "needs_manual_review"


class ConfidenceLevel(Enum):
    """Confidence levels for verification"""
    VERY_HIGH = "very_high"  # 90-100%
    HIGH = "high"            # 75-89%
    MEDIUM = "medium"        # 50-74%
    LOW = "low"              # 25-49%
    VERY_LOW = "very_low"    # 0-24%


@dataclass
class VerificationResult:
    """Result of vulnerability verification"""
    status: VerificationStatus
    confidence: ConfidenceLevel
    confidence_score: float  # 0.0 - 1.0
    evidence: List[str]
    verification_methods: List[str]
    false_positive_indicators: List[str]
    recommendations: List[str]


class VulnerabilityVerificationModule:
    """Multi-stage verification module for ML predictions"""

    def __init__(self):
        self.verification_rules = self._load_verification_rules()

    def _load_verification_rules(self) -> Dict:
        """Load verification rules for different vulnerability types"""
        return {
            'XSS': {
                'patterns': [
                    r'dangerouslySetInnerHTML',
                    r'innerHTML\s*=',
                    r'document\.write\s*\(',
                    r'eval\s*\(',
                ],
                'context_required': ['user_input', 'external_data'],
                'false_positive_patterns': [
                    r'DOMPurify\.sanitize',
                    r'escapeHtml',
                    r'sanitize\(',
                ]
            },
            'PROTOTYPE_POLLUTION': {
                'patterns': [
                    r'Object\.assign\s*\(',
                    r'\.\.\..*(?:req\.body|req\.query|params)',
                    r'merge\s*\(',
                    r'extend\s*\(',
                ],
                'context_required': ['object_merge', 'user_input'],
                'false_positive_patterns': [
                    r'Object\.create\(null\)',
                    r'Object\.freeze\(',
                    r'hasOwnProperty',
                ]
            },
            'SENSITIVE_DATA_EXPOSURE': {
                'patterns': [
                    r'localStorage\.setItem.*(?:token|key|secret|password)',
                    r'sessionStorage\.setItem.*(?:token|key|secret)',
                ],
                'context_required': ['storage', 'sensitive_data'],
                'false_positive_patterns': [
                    r'encrypt\(',
                    r'hash\(',
                    r'cipher\(',
                ]
            },
            'SSRF': {
                'patterns': [
                    r'fetch\s*\([^)]*(?:req\.(?:query|body|params)|params|query)',
                    r'axios\.[get|post]\s*\([^)]*(?:req\.(?:query|body|params))',
                ],
                'context_required': ['url_parameter', 'http_request'],
                'false_positive_patterns': [
                    r'validate.*url',
                    r'whitelist',
                    r'isValidUrl',
                ]
            },
            'SQL_INJECTION': {
                'patterns': [
                    r'query\s*\(\s*["\'].*\$\{',
                    r'execute\s*\(\s*["\'].*\+',
                    r'\.raw\s*\(',
                ],
                'context_required': ['database', 'user_input'],
                'false_positive_patterns': [
                    r'prepared',
                    r'parameterized',
                    r'sanitize',
                ]
            },
            'CODE_INJECTION': {
                'patterns': [
                    r'\beval\s*\(',
                    r'Function\s*\(',
                    r'setTimeout\s*\([^,]*[+$]',
                ],
                'context_required': ['code_execution', 'user_input'],
                'false_positive_patterns': [
                    r'JSON\.parse',
                    r'parseInt',
                    r'parseFloat',
                ]
            }
        }

    def verify_vulnerability(
        self,
        vuln_type: str,
        code_snippet: str,
        file_path: str,
        ml_confidence: float,
        context: Dict = None
    ) -> VerificationResult:
        """
        Perform multi-stage verification of vulnerability

        Args:
            vuln_type: Type of vulnerability
            code_snippet: Code containing potential vulnerability
            file_path: Path to file
            ml_confidence: ML model's confidence score
            context: Additional context information

        Returns:
            VerificationResult with detailed verification status
        """

        evidence = []
        verification_methods = []
        false_positive_indicators = []

        # Stage 1: Pattern-based verification
        pattern_score, pattern_evidence = self._verify_patterns(vuln_type, code_snippet)
        if pattern_evidence:
            evidence.extend(pattern_evidence)
            verification_methods.append("pattern_matching")

        # Stage 2: Context analysis
        context_score, context_evidence = self._verify_context(vuln_type, code_snippet, context)
        if context_evidence:
            evidence.extend(context_evidence)
            verification_methods.append("context_analysis")

        # Stage 3: False positive detection
        fp_score, fp_indicators = self._detect_false_positives(vuln_type, code_snippet)
        false_positive_indicators.extend(fp_indicators)
        if fp_indicators:
            verification_methods.append("false_positive_detection")

        # Stage 4: AST analysis (if possible)
        ast_score, ast_evidence = self._verify_ast(code_snippet, vuln_type)
        if ast_evidence:
            evidence.extend(ast_evidence)
            verification_methods.append("ast_analysis")

        # Stage 5: Code flow analysis
        flow_score, flow_evidence = self._analyze_code_flow(code_snippet, vuln_type)
        if flow_evidence:
            evidence.extend(flow_evidence)
            verification_methods.append("code_flow_analysis")

        # Calculate final confidence score
        weights = {
            'pattern': 0.25,
            'context': 0.20,
            'false_positive': 0.20,
            'ast': 0.20,
            'flow': 0.15
        }

        confidence_score = (
            pattern_score * weights['pattern'] +
            context_score * weights['context'] +
            (1.0 - fp_score) * weights['false_positive'] +
            ast_score * weights['ast'] +
            flow_score * weights['flow']
        )

        # Adjust based on ML confidence
        confidence_score = (confidence_score * 0.7) + (ml_confidence * 0.3)

        # Determine verification status
        status = self._determine_status(confidence_score, fp_score)
        confidence_level = self._determine_confidence_level(confidence_score)

        # Generate recommendations
        recommendations = self._generate_recommendations(
            vuln_type,
            status,
            confidence_score,
            false_positive_indicators
        )

        return VerificationResult(
            status=status,
            confidence=confidence_level,
            confidence_score=confidence_score,
            evidence=evidence,
            verification_methods=verification_methods,
            false_positive_indicators=false_positive_indicators,
            recommendations=recommendations
        )

    def _verify_patterns(self, vuln_type: str, code: str) -> Tuple[float, List[str]]:
        """Verify using pattern matching"""
        if vuln_type not in self.verification_rules:
            return 0.5, []

        rules = self.verification_rules[vuln_type]
        patterns = rules['patterns']

        matches = []
        for pattern in patterns:
            if re.search(pattern, code, re.IGNORECASE):
                matches.append(f"Pattern matched: {pattern}")

        score = min(1.0, len(matches) / len(patterns))
        return score, matches

    def _verify_context(self, vuln_type: str, code: str, context: Dict = None) -> Tuple[float, List[str]]:
        """Verify contextual indicators"""
        if vuln_type not in self.verification_rules:
            return 0.5, []

        rules = self.verification_rules[vuln_type]
        required_context = rules.get('context_required', [])

        evidence = []
        score = 0.0

        # Check for context indicators in code
        context_indicators = {
            'user_input': [r'req\.body', r'req\.query', r'params', r'input', r'user'],
            'external_data': [r'fetch\(', r'axios', r'http', r'api'],
            'storage': [r'localStorage', r'sessionStorage', r'cookie'],
            'sensitive_data': [r'token', r'password', r'secret', r'key', r'credential'],
            'database': [r'query', r'execute', r'sql', r'database'],
            'code_execution': [r'eval', r'Function', r'exec'],
            'object_merge': [r'assign', r'merge', r'extend', r'spread'],
            'url_parameter': [r'url', r'uri', r'href', r'link'],
            'http_request': [r'fetch', r'axios', r'request', r'http'],
        }

        matched_contexts = 0
        for ctx in required_context:
            if ctx in context_indicators:
                for pattern in context_indicators[ctx]:
                    if re.search(pattern, code, re.IGNORECASE):
                        evidence.append(f"Context indicator found: {ctx}")
                        matched_contexts += 1
                        break

        if required_context:
            score = matched_contexts / len(required_context)
        else:
            score = 0.5

        return score, evidence

    def _detect_false_positives(self, vuln_type: str, code: str) -> Tuple[float, List[str]]:
        """Detect false positive indicators"""
        if vuln_type not in self.verification_rules:
            return 0.0, []

        rules = self.verification_rules[vuln_type]
        fp_patterns = rules.get('false_positive_patterns', [])

        indicators = []
        for pattern in fp_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                indicators.append(f"Mitigation found: {pattern}")

        # Higher score = more false positive indicators = less confident in vulnerability
        score = min(1.0, len(indicators) / max(1, len(fp_patterns)))
        return score, indicators

    def _verify_ast(self, code: str, vuln_type: str) -> Tuple[float, List[str]]:
        """Verify using AST analysis"""
        try:
            # Try to parse as Python (for demo purposes)
            # In production, use language-specific parsers
            tree = ast.parse(code)

            evidence = []
            dangerous_nodes = []

            # Look for dangerous patterns in AST
            for node in ast.walk(tree):
                if isinstance(node, ast.Call):
                    if hasattr(node.func, 'id'):
                        func_name = node.func.id
                        if func_name in ['eval', 'exec', 'compile']:
                            dangerous_nodes.append(func_name)
                            evidence.append(f"Dangerous function call: {func_name}")

            score = min(1.0, len(dangerous_nodes) / 3.0) if dangerous_nodes else 0.5
            return score, evidence

        except SyntaxError:
            # Not valid Python, or JavaScript code
            # Use regex-based AST simulation
            return self._simulate_ast_analysis(code, vuln_type)

    def _simulate_ast_analysis(self, code: str, vuln_type: str) -> Tuple[float, List[str]]:
        """Simulate AST analysis using pattern matching"""
        evidence = []
        score = 0.0

        # Look for function calls
        function_calls = re.findall(r'(\w+)\s*\(', code)
        dangerous_functions = {
            'eval', 'exec', 'Function', 'setTimeout', 'setInterval',
            'dangerouslySetInnerHTML', 'innerHTML'
        }

        found_dangerous = [f for f in function_calls if f in dangerous_functions]
        if found_dangerous:
            evidence.extend([f"Dangerous function: {f}" for f in found_dangerous])
            score = min(1.0, len(found_dangerous) / 2.0)
        else:
            score = 0.3

        return score, evidence

    def _analyze_code_flow(self, code: str, vuln_type: str) -> Tuple[float, List[str]]:
        """Analyze code flow for vulnerability patterns"""
        evidence = []
        score = 0.0

        # Check for data flow from input to sink
        input_sources = [r'req\.body', r'req\.query', r'params', r'input']
        sinks = {
            'XSS': [r'innerHTML', r'dangerouslySetInnerHTML', r'document\.write'],
            'CODE_INJECTION': [r'eval', r'Function', r'exec'],
            'SQL_INJECTION': [r'query', r'execute', r'raw'],
            'SSRF': [r'fetch', r'axios', r'http'],
        }

        # Check if input flows to sink
        has_input = any(re.search(pattern, code) for pattern in input_sources)
        has_sink = False

        if vuln_type in sinks:
            has_sink = any(re.search(pattern, code) for pattern in sinks[vuln_type])

        if has_input and has_sink:
            evidence.append("Data flow: user input â†’ vulnerable sink")
            score = 0.8
        elif has_sink:
            evidence.append("Vulnerable sink detected (no clear input source)")
            score = 0.5
        else:
            score = 0.2

        return score, evidence

    def _determine_status(self, confidence_score: float, fp_score: float) -> VerificationStatus:
        """Determine verification status"""
        if fp_score > 0.7:
            return VerificationStatus.FALSE_POSITIVE
        elif confidence_score >= 0.85:
            return VerificationStatus.VERIFIED
        elif confidence_score >= 0.65:
            return VerificationStatus.LIKELY
        elif confidence_score >= 0.40:
            return VerificationStatus.POSSIBLE
        else:
            return VerificationStatus.NEEDS_MANUAL_REVIEW

    def _determine_confidence_level(self, score: float) -> ConfidenceLevel:
        """Determine confidence level"""
        if score >= 0.90:
            return ConfidenceLevel.VERY_HIGH
        elif score >= 0.75:
            return ConfidenceLevel.HIGH
        elif score >= 0.50:
            return ConfidenceLevel.MEDIUM
        elif score >= 0.25:
            return ConfidenceLevel.LOW
        else:
            return ConfidenceLevel.VERY_LOW

    def _generate_recommendations(
        self,
        vuln_type: str,
        status: VerificationStatus,
        confidence: float,
        fp_indicators: List[str]
    ) -> List[str]:
        """Generate recommendations based on verification"""
        recommendations = []

        if status == VerificationStatus.VERIFIED:
            recommendations.append("HIGH PRIORITY: Immediate remediation required")
            recommendations.append("Vulnerability confirmed through multi-stage verification")
        elif status == VerificationStatus.LIKELY:
            recommendations.append("MEDIUM PRIORITY: Manual review and remediation recommended")
            recommendations.append("High probability of vulnerability - verify in context")
        elif status == VerificationStatus.POSSIBLE:
            recommendations.append("LOW PRIORITY: Manual review suggested")
            recommendations.append("Potential vulnerability - assess based on application context")
        elif status == VerificationStatus.FALSE_POSITIVE:
            recommendations.append("Likely false positive - mitigations detected")
            recommendations.append("Review to confirm security controls are effective")
        else:
            recommendations.append("MANUAL REVIEW REQUIRED: Insufficient confidence")
            recommendations.append("Expert analysis needed for accurate assessment")

        # Add specific recommendations
        if fp_indicators:
            recommendations.append(f"Security controls detected: {len(fp_indicators)}")
            recommendations.append("Verify that existing mitigations are properly implemented")

        if confidence < 0.5:
            recommendations.append("Consider additional testing (DAST, manual testing)")

        return recommendations

    def batch_verify(
        self,
        vulnerabilities: List[Dict]
    ) -> List[Tuple[Dict, VerificationResult]]:
        """Verify multiple vulnerabilities in batch"""
        results = []

        for vuln in vulnerabilities:
            result = self.verify_vulnerability(
                vuln_type=vuln.get('type', 'UNKNOWN'),
                code_snippet=vuln.get('code_snippet', ''),
                file_path=vuln.get('file', ''),
                ml_confidence=vuln.get('confidence_score', 0.5),
                context=vuln.get('context', {})
            )
            results.append((vuln, result))

        return results


def main():
    """Test verification module"""

    # Example usage
    verifier = VulnerabilityVerificationModule()

    # Test case 1: XSS vulnerability
    result = verifier.verify_vulnerability(
        vuln_type='XSS',
        code_snippet='element.innerHTML = userInput;',
        file_path='test.js',
        ml_confidence=0.85
    )

    print(f"Status: {result.status.value}")
    print(f"Confidence: {result.confidence.value} ({result.confidence_score:.2%})")
    print(f"Evidence: {result.evidence}")
    print(f"Methods: {result.verification_methods}")
    print(f"Recommendations: {result.recommendations}")


if __name__ == '__main__':
    main()
