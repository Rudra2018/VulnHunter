#!/usr/bin/env python3
"""
BEAST MODE Binary Vulnerability Trainer
Advanced machine learning training pipeline for binary vulnerability detection
"""

import os
import json
import pickle
import logging
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.neural_network import MLPClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support
from sklearn.feature_selection import SelectKBest, f_classif, RFE

# Import our custom components
from .binary_dataset_builder import BinaryDatasetBuilder, BinaryVulnerability
from .binary_feature_extractor import BinaryFeatureExtractor
from .assembly_vulnerability_analyzer import AssemblyVulnerabilityAnalyzer

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class BinaryVulnerabilityTrainer:
    """Advanced ML training pipeline for binary vulnerability detection"""

    def __init__(self):
        self.dataset_builder = BinaryDatasetBuilder()
        self.feature_extractor = BinaryFeatureExtractor()
        self.assembly_analyzer = AssemblyVulnerabilityAnalyzer()

        # ML components
        self.models = {}
        self.scalers = {}
        self.label_encoder = LabelEncoder()
        self.feature_selector = None
        self.feature_names = []
        self.trained = False

        # Training metrics
        self.training_metrics = {}
        self.model_performance = {}

        logger.info("ğŸ¦¾ Binary Vulnerability Trainer initialized")

    def build_training_dataset(self, target_size: int = 5000) -> List[BinaryVulnerability]:
        """Build comprehensive training dataset"""
        logger.info(f"ğŸ”„ Building binary training dataset (target: {target_size} samples)")

        # Build dataset using the dataset builder
        dataset = self.dataset_builder.build_comprehensive_dataset(target_size)

        logger.info(f"âœ… Training dataset built: {len(dataset)} samples")
        return dataset

    def extract_ml_features(self, dataset: List[BinaryVulnerability]) -> Tuple[np.ndarray, np.ndarray, List[str]]:
        """Extract comprehensive ML features from dataset"""
        logger.info("ğŸ”„ Extracting ML features from binary dataset...")

        features_list = []
        labels = []

        for i, binary_sample in enumerate(dataset):
            if i % 1000 == 0:
                logger.info(f"   Processing sample {i}/{len(dataset)}")

            try:
                # Extract binary features
                binary_features = self.feature_extractor.extract_comprehensive_features(
                    binary_sample.binary_path
                )

                # Extract assembly vulnerabilities
                assembly_vulns = self.assembly_analyzer.analyze_disassembly(
                    binary_sample.binary_path
                )
                assembly_summary = self.assembly_analyzer.get_vulnerability_summary(assembly_vulns)

                # Combine all features
                combined_features = self._combine_features(
                    binary_features,
                    assembly_summary,
                    binary_sample
                )

                features_list.append(combined_features)
                labels.append(self._encode_vulnerability_label(binary_sample))

            except Exception as e:
                logger.warning(f"Feature extraction failed for {binary_sample.binary_path}: {e}")
                continue

        # Convert to DataFrame for easier processing
        features_df = pd.DataFrame(features_list)

        # Handle missing values
        features_df = features_df.fillna(0)

        # Get feature names
        feature_names = list(features_df.columns)

        # Convert to numpy arrays
        X = features_df.values
        y = np.array(labels)

        logger.info(f"âœ… Feature extraction complete: {X.shape[0]} samples, {X.shape[1]} features")

        return X, y, feature_names

    def _combine_features(self, binary_features: Dict, assembly_summary: Dict,
                         binary_sample: BinaryVulnerability) -> Dict[str, Any]:
        """Combine different feature types into comprehensive feature vector"""
        combined = {}

        # Basic binary features
        combined.update(binary_features)

        # Assembly analysis features
        combined.update({
            'asm_total_vulnerabilities': assembly_summary.get('total_vulnerabilities', 0),
            'asm_risk_score': assembly_summary.get('risk_score', 0.0),
            'asm_high_confidence_vulns': assembly_summary.get('confidence_stats', {}).get('high_confidence_count', 0),
            'asm_avg_confidence': assembly_summary.get('confidence_stats', {}).get('average', 0.0),
        })

        # Severity distribution features
        severity_dist = assembly_summary.get('severity_distribution', {})
        combined.update({
            'asm_critical_count': severity_dist.get('critical', 0),
            'asm_high_count': severity_dist.get('high', 0),
            'asm_medium_count': severity_dist.get('medium', 0),
            'asm_low_count': severity_dist.get('low', 0),
        })

        # Vulnerability type distribution features
        type_dist = assembly_summary.get('type_distribution', {})
        combined.update({
            'asm_buffer_overflow': type_dist.get('buffer_overflow', 0),
            'asm_integer_overflow': type_dist.get('integer_overflow', 0),
            'asm_use_after_free': type_dist.get('use_after_free', 0),
            'asm_format_string': type_dist.get('format_string', 0),
            'asm_privilege_escalation': type_dist.get('privilege_escalation', 0),
        })

        # Platform-specific features
        combined.update({
            'platform_macos': 1 if binary_sample.platform == 'macos' else 0,
            'platform_windows': 1 if binary_sample.platform == 'windows' else 0,
            'platform_linux': 1 if binary_sample.platform == 'linux' else 0,
        })

        # Binary type features
        combined.update({
            'binary_type_pe': 1 if binary_sample.binary_type == 'PE32' else 0,
            'binary_type_elf': 1 if binary_sample.binary_type in ['ELF32', 'ELF64'] else 0,
            'binary_type_macho': 1 if binary_sample.binary_type == 'Mach-O' else 0,
        })

        # CVE-based features
        combined.update({
            'has_cve': 1 if binary_sample.cve_id != 'N/A' else 0,
            'cve_year': self._extract_cve_year(binary_sample.cve_id),
        })

        # Ensure all values are numeric
        for key, value in combined.items():
            if isinstance(value, bool):
                combined[key] = int(value)
            elif not isinstance(value, (int, float)):
                combined[key] = 0

        return combined

    def _encode_vulnerability_label(self, binary_sample: BinaryVulnerability) -> str:
        """Encode vulnerability type as label"""
        if binary_sample.vulnerability_type == 'none':
            return 'benign'
        else:
            return binary_sample.vulnerability_type

    def _extract_cve_year(self, cve_id: str) -> int:
        """Extract year from CVE ID"""
        if cve_id == 'N/A':
            return 0
        try:
            # CVE format: CVE-YYYY-NNNN
            parts = cve_id.split('-')
            if len(parts) >= 2:
                return int(parts[1])
        except (ValueError, IndexError):
            pass
        return 0

    def train_ensemble_models(self, X: np.ndarray, y: np.ndarray, feature_names: List[str]):
        """Train ensemble of ML models for binary vulnerability detection"""
        logger.info("ğŸ”„ Training ensemble models for binary vulnerability detection...")

        self.feature_names = feature_names

        # Encode labels
        y_encoded = self.label_encoder.fit_transform(y)

        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded
        )

        # Feature selection
        logger.info("   Performing feature selection...")
        self.feature_selector = SelectKBest(score_func=f_classif, k=min(500, X.shape[1]))
        X_train_selected = self.feature_selector.fit_transform(X_train, y_train)
        X_test_selected = self.feature_selector.transform(X_test)

        # Scale features for algorithms that need it
        self.scalers['standard'] = StandardScaler()
        self.scalers['minmax'] = MinMaxScaler()

        X_train_scaled = self.scalers['standard'].fit_transform(X_train_selected)
        X_test_scaled = self.scalers['standard'].transform(X_test_selected)

        X_train_minmax = self.scalers['minmax'].fit_transform(X_train_selected)
        X_test_minmax = self.scalers['minmax'].transform(X_test_selected)

        # Train models
        logger.info("   Training Random Forest...")
        self.models['random_forest'] = self._train_random_forest(X_train_selected, y_train)

        logger.info("   Training Gradient Boosting...")
        self.models['gradient_boosting'] = self._train_gradient_boosting(X_train_selected, y_train)

        logger.info("   Training Neural Network...")
        self.models['neural_network'] = self._train_neural_network(X_train_scaled, y_train)

        logger.info("   Training SVM...")
        self.models['svm'] = self._train_svm(X_train_minmax, y_train)

        logger.info("   Training Naive Bayes...")
        self.models['naive_bayes'] = self._train_naive_bayes(X_train_selected, y_train)

        self.trained = True

        # Evaluate models
        self._evaluate_models(X_test_selected, X_test_scaled, X_test_minmax, y_test)

        logger.info(f"âœ… Training complete: {len(self.models)} models trained")

    def _train_random_forest(self, X_train: np.ndarray, y_train: np.ndarray) -> RandomForestClassifier:
        """Train Random Forest classifier with hyperparameter tuning"""
        # Basic Random Forest (for speed in demo)
        rf = RandomForestClassifier(
            n_estimators=200,
            max_depth=20,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1,
            class_weight='balanced'
        )
        rf.fit(X_train, y_train)
        return rf

    def _train_gradient_boosting(self, X_train: np.ndarray, y_train: np.ndarray) -> GradientBoostingClassifier:
        """Train Gradient Boosting classifier"""
        gb = GradientBoostingClassifier(
            n_estimators=150,
            learning_rate=0.1,
            max_depth=8,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42
        )
        gb.fit(X_train, y_train)
        return gb

    def _train_neural_network(self, X_train: np.ndarray, y_train: np.ndarray) -> MLPClassifier:
        """Train Neural Network classifier"""
        nn = MLPClassifier(
            hidden_layer_sizes=(256, 128, 64),
            activation='relu',
            solver='adam',
            alpha=0.001,
            learning_rate='adaptive',
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )
        nn.fit(X_train, y_train)
        return nn

    def _train_svm(self, X_train: np.ndarray, y_train: np.ndarray) -> SVC:
        """Train Support Vector Machine classifier"""
        svm = SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=42,
            class_weight='balanced'
        )
        svm.fit(X_train, y_train)
        return svm

    def _train_naive_bayes(self, X_train: np.ndarray, y_train: np.ndarray) -> GaussianNB:
        """Train Naive Bayes classifier"""
        nb = GaussianNB()
        nb.fit(X_train, y_train)
        return nb

    def _evaluate_models(self, X_test: np.ndarray, X_test_scaled: np.ndarray,
                        X_test_minmax: np.ndarray, y_test: np.ndarray):
        """Evaluate all trained models"""
        logger.info("ğŸ“Š Evaluating models...")

        test_data = {
            'random_forest': X_test,
            'gradient_boosting': X_test,
            'neural_network': X_test_scaled,
            'svm': X_test_minmax,
            'naive_bayes': X_test
        }

        for name, model in self.models.items():
            X_test_model = test_data[name]
            predictions = model.predict(X_test_model)
            probabilities = model.predict_proba(X_test_model)

            accuracy = accuracy_score(y_test, predictions)
            precision, recall, f1, _ = precision_recall_fscore_support(y_test, predictions, average='weighted')

            self.model_performance[name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1_score': f1,
                'predictions': predictions,
                'probabilities': probabilities
            }

            logger.info(f"   {name}: Accuracy={accuracy:.4f}, F1={f1:.4f}")

        # Ensemble prediction
        ensemble_predictions = self._ensemble_predict(X_test, X_test_scaled, X_test_minmax)
        ensemble_accuracy = accuracy_score(y_test, ensemble_predictions)
        logger.info(f"   ensemble: Accuracy={ensemble_accuracy:.4f}")

        # Detailed report for best individual model
        best_model = max(self.model_performance.keys(),
                        key=lambda x: self.model_performance[x]['accuracy'])
        logger.info(f"\nğŸ“‹ Detailed Classification Report ({best_model}):")

        best_predictions = self.model_performance[best_model]['predictions']
        labels = self.label_encoder.classes_
        print(classification_report(y_test, best_predictions, target_names=labels))

    def _ensemble_predict(self, X_test: np.ndarray, X_test_scaled: np.ndarray,
                         X_test_minmax: np.ndarray) -> np.ndarray:
        """Make ensemble predictions using voting"""
        test_data = {
            'random_forest': X_test,
            'gradient_boosting': X_test,
            'neural_network': X_test_scaled,
            'svm': X_test_minmax,
            'naive_bayes': X_test
        }

        predictions = []
        for name, model in self.models.items():
            X_test_model = test_data[name]
            pred = model.predict(X_test_model)
            predictions.append(pred)

        # Majority voting
        predictions = np.array(predictions)
        ensemble_pred = []

        for i in range(predictions.shape[1]):
            votes = predictions[:, i]
            unique, counts = np.unique(votes, return_counts=True)
            ensemble_pred.append(unique[np.argmax(counts)])

        return np.array(ensemble_pred)

    def predict_binary_vulnerability(self, binary_path: str) -> Dict[str, Any]:
        """Predict vulnerability for a single binary"""
        if not self.trained:
            raise ValueError("Models not trained. Call train_ensemble_models() first.")

        logger.info(f"ğŸ” Analyzing binary: {binary_path}")

        # Extract features
        binary_features = self.feature_extractor.extract_comprehensive_features(binary_path)
        assembly_vulns = self.assembly_analyzer.analyze_disassembly(binary_path)
        assembly_summary = self.assembly_analyzer.get_vulnerability_summary(assembly_vulns)

        # Create mock binary sample for feature combination
        mock_sample = BinaryVulnerability(
            platform='unknown',
            binary_type='unknown',
            application='unknown',
            cve_id='N/A',
            vulnerability_type='unknown',
            severity='unknown',
            binary_path=binary_path,
            binary_hash='',
            features={},
            metadata={}
        )

        # Combine features
        combined_features = self._combine_features(binary_features, assembly_summary, mock_sample)

        # Convert to DataFrame and align with training features
        features_df = pd.DataFrame([combined_features])
        features_df = features_df.reindex(columns=self.feature_names, fill_value=0)
        X = features_df.values

        # Apply feature selection
        X_selected = self.feature_selector.transform(X)
        X_scaled = self.scalers['standard'].transform(X_selected)
        X_minmax = self.scalers['minmax'].transform(X_selected)

        # Get predictions from all models
        predictions = {}
        probabilities = {}

        test_data = {
            'random_forest': X_selected,
            'gradient_boosting': X_selected,
            'neural_network': X_scaled,
            'svm': X_minmax,
            'naive_bayes': X_selected
        }

        for name, model in self.models.items():
            X_model = test_data[name]
            pred = model.predict(X_model)[0]
            prob = model.predict_proba(X_model)[0]

            predictions[name] = self.label_encoder.inverse_transform([pred])[0]
            probabilities[name] = prob.max()

        # Ensemble prediction
        ensemble_pred = self._ensemble_predict(X_selected, X_scaled, X_minmax)[0]
        ensemble_label = self.label_encoder.inverse_transform([ensemble_pred])[0]

        # Calculate overall confidence
        avg_confidence = np.mean(list(probabilities.values()))

        return {
            'prediction': ensemble_label,
            'confidence': avg_confidence,
            'individual_predictions': predictions,
            'individual_confidences': probabilities,
            'assembly_vulnerabilities': assembly_vulns,
            'risk_assessment': self._assess_risk(ensemble_label, avg_confidence, assembly_summary),
            'recommendations': self._get_security_recommendations(ensemble_label, assembly_vulns)
        }

    def _assess_risk(self, prediction: str, confidence: float, assembly_summary: Dict) -> Dict[str, Any]:
        """Assess overall risk based on prediction and analysis"""
        risk_scores = {
            'benign': 0,
            'buffer_overflow': 9,
            'use_after_free': 9,
            'privilege_escalation': 10,
            'integer_overflow': 7,
            'format_string': 8,
            'stack_overflow': 8,
            'heap_overflow': 8,
            'null_pointer_dereference': 5,
            'race_condition': 6,
            'memory_leak': 3
        }

        base_risk = risk_scores.get(prediction, 5)
        assembly_risk = assembly_summary.get('risk_score', 0)

        # Combine ML prediction risk with assembly analysis risk
        combined_risk = (base_risk * confidence + assembly_risk) / 2
        combined_risk = min(combined_risk, 10.0)

        if combined_risk >= 8:
            risk_level = "CRITICAL"
        elif combined_risk >= 6:
            risk_level = "HIGH"
        elif combined_risk >= 4:
            risk_level = "MEDIUM"
        elif combined_risk >= 2:
            risk_level = "LOW"
        else:
            risk_level = "MINIMAL"

        return {
            'score': round(combined_risk, 2),
            'level': risk_level,
            'ml_confidence': confidence,
            'assembly_risk': assembly_risk
        }

    def _get_security_recommendations(self, prediction: str, assembly_vulns: List) -> List[str]:
        """Get security recommendations based on analysis"""
        recommendations = []

        if prediction == 'benign':
            recommendations.append("Binary appears safe based on ML analysis")
        else:
            recommendations.append(f"Binary contains {prediction} vulnerability patterns")
            recommendations.append("Conduct thorough security review before deployment")

        if assembly_vulns:
            recommendations.append("Assembly analysis detected potential vulnerabilities:")
            for vuln in assembly_vulns[:3]:  # Top 3 vulnerabilities
                recommendations.append(f"â€¢ {vuln.vulnerability_type.value}: {vuln.mitigation}")

        recommendations.extend([
            "Run binary in sandboxed environment",
            "Monitor for suspicious behavior during execution",
            "Keep system and dependencies updated"
        ])

        return recommendations

    def save_models(self, filename: str = None) -> str:
        """Save trained models to file"""
        if not self.trained:
            raise ValueError("No trained models to save")

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"binary_vulnerability_models_{timestamp}.pkl"

        model_data = {
            'models': self.models,
            'scalers': self.scalers,
            'label_encoder': self.label_encoder,
            'feature_selector': self.feature_selector,
            'feature_names': self.feature_names,
            'model_performance': self.model_performance,
            'metadata': {
                'training_timestamp': datetime.now().isoformat(),
                'model_count': len(self.models),
                'feature_count': len(self.feature_names)
            }
        }

        with open(filename, 'wb') as f:
            pickle.dump(model_data, f)

        logger.info(f"ğŸ’¾ Models saved: {filename}")
        return filename

    def load_models(self, filename: str):
        """Load pre-trained models from file"""
        logger.info(f"ğŸ“‚ Loading models from: {filename}")

        with open(filename, 'rb') as f:
            model_data = pickle.load(f)

        self.models = model_data['models']
        self.scalers = model_data['scalers']
        self.label_encoder = model_data['label_encoder']
        self.feature_selector = model_data['feature_selector']
        self.feature_names = model_data['feature_names']
        self.model_performance = model_data['model_performance']

        self.trained = True

        logger.info(f"âœ… Models loaded: {len(self.models)} models, {len(self.feature_names)} features")

    def get_feature_importance(self, top_k: int = 20) -> Dict[str, float]:
        """Get top feature importance from Random Forest"""
        if 'random_forest' not in self.models:
            return {}

        rf_model = self.models['random_forest']
        feature_importance = rf_model.feature_importances_

        # Get selected feature names
        selected_features = self.feature_selector.get_support()
        selected_feature_names = [name for i, name in enumerate(self.feature_names) if selected_features[i]]

        # Create importance dictionary
        importance_dict = dict(zip(selected_feature_names, feature_importance))

        # Sort and return top k
        sorted_features = sorted(importance_dict.items(), key=lambda x: x[1], reverse=True)
        return dict(sorted_features[:top_k])

def main():
    """Main execution function for testing"""
    trainer = BinaryVulnerabilityTrainer()

    # Build training dataset
    logger.info("ğŸ”„ Building training dataset...")
    dataset = trainer.build_training_dataset(target_size=1000)  # Smaller for demo

    # Extract features
    logger.info("ğŸ”„ Extracting ML features...")
    X, y, feature_names = trainer.extract_ml_features(dataset)

    # Train models
    logger.info("ğŸ”„ Training models...")
    trainer.train_ensemble_models(X, y, feature_names)

    # Save models
    model_file = trainer.save_models()

    # Test prediction
    test_binary = "samples/windows/vulnerable/WinRAR.exe"
    logger.info(f"ğŸ” Testing prediction on: {test_binary}")
    result = trainer.predict_binary_vulnerability(test_binary)

    print(f"\nğŸ“Š Prediction Results:")
    print(f"   Prediction: {result['prediction']}")
    print(f"   Confidence: {result['confidence']:.2%}")
    print(f"   Risk Level: {result['risk_assessment']['level']}")

    # Feature importance
    importance = trainer.get_feature_importance()
    print(f"\nğŸ” Top 10 Important Features:")
    for feature, score in list(importance.items())[:10]:
        print(f"   {feature}: {score:.4f}")

if __name__ == "__main__":
    main()