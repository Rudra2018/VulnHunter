#!/usr/bin/env python3
"""
Comprehensive Vulnerability Validation System
Enhanced validation based on Oort Protocol analysis lessons learned
Validates vulnerability claims with professional bug bounty standards
"""

import os
import json
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Tuple, Optional
from pathlib import Path
import logging
import subprocess
import re
import hashlib

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('comprehensive_vulnerability_validation.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('ComprehensiveVulnValidator')

class ComprehensiveVulnerabilityValidator:
    """
    Comprehensive vulnerability validation system incorporating lessons from Oort Protocol analysis
    Validates vulnerability claims against professional bug bounty standards
    """

    def __init__(self, results_dir: str = "validation_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)

        # Critical validation criteria based on Oort Protocol lessons
        self.go_criteria = {
            "file_existence": {"weight": 0.15, "critical": True},
            "production_code": {"weight": 0.25, "critical": True},
            "independent_confirmation": {"weight": 0.20, "critical": True},
            "working_exploits": {"weight": 0.20, "critical": False},
            "scope_compliance": {"weight": 0.15, "critical": True},
            "concrete_evidence": {"weight": 0.05, "critical": False}
        }

        self.no_go_indicators = [
            "test_files_as_production",
            "mock_contracts_as_production",
            "no_independent_confirmation",
            "theoretical_only_impact",
            "out_of_scope_claims",
            "generic_exploits"
        ]

    def validate_repository_structure(self, repo_path: str) -> Dict[str, Any]:
        """Validate repository structure and identify production vs test code"""

        logger.info("ðŸ—ï¸ Analyzing Repository Structure...")

        repo_path = Path(repo_path)
        if not repo_path.exists():
            return {"error": f"Repository path {repo_path} does not exist"}

        structure_analysis = {
            "repository_type": self.identify_repository_type(repo_path),
            "production_files": {},
            "test_files": {},
            "smart_contracts": {
                "production": [],
                "test": [],
                "total": 0
            },
            "cpp_files": {
                "production": [],
                "test": [],
                "total": 0
            },
            "scope_assessment": {}
        }

        # Analyze Solidity files
        sol_files = list(repo_path.rglob("*.sol"))
        structure_analysis["smart_contracts"]["total"] = len(sol_files)

        for sol_file in sol_files:
            rel_path = sol_file.relative_to(repo_path)
            if self.is_test_file(rel_path):
                structure_analysis["smart_contracts"]["test"].append(str(rel_path))
            else:
                structure_analysis["smart_contracts"]["production"].append(str(rel_path))

        # Analyze C++ files
        cpp_files = list(repo_path.rglob("*.cpp")) + list(repo_path.rglob("*.hpp"))
        structure_analysis["cpp_files"]["total"] = len(cpp_files)

        for cpp_file in cpp_files:
            rel_path = cpp_file.relative_to(repo_path)
            if self.is_test_file(rel_path):
                structure_analysis["cpp_files"]["test"].append(str(rel_path))
            else:
                structure_analysis["cpp_files"]["production"].append(str(rel_path))

        # Critical findings
        production_sol = len(structure_analysis["smart_contracts"]["production"])
        test_sol = len(structure_analysis["smart_contracts"]["test"])

        logger.info(f"  ðŸ“ Repository Type: {structure_analysis['repository_type']}")
        logger.info(f"  ðŸ“Š Smart Contracts: {production_sol} production, {test_sol} test")
        logger.info(f"  ðŸ”§ C++ Files: {len(structure_analysis['cpp_files']['production'])} production, {len(structure_analysis['cpp_files']['test'])} test")

        if test_sol > 0 and production_sol == 0:
            logger.warning("âš ï¸ CRITICAL: All Solidity files are in test directories!")
            structure_analysis["critical_findings"] = ["ALL_SOLIDITY_FILES_ARE_TESTS"]

        return structure_analysis

    def identify_repository_type(self, repo_path: Path) -> str:
        """Identify the primary technology and purpose of repository"""

        # Check for key indicators
        has_package_json = (repo_path / "package.json").exists()
        has_cargo_toml = (repo_path / "Cargo.toml").exists()
        has_cmake = (repo_path / "CMakeLists.txt").exists()
        has_hardhat = (repo_path / "hardhat.config.js").exists()
        has_truffle = (repo_path / "truffle-config.js").exists()

        sol_files = len(list(repo_path.rglob("*.sol")))
        cpp_files = len(list(repo_path.rglob("*.cpp")))
        rs_files = len(list(repo_path.rglob("*.rs")))

        if has_cmake and cpp_files > 50:
            return "cpp_blockchain_node"
        elif (has_hardhat or has_truffle) and sol_files > 10:
            return "solidity_defi_protocol"
        elif has_cargo_toml and rs_files > 20:
            return "rust_blockchain"
        elif sol_files > cpp_files and sol_files > 5:
            return "smart_contract_focused"
        elif cpp_files > sol_files and cpp_files > 10:
            return "cpp_application"
        else:
            return "mixed_or_unknown"

    def is_test_file(self, file_path: Path) -> bool:
        """Determine if a file is test code or production code"""

        path_str = str(file_path).lower()

        # Test directory indicators
        test_indicators = [
            "/test/", "/tests/", "/testing/",
            "/spec/", "/specs/", "/__tests__/",
            "/mock/", "/mocks/", "/fixtures/",
            "/examples/", "/samples/"
        ]

        # File name indicators
        filename = file_path.name.lower()
        test_filename_patterns = [
            r".*test.*\.(sol|cpp|hpp|js|ts)$",
            r".*spec.*\.(sol|cpp|hpp|js|ts)$",
            r".*mock.*\.(sol|cpp|hpp|js|ts)$",
            r".*example.*\.(sol|cpp|hpp|js|ts)$"
        ]

        # Check path
        for indicator in test_indicators:
            if indicator in path_str:
                return True

        # Check filename
        for pattern in test_filename_patterns:
            if re.match(pattern, filename):
                return True

        return False

    def validate_vulnerability_claims(self, claims: List[Dict[str, Any]], repo_path: str) -> Dict[str, Any]:
        """Validate specific vulnerability claims against repository"""

        logger.info("ðŸ” Validating Vulnerability Claims...")

        repo_structure = self.validate_repository_structure(repo_path)
        validation_results = {
            "repository_analysis": repo_structure,
            "claim_validations": [],
            "overall_assessment": {},
            "go_no_go_decision": {},
            "recommendations": []
        }

        for i, claim in enumerate(claims):
            logger.info(f"  ðŸ“‹ Validating Claim {i+1}: {claim.get('name', 'Unknown')}")

            claim_validation = self.validate_single_claim(claim, repo_path, repo_structure)
            validation_results["claim_validations"].append(claim_validation)

            # Log critical findings
            if claim_validation["critical_issues"]:
                logger.warning(f"    âš ï¸ Critical Issues Found:")
                for issue in claim_validation["critical_issues"]:
                    logger.warning(f"      - {issue}")

        # Overall assessment
        validation_results["overall_assessment"] = self.assess_overall_validity(validation_results)
        validation_results["go_no_go_decision"] = self.make_go_no_go_decision(validation_results)
        validation_results["recommendations"] = self.generate_recommendations(validation_results)

        return validation_results

    def validate_single_claim(self, claim: Dict[str, Any], repo_path: str, repo_structure: Dict[str, Any]) -> Dict[str, Any]:
        """Validate a single vulnerability claim"""

        claim_validation = {
            "claim_details": claim,
            "file_validation": {},
            "scope_validation": {},
            "evidence_validation": {},
            "critical_issues": [],
            "validity_score": 0.0,
            "recommendation": ""
        }

        # Validate file existence and production status
        claimed_file = claim.get("file", "")
        if claimed_file:
            file_path = Path(repo_path) / claimed_file

            claim_validation["file_validation"] = {
                "exists": file_path.exists(),
                "is_production": not self.is_test_file(Path(claimed_file)),
                "file_type": self.get_file_type(claimed_file),
                "in_scope_directory": self.is_in_scope_directory(claimed_file, repo_structure)
            }

            if not file_path.exists():
                claim_validation["critical_issues"].append("CLAIMED_FILE_DOES_NOT_EXIST")
            elif self.is_test_file(Path(claimed_file)):
                claim_validation["critical_issues"].append("CLAIMED_FILE_IS_TEST_CODE")

        # Validate scope compliance
        claim_validation["scope_validation"] = self.validate_scope_compliance(claim, repo_structure)

        # Validate evidence quality
        claim_validation["evidence_validation"] = self.validate_evidence_quality(claim)

        # Calculate validity score
        claim_validation["validity_score"] = self.calculate_validity_score(claim_validation)

        # Generate recommendation
        if claim_validation["validity_score"] >= 0.8:
            claim_validation["recommendation"] = "SUBMIT_TO_BUG_BOUNTY"
        elif claim_validation["validity_score"] >= 0.6:
            claim_validation["recommendation"] = "REQUIRES_ADDITIONAL_VALIDATION"
        else:
            claim_validation["recommendation"] = "DO_NOT_SUBMIT"

        return claim_validation

    def validate_scope_compliance(self, claim: Dict[str, Any], repo_structure: Dict[str, Any]) -> Dict[str, Any]:
        """Validate if claim complies with typical bug bounty scope"""

        scope_validation = {
            "file_in_scope": True,
            "vulnerability_type_in_scope": True,
            "impact_realistic": True,
            "scope_violations": []
        }

        # Check file scope
        claimed_file = claim.get("file", "")
        if claimed_file and self.is_test_file(Path(claimed_file)):
            scope_validation["file_in_scope"] = False
            scope_validation["scope_violations"].append("TEST_FILE_OUT_OF_SCOPE")

        # Check vulnerability type scope
        vuln_type = claim.get("vulnerability_type", "")
        if vuln_type in ["theoretical_analysis", "code_review_only"]:
            scope_validation["vulnerability_type_in_scope"] = False
            scope_validation["scope_violations"].append("THEORETICAL_VULNERABILITY_NOT_ACCEPTED")

        # Check impact realism
        impact = claim.get("economic_impact", "")
        if "theoretical" in impact.lower() or "estimated without PoC" in impact.lower():
            scope_validation["impact_realistic"] = False
            scope_validation["scope_violations"].append("UNREALISTIC_IMPACT_CLAIMS")

        return scope_validation

    def validate_evidence_quality(self, claim: Dict[str, Any]) -> Dict[str, Any]:
        """Validate quality of evidence provided"""

        evidence_validation = {
            "has_specific_lines": bool(claim.get("line_number")),
            "has_code_snippet": bool(claim.get("code_snippet")),
            "has_poc": bool(claim.get("proof_of_concept")),
            "has_impact_analysis": bool(claim.get("economic_impact")),
            "evidence_quality_score": 0.0
        }

        # Calculate evidence quality
        evidence_score = 0.0
        if evidence_validation["has_specific_lines"]:
            evidence_score += 0.3
        if evidence_validation["has_code_snippet"]:
            evidence_score += 0.3
        if evidence_validation["has_poc"]:
            evidence_score += 0.2
        if evidence_validation["has_impact_analysis"]:
            evidence_score += 0.2

        evidence_validation["evidence_quality_score"] = evidence_score

        return evidence_validation

    def calculate_validity_score(self, claim_validation: Dict[str, Any]) -> float:
        """Calculate overall validity score for a claim"""

        score = 0.0

        # File validation (40% weight)
        if claim_validation["file_validation"].get("exists", False):
            score += 0.15
        if claim_validation["file_validation"].get("is_production", False):
            score += 0.25

        # Scope validation (30% weight)
        if claim_validation["scope_validation"].get("file_in_scope", False):
            score += 0.15
        if claim_validation["scope_validation"].get("vulnerability_type_in_scope", False):
            score += 0.15

        # Evidence validation (30% weight)
        evidence_score = claim_validation["evidence_validation"].get("evidence_quality_score", 0.0)
        score += evidence_score * 0.30

        # Critical issue penalties
        for issue in claim_validation["critical_issues"]:
            if issue in ["CLAIMED_FILE_DOES_NOT_EXIST", "CLAIMED_FILE_IS_TEST_CODE"]:
                score = max(0.0, score - 0.5)  # Major penalty
            else:
                score = max(0.0, score - 0.1)  # Minor penalty

        return min(1.0, score)

    def assess_overall_validity(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Assess overall validity of all claims"""

        claim_scores = [cv["validity_score"] for cv in validation_results["claim_validations"]]

        overall_assessment = {
            "total_claims": len(claim_scores),
            "high_validity_claims": len([s for s in claim_scores if s >= 0.8]),
            "medium_validity_claims": len([s for s in claim_scores if 0.6 <= s < 0.8]),
            "low_validity_claims": len([s for s in claim_scores if s < 0.6]),
            "average_validity_score": np.mean(claim_scores) if claim_scores else 0.0,
            "repository_suitability": self.assess_repository_suitability(validation_results["repository_analysis"])
        }

        return overall_assessment

    def assess_repository_suitability(self, repo_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Assess if repository is suitable for bug bounty claims"""

        suitability = {
            "has_production_contracts": len(repo_analysis.get("smart_contracts", {}).get("production", [])) > 0,
            "has_production_cpp": len(repo_analysis.get("cpp_files", {}).get("production", [])) > 0,
            "repo_type_appropriate": repo_analysis.get("repository_type") in ["cpp_blockchain_node", "solidity_defi_protocol"],
            "critical_issues": repo_analysis.get("critical_findings", []),
            "suitability_score": 0.0
        }

        score = 0.0
        if suitability["has_production_contracts"] or suitability["has_production_cpp"]:
            score += 0.5
        if suitability["repo_type_appropriate"]:
            score += 0.3
        if not suitability["critical_issues"]:
            score += 0.2

        suitability["suitability_score"] = score

        return suitability

    def make_go_no_go_decision(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Make final go/no-go decision for bug bounty submission"""

        overall_assessment = validation_results["overall_assessment"]
        repo_suitability = overall_assessment["repository_suitability"]

        # Critical failure conditions
        critical_failures = []

        if overall_assessment["high_validity_claims"] == 0:
            critical_failures.append("NO_HIGH_VALIDITY_CLAIMS")

        if repo_suitability["suitability_score"] < 0.5:
            critical_failures.append("REPOSITORY_NOT_SUITABLE")

        if "ALL_SOLIDITY_FILES_ARE_TESTS" in repo_suitability.get("critical_issues", []):
            critical_failures.append("ALL_CONTRACTS_ARE_TEST_FILES")

        # Make decision
        decision = {
            "recommendation": "NO_GO" if critical_failures else "GO",
            "confidence": "HIGH",
            "critical_failures": critical_failures,
            "success_probability": self.calculate_success_probability(validation_results),
            "risk_assessment": self.assess_submission_risks(validation_results)
        }

        if decision["recommendation"] == "GO":
            if overall_assessment["average_validity_score"] > 0.8:
                decision["recommendation"] = "STRONG_GO"
            elif overall_assessment["average_validity_score"] > 0.6:
                decision["recommendation"] = "CAUTIOUS_GO"
            else:
                decision["recommendation"] = "WEAK_GO"

        return decision

    def calculate_success_probability(self, validation_results: Dict[str, Any]) -> float:
        """Calculate probability of successful bug bounty submission"""

        overall_assessment = validation_results["overall_assessment"]

        base_probability = overall_assessment["average_validity_score"]

        # Adjust for repository quality
        repo_score = overall_assessment["repository_suitability"]["suitability_score"]
        adjusted_probability = (base_probability * 0.7) + (repo_score * 0.3)

        # Penalty for critical issues
        total_critical_issues = sum(len(cv["critical_issues"]) for cv in validation_results["claim_validations"])
        penalty = min(0.5, total_critical_issues * 0.1)

        final_probability = max(0.0, adjusted_probability - penalty)

        return final_probability

    def assess_submission_risks(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Assess risks of bug bounty submission"""

        risks = {
            "duplicate_risk": "LOW",
            "out_of_scope_risk": "LOW",
            "invalid_claim_risk": "LOW",
            "reputation_risk": "LOW",
            "major_risks": []
        }

        # Check for high-risk indicators
        for claim_validation in validation_results["claim_validations"]:
            if "CLAIMED_FILE_IS_TEST_CODE" in claim_validation["critical_issues"]:
                risks["out_of_scope_risk"] = "HIGH"
                risks["major_risks"].append("Claims based on test files")

            if claim_validation["validity_score"] < 0.3:
                risks["invalid_claim_risk"] = "HIGH"
                risks["major_risks"].append("Low-quality vulnerability claims")

        repo_issues = validation_results["repository_analysis"].get("critical_findings", [])
        if "ALL_SOLIDITY_FILES_ARE_TESTS" in repo_issues:
            risks["reputation_risk"] = "HIGH"
            risks["major_risks"].append("Fundamental misunderstanding of codebase")

        return risks

    def generate_recommendations(self, validation_results: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations"""

        recommendations = []
        decision = validation_results.get("go_no_go_decision", {})

        if decision.get("recommendation") == "NO_GO":
            recommendations.extend([
                "âŒ DO NOT SUBMIT current vulnerability claims to bug bounty program",
                "ðŸ” Focus on production code rather than test files",
                "ðŸŽ¯ Develop working proof-of-concept exploits",
                "ðŸ“Š Validate claims with independent static analysis tools"
            ])
        elif decision.get("recommendation") in ["GO", "STRONG_GO", "CAUTIOUS_GO"]:
            recommendations.extend([
                "âœ… Proceed with bug bounty submission for validated claims",
                "ðŸ“‹ Include detailed proof-of-concept demonstrations",
                "ðŸŽ¯ Focus on high-validity claims first",
                "âš ï¸ Ensure all claims target production code only"
            ])
        else:
            recommendations.extend([
                "âš ï¸ Additional validation required before submission",
                "ðŸ”§ Improve evidence quality and proof-of-concept demonstrations",
                "ðŸŽ¯ Focus research on production code vulnerabilities"
            ])

        # Repository-specific recommendations
        repo_analysis = validation_results["repository_analysis"]
        if repo_analysis.get("repository_type") == "cpp_blockchain_node":
            recommendations.append("ðŸ”§ Focus on C++ networking and consensus vulnerabilities")
        elif repo_analysis.get("repository_type") == "solidity_defi_protocol":
            recommendations.append("ðŸ’° Focus on economic attack vectors in DeFi logic")

        return recommendations

    def get_file_type(self, filename: str) -> str:
        """Get file type from filename"""
        ext = Path(filename).suffix.lower()

        type_mapping = {
            ".sol": "solidity",
            ".cpp": "cpp",
            ".hpp": "cpp_header",
            ".c": "c",
            ".h": "c_header",
            ".rs": "rust",
            ".js": "javascript",
            ".ts": "typescript",
            ".py": "python"
        }

        return type_mapping.get(ext, "unknown")

    def is_in_scope_directory(self, filepath: str, repo_structure: Dict[str, Any]) -> bool:
        """Check if file is in typical bug bounty scope"""

        # Production directories are typically in scope
        if not self.is_test_file(Path(filepath)):
            return True

        # Test directories typically out of scope
        return False

    def run_vuln_hunter_analysis(self, repo_path: str) -> Dict[str, Any]:
        """Run VulnHunter AI analysis on repository"""

        logger.info("ðŸ¤– Running VulnHunter AI Analysis...")

        try:
            # Assuming VulnHunter is available in the system
            result = subprocess.run([
                "python3", "core/enhanced_vulnhunter_system.py", repo_path
            ], capture_output=True, text=True, timeout=300)

            if result.returncode == 0:
                return {"success": True, "output": result.stdout}
            else:
                return {"success": False, "error": result.stderr}

        except subprocess.TimeoutExpired:
            return {"success": False, "error": "VulnHunter analysis timed out"}
        except FileNotFoundError:
            return {"success": False, "error": "VulnHunter system not found"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def generate_comprehensive_report(self, validation_results: Dict[str, Any]) -> Dict[str, Any]:
        """Generate comprehensive validation report"""

        logger.info("ðŸ“„ Generating Comprehensive Validation Report...")

        report = {
            "metadata": {
                "validation_date": datetime.now().isoformat(),
                "validator_version": "Comprehensive Vulnerability Validator v1.0",
                "validation_framework": "Professional Bug Bounty Standards"
            },
            "executive_summary": {
                "total_claims_analyzed": validation_results["overall_assessment"]["total_claims"],
                "high_validity_claims": validation_results["overall_assessment"]["high_validity_claims"],
                "go_no_go_decision": validation_results["go_no_go_decision"]["recommendation"],
                "success_probability": validation_results["go_no_go_decision"]["success_probability"],
                "primary_recommendation": validation_results["recommendations"][0] if validation_results["recommendations"] else "No recommendations generated"
            },
            "detailed_analysis": validation_results,
            "actionable_insights": self.generate_actionable_insights(validation_results),
            "lessons_learned": self.extract_lessons_learned(validation_results)
        }

        return report

    def generate_actionable_insights(self, validation_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate actionable insights from validation"""

        insights = []

        # Repository structure insights
        repo_analysis = validation_results["repository_analysis"]
        if repo_analysis.get("smart_contracts", {}).get("production"):
            insights.append({
                "category": "Repository Analysis",
                "insight": "Production smart contracts identified",
                "action": "Focus vulnerability research on production contract files",
                "priority": "HIGH"
            })
        elif not repo_analysis.get("smart_contracts", {}).get("production"):
            insights.append({
                "category": "Repository Analysis",
                "insight": "No production smart contracts found",
                "action": "Pivot research to focus on actual production code (C++/Rust)",
                "priority": "CRITICAL"
            })

        # Claim quality insights
        low_validity_claims = validation_results["overall_assessment"]["low_validity_claims"]
        if low_validity_claims > 0:
            insights.append({
                "category": "Claim Quality",
                "insight": f"{low_validity_claims} claims have low validity scores",
                "action": "Improve evidence quality and develop working proof-of-concepts",
                "priority": "HIGH"
            })

        return insights

    def extract_lessons_learned(self, validation_results: Dict[str, Any]) -> List[str]:
        """Extract key lessons learned from validation process"""

        lessons = [
            "Always verify claimed vulnerable files exist in production code, not test directories",
            "Independent tool confirmation is essential for credible vulnerability claims",
            "Bug bounty scope typically excludes test files, mock implementations, and example code",
            "Repository architecture understanding prevents fundamental misanalysis",
            "Working proof-of-concept exploits significantly strengthen vulnerability claims"
        ]

        # Add specific lessons based on validation results
        repo_type = validation_results["repository_analysis"].get("repository_type")
        if repo_type == "cpp_blockchain_node":
            lessons.append("C++ blockchain nodes require focus on networking, consensus, and memory safety vulnerabilities")
        elif repo_type == "solidity_defi_protocol":
            lessons.append("DeFi protocols require focus on economic logic, oracle manipulation, and reentrancy vulnerabilities")

        return lessons

    def save_validation_report(self, report: Dict[str, Any], filename: str = None):
        """Save validation report to file"""

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"comprehensive_validation_report_{timestamp}.json"

        report_path = self.results_dir / filename

        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)

        logger.info(f"ðŸ’¾ Validation report saved: {report_path}")

        # Also save executive summary
        exec_summary_path = self.results_dir / f"executive_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(exec_summary_path, 'w') as f:
            json.dump(report["executive_summary"], f, indent=2)

        logger.info(f"ðŸ’¾ Executive summary saved: {exec_summary_path}")

        return report_path


def main():
    """Execute comprehensive vulnerability validation"""

    logger.info("ðŸŽ¬ Initializing Comprehensive Vulnerability Validator")
    logger.info("=" * 80)

    # Example usage - validate Oort Protocol claims
    validator = ComprehensiveVulnerabilityValidator()

    # Example claims to validate
    example_claims = [
        {
            "name": "Oracle Price Manipulation",
            "file": "test/contracts/mcp-dydx/contracts/protocol/Getters.sol",
            "line_number": 24,
            "vulnerability_type": "economic_manipulation",
            "code_snippet": 'import { IPriceOracle } from "./interfaces/IPriceOracle.sol";',
            "economic_impact": "Estimated $10M+ manipulation potential",
            "proof_of_concept": "Theoretical analysis of price oracle calls"
        },
        {
            "name": "P2P Network Buffer Overflow",
            "file": "mcp/p2p/peer.cpp",
            "line_number": 106,
            "vulnerability_type": "buffer_overflow",
            "code_snippet": "ba::async_read(*socket, boost::asio::buffer(read_header_buffer, read_header_buffer.size())",
            "economic_impact": "Network DoS and potential RCE",
            "proof_of_concept": "Requires network packet crafting analysis"
        }
    ]

    repo_path = "/Users/ankitthakur/vuln_ml_research/Olympus"

    # Run validation
    validation_results = validator.validate_vulnerability_claims(example_claims, repo_path)

    # Generate comprehensive report
    comprehensive_report = validator.generate_comprehensive_report(validation_results)

    # Save report
    report_path = validator.save_validation_report(comprehensive_report)

    # Display summary
    logger.info("ðŸŽ‰ COMPREHENSIVE VALIDATION COMPLETED!")
    logger.info("=" * 80)

    exec_summary = comprehensive_report["executive_summary"]
    logger.info(f"ðŸ“Š VALIDATION SUMMARY:")
    logger.info(f"  ðŸŽ¯ Total Claims Analyzed: {exec_summary['total_claims_analyzed']}")
    logger.info(f"  âœ… High Validity Claims: {exec_summary['high_validity_claims']}")
    logger.info(f"  ðŸš¦ Decision: {exec_summary['go_no_go_decision']}")
    logger.info(f"  ðŸ“ˆ Success Probability: {exec_summary['success_probability']:.1%}")
    logger.info(f"  ðŸ’¡ Primary Recommendation: {exec_summary['primary_recommendation']}")

    logger.info("=" * 80)

    return comprehensive_report


if __name__ == "__main__":
    comprehensive_report = main()