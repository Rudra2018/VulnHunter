code,context,function,category,severity,cwe,language,project_type,is_vulnerable,source
proc = subprocess.Popen(,"                        url, json_req)
    repeat_command = 'for n in {{1..3}}; do {} & sleep 1; done;'.format(
        exec_command)
    proc = subprocess.Popen(
        repeat_command,
        shell=True,
        stdout=subprocess.PIPE,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Serving
"shell=True,","        exec_command)
    proc = subprocess.Popen(
        repeat_command,
        shell=True,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE)
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Serving
"MODEL_URL_PATTERN = re.compile(r""https://download[.]pytorch[.]org/models/.+?[.]pth"")","import re
import sys

MODEL_URL_PATTERN = re.compile(r""https://download[.]pytorch[.]org/models/.+?[.]pth"")


def main(*roots):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)","
def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    output, err = p.communicate()
    rc = p.returncode
    enc = locale.getpreferredencoding()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchVision
"p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)","
def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)
    output, err = p.communicate()
    rc = p.returncode
    enc = locale.getpreferredencoding()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    rpn_pre_nms_top_n_test=150,
)

model.eval()
script_model = torch.jit.script(model)
opt_script_model = optimize_for_mobile(script_model)
opt_script_model.save(""app/src/main/assets/frcnn_mnetv3.pt"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    rpn_pre_nms_top_n_test=150,
)

model.eval()
script_model = torch.jit.script(model)
opt_script_model = optimize_for_mobile(script_model)
opt_script_model.save(""VisionTestApp/frcnn_mnetv3.pt"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"self.keys = pickle.load(open(cache_file, ""rb""))","            self.length = txn.stat()[""entries""]
        cache_file = ""_cache_"" + """".join(c for c in root if c in string.ascii_letters)
        if os.path.isfile(cache_file):
            self.keys = pickle.load(open(cache_file, ""rb""))
        else:
            with self.env.begin(write=False) as txn:
                self.keys = [key for key in txn.cursor().iternext(keys=True, values=False)]",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"return torch.load(file, weights_only=True)","    file = os.path.join(root, file)

    if check_integrity(file):
        return torch.load(file, weights_only=True)
    else:
        msg = (
            ""The meta file {} is not present in the root directory or is corrupted. """,torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"data = pickle.load(infile, encoding=""latin1"")","        if not check_integrity(path, self.meta[""md5""]):
            raise RuntimeError(""Dataset metadata file not found or corrupted. You can use download=True to download it"")
        with open(path, ""rb"") as infile:
            data = pickle.load(infile, encoding=""latin1"")
            self.classes = data[self.meta[""key""]]
        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}
",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"entry = pickle.load(f, encoding=""latin1"")","        for file_name, checksum in downloaded_list:
            file_path = os.path.join(self.root, self.base_folder, file_name)
            with open(file_path, ""rb"") as f:
                entry = pickle.load(f, encoding=""latin1"")
                self.data.append(entry[""data""])
                if ""labels"" in entry:
                    self.targets.extend(entry[""labels""])",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"entry = pickle.load(f, encoding=""latin1"")","        for file_name, checksum in downloaded_list:
            file_path = os.path.join(self.root, self.base_folder, file_name)
            with open(file_path, ""rb"") as f:
                entry = pickle.load(f, encoding=""latin1"")
                self.data.append(entry[""data""])
                if ""labels"" in entry:
                    self.targets.extend(entry[""labels""])",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"data = pickle.load(infile, encoding=""latin1"")","        if not check_integrity(path, self.meta[""md5""]):
            raise RuntimeError(""Dataset metadata file not found or corrupted. You can use download=True to download it"")
        with open(path, ""rb"") as infile:
            data = pickle.load(infile, encoding=""latin1"")
            self.classes = data[self.meta[""key""]]
        self.class_to_idx = {_class: i for i, _class in enumerate(self.classes)}
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"self.data, self.labels, self.matches = torch.load(self.data_file, weights_only=True)","            self.cache()

        # load the serialized data
        self.data, self.labels, self.matches = torch.load(self.data_file, weights_only=True)

    def __getitem__(self, index: int) -> Union[torch.Tensor, tuple[Any, Any, torch.Tensor]]:
        """"""",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"return torch.load(os.path.join(self.processed_folder, data_file), weights_only=True)","        # This is for BC only. We no longer cache the data in a custom binary, but simply read from the raw data
        # directly.
        data_file = self.training_file if self.train else self.test_file
        return torch.load(os.path.join(self.processed_folder, data_file), weights_only=True)

    def _load_data(self):
        image_file = f""{'train' if self.train else 't10k'}-images-idx3-ubyte""",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
base_module = importlib.import_module(base_module_name),"        raise ValueError(f""Invalid weight name provided: '{name}'."")

    base_module_name = ""."".join(sys.modules[__name__].__name__.split(""."")[:-1])
    base_module = importlib.import_module(base_module_name)
    model_modules = [base_module] + [
        x[1]
        for x in inspect.getmembers(base_module, inspect.ismodule)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
pattern = re.compile(,"    # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.
    # They are also in the checkpoints in model_urls. This pattern is used
    # to find such keys.
    pattern = re.compile(
        r""^(.*denselayer\d+\.(?:norm|relu|conv))\.((?:[12])\.(?:weight|bias|running_mean|running_var))$""
    )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"eval_tracer.trace(model.eval(), concrete_args=concrete_args)","    train_tracer = NodePathTracer(**tracer_kwargs)
    train_tracer.trace(model.train(), concrete_args=concrete_args)
    eval_tracer = NodePathTracer(**tracer_kwargs)
    eval_tracer.trace(model.eval(), concrete_args=concrete_args)
    train_nodes = list(train_tracer.node_to_qualname.values())
    eval_nodes = list(eval_tracer.node_to_qualname.values())
    if not suppress_diff_warning:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
NOTE this should be safe when calling model.eval() because that just,"    def train(self, mode=True):
        """"""
        Swap out the graph depending on the selected training mode.
        NOTE this should be safe when calling model.eval() because that just
        calls this with mode == False.
        """"""
        # NOTE: Only set self.graph if the current graph is not the desired",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"        if mode == ""train"":
            model.train()
        elif mode == ""eval"":
            model.eval()

        # Instantiate our NodePathTracer and use that to trace the model
        tracer = NodePathTracer(**tracer_kwargs)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"compiled_fn = torch.compile(fn, **compile_kwargs)","    def decorate_fn(fn):
        @functools.wraps(fn)
        def compile_hook(*args, **kwargs):
            compiled_fn = torch.compile(fn, **compile_kwargs)
            globals()[fn.__name__] = functools.wraps(fn)(compiled_fn)
            return compiled_fn(*args, **kwargs)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
_META_FILE_PATTERN = re.compile(,"        ]
    )

    _META_FILE_PATTERN = re.compile(
        rf""(?P<annotations>({'|'.join(_ANN_DECODERS.keys())}))_(?P<split>[a-zA-Z]+)(?P<year>\d+)[.]json""
    )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"_TRAIN_IMAGE_NAME_PATTERN = re.compile(r""(?P<wnid>n\d{8})_\d+[.]JPEG"")","
        return resources

    _TRAIN_IMAGE_NAME_PATTERN = re.compile(r""(?P<wnid>n\d{8})_\d+[.]JPEG"")

    def _prepare_train_data(self, data: tuple[str, BinaryIO]) -> tuple[tuple[Label, str], tuple[str, BinaryIO]]:
        path = pathlib.Path(data[0])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"_VAL_TEST_IMAGE_NAME_PATTERN = re.compile(r""ILSVRC2012_(val|test)_(?P<id>\d{8})[.]JPEG"")","            ""ILSVRC2012_validation_ground_truth.txt"": ImageNetDemux.LABEL,
        }.get(pathlib.Path(data[0]).name)

    _VAL_TEST_IMAGE_NAME_PATTERN = re.compile(r""ILSVRC2012_(val|test)_(?P<id>\d{8})[.]JPEG"")

    def _val_test_image_key(self, path: pathlib.Path) -> int:
        return int(self._VAL_TEST_IMAGE_NAME_PATTERN.match(path.name)[""id""])  # type: ignore[index]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"pattern = re.compile(r""\s*'(?P<category>\w+)';\s*%(?P<label>\d+)"")","        dp = Mapper(dp, bytes.decode, input_col=1)
        lines = tuple(zip(*iter(dp)))[1]

        pattern = re.compile(r""\s*'(?P<category>\w+)';\s*%(?P<label>\d+)"")
        categories_and_labels = cast(
            list[tuple[str, ...]],
            [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"content = cast(dict[str, Any], pickle.load(file, encoding=""latin1""))","
    def _unpickle(self, data: tuple[str, io.BytesIO]) -> dict[str, Any]:
        _, file = data
        content = cast(dict[str, Any], pickle.load(file, encoding=""latin1""))
        file.close()
        return content
",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"_IMAGES_NAME_PATTERN = re.compile(r""image_(?P<id>\d+)[.]jpg"")","        )
        return [images, anns]

    _IMAGES_NAME_PATTERN = re.compile(r""image_(?P<id>\d+)[.]jpg"")
    _ANNS_NAME_PATTERN = re.compile(r""annotation_(?P<id>\d+)[.]mat"")
    _ANNS_CATEGORY_MAP = {
        ""Faces_2"": ""Faces"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"_ANNS_NAME_PATTERN = re.compile(r""annotation_(?P<id>\d+)[.]mat"")","        return [images, anns]

    _IMAGES_NAME_PATTERN = re.compile(r""image_(?P<id>\d+)[.]jpg"")
    _ANNS_NAME_PATTERN = re.compile(r""annotation_(?P<id>\d+)[.]mat"")
    _ANNS_CATEGORY_MAP = {
        ""Faces_2"": ""Faces"",
        ""Faces_3"": ""Faces_easy"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
data = pickle.load(fobj),"
    def __iter__(self) -> Iterator[Any]:
        for _, fobj in self.source_datapipe:
            data = pickle.load(fobj)
            for _, d in enumerate(data):
                yield d
",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
importlib.import_module(dependency),"    ) -> None:
        for dependency in dependencies:
            try:
                importlib.import_module(dependency)
            except ModuleNotFoundError:
                raise ModuleNotFoundError(
                    f""{type(self).__name__}() depends on the third-party package '{dependency}'. """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    if backend not in torch.backends.quantized.supported_engines:
        raise RuntimeError(""Quantized backend not supported "")
    torch.backends.quantized.engine = backend
    model.eval()
    # Make sure that weight qconfig matches that of the serialized models
    if backend == ""fbgemm"":
        model.qconfig = torch.ao.quantization.QConfig(  # type: ignore[assignment]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"
    if quantize:
        torch.ao.quantization.convert(model, inplace=True)
        model.eval()

    return model
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
# and frozen for the rest of the training process (i.e. set as eval()). The bias term is thus still useful,"        # Note regarding bias=True:
        # Usually we can pass bias=False in conv layers followed by a norm layer.
        # But in the RAFT training reference, the BatchNorm2d layers are only activated for the first dataset,
        # and frozen for the rest of the training process (i.e. set as eval()). The bias term is thus still useful
        # for the rest of the datasets. Technically, we could remove the bias for other norm layers like Instance norm
        # because these aren't frozen, but we don't bother (also, we wouldn't be able to load the original weights).
        self.convnormrelu1 = Conv2dNormActivation(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example:

        >>> model = torchvision.models.detection.ssdlite320_mobilenet_v3_large(weights=SSDLite320_MobileNet_V3_Large_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 320, 320), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example:

        >>> model = torchvision.models.detection.ssd300_vgg16(weights=SSD300_VGG16_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 300), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"        >>>                    num_classes=2,
        >>>                    rpn_anchor_generator=anchor_generator,
        >>>                    box_roi_pool=roi_pooler)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"        >>>     targets.append(d)
        >>> output = model(images, targets)
        >>> # For inference
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
        >>>",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example::

        >>> model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example::

        >>> model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"        >>>     num_classes=80,
        >>>     anchor_generator=anchor_generator,
        >>> )
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example:

        >>> model = torchvision.models.detection.fcos_resnet50_fpn(weights=FCOS_ResNet50_FPN_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"        >>>                  rpn_anchor_generator=anchor_generator,
        >>>                  box_roi_pool=roi_pooler,
        >>>                  mask_roi_pool=mask_roi_pooler)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example::

        >>> model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
        >>>",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"        >>> model = RetinaNet(backbone,
        >>>                   num_classes=2,
        >>>                   anchor_generator=anchor_generator)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example::

        >>> model = torchvision.models.detection.retinanet_resnet50_fpn(weights=RetinaNet_ResNet50_FPN_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"        >>>                      rpn_anchor_generator=anchor_generator,
        >>>                      box_roi_pool=roi_pooler,
        >>>                      keypoint_roi_pool=keypoint_roi_pooler)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
>>> model.eval(),"    Example::

        >>> model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT)
        >>> model.eval()
        >>> x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]
        >>> predictions = model(x)
        >>>",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"        out_channels (List[int]): A list of the output channels of the model.
    """"""
    in_training = model.training
    model.eval()

    with torch.no_grad():
        # Use dummy data to retrieve the feature map sizes to avoid hard-coding their values",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model = model.eval(),"device = ""cuda"" if torch.cuda.is_available() else ""cpu""

model = raft_large(weights=Raft_Large_Weights.DEFAULT, progress=False).to(device)
model = model.eval()

list_of_flows = model(img1_batch.to(device), img2_batch.to(device))
print(f""type = {type(list_of_flows)}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model = model.eval(),"images = [transforms(d) for d in dog_list]

model = fasterrcnn_resnet50_fpn(weights=weights, progress=False)
model = model.eval()

outputs = model(images)
print(outputs)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model = model.eval(),"transforms = weights.transforms(resize_size=None)

model = fcn_resnet50(weights=weights, progress=False)
model = model.eval()

batch = torch.stack([transforms(d) for d in dog_list])
output = model(batch)['out']",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model = model.eval(),"images = [transforms(d) for d in dog_list]

model = maskrcnn_resnet50_fpn(weights=weights, progress=False)
model = model.eval()

output = model(images)
print(output)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model = model.eval(),"person_float = transforms(person_int)

model = keypointrcnn_resnet50_fpn(weights=weights, progress=False)
model = model.eval()

outputs = model([person_float])
print(outputs)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"self.resnet18 = resnet18(weights=weights, progress=False).eval()","    def __init__(self):
        super().__init__()
        weights = ResNet18_Weights.DEFAULT
        self.resnet18 = resnet18(weights=weights, progress=False).eval()
        self.transforms = weights.transforms(antialias=True)

    def forward(self, x: torch.Tensor) -> torch.Tensor:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)","        model_without_ddp = model.module

    if args.resume:
        checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)
        model_without_ddp.load_state_dict(checkpoint[""model""])
        optimizer.load_state_dict(checkpoint[""optimizer""])
        lr_scheduler.load_state_dict(checkpoint[""lr_scheduler""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"        data_loader_calibration = torch.utils.data.DataLoader(
            ds, batch_size=args.batch_size, shuffle=False, num_workers=args.workers, pin_memory=True
        )
        model.eval()
        model.fuse_model(is_qat=False)
        model.qconfig = torch.ao.quantization.get_default_qconfig(args.qbackend)
        torch.ao.quantization.prepare(model, inplace=True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
quantized_eval_model.eval(),"
            evaluate(model, criterion, data_loader_test, device=device, log_suffix=""QAT"")
            quantized_eval_model = copy.deepcopy(model_without_ddp)
            quantized_eval_model.eval()
            quantized_eval_model.to(torch.device(""cpu""))
            torch.ao.quantization.convert(quantized_eval_model, inplace=True)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","
    # Deep copy to avoid side effects on the model object.
    model = copy.deepcopy(model)
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    # Load the weights to the model to validate that everything works
    # and remove unnecessary weights (such as auxiliaries, etc.)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
state = torch.load(,"    num_models = len(inputs)
    for fpath in inputs:
        with open(fpath, ""rb"") as f:
            state = torch.load(
                f, map_location=(lambda s, _: torch.serialization.default_restore_location(s, ""cpu"")), weights_only=True
            )
        # Copies over the settings from the first checkpoint",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"dataset, _ = torch.load(cache_path, weights_only=False)","        # Attention, as the transforms are also cached!
        print(f""Loading dataset_train from {cache_path}"")
        # TODO: this could probably be weights_only=True
        dataset, _ = torch.load(cache_path, weights_only=False)
    else:
        # We need a default value for the variables below because args may come
        # from train_quantization.py which doesn't define them.",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"dataset_test, _ = torch.load(cache_path, weights_only=False)","        # Attention, as the transforms are also cached!
        print(f""Loading dataset_test from {cache_path}"")
        # TODO: this could probably be weights_only=True
        dataset_test, _ = torch.load(cache_path, weights_only=False)
    else:
        if args.weights and args.test_only:
            weights = torchvision.models.get_weight(args.weights)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)","        model_ema = utils.ExponentialMovingAverage(model_without_ddp, device=device, decay=1.0 - alpha)

    if args.resume:
        checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)
        model_without_ddp.load_state_dict(checkpoint[""model""])
        if not args.test_only:
            optimizer.load_state_dict(checkpoint[""optimizer""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"

def evaluate(model, criterion, data_loader, device, print_freq=100, log_suffix=""""):
    model.eval()
    metric_logger = utils.MetricLogger(delimiter=""  "")
    header = f""Test: {log_suffix}""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)","        lr_scheduler = main_lr_scheduler

    if args.resume:
        checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)
        model_without_ddp.load_state_dict(checkpoint[""model""], strict=not args.test_only)
        if not args.test_only:
            optimizer.load_state_dict(checkpoint[""optimizer""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"

def evaluate(model, data_loader, device, num_classes):
    model.eval()
    confmat = utils.ConfusionMatrix(num_classes)
    metric_logger = utils.MetricLogger(delimiter=""  "")
    header = ""Test:""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"dataset, _ = torch.load(cache_path, weights_only=False)","
    if args.cache_dataset and os.path.exists(cache_path):
        print(f""Loading dataset_train from {cache_path}"")
        dataset, _ = torch.load(cache_path, weights_only=False)
        dataset.transform = transform_train
    else:
        if args.distributed:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"dataset_test, _ = torch.load(cache_path, weights_only=False)","
    if args.cache_dataset and os.path.exists(cache_path):
        print(f""Loading dataset_test from {cache_path}"")
        dataset_test, _ = torch.load(cache_path, weights_only=False)
        dataset_test.transform = transform_test
    else:
        if args.distributed:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)","        model_without_ddp = model.module

    if args.resume:
        checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)
        model_without_ddp.load_state_dict(checkpoint[""model""])
        optimizer.load_state_dict(checkpoint[""optimizer""])
        lr_scheduler.load_state_dict(checkpoint[""lr_scheduler""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"

def evaluate(model, criterion, data_loader, device):
    model.eval()
    metric_logger = utils.MetricLogger(delimiter=""  "")
    header = ""Test:""
    num_processed_samples = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"model.load_state_dict(torch.load(args.resume, weights_only=True))","
    model = EmbeddingNet()
    if args.resume:
        model.load_state_dict(torch.load(args.resume, weights_only=True))

    model.to(device)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"
@torch.inference_mode()
def evaluate(model, loader, device):
    model.eval()
    embeds, labels = [], []
    dists, targets = None, None
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
m.eval(),"def freeze_batch_norm(model):
    for m in model.modules():
        if isinstance(m, torch.nn.BatchNorm2d):
            m.eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)","        model_without_ddp = model

    if args.resume is not None:
        checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)
        model_without_ddp.load_state_dict(checkpoint[""model""])

    if args.test_only:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    batch_size = batch_size or args.batch_size
    device = torch.device(args.device)

    model.eval()

    if args.distributed:
        sampler = torch.utils.data.distributed.DistributedSampler(val_dataset, shuffle=False, drop_last=True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    # FIXME remove this and make paste_masks_in_image run on the GPU
    torch.set_num_threads(1)
    cpu_device = torch.device(""cpu"")
    model.eval()
    metric_logger = utils.MetricLogger(delimiter=""  "")
    header = ""Test:""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)","        )

    if args.resume:
        checkpoint = torch.load(args.resume, map_location=""cpu"", weights_only=True)
        model_without_ddp.load_state_dict(checkpoint[""model""])
        optimizer.load_state_dict(checkpoint[""optimizer""])
        lr_scheduler.load_state_dict(checkpoint[""lr_scheduler""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume_path, map_location=""cpu"", weights_only=True)","    # load them from checkpoint if needed
    args.start_step = 0
    if args.resume_path is not None:
        checkpoint = torch.load(args.resume_path, map_location=""cpu"", weights_only=True)
        if ""model"" in checkpoint:
            # this means the user requested to resume from a training checkpoint
            model_without_ddp.load_state_dict(checkpoint[""model""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.resume_path, map_location=""cpu"", weights_only=True)","    # load them from checkpoint if needed
    args.start_step = 0
    if args.resume_path is not None:
        checkpoint = torch.load(args.resume_path, map_location=""cpu"", weights_only=True)
        if ""model"" in checkpoint:
            # this means the user requested to resume from a training checkpoint
            model_without_ddp.load_state_dict(checkpoint[""model""])",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    header=None,
):
    """"""Helper function to compute various metrics (epe, etc.) for a model on a given dataset.""""""
    model.eval()
    header = header or ""Test:""
    device = torch.device(args.device)
    metric_logger = utils.MetricLogger(delimiter=""  "")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"checkpoint = torch.load(args.checkpoint, map_location=torch.device(""cpu""), weights_only=True)","    utils.setup_ddp(args)

    if not args.weights:
        checkpoint = torch.load(args.checkpoint, map_location=torch.device(""cpu""), weights_only=True)
        if ""model"" in checkpoint:
            experiment_args = checkpoint[""args""]
            model = torchvision.prototype.models.depth.stereo.__dict__[experiment_args.model](weights=None)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchVision
model.eval(),"    """"""Helper function to compute various metrics (epe, etc.) for a model on a given dataset.
    We process as many samples as possible with ddp.
    """"""
    model.eval()
    header = header or ""Test:""
    device = torch.device(args.device)
    metric_logger = utils.MetricLogger(delimiter=""  "")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
m.eval(),"def freeze_batch_norm(model):
    for m in model.modules():
        if isinstance(m, torch.nn.BatchNorm2d):
            m.eval()


def unfreeze_batch_norm(model):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchVision
"model.compile(optimizer='adam', loss='binary_crossentropy')","      kernel_initializer='ones',
      name=layer_name_prefix + '2')(dense1)
  model = tf_keras.models.Model(inputs=[input_tensor], outputs=[output])
  model.compile(optimizer='adam', loss='binary_crossentropy')
  model.predict(tf.ones((1, 3)), steps=1)

  if h5_path:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","    model.add(tf_keras.layers.Reshape([2, 3], input_shape=[6]))
    model.add(tf_keras.layers.LSTM(10))
    model.add(tf_keras.layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy')
    model.predict(tf.ones((1, 6)), steps=1)
    return model
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","    model = tf_keras.Sequential()
    model.add(tf_keras.layers.Dense(6, input_shape=[10], activation='relu'))
    model.add(self._createSimpleSequentialModel())
    model.compile(optimizer='adam', loss='binary_crossentropy')
    model.predict(tf.ones((1, 10)), steps=1)
    return model
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","    y = tf_keras.layers.Concatenate()([input1, input2])
    y = tf_keras.layers.Dense(4, activation='softmax')(y)
    model = tf_keras.Model([input1, input2], y)
    model.compile(optimizer='adam', loss='binary_crossentropy')
    model.predict([tf.ones((1, 8)), tf.ones((1, 10))], steps=1)
    return model
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","      model = tf_keras.Sequential()
      model.add(tf_keras.layers.Dense(10, activation='relu', input_shape=[4]))
      model.add(tf_keras.layers.Dense(1, activation='sigmoid'))
      model.compile(optimizer='adam', loss='binary_crossentropy')
      model.predict(tf.ones((1, 4)), steps=1)

      h5_path = os.path.join(self._tmp_dir, 'model.h5')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","      model = tf_keras.Sequential()
      model.add(tf_keras.layers.Dense(10, activation='relu', input_shape=[4]))
      model.add(tf_keras.layers.Dense(1, activation='sigmoid'))
      model.compile(optimizer='adam', loss='binary_crossentropy')
      model.predict(tf.ones((1, 4)), steps=1)

      h5_path = os.path.join(self._tmp_dir, 'model.h5')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(loss='mean_squared_error', optimizer='sgd')","    model = tf_keras.Model(t_input, t_output)

    # `tf_keras.Model`s must be compiled before they can be saved.
    model.compile(loss='mean_squared_error', optimizer='sgd')

    conversion.save_keras_model(model, self._tmp_dir)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(loss='mean_squared_error', optimizer='sgd')","    model = tf_keras.Sequential([tf_keras.layers.Dense(1, input_shape=[2])])

    # `tf_keras.Model`s must be compiled before they can be saved.
    model.compile(loss='mean_squared_error', optimizer='sgd')

    conversion.save_keras_model(model, self._tmp_dir)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
TFHUB_VALID_URL_REGEX = re.compile(,"from tensorflowjs.converters import common

# regex for recognizing valid url for TFHub module.
TFHUB_VALID_URL_REGEX = re.compile(
    # http:// or https://
    r'^(http)s?://', re.IGNORECASE)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"outer_model.compile(loss='binary_crossentropy', optimizer='sgd')","      outer_model = tf_keras.Sequential()
      outer_model.add(inner_model)
      outer_model.add(tf_keras.layers.Dense(1, activation='sigmoid'))
      outer_model.compile(loss='binary_crossentropy', optimizer='sgd')

      x = np.ones([1, 3], dtype=np.float32)
      predict_out = outer_model.predict(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(loss='binary_crossentropy', optimizer='sgd')","      y = tf_keras.layers.Dense(1, activation='sigmoid')(y)

      model = tf_keras.Model([input1, input2], y)
      model.compile(loss='binary_crossentropy', optimizer='sgd')

      input1_val = np.ones([1, 4])
      input2_val = np.ones([1, 10])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(loss='binary_crossentropy', optimizer='sgd')","      y = tf_keras.layers.Dense(1, activation='sigmoid')(y)

      model = tf_keras.Model([input1, input2], y)
      model.compile(loss='binary_crossentropy', optimizer='sgd')

      input1_val = np.ones([1, 4])
      input2_val = np.ones([1, 10])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","    model.add(tf_keras.layers.Reshape([2, 3], input_shape=[6]))
    model.add(tf_keras.layers.LSTM(10))
    model.add(tf_keras.layers.Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy')
    model.predict(tf.ones((1, 6)), steps=1)
    tf_keras.backend.set_learning_phase(0)
    return model",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","    model = tf_keras.Sequential()
    model.add(tf_keras.layers.Dense(6, input_shape=[10], activation='relu'))
    model.add(self._createSimpleSequentialModel())
    model.compile(optimizer='adam', loss='binary_crossentropy')
    model.predict(tf.ones((1, 10)), steps=1)
    return model
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"model.compile(optimizer='adam', loss='binary_crossentropy')","    y = tf_keras.layers.Concatenate()([input1, input2])
    y = tf_keras.layers.Dense(4, activation='softmax')(y)
    model = tf_keras.Model([input1, input2], y)
    model.compile(optimizer='adam', loss='binary_crossentropy')
    model.predict([tf.ones((1, 8)), tf.ones((1, 10))], steps=1)
    return model
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"save_dir = os.path.join('..', 'test_objects', 'saved_model', 'int64_multiply')","root.f = def_function.function(lambda x: root.v1 * root.v2 * x)
to_save = root.f.get_concrete_function(input_data)

save_dir = os.path.join('..', 'test_objects', 'saved_model', 'int64_multiply')
save(root, save_dir, to_save)
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"save_dir = os.path.join('..', 'test_objects', 'saved_model', 'uint8_multiply')","root.f = def_function.function(lambda x: root.v1 * root.v2 * tf.cast(x, tf.int32))
to_save = root.f.get_concrete_function(input_data)

save_dir = os.path.join('..', 'test_objects', 'saved_model', 'uint8_multiply')
save(root, save_dir, to_save)
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
_TYPES = re.compile(,"    ])

# Type names
_TYPES = re.compile(
    r'^(?:'
    # [dcl.type.simple]
    r'(char(16_t|32_t)?)|wchar_t|'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_THIRD_PARTY_HEADERS_PATTERN = re.compile(,"# - Anything not following google file name conventions (containing an
#   uppercase character, such as Python.h or nsStringAPI.h, for example).
# - Lua headers.
_THIRD_PARTY_HEADERS_PATTERN = re.compile(
    r'^(?:[^/]*[A-Z][^/]*\.h|lua\.h|lauxlib\.h|lualib\.h)$')

# Pattern for matching FileInfo.BaseName() against test file name",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"_EMPTY_CONDITIONAL_BODY_PATTERN = re.compile(r'^\s*$', re.DOTALL)","_TEST_FILE_SUFFIX = r'(_test|_unittest|_regtest)$'

# Pattern that matches only complete whitespace, possibly across multiple lines.
_EMPTY_CONDITIONAL_BODY_PATTERN = re.compile(r'^\s*$', re.DOTALL)

# Assertion macros.  These are defined in base/logging.h and
# testing/base/public/gunit.h.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_ALT_TOKEN_REPLACEMENT_PATTERN = re.compile(,"#
# False positives include C-style multi-line comments and multi-line strings
# but those have always been troublesome for cpplint.
_ALT_TOKEN_REPLACEMENT_PATTERN = re.compile(
    r'[ =()](' + ('|'.join(_ALT_TOKEN_REPLACEMENT.keys())) + r')(?=[ (]|$)')

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_MATCH_ASM = re.compile(r'^\s*(?:asm|_asm|__asm|__asm__)',"_BLOCK_ASM = 3    # The whole block is an inline assembly block

# Match start of assembly blocks
_MATCH_ASM = re.compile(r'^\s*(?:asm|_asm|__asm|__asm__)'
                        r'(?:\s+(volatile|__volatile__))?'
                        r'\s*[{(]')
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_SEARCH_C_FILE = re.compile(r'\b(?:LINT_C_FILE|',"                        r'\s*[{(]')

# Match strings that indicate we're working on a C (not C++) file.
_SEARCH_C_FILE = re.compile(r'\b(?:LINT_C_FILE|'
                            r'vim?:\s*.*(\s*|:)filetype=c(\s*|:|$))')

# Match string that indicates we're working on a Linux Kernel file.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_SEARCH_KERNEL_FILE = re.compile(r'\b(?:LINT_KERNEL_FILE)'),"                            r'vim?:\s*.*(\s*|:)filetype=c(\s*|:|$))')

# Match string that indicates we're working on a Linux Kernel file.
_SEARCH_KERNEL_FILE = re.compile(r'\b(?:LINT_KERNEL_FILE)')

_regexp_compile_cache = {}
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_regexp_compile_cache[pattern] = sre_compile.compile(pattern),"  # performance reasons; factoring it out into a separate function turns out
  # to be noticeably expensive.
  if pattern not in _regexp_compile_cache:
    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)
  return _regexp_compile_cache[pattern].match(s)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_regexp_compile_cache[pattern] = sre_compile.compile(pattern),"    string with replacements made (or original string if no replacements)
  """"""
  if pattern not in _regexp_compile_cache:
    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)
  return _regexp_compile_cache[pattern].sub(rep, s)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_regexp_compile_cache[pattern] = sre_compile.compile(pattern),"def Search(pattern, s):
  """"""Searches the string for the pattern, caching the compiled regexp.""""""
  if pattern not in _regexp_compile_cache:
    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)
  return _regexp_compile_cache[pattern].search(s)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_CLEANSE_LINE_ESCAPES = re.compile(,"

# Matches standard C++ escape sequences per 2.13.2.3 of the C++ standard.
_RE_PATTERN_CLEANSE_LINE_ESCAPES = re.compile(
    r'\\([abfnrtv?""\\\']|\d+|x[0-9a-fA-F]+)')
# Match a single C style comment on the same line.
_RE_PATTERN_C_COMMENTS = r'/\*(?:[^*]|\*(?!/))*\*/'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_CLEANSE_LINE_C_COMMENTS = re.compile(,"# end of the line. Otherwise, we try to remove spaces from the right side,
# if this doesn't work we try on left side but only if there's a non-character
# on the right.
_RE_PATTERN_CLEANSE_LINE_C_COMMENTS = re.compile(
    r'(\s*' + _RE_PATTERN_C_COMMENTS + r'\s*$|' +
    _RE_PATTERN_C_COMMENTS + r'\s+|' +
    r'\s+' + _RE_PATTERN_C_COMMENTS + r'(?=\W)|' +",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_INVALID_INCREMENT = re.compile(,"
# Matches invalid increment: *count++, which moves pointer instead of
# incrementing a value.
_RE_PATTERN_INVALID_INCREMENT = re.compile(
    r'^\s*\*\w+(\+\+|--);')

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_TODO = re.compile(r'^//(\s*)TODO(\(.+?\))?:?(\s|$)?'),"    function_state.Count()  # Count non-blank/non-comment lines.


_RE_PATTERN_TODO = re.compile(r'^//(\s*)TODO(\(.+?\))?:?(\s|$)?')


def CheckComment(line, filename, linenum, next_line_start, error):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"_RE_PATTERN_INCLUDE = re.compile(r'^\s*#\s*include\s*([<""])([^>""]*)[>""].*$')","    CheckSectionSpacing(filename, clean_lines, classinfo, linenum, error)


_RE_PATTERN_INCLUDE = re.compile(r'^\s*#\s*include\s*([<""])([^>""]*)[>""].*$')
# Matches the first component of a filename delimited by -s and _s. That is:
#  _RE_FIRST_COMPONENT.match('foo').group(0) == 'foo'
#  _RE_FIRST_COMPONENT.match('foo.cc').group(0) == 'foo'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_FIRST_COMPONENT = re.compile(r'^[^-_.]+'),"#  _RE_FIRST_COMPONENT.match('foo.cc').group(0) == 'foo'
#  _RE_FIRST_COMPONENT.match('foo-bar_baz.cc').group(0) == 'foo'
#  _RE_FIRST_COMPONENT.match('foo_bar-baz.cc').group(0) == 'foo'
_RE_FIRST_COMPONENT = re.compile(r'^[^-_.]+')


def _DropCommonSuffixes(filename):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_REF_PARAM = re.compile(,"    r'\s*<(?:<(?:<[^<>]*>|[^<>])*>|[^<>])*>|'
    r'::)+')
# A call-by-reference parameter ends with '& identifier'.
_RE_PATTERN_REF_PARAM = re.compile(
    r'(' + _RE_PATTERN_TYPE + r'(?:\s*(?:\bconst\b|[*]))*\s*'
    r'&\s*' + _RE_PATTERN_IDENT + r')\s*(?:=[^,()]+)?[,)]')
# A call-by-const-reference parameter either ends with 'const& identifier'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_STRING = re.compile(r'\bstring\b'),"    ('<utility>', ('forward', 'make_pair', 'move', 'swap')),
    )

_RE_PATTERN_STRING = re.compile(r'\bstring\b')

_re_pattern_headers_maybe_templates = []
for _header, _templates in _HEADERS_MAYBE_TEMPLATES:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"(re.compile(r'[^>.]\b' + _template + r'(<.*?>)?\([^\)]'),","    # Match max<type>(..., ...), max(..., ...), but not foo->max, foo.max or
    # type::max().
    _re_pattern_headers_maybe_templates.append(
        (re.compile(r'[^>.]\b' + _template + r'(<.*?>)?\([^\)]'),
            _template,
            _header))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"(re.compile(r'(\<|\b)' + _template + r'\s*\<'),","for _header, _templates in _HEADERS_CONTAINING_TEMPLATES:
  for _template in _templates:
    _re_pattern_templates.append(
        (re.compile(r'(\<|\b)' + _template + r'\s*\<'),
         _template + '<>',
         _header))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
_RE_PATTERN_EXPLICIT_MAKEPAIR = re.compile(r'\bmake_pair\s*<'),"            'Add #include ' + required_header_unstripped + ' for ' + template)


_RE_PATTERN_EXPLICIT_MAKEPAIR = re.compile(r'\bmake_pair\s*<')


def CheckMakePairUsesDeduction(filename, clean_lines, linenum, error):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
pattern = re.compile(val),"            # file's ""exclude_files"" filter is meant to be checked against ""bar""
            # and not ""baz"" nor ""bar/baz.cc"".
            if base_name:
              pattern = re.compile(val)
              if pattern.match(base_name):
                if _cpplint_state.quiet:
                  # Suppress ""Ignoring file"" warning when using --quiet.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","  def testInference(self):
    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model.json')","    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()

      inference.predict(binary_path, model_path, test_data_dir, tmp_dir, backend=backend)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","
  # Todo(linazhao): Add a test model that outputs multiple tensors.
  def testInferenceWithOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model.json')","  # Todo(linazhao): Add a test model that outputs multiple tensors.
  def testInferenceWithOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"  def testInferenceWithOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()

    inference.predict(binary_path, model_path, test_data_dir, tmp_dir, tf_output_name_file='tf_output_name.json')",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","    shutil.rmtree(tmp_dir)

  def testInferenceWithNonExistingOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model.json')","
  def testInferenceWithNonExistingOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"  def testInferenceWithNonExistingOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()

    # Throws an error",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","  def testInferenceWithStructuredOutputKeys(self):
    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model_structured_outputs.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model_structured_outputs.json')","    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model_structured_outputs.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model_structured_outputs.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()

      inference.predict(binary_path, model_path, test_data_dir, tmp_dir, backend=backend)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","  def testInference(self):
    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model.json')","    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()

      inference.predict(binary_path, model_path, test_data_dir, tmp_dir, backend=backend)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","
  # Todo(linazhao): Add a test model that outputs multiple tensors.
  def testInferenceWithOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model.json')","  # Todo(linazhao): Add a test model that outputs multiple tensors.
  def testInferenceWithOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"  def testInferenceWithOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()

    inference.predict(binary_path, model_path, test_data_dir, tmp_dir, tf_output_name_file='tf_output_name.json')",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","    shutil.rmtree(tmp_dir)

  def testInferenceWithNonExistingOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model.json')","
  def testInferenceWithNonExistingOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"  def testInferenceWithNonExistingOutputNameFile(self):
    binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
    model_path = os.path.join('../test_data', 'model.json')
    test_data_dir = os.path.join('../test_data')
    tmp_dir = tempfile.mkdtemp()

    # Throws an error",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"binary_path = os.path.join('../binaries', 'tfjs-inference-linux')","  def testInferenceWithStructuredOutputKeys(self):
    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model_structured_outputs.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"model_path = os.path.join('../test_data', 'model_structured_outputs.json')","    backends = ['cpu', 'wasm']
    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model_structured_outputs.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
test_data_dir = os.path.join('../test_data'),"    for backend in backends:
      binary_path = os.path.join('../binaries', 'tfjs-inference-linux')
      model_path = os.path.join('../test_data', 'model_structured_outputs.json')
      test_data_dir = os.path.join('../test_data')
      tmp_dir = tempfile.mkdtemp()

      inference.predict(binary_path, model_path, test_data_dir, tmp_dir, backend=backend)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow.js
"subprocess.check_call(build_frontend_command[platform.system()], shell=True)","            os.remove(self.source_server_file)

        try:
            subprocess.check_call(build_frontend_command[platform.system()], shell=True)
        except OSError:
            assert 0, ""build failed""
        copy2(self.source_server_file, self.dest_file_name)",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"build_plugins_command[platform.system()], shell=True","        try:
            if self.plugins == ""endpoints"":
                subprocess.check_call(
                    build_plugins_command[platform.system()], shell=True
                )
            else:
                raise OSError(""No such rule exists"")",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"subprocess.check_call(cmd, shell=True)","    )
    print(f""## In directory: {os.getcwd()} | Executing command: {cmd}\n"")
    try:
        subprocess.check_call(cmd, shell=True)
        marfile = ""{}.mar"".format(model[""model_name""])
        print(""## {} is generated.\n"".format(marfile))
        mar_set.add(marfile)",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
p = subprocess.Popen(,"
def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
    )
    output, err = p.communicate()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True","def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
    )
    output, err = p.communicate()
    rc = p.returncode",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"subprocess.run([cmd], shell=True, check=True)","        print(f""Executing command: {cmd}"")
    else:
        try:
            subprocess.run([cmd], shell=True, check=True)
        except subprocess.CalledProcessError as e:
            raise (e)
",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"subprocess.run([cmd], shell=True, check=True)","        print(f""Executing command: {cmd}"")
    else:
        try:
            subprocess.run([cmd], shell=True, check=True)
        except subprocess.CalledProcessError as e:
            raise (e)
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"subprocess.run([cmd], shell=True, check=True)","        print(f""Executing command: {cmd}"")
    else:
        try:
            subprocess.run([cmd], shell=True, check=True)
        except subprocess.CalledProcessError as e:
            raise (e)
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""","    print(""## Started regression pytests"")
    os.chdir(os.path.join(REPO_ROOT, ""test"", ""pytest""))
    cmd = (
        ""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""
        "" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""
        "" ../../frontend/server/src/main/resources/proto/management.proto""
    )",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
"""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""","    print(""## Started regression pytests"")
    os.chdir(os.path.join(REPO_ROOT, ""test"", ""pytest""))
    cmd = (
        ""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""
        "" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""
        "" ../../frontend/server/src/main/resources/proto/management.proto""
    )",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
"""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""","    print(""## Started regression pytests"")
    os.chdir(os.path.join(REPO_ROOT, ""test"", ""pytest""))
    cmd = (
        ""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""
        "" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""
        "" ../../frontend/server/src/main/resources/proto/management.proto""
    )",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
"""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""","    print(""## Started regression pytests"")
    os.chdir(os.path.join(REPO_ROOT, ""test"", ""pytest""))
    cmd = (
        ""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""
        "" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""
        "" ../../frontend/server/src/main/resources/proto/management.proto""
    )",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
""" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""","    os.chdir(os.path.join(REPO_ROOT, ""test"", ""pytest""))
    cmd = (
        ""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""
        "" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""
        "" ../../frontend/server/src/main/resources/proto/management.proto""
    )
    status = os.system(cmd)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
""" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""","    os.chdir(os.path.join(REPO_ROOT, ""test"", ""pytest""))
    cmd = (
        ""python -m grpc_tools.protoc -I ../../third_party/google/rpc --proto_path=../../frontend/server/src/main/resources/proto/""
        "" --python_out=. --grpc_python_out=. ../../frontend/server/src/main/resources/proto/inference.proto""
        "" ../../frontend/server/src/main/resources/proto/management.proto""
    )
    status = os.system(cmd)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
ret_code = subprocess.run(,"        s3_command = f""{self.s3_command} --exclude '*' --include '*.whl' {local_folder_path} {self.s3_bucket.rstrip('/')}/whl/{self.channel}""

        try:
            ret_code = subprocess.run(
                s3_command, check=True, stdout=subprocess.PIPE, universal_newlines=True, shell=True
            )
        except subprocess.CalledProcessError as e:",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
ret_code = subprocess.run(,"        s3_command = f""{self.s3_command} --exclude '*' --include '*.whl' {local_folder_path} {self.s3_bucket.rstrip('/')}/whl/{self.channel}""

        try:
            ret_code = subprocess.run(
                s3_command, check=True, stdout=subprocess.PIPE, universal_newlines=True, shell=True
            )
        except subprocess.CalledProcessError as e:",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"s3_command, check=True, stdout=subprocess.PIPE, universal_newlines=True, shell=True","
        try:
            ret_code = subprocess.run(
                s3_command, check=True, stdout=subprocess.PIPE, universal_newlines=True, shell=True
            )
        except subprocess.CalledProcessError as e:
            LOGGER.info(f""S3 upload command failed: {s3_command}. Exception: {e}"")",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"run_process(ag_call, shell=True)","        # run AggregateReport
        ag_call = 'java -jar {} --tool Reporter --generate-csv {} --input-jtl {} --plugin-type AggregateReport'.format(CMDRUNNER, outfile, tmpfile)
        if PLATFORM == 'Windows':
            run_process(ag_call, shell=True)
        else:
            run_process(ag_call)
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"p = subprocess.Popen(command, stdout=subprocess.PIPE,","
def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(command, stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE, shell=True)
    raw_output, raw_err = p.communicate()
    rc = p.returncode",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"stderr=subprocess.PIPE, shell=True)","def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(command, stdout=subprocess.PIPE,
                         stderr=subprocess.PIPE, shell=True)
    raw_output, raw_err = p.communicate()
    rc = p.returncode
    enc = locale.getpreferredencoding()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
yaml_dict = yaml.load(f),"def load_benchmark_config(bm_config_path, skip_ts_install, skip_upload):
    yaml = ruamel.yaml.YAML()
    with open(bm_config_path, ""r"") as f:
        yaml_dict = yaml.load(f)

        benchmark_config = BenchmarkConfig(yaml_dict, skip_ts_install, skip_upload)
        benchmark_config.load_config()",yaml.load,Unsafe YAML,HIGH,CWE-502,Python,ML/AI,1,ML/AI:TorchServe
"def execute(command, wait=False, stdout=None, stderr=None, shell=True):","    shutil.rmtree(WF_STORE, ignore_errors=True)


def execute(command, wait=False, stdout=None, stderr=None, shell=True):
    print(""execute: {}"".format(command))
    cmd = Popen(
        command,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
"pattern = re.compile(r""(.+=)?http(s)?://.+"", re.IGNORECASE)","            cmd.append(""-m"")
            cmd.extend(args.models)
            if not args.model_store:
                pattern = re.compile(r""(.+=)?http(s)?://.+"", re.IGNORECASE)
                for model_url in args.models:
                    if not pattern.match(model_url) and model_url != ""ALL"":
                        print(""--model-store is required to load model locally."")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
module = importlib.import_module(module_name),"        if module_name.endswith("".py""):
            module_name = module_name[:-3]
        module_name = module_name.split(""/"")[-1]
        module = importlib.import_module(module_name)
        function_name = None if len(temp) == 1 else temp[1]
        return module, function_name
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"module = importlib.import_module(module_name, ""ts.torch_handler"")","
    def _load_default_handler(self, handler):
        module_name = "".{0}"".format(handler)
        module = importlib.import_module(module_name, ""ts.torch_handler"")
        return module

    def _load_default_envelope(self, envelope):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
module = importlib.import_module(,"
    def _load_default_envelope(self, envelope):
        module_name = "".{0}"".format(envelope)
        module = importlib.import_module(
            module_name, ""ts.torch_handler.request_envelope""
        )
        envelope_class = list_classes_from_module(module)[0]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
self.model.eval(),"                else ""cpu""
            )
            self.model.to(self.device)
            self.model.eval()
            self.initialized = True

    def postprocess(self, data):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"state_dict = torch.load(model_pt_path, map_location=self.map_location)","            )

        model_class = model_class_definitions[0]
        state_dict = torch.load(model_pt_path, map_location=self.map_location)
        model = model_class()
        model.load_state_dict(state_dict)
        return model",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchServe
tensor = torch.load(io.BytesIO(image)),"        values = []
        for row in data:
            image = row.get(""data"") or row.get(""body"")
            tensor = torch.load(io.BytesIO(image))
            values.append(tensor)
        data = self.inference(torch.stack(values))
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchServe
self.model.eval(),"            self.model = self._load_torchscript_model(model_pt_path)

        self.model.to(self.device)
        self.model.eval()

        logger.debug(""Model file %s loaded successfully"", model_pt_path)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"module = importlib.import_module(model_file.split(""."")[0])","        if not os.path.isfile(model_def_path):
            raise RuntimeError(""Missing the model.py file"")

        module = importlib.import_module(model_file.split(""."")[0])
        model_class_definitions = list_classes_from_module(module)
        if len(model_class_definitions) != 1:
            raise ValueError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"state_dict = torch.load(model_pt_path, map_location=map_location)","            map_location = (
                None if (XLA_AVAILABLE and self.map_location is None) else self.device
            )
            state_dict = torch.load(model_pt_path, map_location=map_location)
            model.load_state_dict(state_dict)
        return model
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchServe
self.model.eval(),"                model_dir, model_file, self.model_pt_path
            )
            self.model.to(self.device)
            self.model.eval()

        # Convert your model by following instructions: https://pytorch.org/tutorials/intermediate/nvfuser_intro_tutorial.html
        # For TensorRT support follow instructions here: https://pytorch.org/TensorRT/getting_started/getting_started_with_python_api.html#getting-started-with-python-api",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
self.model.eval(),"        # For TensorRT support follow instructions here: https://pytorch.org/TensorRT/getting_started/getting_started_with_python_api.html#getting-started-with-python-api
        elif self.model_pt_path.endswith("".pt""):
            self.model = self._load_torchscript_model(self.model_pt_path)
            self.model.eval()

        # Convert your model by following instructions: https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html
        elif self.model_pt_path.endswith("".onnx"") and ONNX_AVAILABLE:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"""This approach of specifying torch.compile() options is deprecated. The new standard approach is mentioned in https://github.com/pytorch/serve/issues/3164""","                )

                logger.warning(
                    ""This approach of specifying torch.compile() options is deprecated. The new standard approach is mentioned in https://github.com/pytorch/serve/issues/3164""
                )
        else:
            valid_backend = False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
self.model = torch.compile(,"            )
            # Compilation will delay your model initialization
            try:
                self.model = torch.compile(
                    self.model,
                    **compile_options,
                )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"module = importlib.import_module(model_file.split(""."")[0])","        if not os.path.isfile(model_def_path):
            raise RuntimeError(""Missing the model.py file"")

        module = importlib.import_module(model_file.split(""."")[0])
        model_class_definitions = list_classes_from_module(module)
        if len(model_class_definitions) != 1:
            raise ValueError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
self.source_vocab = torch.load(source_vocab),"        )
        if source_vocab:
            # Backward compatibility
            self.source_vocab = torch.load(source_vocab)
        else:
            self.source_vocab = torch.load(self.get_source_vocab_path(context))
        # Captum initialization",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchServe
self.source_vocab = torch.load(self.get_source_vocab_path(context)),"            # Backward compatibility
            self.source_vocab = torch.load(source_vocab)
        else:
            self.source_vocab = torch.load(self.get_source_vocab_path(context))
        # Captum initialization
        self.lig = LayerIntegratedGradients(self.model, self.model.embedding)
        self.initialized = True",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TorchServe
CONTRACTIONS_PATTERN = re.compile(,"logger = logging.getLogger(__name__)


CONTRACTIONS_PATTERN = re.compile(
    ""({})"".format(""|"".join(CONTRACTION_MAP.keys())),
    flags=re.IGNORECASE | re.DOTALL,
)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"CLEANUP_REGEX = re.compile(""<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});"")","
logger = logging.getLogger(__name__)

CLEANUP_REGEX = re.compile(""<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});"")


def list_classes_from_module(module, parent_class=None):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))","    "" "",
]

_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))


def _basic_english_normalize(line):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"importlib.import_module(f""{module_prefix}.{module_name}"")","
    if module_prefix:
        module = (
            importlib.import_module(f""{module_prefix}.{module_name}"")
            if len(module_name) > 0
            else importlib.import_module(module_prefix)
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
else importlib.import_module(module_prefix),"        module = (
            importlib.import_module(f""{module_prefix}.{module_name}"")
            if len(module_name) > 0
            else importlib.import_module(module_prefix)
        )
    elif len(module_name) > 0:
        module = importlib.import_module(module_name)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
module = importlib.import_module(module_name),"            else importlib.import_module(module_prefix)
        )
    elif len(module_name) > 0:
        module = importlib.import_module(module_name)
    else:
        raise ImportError(f""module name is not defined."")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
model.eval(),"
    checkpoint_prefix = None
    # Set the model to evaluation mode
    model.eval()

    # Extract the concrete arguments for the model's forward method
    sig = inspect.signature(model.forward)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"REPO_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../.."")","from argparse import ArgumentParser

# To help discover local modules
REPO_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../.."")
sys.path.append(REPO_ROOT)

from ts_scripts.utils import check_ts_version, try_and_handle",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
"REPO_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../.."")","import sys

# To help discover local modules
REPO_ROOT = os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../.."")
sys.path.append(REPO_ROOT)

from setup import get_nightly_version",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
"pattern = re.compile("" TS_METRICS | MODEL_METRICS "")","        lines = logfile.readlines()

    metrics_dict_list = []
    pattern = re.compile("" TS_METRICS | MODEL_METRICS "")
    for line in lines:
        if pattern.search(line):
            segments = line.split(""|"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"def execute(command, wait=False, stdout=None, stderr=None, shell=True):","from subprocess import Popen


def execute(command, wait=False, stdout=None, stderr=None, shell=True):
    print(command)
    cmd = Popen(
        command,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TorchServe
pattern = re.compile(v),"
    for k, v in metrics.items():
        all_lines = []
        pattern = re.compile(v)
        for line in lines:
            if pattern.search(line):
                all_lines.append(line.split(""|"")[0].split("":"")[3].strip())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
pattern = re.compile(pattern),"

def extract_entity(data, pattern, index, delim="" ""):
    pattern = re.compile(pattern)
    for line in data:
        if pattern.search(line):
            return line.split(delim)[index].strip()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TorchServe
"REPO_ROOT = os.path.join(conda_build_dir, "".."", "".."")","from ts_scripts.utils import try_and_handle

conda_build_dir = os.path.dirname(os.path.abspath(__file__))
REPO_ROOT = os.path.join(conda_build_dir, "".."", "".."")
MINICONDA_DOWNLOAD_URL = (
    ""https://repo.anaconda.com/miniconda/Miniconda3-py39_4.9.2-Linux-x86_64.sh""
)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TorchServe
"""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),","

REPLACE_PATTERNS = {
    ""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),
    ""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),
}
REPLACE_FILES = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),","
REPLACE_PATTERNS = {
    ""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),
    ""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),
}
REPLACE_FILES = {
    ""init"": ""src/datasets/__init__.py"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"patterns.append(re.compile(f"".*({ext_pattern})(\\..+)?$""))","        patterns = []
        if extensions:
            ext_pattern = ""|"".join(re.escape(ext) for ext in extensions)
            patterns.append(re.compile(f"".*({ext_pattern})(\\..+)?$""))
        if file_names:
            fn_pattern = ""|"".join(re.escape(fn) for fn in file_names)
            patterns.append(re.compile(rf"".*[\/]?({fn_pattern})$""))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"patterns.append(re.compile(rf"".*[\/]?({fn_pattern})$""))","            patterns.append(re.compile(f"".*({ext_pattern})(\\..+)?$""))
        if file_names:
            fn_pattern = ""|"".join(re.escape(fn) for fn in file_names)
            patterns.append(re.compile(rf"".*[\/]?({fn_pattern})$""))
        if patterns:
            return DataFilesList(
                [data_file for data_file in self if any(pattern.match(data_file) for pattern in patterns)],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"_uppercase_uppercase_re = re.compile(r""([A-Z]+)([A-Z][a-z])"")","import re


_uppercase_uppercase_re = re.compile(r""([A-Z]+)([A-Z][a-z])"")
_lowercase_uppercase_re = re.compile(r""([a-z\d])([A-Z])"")

_single_underscore_re = re.compile(r""(?<!_)_(?!_)"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"_lowercase_uppercase_re = re.compile(r""([a-z\d])([A-Z])"")","

_uppercase_uppercase_re = re.compile(r""([A-Z]+)([A-Z][a-z])"")
_lowercase_uppercase_re = re.compile(r""([a-z\d])([A-Z])"")

_single_underscore_re = re.compile(r""(?<!_)_(?!_)"")
_multiple_underscores_re = re.compile(r""(_{2,})"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"_single_underscore_re = re.compile(r""(?<!_)_(?!_)"")","_uppercase_uppercase_re = re.compile(r""([A-Z]+)([A-Z][a-z])"")
_lowercase_uppercase_re = re.compile(r""([a-z\d])([A-Z])"")

_single_underscore_re = re.compile(r""(?<!_)_(?!_)"")
_multiple_underscores_re = re.compile(r""(_{2,})"")

_split_re = r""^\w+(\.\w+)*$""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"_multiple_underscores_re = re.compile(r""(_{2,})"")","_lowercase_uppercase_re = re.compile(r""([a-z\d])([A-Z])"")

_single_underscore_re = re.compile(r""(?<!_)_(?!_)"")
_multiple_underscores_re = re.compile(r""(_{2,})"")

_split_re = r""^\w+(\.\w+)*$""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
_SUB_SPEC_RE = re.compile(,"
HF_GCP_BASE_URL = ""https://storage.googleapis.com/huggingface-nlp/cache/datasets""

_SUB_SPEC_RE = re.compile(
    rf""""""
^
 (?P<split>{_split_re[1:-1]})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"_ADDITION_SEP_RE = re.compile(r""\s*\+\s*"")","    re.X,
)

_ADDITION_SEP_RE = re.compile(r""\s*\+\s*"")


class DatasetNotOnHfGcsError(ConnectionError):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
module = importlib.import_module(module_path),"
def import_main_class(module_path) -> Optional[type[DatasetBuilder]]:
    """"""Import a module at module_path and return its main class: a DatasetBuilder""""""
    module = importlib.import_module(module_path)
    # Find the main class in our imported module
    module_main_cls = None
    for name, obj in module.__dict__.items():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
module = importlib.import_module(module_path),"        download_config: Mainly use `token` or `storage_options` to support different platforms and auth types.
    """"""

    module = importlib.import_module(module_path)

    # TODO(QL): always update the module to add subsequent new authentication without removing old ones
    if hasattr(module, ""_patched_for_streaming"") and module._patched_for_streaming:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"_VERSION_REG = re.compile(r""^(?P<major>\d+)"" r""\.(?P<minor>\d+)"" r""\.(?P<patch>\d+)$"")","from typing import Optional, Union


_VERSION_REG = re.compile(r""^(?P<major>\d+)"" r""\.(?P<minor>\d+)"" r""\.(?P<patch>\d+)$"")


@total_ordering",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
>>> snli_module = importlib.import_module(dataset_module.module_path),"        >>> from datasets.streaming import patch_submodule, xjoin
        >>>
        >>> dataset_module = dataset_module_factory(""snli"")
        >>> snli_module = importlib.import_module(dataset_module.module_path)
        >>> patcher = patch_submodule(snli_module, ""os.path.join"", xjoin)
        >>> patcher.start()
        >>> assert snli_module.os.path.join is xjoin",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"SINGLE_SLASH_AFTER_PROTOCOL_PATTERN = re.compile(r""(?<!:):/"")","    for extension in fs_class.extensions
}
SINGLE_FILE_COMPRESSION_PROTOCOLS = {fs_class.protocol for fs_class in COMPRESSION_FILESYSTEMS}
SINGLE_SLASH_AFTER_PROTOCOL_PATTERN = re.compile(r""(?<!:):/"")


MAGIC_NUMBER_TO_COMPRESSION_PROTOCOL = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"def joinpath(self, *p: tuple[str, ...]) -> ""xPath"":","        """"""
        return xopen(str(self), *args, **kwargs)

    def joinpath(self, *p: tuple[str, ...]) -> ""xPath"":
        """"""Extend :func:`xjoin` to support argument of type :obj:`~pathlib.Path`.

        Args:",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:Datasets
"return torch.load(io.BytesIO(data), weights_only=True)","def torch_loads(data: bytes):
    import torch

    return torch.load(io.BytesIO(data), weights_only=True)


# Obtained by checking `decoders` in `webdataset.autodecode`",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Datasets
sql_str = str(sql.compile(dialect=engine.dialect)),"
                if isinstance(sql, sqlalchemy.sql.Selectable):
                    engine = sqlalchemy.create_engine(config_kwargs[""con""].split(""://"")[0] + ""://"")
                    sql_str = str(sql.compile(dialect=engine.dialect))
                    config_kwargs[""sql""] = sql_str
                else:
                    raise TypeError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Datasets
"unk_token_regex = re.compile(""(.{1}\b)?(unk|oov)(\b.{1})?"", flags=re.IGNORECASE)","            an Annotation object
    """"""

    unk_token_regex = re.compile(""(.{1}\b)?(unk|oov)(\b.{1})?"", flags=re.IGNORECASE)

    def __init__(
        self,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Tokenizers
def eval(self):,"            # the deepspeed optimizer further wraps the optimizer
            self.optimizer.optimizer.train()

    def eval(self):
        """"""
        Sets the optimizer to ""eval"" mode. Useful for optimizers like `schedule_free`
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
self.optimizer.eval(),"        Sets the optimizer to ""eval"" mode. Useful for optimizers like `schedule_free`
        """"""
        if hasattr(self.optimizer, ""eval"") and callable(self.optimizer.eval):
            self.optimizer.eval()

    def step(self, closure=None):
        if is_lomo_available():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
scaler_state = torch.load(input_scaler_file),"        scaler = None
        if self.scaler is not None and self.is_fsdp2:
            input_scaler_file = os.path.join(input_dir, SCALER_NAME)
            scaler_state = torch.load(input_scaler_file)
            self.scaler.load_state_dict(scaler_state)
            # We also need to call the `_lazy_init_scale_growth_tracker` to initialize the scaler, as it would else be called
            # on the first call to scale",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Accelerate
"model = torch.compile(model, **self.state.dynamo_plugin.to_kwargs())","            if self.state.dynamo_plugin.use_regional_compilation:
                model = compile_regions(model, **self.state.dynamo_plugin.to_kwargs())
            else:
                model = torch.compile(model, **self.state.dynamo_plugin.to_kwargs())

        # Get old params and canonicalize - we canonicalize to have the mapping easy
        old_named_params = fsdp2_canonicalize_names(self._get_named_parameters(*tuple(result), drop_refs=True))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"reg = re.compile(kwargs[""ignored_modules""])","                    }

                    if isinstance(kwargs[""ignored_modules""], str):
                        reg = re.compile(kwargs[""ignored_modules""])
                        ignored = []
                        for name, module in model.named_modules():
                            if reg.fullmatch(name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"model = torch.compile(model, **self.state.dynamo_plugin.to_kwargs())","            if self.state.dynamo_plugin.use_regional_compilation:
                model = compile_regions(model, **self.state.dynamo_plugin.to_kwargs())
            else:
                model = torch.compile(model, **self.state.dynamo_plugin.to_kwargs())
        return model

    def _prepare_ao(self, *args):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"engine.compile(backend=compile_kwargs.pop(""backend""), compile_kwargs=compile_kwargs)","                if self.state.dynamo_plugin.use_regional_compilation:
                    compile_regions_deepspeed(engine.module, **compile_kwargs)
                else:
                    engine.compile(backend=compile_kwargs.pop(""backend""), compile_kwargs=compile_kwargs)
            if optimizer is not None:
                optimizer = DeepSpeedOptimizerWrapper(optimizer)
            if scheduler is not None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"reg = re.compile(r""(.*?)-\d{5}-of-\d{5}"")","
            # make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005
            filename_no_suffix = filename.replace("".bin"", """")
            reg = re.compile(r""(.*?)-\d{5}-of-\d{5}"")

            if (
                filename.startswith(weights_no_suffix)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
Check whether the module was compiled with torch.compile(),"
def is_compiled_module(module: torch.nn.Module) -> bool:
    """"""
    Check whether the module was compiled with torch.compile()
    """"""
    if not hasattr(torch, ""_dynamo""):
        return False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
Check whether the module has submodules that were compiled with `torch.compile()`.,"
def has_compiled_regions(module: torch.nn.Module) -> bool:
    """"""
    Check whether the module has submodules that were compiled with `torch.compile()`.
    """"""
    if not hasattr(torch, ""_dynamo""):
        return False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
Additional keyword arguments to pass to `torch.compile()`.,"        module (`torch.nn.Module`):
            The model to compile.
        **compile_kwargs:
            Additional keyword arguments to pass to `torch.compile()`.

    Returns:
        `torch.nn.Module`: A new instance of the model with some compiled regions.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"new_module.append(torch.compile(submodule, **compile_kwargs))","        if is_repeated_blocks(module):
            new_module = torch.nn.ModuleList()
            for submodule in module:
                new_module.append(torch.compile(submodule, **compile_kwargs))
        elif has_repeated_blocks(module):
            new_module = module.__class__.__new__(module.__class__)
            new_module.__dict__.update(module.__dict__)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"new_module = torch.compile(module, **compile_kwargs)","            for name, submodule in module.named_children():
                new_module.add_module(name, _compile_regions(submodule, **compile_kwargs))
        else:
            new_module = torch.compile(module, **compile_kwargs)

        return new_module
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"`torch.compile(...)` interferes with, version of trgional compilation uses the inplace `module.compile()` method","    """"""
    Performs regional compilation the same way as `compile_regions`, but specifically for `DeepSpeedEngine.module`.
    Since the model is wrapped in a `DeepSpeedEngine` and has many added hooks, offloaded parameters, etc that
    `torch.compile(...)` interferes with, version of trgional compilation uses the inplace `module.compile()` method
    instead.

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"`torch.compile(...)` interferes with, version of trgional compilation uses the inplace `module.compile()` method","    """"""
    Performs regional compilation the same way as `compile_regions`, but specifically for `DeepSpeedEngine.module`.
    Since the model is wrapped in a `DeepSpeedEngine` and has many added hooks, offloaded parameters, etc that
    `torch.compile(...)` interferes with, version of trgional compilation uses the inplace `module.compile()` method
    instead.

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
Additional keyword arguments to pass to `module.compile()`.,"        module (`torch.nn.Module`):
            The model to compile.
        **compile_kwargs:
            Additional keyword arguments to pass to `module.compile()`.
    """"""

    if is_repeated_blocks(module):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
submodule.compile(**compile_kwargs),"
    if is_repeated_blocks(module):
        for submodule in module:
            submodule.compile(**compile_kwargs)
    elif has_repeated_blocks(module):
        for child in module.children():
            compile_regions_deepspeed(child, **compile_kwargs)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
module.compile(**compile_kwargs),"        for child in module.children():
            compile_regions_deepspeed(child, **compile_kwargs)
    else:  # leaf node
        module.compile(**compile_kwargs)


def model_has_dtensor(model: torch.nn.Module) -> bool:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
Turn torch.compile() into a no-op for testing,"        options (`Any`, defaults to `None`):
            A dictionary of options to pass to the backend.
        disable (`bool`, defaults to `False`):
            Turn torch.compile() into a no-op for testing
        use_regional_compilation (`bool`, defaults to `None`):
            Use it to reduce the cold start compilation time of torch.compile() by targeting repeated blocks of the
            same class and compiling them sequentially to hit the compiler's cache. For example, in `GPT2LMHeadModel`,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
Use it to reduce the cold start compilation time of torch.compile() by targeting repeated blocks of the,"        disable (`bool`, defaults to `False`):
            Turn torch.compile() into a no-op for testing
        use_regional_compilation (`bool`, defaults to `None`):
            Use it to reduce the cold start compilation time of torch.compile() by targeting repeated blocks of the
            same class and compiling them sequentially to hit the compiler's cache. For example, in `GPT2LMHeadModel`,
            the repeated block/class is `GPT2Block`, and can be accessed as `model.transformer.h[0]`. The rest of the
            model (e.g model.lm_head) is compiled separately.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"metadata={""help"": ""Turn torch.compile() into a no-op for testing""},","    )
    disable: bool = field(
        default=False,
        metadata={""help"": ""Turn torch.compile() into a no-op for testing""},
    )

    use_regional_compilation: bool = field(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Use it to reduce the cold start compilation time of torch.compile() by targeting repeated ""","        metadata={
            ""help"": (
                # https://pytorch.org/tutorials/recipes/regional_compilation.html
                ""Use it to reduce the cold start compilation time of torch.compile() by targeting repeated ""
                ""blocks of the same class and compiling them sequentially to hit the compiler's cache. For ""
                ""example, in `GPT2LMHeadModel`, the repeated block/class is `GPT2Block`, and can be accessed ""
                ""as `model.transformer.h[0]`. The rest of the model (e.g model.lm_head) is compiled separately.""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"return torch.load(checkpoint_file, map_location=torch.device(""cpu""), weights_only=True)","
            return tensors
    else:
        return torch.load(checkpoint_file, map_location=torch.device(""cpu""), weights_only=True)


def get_state_dict_offloaded_model(model: nn.Module):",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Accelerate
def eval(self):,"
        self.log_eval_results()

    def eval(self):
        for model_module in self.module:
            model_module.eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model_module.eval(),"
    def eval(self):
        for model_module in self.module:
            model_module.eval()

        if self.module_config is None:
            self.module_config = self.get_module_config()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"state_dict = torch.load(input_model_file, weights_only=True)","            # we want an empty state dict for FSDP2 as we use `broadcast_from_rank0`
            load_model = not accelerator.is_fsdp2 or accelerator.is_main_process
            if load_model:
                state_dict = torch.load(input_model_file, weights_only=True)
            else:
                state_dict = {}
            logger.info(f""Model loaded from {input_model_file}"")",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Accelerate
"state_dict = torch.load(input_model_file, weights_only=True)","            )
            input_model_file = os.path.join(input_dir, weights_name)
            logger.info(f""Loading model from {input_model_file}"")
            state_dict = torch.load(input_model_file, weights_only=True)
            logger.info(f""Model loaded from {input_model_file}"")
        elif fsdp_plugin.state_dict_type == StateDictType.SHARDED_STATE_DICT:
            ckpt_dir = (",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Accelerate
"optim_state = torch.load(input_optimizer_file, weights_only=True)","                )
                input_optimizer_file = os.path.join(input_dir, optimizer_name)
                logger.info(f""Loading Optimizer state from {input_optimizer_file}"")
                optim_state = torch.load(input_optimizer_file, weights_only=True)
                logger.info(f""Optimizer state loaded from {input_optimizer_file}"")
        else:
            ckpt_dir = (",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Accelerate
reg = re.compile(modules),"    parameters = []
    # code taken from accelerate while preparing kwargs for FSDP
    if isinstance(modules, str):
        reg = re.compile(modules)
        mapped_modules = []
        for name, module in model.named_modules():
            if reg.fullmatch(name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
mod = importlib.import_module(mod_name),"        sys.path.append(str(script_path.parent.resolve()))
        mod_name = script_path.stem

    mod = importlib.import_module(mod_name)
    if not hasattr(mod, args.main_training_function):
        raise ValueError(
            f""Your training script should have a function named {args.main_training_function}, or you should pass a """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
unpickled_accelerator = pickle.loads(pickled_accelerator),"    data_loader = create_dataloader(accelerator, dataset_size=32, batch_size=4)
    _ = accelerator.prepare(data_loader)
    pickled_accelerator = pickle.dumps(accelerator)
    unpickled_accelerator = pickle.loads(pickled_accelerator)
    # TODO: Maybe this should be implemented as __eq__ for AcceleratorState?
    assert accelerator.state.__dict__ == unpickled_accelerator.state.__dict__
",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"    # First do baseline
    model, dataloader, device = setup[""no""]
    model.to(device)
    model.eval()
    for batch in dataloader:
        batch.to(device)
        with torch.inference_mode():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
    # Then do distributed
    model, dataloader, device = setup[""ddp""]
    model.eval()
    for batch in dataloader:
        with torch.inference_mode():
            outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
inference_model.eval(),"    assert get_active_deepspeed_plugin(accelerator.state) is zero3_plugin
    inference_model = NoiseModel()
    inference_model = accelerator.prepare(inference_model)
    inference_model.eval()

    # Run training loop
    accelerator.state.select_deepspeed_plugin(""training"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
train_model.eval(),"                lr_scheduler.step()
                optimizer.zero_grad()

        train_model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                outputs = train_model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
zero2_model.eval(),"                zero3_lr_scheduler.step()
                zero3_optimizer.zero_grad()

        zero2_model.eval()
        zero3_model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
zero3_model.eval(),"                zero3_optimizer.zero_grad()

        zero2_model.eval()
        zero3_model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                logits_a = zero2_model(**batch).logits",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"                    )
                    lr_scheduler_check_completed = True

        model.eval()
        samples_seen = 0
        for step, batch in enumerate(eval_dataloader):
            # We could avoid this line since we set the accelerator with `device_placement=True`.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"

def evaluation_loop(accelerator, model, eval_dataloader, metric):
    model.eval()
    samples_seen = 0
    for step, batch in enumerate(eval_dataloader):
        # We could avoid this line since we set the accelerator with `device_placement=True`.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model = AutoModelForCausalLM.from_config(config).to(dtype=torch.float16).eval(),"]:
    with torch.device(torch_device_type):
        config = AutoConfig.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_config(config).to(dtype=torch.float16).eval()

    full_compilation_model = torch.compile(model)
    regional_compilation_model = compile_regions(model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
full_compilation_model = torch.compile(model),"        config = AutoConfig.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_config(config).to(dtype=torch.float16).eval()

    full_compilation_model = torch.compile(model)
    regional_compilation_model = compile_regions(model)

    for model, sub_label, description, stmt, iters in [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""","

def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():
            # W/ MS-AMP, we need to cast while evaluating",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""","

def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():
            outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""","

def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():
            outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""","

def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():
            outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""","

def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():
            outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
"""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""","

def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
model.eval(),"
def evaluate_model(model, dataloader, metric, accelerator=None):
    ""Turns model to .eval(), runs dataloader, calculates metric, then turns eval back on""
    model.eval()
    for step, batch in enumerate(dataloader):
        with torch.no_grad():
            outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Accelerate
importlib.import_module(importname),"
def check_pydep(importname: str, module: str) -> None:
    try:
        importlib.import_module(importname)
    except ImportError as e:
        raise RuntimeError(
            missing_pydep.format(importname=importname, module=module)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(python_bytecode, exec_globals)","    exec_globals[""OUT_STREAM""] = output_stream

    python_bytecode = compile(""\n"".join(python_lines), input_path, ""exec"")
    exec(python_bytecode, exec_globals)

    return output_stream.getvalue()
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(python_bytecode, exec_globals)","    exec_globals[""OUT_STREAM""] = output_stream

    python_bytecode = compile(""\n"".join(python_lines), input_path, ""exec"")
    exec(python_bytecode, exec_globals)

    return output_stream.getvalue()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"python_bytecode = compile(""\n"".join(python_lines), input_path, ""exec"")","    output_stream = io.StringIO()
    exec_globals[""OUT_STREAM""] = output_stream

    python_bytecode = compile(""\n"".join(python_lines), input_path, ""exec"")
    exec(python_bytecode, exec_globals)

    return output_stream.getvalue()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RELEASE_PATTERN = re.compile(r""/v[0-9]+(\.[0-9]+)*(-rc[0-9]+)?/"")","

UNKNOWN = ""Unknown""
RELEASE_PATTERN = re.compile(r""/v[0-9]+(\.[0-9]+)*(-rc[0-9]+)?/"")


def get_sha(pytorch_root: str | Path) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","        try:
            output = subprocess.check_output(
                build_custom_step,
                shell=True,
                stderr=subprocess.STDOUT,
                text=True,
            )",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"SHA1_RE = re.compile(r""(?P<sha1>[0-9a-fA-F]{40})"")","LOGGER: logging.Logger | None = None
VERBOSE: bool = False
DATETIME_FORMAT = ""%Y-%m-%d_%Hh%Mm%Ss""
SHA1_RE = re.compile(r""(?P<sha1>[0-9a-fA-F]{40})"")
USERNAME_PASSWORD_RE = re.compile(r"":\/\/(.*?)\@"")
LOG_DIRNAME_RE = re.compile(
    r""(?P<datetime>\d{4}-\d\d-\d\d_\d\dh\d\dm\d\ds)_""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"USERNAME_PASSWORD_RE = re.compile(r"":\/\/(.*?)\@"")","VERBOSE: bool = False
DATETIME_FORMAT = ""%Y-%m-%d_%Hh%Mm%Ss""
SHA1_RE = re.compile(r""(?P<sha1>[0-9a-fA-F]{40})"")
USERNAME_PASSWORD_RE = re.compile(r"":\/\/(.*?)\@"")
LOG_DIRNAME_RE = re.compile(
    r""(?P<datetime>\d{4}-\d\d-\d\d_\d\dh\d\dm\d\ds)_""
    r""(?P<uuid>[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
LOG_DIRNAME_RE = re.compile(,"DATETIME_FORMAT = ""%Y-%m-%d_%Hh%Mm%Ss""
SHA1_RE = re.compile(r""(?P<sha1>[0-9a-fA-F]{40})"")
USERNAME_PASSWORD_RE = re.compile(r"":\/\/(.*?)\@"")
LOG_DIRNAME_RE = re.compile(
    r""(?P<datetime>\d{4}-\d\d-\d\d_\d\dh\d\dm\d\ds)_""
    r""(?P<uuid>[0-9a-f]{8}-(?:[0-9a-f]{4}-){3}[0-9a-f]{12})"",
)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"NAMELESS_SCHEMA = re.compile(r""\(.*\) -> .*"")","_keep_alive: list[Library] = []


NAMELESS_SCHEMA = re.compile(r""\(.*\) -> .*"")


@functools.singledispatch",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"regx = re.compile(r""\n\s{4}(?!\s)"")","    }
    """"""
    # Split on exactly 4 spaces after a newline
    regx = re.compile(r""\n\s{4}(?!\s)"")
    kwargs = [section.strip() for section in regx.split(desc)]
    kwargs = [section for section in kwargs if len(section) > 0]
    return {desc.split("" "")[0]: desc for desc in kwargs}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
importlib.import_module(module),"            module (str): The name of the Python module to import

        """"""
        importlib.import_module(module)

    def load_library(self, path):
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# weights = torch.load(buf, weights_only = True)","# For example:
# data = urllib.request.urlopen('https://download.pytorch.org/models/resnet50-0676ba61.pth').read()
# buf = io.BytesIO(data)
# weights = torch.load(buf, weights_only = True)

import functools as _functools
import warnings",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
#   torch.load(file_a),"
# Separate from _get_allowed_globals because of the lru_cache on _get_allowed_globals
# For example if user had a script like
#   torch.load(file_a)
#   torch.serialization._add_safe_globals([torch.foo])
#   torch.load(file_b)
# the dynamic additions to safe_globals would not be picked up by",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
#   torch.load(file_b),"# For example if user had a script like
#   torch.load(file_a)
#   torch.serialization._add_safe_globals([torch.foo])
#   torch.load(file_b)
# the dynamic additions to safe_globals would not be picked up by
# _get_allowed_globals due to the lru_cache
def _get_user_allowed_globals():",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,"

@_overload
def compile(
    model: _Callable[_InputT, _RetT],
    *,
    fullgraph: builtins.bool = False,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,"

@_overload
def compile(
    model: None = None,
    *,
    fullgraph: builtins.bool = False,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,") -> _Callable[[_Callable[_InputT, _RetT]], _Callable[_InputT, _RetT]]: ...


def compile(
    model: _Optional[_Callable[_InputT, _RetT]] = None,
    *,
    fullgraph: builtins.bool = False,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
disable (bool): Turn torch.compile() into a no-op for testing,"          - `torch.compiler.keep_tensor_guards_unsafe`

        - For inductor you can see the full list of configs that it supports by calling `torch._inductor.list_options()`
       disable (bool): Turn torch.compile() into a no-op for testing

    Example::
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(options={""triton.cudagraphs"": True}, fullgraph=True)","
    Example::

        @torch.compile(options={""triton.cudagraphs"": True}, fullgraph=True)
        def foo(x):
            return torch.sin(x) + torch.cos(x)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return compile(  # pyrefly: ignore  # no-matching-overload,"        def fn(model: _Callable[_InputT, _RetT]) -> _Callable[_InputT, _RetT]:
            if model is None:
                raise RuntimeError(""Model can't be None"")
            return compile(  # pyrefly: ignore  # no-matching-overload
                model,
                fullgraph=fullgraph,
                dynamic=dynamic,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""To capture an useful graph, we will implicitly switch to torch.compile(backend=eager)""","    if torch.compiler.is_exporting():
        warnings.warn(
            ""You are calling torch.compile inside torch.export region. ""
            ""To capture an useful graph, we will implicitly switch to torch.compile(backend=eager)""
        )
        from torch._higher_order_ops.utils import setup_compilation_env
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return importlib.import_module(f"".{name}"", __name__)","
        # Lazy modules
        if name in _lazy_modules:
            return importlib.import_module(f"".{name}"", __name__)

        raise AttributeError(f""module '{__name__}' has no attribute '{name}'"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.load(,"        f.extractall(model_dir)
        extraced_name = members[0].filename
        extracted_file = os.path.join(model_dir, extraced_name)
    return torch.load(
        extracted_file, map_location=map_location, weights_only=weights_only
    )
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch.load(cached_file, map_location=map_location, weights_only=weights_only)","
    if _is_legacy_zip_format(cached_file):
        return _legacy_zip_load(cached_file, model_dir, map_location, weights_only)
    return torch.load(cached_file, map_location=map_location, weights_only=weights_only)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"HASH_REGEX = re.compile(r""-([a-f0-9]*)\."")","]

# matches bfd8deac from resnet18-bfd8deac.pth
HASH_REGEX = re.compile(r""-([a-f0-9]*)\."")

_TRUSTED_REPO_OWNERS = (
    ""facebookresearch"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ENV_GITHUB_TOKEN = ""GITHUB_TOKEN""","    ""pytorch"",
    ""fairinternal"",
)
ENV_GITHUB_TOKEN = ""GITHUB_TOKEN""
ENV_TORCH_HOME = ""TORCH_HOME""
ENV_XDG_CACHE_HOME = ""XDG_CACHE_HOME""
DEFAULT_CACHE_DIR = ""~/.cache""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch.load(io.BytesIO(b), weights_only=False)","

def _load_from_bytes(b):
    return torch.load(io.BytesIO(b), weights_only=False)


@functools.cache",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"COMPILE_ID_PATTERN = re.compile(r""^(?P<frame_id>\d+)/(?P<frame_compile_id>\d+)$"")","and no guard installation notions here.
""""""

COMPILE_ID_PATTERN = re.compile(r""^(?P<frame_id>\d+)/(?P<frame_compile_id>\d+)$"")
CA_COMPILE_ID_PATTERN = re.compile(
    r""^!(?P<compiled_autograd_id>\d+)(?:/(?P<frame_id>\d+)/(?P<frame_compile_id>\d+))?$""
)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
CA_COMPILE_ID_PATTERN = re.compile(,"""""""

COMPILE_ID_PATTERN = re.compile(r""^(?P<frame_id>\d+)/(?P<frame_compile_id>\d+)$"")
CA_COMPILE_ID_PATTERN = re.compile(
    r""^!(?P<compiled_autograd_id>\d+)(?:/(?P<frame_id>\d+)/(?P<frame_compile_id>\d+))?$""
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"regex = re.compile(r""registered at .*FallbackKernel\.cpp.*(\[)"")","        output = self._format_header(""Computed Dispatch Table"")
        table = self.rawDispatchTable()
        table_entries = table.split(""\n"")
        regex = re.compile(r""registered at .*FallbackKernel\.cpp.*(\[)"")
        for line in table_entries:
            k = line.split("":"")[0]
            if k in self.runtime_keys:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Indicates whether we are tracing/compiling with torch.compile() or torch.export().,"
    def is_compiling() -> bool:
        """"""
        Indicates whether we are tracing/compiling with torch.compile() or torch.export().
        """"""
        warnings.warn(  # use `warnings.warn` instead of `@deprecated`
            ""`torch._utils.is_compiling` is deprecated. Use `torch.compiler.is_compiling` instead."",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
obj = __import__(components[0]),"
def _import_dotted_name(name):
    components = name.split(""."")
    obj = __import__(components[0])
    for component in components[1:]:
        obj = getattr(obj, component)
    return obj",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"substitution = re.compile(substitution_str, re.MULTILINE)","
class CodeTemplate:
    substitution_str = r""(^[^\n\S]*)?\$([^\d\W]\w*|\{,?[^\d\W]\w*\,?})""
    substitution = re.compile(substitution_str, re.MULTILINE)

    pattern: str
    filename: str",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"decl_re = re.compile(r""(?P<name>[^\(]+)\((?P<args>.*)\) -> (?P<returns>.*)"")","            self.arguments.out,
        )

    decl_re = re.compile(r""(?P<name>[^\(]+)\((?P<args>.*)\) -> (?P<returns>.*)"")

    @staticmethod
    def parse(func: str) -> FunctionSchema:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
exec(custom_rearrange_callable_code),"        )
    )

    exec(custom_rearrange_callable_code)
    return locals()[custom_rearrange_callable_name]

",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
exec(custom_rearrange_callable_code),"        )
    )

    exec(custom_rearrange_callable_code)
    return locals()[custom_rearrange_callable_name]

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"eval_result: EvalResults = eval(dataset_name, dataset)","                return evaluator.get_results()

            for dataset_name, dataset in datasets.items():
                eval_result: EvalResults = eval(dataset_name, dataset)
                eval_result_metrics = eval_result.to_map()
                if dataset_name == ""val"":
                    num_correct = eval_result.accuracy.num_correct",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(name, df):","            safe_proba = evaluator.get_safe_proba()
            print(f""safe_proba={safe_proba}"")

            def eval(name, df):
                if ranking:
                    # when ranking is enabled, we duplicate each input for each choice that
                    # is almost as good as the best choice",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"eval_result: EvalResults = eval(dataset_name, dataset)","                return evaluator.get_results()

            for dataset_name, dataset in datasets.items():
                eval_result: EvalResults = eval(dataset_name, dataset)
                eval_result_metrics = eval_result.to_map()
                if dataset_name == ""val"":
                    num_correct = eval_result.accuracy.num_correct",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
cf = torch.compile(mm_mat1_mat2_prepadded),"                return torch.mm(a + 1, b + 1)

            if prepadded_left and prepadded_right:
                cf = torch.compile(mm_mat1_mat2_prepadded)
            elif prepadded_left:
                cf = torch.compile(mm_mat1_prepadded)
            elif prepadded_right:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
cf = torch.compile(mm_mat1_prepadded),"            if prepadded_left and prepadded_right:
                cf = torch.compile(mm_mat1_mat2_prepadded)
            elif prepadded_left:
                cf = torch.compile(mm_mat1_prepadded)
            elif prepadded_right:
                cf = torch.compile(mm_mat2_prepadded)
            else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
cf = torch.compile(mm_mat2_prepadded),"            elif prepadded_left:
                cf = torch.compile(mm_mat1_prepadded)
            elif prepadded_right:
                cf = torch.compile(mm_mat2_prepadded)
            else:
                cf = torch.compile(mm)
            cf(a, b)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
cf = torch.compile(mm),"            elif prepadded_right:
                cf = torch.compile(mm_mat2_prepadded)
            else:
                cf = torch.compile(mm)
            cf(a, b)
            torch.compiler.reset()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"cf = torch.compile(mixed_mm, mode=""max-autotune-no-cudagraphs"")","            def mixed_mm(A, B):
                return torch.mm(A, B.to(A.dtype))

            cf = torch.compile(mixed_mm, mode=""max-autotune-no-cudagraphs"")
            cf(a, b)
            torch.compiler.reset()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"cf = torch.compile(mixed_mm, mode=""max-autotune-no-cudagraphs"")","                def mixed_mm(A: Any, B: Any) -> Any:
                    return torch.mm(A, B)

                cf = torch.compile(mixed_mm, mode=""max-autotune-no-cudagraphs"")
                cf(a, b)
                torch.compiler.reset()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_GHSTACK_HEAD_REF = re.compile(r""^(gh/[^/]+/[0-9]+/)head$"")","}
""""""

RE_GHSTACK_HEAD_REF = re.compile(r""^(gh/[^/]+/[0-9]+/)head$"")
RE_GHSTACK_DESC = re.compile(r""Stack.*:\r?\n(\* [^\r\n]+\r?\n)+"", re.MULTILINE)
RE_PULL_REQUEST_RESOLVED = re.compile(
    r""(Pull Request resolved|Pull-Request-resolved|Pull-Request): """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_GHSTACK_DESC = re.compile(r""Stack.*:\r?\n(\* [^\r\n]+\r?\n)+"", re.MULTILINE)","""""""

RE_GHSTACK_HEAD_REF = re.compile(r""^(gh/[^/]+/[0-9]+/)head$"")
RE_GHSTACK_DESC = re.compile(r""Stack.*:\r?\n(\* [^\r\n]+\r?\n)+"", re.MULTILINE)
RE_PULL_REQUEST_RESOLVED = re.compile(
    r""(Pull Request resolved|Pull-Request-resolved|Pull-Request): ""
    r""https://github.com/(?P<owner>[^/]+)/(?P<repo>[^/]+)/pull/(?P<number>[0-9]+)"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RE_PULL_REQUEST_RESOLVED = re.compile(,"
RE_GHSTACK_HEAD_REF = re.compile(r""^(gh/[^/]+/[0-9]+/)head$"")
RE_GHSTACK_DESC = re.compile(r""Stack.*:\r?\n(\* [^\r\n]+\r?\n)+"", re.MULTILINE)
RE_PULL_REQUEST_RESOLVED = re.compile(
    r""(Pull Request resolved|Pull-Request-resolved|Pull-Request): ""
    r""https://github.com/(?P<owner>[^/]+)/(?P<repo>[^/]+)/pull/(?P<number>[0-9]+)"",
    re.MULTILINE,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_PR_CC_LINE = re.compile(r""^cc:? @\w+.*\r?\n?$"", re.MULTILINE)","    r""https://github.com/(?P<owner>[^/]+)/(?P<repo>[^/]+)/pull/(?P<number>[0-9]+)"",
    re.MULTILINE,
)
RE_PR_CC_LINE = re.compile(r""^cc:? @\w+.*\r?\n?$"", re.MULTILINE)
RE_DIFF_REV = re.compile(r""^Differential Revision:.+?(D[0-9]+)"", re.MULTILINE)
CIFLOW_LABEL = re.compile(r""^ciflow/.+"")
CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_DIFF_REV = re.compile(r""^Differential Revision:.+?(D[0-9]+)"", re.MULTILINE)","    re.MULTILINE,
)
RE_PR_CC_LINE = re.compile(r""^cc:? @\w+.*\r?\n?$"", re.MULTILINE)
RE_DIFF_REV = re.compile(r""^Differential Revision:.+?(D[0-9]+)"", re.MULTILINE)
CIFLOW_LABEL = re.compile(r""^ciflow/.+"")
CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")
MERGE_RULE_PATH = Path("".github"") / ""merge_rules.yaml""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"CIFLOW_LABEL = re.compile(r""^ciflow/.+"")",")
RE_PR_CC_LINE = re.compile(r""^cc:? @\w+.*\r?\n?$"", re.MULTILINE)
RE_DIFF_REV = re.compile(r""^Differential Revision:.+?(D[0-9]+)"", re.MULTILINE)
CIFLOW_LABEL = re.compile(r""^ciflow/.+"")
CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")
MERGE_RULE_PATH = Path("".github"") / ""merge_rules.yaml""
REMOTE_MAIN_BRANCH = ""origin/main""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")","RE_PR_CC_LINE = re.compile(r""^cc:? @\w+.*\r?\n?$"", re.MULTILINE)
RE_DIFF_REV = re.compile(r""^Differential Revision:.+?(D[0-9]+)"", re.MULTILINE)
CIFLOW_LABEL = re.compile(r""^ciflow/.+"")
CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")
MERGE_RULE_PATH = Path("".github"") / ""merge_rules.yaml""
REMOTE_MAIN_BRANCH = ""origin/main""
DRCI_CHECKRUN_NAME = ""Dr.CI""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"REMOVE_JOB_NAME_SUFFIX_REGEX = re.compile(r"", [0-9]+, [0-9]+, .+\)$"")","    return failures.get(str(pr_num), {}) if failures else {}


REMOVE_JOB_NAME_SUFFIX_REGEX = re.compile(r"", [0-9]+, [0-9]+, .+\)$"")


def remove_job_name_suffix(name: str, replacement: str = "")"") -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"CIFLOW_LABEL = re.compile(r""^ciflow/.+"")","
BOT_COMMANDS_WIKI = ""https://github.com/pytorch/pytorch/wiki/Bot-commands""

CIFLOW_LABEL = re.compile(r""^ciflow/.+"")
CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")

OFFICE_HOURS_LINK = ""https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")","BOT_COMMANDS_WIKI = ""https://github.com/pytorch/pytorch/wiki/Bot-commands""

CIFLOW_LABEL = re.compile(r""^ciflow/.+"")
CIFLOW_TRUNK_LABEL = re.compile(r""^ciflow/trunk"")

OFFICE_HOURS_LINK = ""https://github.com/pytorch/pytorch/wiki/Dev-Infra-Office-Hours""
CONTACT_US = f""Questions? Feedback? Please reach out to the [PyTorch DevX Team]({OFFICE_HOURS_LINK})""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"LEADING_V_PATTERN = re.compile(""^v"")","from pathlib import Path


LEADING_V_PATTERN = re.compile(""^v"")
TRAILING_RC_PATTERN = re.compile(""-rc[0-9]*$"")
LEGACY_BASE_VERSION_SUFFIX_PATTERN = re.compile(""a0$"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"TRAILING_RC_PATTERN = re.compile(""-rc[0-9]*$"")","

LEADING_V_PATTERN = re.compile(""^v"")
TRAILING_RC_PATTERN = re.compile(""-rc[0-9]*$"")
LEGACY_BASE_VERSION_SUFFIX_PATTERN = re.compile(""a0$"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"LEGACY_BASE_VERSION_SUFFIX_PATTERN = re.compile(""a0$"")","
LEADING_V_PATTERN = re.compile(""^v"")
TRAILING_RC_PATTERN = re.compile(""-rc[0-9]*$"")
LEGACY_BASE_VERSION_SUFFIX_PATTERN = re.compile(""a0$"")


class NoGitTagException(Exception):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"CIFLOW_TAG_REGEX = re.compile(r""^ciflow\/.*\/(\d{5,6}|[0-9a-f]{40})$"")","
    tags = git_repo._run_git(""tag"").splitlines()

    CIFLOW_TAG_REGEX = re.compile(r""^ciflow\/.*\/(\d{5,6}|[0-9a-f]{40})$"")
    AUTO_REVERT_TAG_REGEX = re.compile(r""^trunk\/[0-9a-f]{40}$"")
    for tag in tags:
        try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"AUTO_REVERT_TAG_REGEX = re.compile(r""^trunk\/[0-9a-f]{40}$"")","    tags = git_repo._run_git(""tag"").splitlines()

    CIFLOW_TAG_REGEX = re.compile(r""^ciflow\/.*\/(\d{5,6}|[0-9a-f]{40})$"")
    AUTO_REVERT_TAG_REGEX = re.compile(r""^trunk\/[0-9a-f]{40}$"")
    for tag in tags:
        try:
            if ESTIMATED_TOKENS[0] > 400:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"JOB_NAME_CFG_REGEX = re.compile(r""(?P<job>[\w-]+)\s+\((?P<cfg>[\w-]+)\)"")","BUILD_JOB_NAME = ""build""
TEST_JOB_NAME = ""test""
BUILD_AND_TEST_JOB_NAME = ""build-and-test""
JOB_NAME_CFG_REGEX = re.compile(r""(?P<job>[\w-]+)\s+\((?P<cfg>[\w-]+)\)"")
EXCLUDED_BRANCHES = [""nightly""]
MEM_LEAK_LABEL = ""enable-mem-leak-check""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"test_config_labels = filter_labels(labels, re.compile(f""{PREFIX}.+""))","            info(msg)
            filtered_test_matrix[""include""].append(entry)

    test_config_labels = filter_labels(labels, re.compile(f""{PREFIX}.+""))
    if not filtered_test_matrix[""include""] and not test_config_labels:
        info(""Found no test-config label on the PR, so all test configs are included"")
        # Found no test-config label and the filtered test matrix is empty, return the same",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"tag_regex = re.compile(r""^ciflow/[\w\-]+/(?P<pr_number>\d+)$"")","
    # If the tag matches, we can get the PR number from it, this is from ciflow
    # workflow dispatcher
    tag_regex = re.compile(r""^ciflow/[\w\-]+/(?P<pr_number>\d+)$"")

    labels = set()
    if pr_number:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_GITHUB_URL_MATCH = re.compile(""^https://.*@?github.com/(.+)/(.+)$"")","
T = TypeVar(""T"")

RE_GITHUB_URL_MATCH = re.compile(""^https://.*@?github.com/(.+)/(.+)$"")


def get_git_remote_name() -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return re.compile(rc),"            else:
                rc += c
    rc += "")""
    return re.compile(rc)


def _shasum(value: str) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","            check_call(
                [f""{SCRIPT_DIR}/amd/package_triton_wheel.sh""],
                cwd=triton_basedir,
                shell=True,
            )
            print(""ROCm libraries setup for triton installation..."")
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
r = yaml.load(contents),"yaml.preserve_quotes = True  # type: ignore[assignment]
yaml.width = 1000  # type: ignore[assignment]
yaml.boolean_representation = [""False"", ""True""]  # type: ignore[attr-defined]
r = yaml.load(contents)

# Cuz ruamel's author intentionally didn't include conversion to string
# https://stackoverflow.com/questions/47614862/best-way-to-use-ruamel-yaml-to-dump-to-string-not-to-stream",yaml.load,Unsafe YAML,HIGH,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"RELEASE_BRANCH_REGEX = re.compile(r""release/(?P<version>.+)"")","    ""critical"",
    ""fixnewfeature"",
}
RELEASE_BRANCH_REGEX = re.compile(r""release/(?P<version>.+)"")


def parse_args() -> Any:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.load(f),"def get_author_map(data_folder: Path, regen_data, assert_stored=False):
    if not regen_data and Path(data_folder / ""author_map.pkl"").exists():
        with open(data_folder / ""author_map.pkl"", ""rb"") as f:
            return pickle.load(f)
    else:
        if assert_stored:
            raise FileNotFoundError(",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.load(f),"def get_file_map(data_folder: Path, regen_data, assert_stored=False):
    if not regen_data and Path(data_folder / ""file_map.pkl"").exists():
        with open(data_folder / ""file_map.pkl"", ""rb"") as f:
            return pickle.load(f)
    else:
        if assert_stored:
            raise FileNotFoundError(",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
commit_classifier.eval(),"        )

    with torch.no_grad():
        commit_classifier.eval()
        val_inpts, val_targets = val_batch
        val_output = commit_classifier(val_inpts)
        val_preds = torch.argmax(val_output, dim=1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"UNKNOWN_TOKEN = ""<Unknown>""","# 94% of all files are captured at len 5, good hyperparameter to play around with.
MAX_LEN_FILE = 6

UNKNOWN_TOKEN = ""<Unknown>""

# Utilities for working with a truncated file graph
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:PyTorch Core
p = subprocess.Popen(,"
def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
    )
    output, err = p.communicate()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True","def run(command):
    """"""Returns (return-code, stdout, stderr)""""""
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
    )
    output, err = p.communicate()
    rc = p.returncode",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.load(Path(""results/classifier/commit_classifier.pt""))","                XLMR_BASE, author_map, file_map, classifier_config
            ).to(device)
            self.classifier.load_state_dict(
                torch.load(Path(""results/classifier/commit_classifier.pt""))
            )
            self.classifier.eval()
        else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
self.classifier.eval(),"            self.classifier.load_state_dict(
                torch.load(Path(""results/classifier/commit_classifier.pt""))
            )
            self.classifier.eval()
        else:
            self.classifier = None
        # Special categories: 'Uncategorized'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
p = subprocess.Popen(,"def system(command):
    """"""Returns (return-code, stdout, stderr)""""""
    print(f""[system] {command}"")
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
    )
    output, err = p.communicate()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True","    """"""Returns (return-code, stdout, stderr)""""""
    print(f""[system] {command}"")
    p = subprocess.Popen(
        command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True
    )
    output, err = p.communicate()
    rc = p.returncode",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_sdpa = torch.compile(,"    kernel_options = get_kernel_options(config.attn_type, config.shape)

    if max_autotune:
        compiled_sdpa = torch.compile(
            flex_attention, dynamic=dynamic, mode=""max-autotune-no-cudagraphs""
        )
    else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_sdpa = torch.compile(flex_attention, dynamic=dynamic)","            flex_attention, dynamic=dynamic, mode=""max-autotune-no-cudagraphs""
        )
    else:
        compiled_sdpa = torch.compile(flex_attention, dynamic=dynamic)

    out_compile = compiled_sdpa(
        query=query,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_block_mask = torch.compile(create_block_mask),"        new_mask_mod = mask_mod

    mask_shape = (1, 1, M, N) if attn_type != ""document_mask"" else (1, 1, M * B, N * B)
    compiled_block_mask = torch.compile(create_block_mask)
    if new_mask_mod:
        block_mask = compiled_block_mask(new_mask_mod, *mask_shape, ""cuda"")
    else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
).eval(),"
    native_mha = torch.nn.MultiheadAttention(
        embed_dim, num_heads, batch_first=True, device=device, dtype=dtype
    ).eval()
    native_mha.in_proj_weight = qkv.weight
    native_mha.in_proj_bias = qkv.bias
    native_mha.out_proj.weight = proj.weight",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"nn_mha = nn_mha.eval().to(""cuda"", config.dtype)","            batch_first=True,
            dropout=dropout_p,
        )
        nn_mha = nn_mha.eval().to(""cuda"", config.dtype)
        composite_mha = build_composite_mha_from_nn_mha(nn_mha)
        qkv, lengths = generate_rand_batch(
            config.batch_size,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_nn_mha = torch.compile(nn_mha),"
        # TorchDynamo will error on NestedTensors
        if config.pad_percentage is None:
            compiled_nn_mha = torch.compile(nn_mha)
            compiled_composite_mha = torch.compile(composite_mha)

            compiled_nn_mha_time = benchmark_torch_function_in_microseconds(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_composite_mha = torch.compile(composite_mha),"        # TorchDynamo will error on NestedTensors
        if config.pad_percentage is None:
            compiled_nn_mha = torch.compile(nn_mha)
            compiled_composite_mha = torch.compile(composite_mha)

            compiled_nn_mha_time = benchmark_torch_function_in_microseconds(
                compiled_nn_mha, qkv, qkv, qkv, mask",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.load(""big_tensor.zip"")","            torch.save(x, ""big_tensor.zip"", _use_new_zipfile_serialization=use_new)

        with Timer() as big2:
            torch.load(""big_tensor.zip"")

        x = [torch.ones(10, 10) for i in range(200)]
        with Timer() as small1:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.load(""small_tensor.zip"")","            torch.save(x, ""small_tensor.zip"", _use_new_zipfile_serialization=use_new)

        with Timer() as small2:
            torch.load(""small_tensor.zip"")

        return {
            ""Big Tensors Save"": big1.ms_duration,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(suite),"    """"""
    skip_tests = set()
    original_dir = abspath(os.getcwd())
    module = importlib.import_module(suite)
    os.chdir(original_dir)
    arch = platform.machine()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","        # Update lookup csv the folder to archived logs
        subprocess.check_call(
            f'echo ""{day},performance,{dtype},{target_dir}"" >> {self.lookup_file}',
            shell=True,
        )

    def archive(self):",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"        ):
            model.train()
        else:
            model.eval()
        gc.collect()
        batch_size = benchmark.batch_size
        if model_name == ""torchrec_dlrm"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(c),"        ]
        for c in candidates:
            try:
                module = importlib.import_module(c)
                break
            except ModuleNotFoundError as e:
                if e.name != c:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"
    if evaluation:
        metric = load_metric(""accuracy"")
        model.eval()
        if not backend:
            opt_model = model
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"help=""number of samples to train/eval (default: 1000)"",","        ""--num-samples"",
        type=int,
        default=1000,
        help=""number of samples to train/eval (default: 1000)"",
    )
    parser.add_argument(
        ""--batch-size"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"help=""sets model.eval() to reduce randomness"",","    parser.add_argument(
        ""--use-eval-mode"",
        action=""store_true"",
        help=""sets model.eval() to reduce randomness"",
    )
    parser.add_argument(
        ""--skip-accuracy-check"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""from transformers import {cls}"")","    pip_install(""git+https://github.com/huggingface/transformers.git#egg=transformers"")
finally:
    for cls in imports:
        exec(f""from transformers import {cls}"")


# These models contain the models present in huggingface_models_list. It is a",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""from transformers import {cls}"")","    pip_install(""git+https://github.com/huggingface/transformers.git#egg=transformers"")
finally:
    for cls in imports:
        exec(f""from transformers import {cls}"")


# These models contain the models present in huggingface_models_list. It is a",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"        ):
            model.train()
        else:
            model.eval()

        self.validate_model(model, example_inputs)
        return device, model_name, model, example_inputs, batch_size",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"mod = importlib.import_module(""transformers"")","

try:
    mod = importlib.import_module(""transformers"")
    for cls in imports:
        if not hasattr(mod, cls):
            raise ModuleNotFoundError",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(module_name),"        ""TrOCRDecoder"": ""transformers.models.trocr.modeling_trocr"",
    }
    module_name = _module_by_model_name.get(model_cls_name, ""transformers"")
    module = importlib.import_module(module_name)
    return getattr(module, model_cls_name)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"    def get_model_and_inputs(model_name, device):
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)
        model.eval()

        model.generation_config.do_sample = False
        model.generation_config.use_cache = True",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"        if is_training and not use_eval_mode:
            model.train()
        else:
            model.eval()

        self.validate_model(model, example_inputs)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"importlib.import_module(""timm"")","

try:
    importlib.import_module(""timm"")
except ModuleNotFoundError:
    print(""Installing PyTorch Image Models..."")
    pip_install(""git+https://github.com/rwightman/pytorch-image-models"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(,"def get_model(args):
    if args.torchbench_model:
        setup_torchbench_cwd()
        module = importlib.import_module(
            f""torchbenchmark.models.{args.torchbench_model}""
        )
        benchmark_cls = getattr(module, ""Model"", None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_mod = torch.compile(mod, dynamic=False)","
            flops = mode.get_total_flops()

            compiled_mod = torch.compile(mod, dynamic=False)

            for _ in range(WARMUP_ITER):
                compiled_mod(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_mod = torch.compile(mod, dynamic=False)","
            x = torch.randn(BS, D, device=device, dtype=dtype)

            compiled_mod = torch.compile(mod, dynamic=False)

            for _ in range(WARMUP_ITER):
                compiled_mod(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_fn = torch.compile(gather_gemv, dynamic=False)","            x = torch.randn(D, device=device, dtype=torch.bfloat16)
            score_idxs = torch.tensor([3, 5], device=device)

            compiled_fn = torch.compile(gather_gemv, dynamic=False)

            for _ in range(WARMUP_ITER):
                compiled_fn(W, score_idxs, x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_fn = torch.compile(gemv, dynamic=False)","            W = torch.randn(D, D, device=device).to(dtype=dtype)
            x = torch.randn(D, device=device, dtype=torch.bfloat16)

            compiled_fn = torch.compile(gemv, dynamic=False)

            for _ in range(WARMUP_ITER):
                compiled_fn(W, x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return model.eval(),"            requires_grad=v.requires_grad,
        )
    model.load_state_dict(state_dict, assign=True)
    return model.eval()


# Only count activated parameters and buffers.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
decode_one_token = torch.compile(,"    global decode_one_token, prefill, compiled
    if not compiled:
        compiled = True
        decode_one_token = torch.compile(
            decode_one_token, mode=""reduce-overhead"", fullgraph=True
        )
        prefill = torch.compile(prefill, fullgraph=True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"prefill = torch.compile(prefill, fullgraph=True)","        decode_one_token = torch.compile(
            decode_one_token, mode=""reduce-overhead"", fullgraph=True
        )
        prefill = torch.compile(prefill, fullgraph=True)

    for i in range(start, num_samples):
        device_sync(device=device)  # MKG",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"
        replace_all_batch_norm_modules_(model)
        # disable dropout for consistency checking
        model.eval()

    model.to(device)
    params, names = extract_weights(model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"
    if has_functorch:
        # disable dropout for consistency checking
        model.eval()

    criterion = nn.NLLLoss()
    params, names = extract_weights(model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_op = torch.compile(,"    for config in group_config.experiments:
        torch._dynamo.reset()
        torch._inductor.utils.clear_caches()
        compiled_op = torch.compile(
            op,
            options=config.to_options(),
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_forward_consume = torch.compile(,"
    def _generate_compile_forward_graph(self):
        """"""generate a compiled graph for the forward function via torch.compile""""""
        compiled_forward_consume = torch.compile(
            self.op_bench.forward_consume_eager, backend=""inductor""
        )
        return compiled_forward_consume",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
state_dict = torch.load(,"
        # Load pretrained weights
        start_load_time = time.time()
        state_dict = torch.load(
            f""{self.model_dir}/resnet18-f37072fd.pth"",
            mmap=True,
            map_location=self.device,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
m.eval(),"        )
        self.metrics_dict[""torch_load_time""] = time.time() - start_load_time
        m.load_state_dict(state_dict, assign=True)
        m.eval()

        if self.compile_model:
            start_compile_time = time.time()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
m.compile(),"
        if self.compile_model:
            start_compile_time = time.time()
            m.compile()
            end_compile_time = time.time()
            self.metrics_dict[""m_compile_time""] = end_compile_time - start_compile_time
        return m",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_cross_entropy = torch.compile(,"
        # Need `lambda` otherwise torch.compile will not trace the function.
        # More discussion: https://github.com/pytorch/pytorch/issues/158455
        compiled_cross_entropy = torch.compile(
            lambda x, target: F.cross_entropy(x, target, reduction=""none""),
            mode=self.compile_mode,
            fullgraph=True,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_cross_entropy = torch.compile(,"        assert kwargs is None
        x, target, dloss = args

        compiled_cross_entropy = torch.compile(
            lambda x, target: F.cross_entropy(x, target, reduction=""none""),
            mode=self.compile_mode,
            fullgraph=True,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_softmax = torch.compile(,"        # Mark batch size as dynamic for realistic workload
        torch._dynamo.mark_dynamic(x, 0)

        compiled_softmax = torch.compile(
            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True
        )
        return lambda: compiled_softmax(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_softmax = torch.compile(,"    def compiled(self, args, kwargs=None) -> Any:
        assert kwargs is None
        x, dy = args
        compiled_softmax = torch.compile(
            lambda x: F.softmax(x, dim=-1), mode=self.compile_mode, fullgraph=True
        )
        y = compiled_softmax(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_rms_norm = torch.compile(,"        # Mark batch size as dynamic for realistic workload
        torch._dynamo.mark_dynamic(x, 0)

        compiled_rms_norm = torch.compile(
            self.rms_norm_ref, mode=self.compile_mode, fullgraph=True
        )
        return lambda: compiled_rms_norm(x, w)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(","    def compiled(self, args, kwargs=None) -> Any:
        assert kwargs is None
        x, w, dy = args
        y = torch.compile(self.rms_norm_ref, mode=self.compile_mode, fullgraph=True)(
            x, w
        )
        return lambda: torch.autograd.grad(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_layernorm = torch.compile(,"        # Mark batch size as dynamic for realistic workload
        torch._dynamo.mark_dynamic(x, 0)

        compiled_layernorm = torch.compile(
            self.layernorm_ref, mode=self.compile_mode, fullgraph=True
        )
        return lambda: compiled_layernorm(x, w, eps=1e-6)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_layernorm = torch.compile(,"    def compiled(self, args, kwargs=None) -> Any:
        assert kwargs is None
        x, w, dy = args
        compiled_layernorm = torch.compile(
            self.layernorm_ref, mode=self.compile_mode, fullgraph=True
        )
        y = compiled_layernorm(x, w)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"bench(""compiled"", torch.compile(add1), False)","def main():
    print(""requires_grad=False"")
    bench(""eager   "", add1, False)
    bench(""compiled"", torch.compile(add1), False)
    print()
    print(""requires_grad=True"")
    bench(""eager   "", add1, True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"bench(""compiled"", torch.compile(add1), True)","    print()
    print(""requires_grad=True"")
    bench(""eager   "", add1, True)
    bench(""compiled"", torch.compile(add1), True)
    print()
    print(""inference_mode()"")
    with torch.inference_mode():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"bench(""compiled"", torch.compile(add1), False)","    print(""inference_mode()"")
    with torch.inference_mode():
        bench(""eager   "", add1, False)
        bench(""compiled"", torch.compile(add1), False)


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(fn_str, globals())","    return x0 + n
""""""

exec(fn_str, globals())
torch._dynamo.config.recompile_limit = 16

",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(fn_str, globals())","    return x0 + n
""""""

exec(fn_str, globals())
torch._dynamo.config.recompile_limit = 16

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"bench(""compiled"", torch.compile(foo, dynamic=False))  # type: ignore[F821]","

def main():
    bench(""compiled"", torch.compile(foo, dynamic=False))  # type: ignore[F821]


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(backend=""eager"", fullgraph=True)","import torch


@torch.compile(backend=""eager"", fullgraph=True)
def symbolic_convert_overhead_stress_test(x, y, n):
    while n > 0:
        n -= 1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
ops = [eval(op)],"    if op == ""all"":
        ops = loader.get_all_ops()
    else:
        ops = [eval(op)]

    max_samples = max_samples + start_idx
    profile_enabled = profile",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
ops = [eval(op)],"    if op == ""all"":
        ops = loader.get_all_ops()
    else:
        ops = [eval(op)]

    max_samples = max_samples + start_idx
    profile_enabled = profile",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_fn = torch.compile(huge_graph, backend=""inductor"")","
    with fresh_cache():
        a = torch.randn(4).cuda()
        compiled_fn = torch.compile(huge_graph, backend=""inductor"")

        # write to cache
        compiled_fn(a)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(inps.strip().strip(""'"").strip('""'), global_vals)","    # f strings introduce quotations we dont want
    for key in dtype_abbrs_parsing:
        inps = inps.replace(f""'{key}'"", key)
    return eval(inps.strip().strip(""'"").strip('""'), global_vals)


class OperatorInputsLoader:",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"cnt = eval(line[len(""cnt: "") : line.find("","")])","            i += 1
            while i < len(lines) and ""Operator: "" not in lines[i]:
                line = lines[i]
                cnt = eval(line[len(""cnt: "") : line.find("","")])
                inps = line[line.find("","") + 2 :].strip(""'"")
                op_inps[inps] += cnt
                i += 1",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
op = eval(key),"    def get_all_ops(self):
        for key in self.operator_db.keys():
            try:
                op = eval(key)
            except AttributeError as ae:
                log.warning(""Evaluating an op name into an OpOverload: %s"", ae)
                continue",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
if skip_non_compute_operators and non_compute_operator(eval(operator)):,"        sorted_operators = sorted(self.func_db.keys())
        with open(output_filename, ""w"") as f:
            for operator in sorted_operators:
                if skip_non_compute_operators and non_compute_operator(eval(operator)):
                    continue
                f.write(f""Operator: {operator}\n"")
                operator_inputs = self.func_db[operator]",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
if skip_non_compute_operators and non_compute_operator(eval(operator)):,"        sorted_operators = sorted(self.func_db.keys())
        with open(output_filename, ""w"") as f:
            for operator in sorted_operators:
                if skip_non_compute_operators and non_compute_operator(eval(operator)):
                    continue
                f.write(f""Operator: {operator}\n"")
                operator_inputs = self.func_db[operator]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(inps.strip().strip(""'"").strip('""'), global_vals)","    # f strings introduce quotations we dont want
    for key in dtype_abbrs_parsing:
        inps = inps.replace(f""'{key}'"", key)
    return eval(inps.strip().strip(""'"").strip('""'), global_vals)


class OperatorInputsLoader:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"cnt = eval(line[len(""cnt: "") : line.find("","")])","            i += 1
            while i < len(lines) and ""Operator: "" not in lines[i]:
                line = lines[i]
                cnt = eval(line[len(""cnt: "") : line.find("","")])
                inps = line[line.find("","") + 2 :].strip(""'"")
                op_inps[inps] += cnt
                i += 1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
op = eval(key),"    def get_all_ops(self):
        for key in self.operator_db.keys():
            try:
                op = eval(key)
            except AttributeError as ae:
                log.warning(""Evaluating an op name into an OpOverload: %s"", ae)
                continue",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(","            fresh_cache(),
            torch._inductor.config.patch(force_shape_pad=self._force_shape_pad),
        ):
            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(
                self.m.cuda() if self._is_gpu else self.m
            )
            opt_m(self.input)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(,"        torch._dynamo.reset()
        self.a = torch.ones(2, device=self.device(), requires_grad=self._requires_grad)

        @torch.compile(
            backend=self.backend(),
            fullgraph=True,
            dynamic=self.is_dynamic(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(backend=self.backend(), fullgraph=True)","        torch._dynamo.reset()

    def _work(self):
        @torch.compile(backend=self.backend(), fullgraph=True)
        def f(x):
            tmps = [x + i for i in range(16)]
            tmps = [x + tmp for tmp in tmps]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(fullgraph=True),"        torch._dynamo.reset()

    def _work(self):
        @torch.compile(fullgraph=True)
        def f(a, b):
            xs = b.tolist()
            for x in xs:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(backend=self.backend(), fullgraph=True)","        torch._dynamo.reset()

    def _work(self):
        @torch.compile(backend=self.backend(), fullgraph=True)
        def f(inp, *weights):
            x = inp
            for w in weights:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(","        with (
            fresh_cache(),
        ):
            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(
                self.m.cuda() if self._is_gpu else self.m
            )
            opt_m(self.input)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(,"        torch._dynamo.reset()

    def _work(self) -> None:
        @torch.compile(
            backend=""inductor"",
            fullgraph=True,
            dynamic=self._dynamic,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(backend=""inductor"")","        torch._dynamo.reset()

    def _work(self):
        @torch.compile(backend=""inductor"")
        def f(x, y):
            return x + y
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(backend=self.backend(), fullgraph=True)","        torch._dynamo.reset()

    def _work(self):
        @torch.compile(backend=self.backend(), fullgraph=True)
        def f(*args):
            outs = [torch.add(x, x) for x in args]
            return outs",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(","        with (
            fresh_cache(),
        ):
            opt_m = torch.compile(backend=self.backend(), dynamic=self.is_dynamic())(
                self.m.cuda() if self._is_gpu else self.m
            )
            opt_m(self.input)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(,"        torch._dynamo.reset()

    def _work(self):
        @torch.compile(
            backend=self.backend(),
            fullgraph=True,
            dynamic=self.is_dynamic(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(fullgraph=True),"        torch._dynamo.reset()

    def _work(self):
        @torch.compile(fullgraph=True)
        def f(a):
            xs = a.tolist()
            y = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
proc = subprocess.run(,"
        for source_cmd in source_cmds or {""""}:
            cmd = f'{source_cmd}{PYTHON_CMD} -c ""import torch""'
            proc = subprocess.run(
                cmd,
                shell=True,
                stdout=subprocess.PIPE,",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
proc = subprocess.run(,"
        for source_cmd in source_cmds or {""""}:
            cmd = f'{source_cmd}{PYTHON_CMD} -c ""import torch""'
            proc = subprocess.run(
                cmd,
                shell=True,
                stdout=subprocess.PIPE,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","            cmd = f'{source_cmd}{PYTHON_CMD} -c ""import torch""'
            proc = subprocess.run(
                cmd,
                shell=True,
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                encoding=""utf-8"",",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","            self.cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            shell=True,
            executable=SHELL,
        )
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.compile(,"                else:
                    backend = ""eager""

                return torch.compile(
                    _invoke_subgraph_placeholder_wrapper,
                    backend=backend,
                    fullgraph=True,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
self.joint_gm.compile(),"                ph.name = f""tangents_{i - self.n_primals}""

        self.joint_gm.graph.lint()
        self.joint_gm.compile()

    def _remove_redundant_sym_size_ops(self) -> None:
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.compile(,"                    )
                else:
                    backend = ""eager""
                return torch.compile(
                    _while_loop_op_wrapper, backend=backend, fullgraph=True
                )(flat_cond_fn, flat_body_fn, tuple(flat_inputs), tuple())
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(backend=""aot_eager"")","        return x.sin().cos()


    @torch.compile(backend=""aot_eager"")
    def f(x):
        return invoke_quant(g, x, scheme=""nf4"")
    ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.compile(,"                else:
                    backend = ""eager""
                with torch._dynamo.utils.disable_cache_limit():
                    return torch.compile(
                        strict_mode_op, backend=backend, fullgraph=True
                    )(callable, operands)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch.compile(_cond_op_wrapper, backend=backend, fullgraph=True)(","    from torch._higher_order_ops.utils import setup_compilation_env

    with setup_compilation_env() as backend:
        return torch.compile(_cond_op_wrapper, backend=backend, fullgraph=True)(
            pred, true_fn, false_fn, operands
        )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch.compile(fn, backend=backend, fullgraph=True)(*args)","def _maybe_compile_and_run_fn(fn, *args):
    if not torch.compiler.is_dynamo_compiling():
        with setup_compilation_env() as backend:  # type: ignore[attr-defined]
            return torch.compile(fn, backend=backend, fullgraph=True)(*args)
    else:
        return fn(*args)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
#      lifted args because it's on the path of torch.compile(dynamic=True).,"# For dynamoed hops, we automatically lift the free symbols in tensors as arguments.
# This has implications for the types of lifted args for different dispatch keys:
#   1. functionalization, FakeTensorMode, ProxyTorchDispatchMode, Autograd need to support torch.Symint
#      lifted args because it's on the path of torch.compile(dynamic=True).
#   2. functionalization, FakeTensorMode, ProxyTorchDispatchMode, Autograd, CompositeExplicitAutograd need
#      to support int arguments. In the eager run case, we re-trace the subgraph in AutogradKey, so inner
#      hops may receive int inputs from the shape of outer tensor inputs.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, namespace)","            grid,
        )
        namespace: dict[str, Any] = {}
        exec(code, namespace)
        grid_fn = namespace[fn_name]

    if tma_descriptor_metadata:",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, namespace)","            grid,
        )
        namespace: dict[str, Any] = {}
        exec(code, namespace)
        grid_fn = namespace[fn_name]

    if tma_descriptor_metadata:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return re.compile(r""((\+|-)?[\w\.]+,\s*)*(\+|-)?[\w\.]+?"")","
# match a comma separated list of loggable names (whitespace allowed after commas)
def _gen_settings_regex():
    return re.compile(r""((\+|-)?[\w\.]+,\s*)*(\+|-)?[\w\.]+?"")


def _validate_settings(settings):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
inputs_meta = pickle.load(f),"    """"""
    inputs = []
    with open(input_data_path, ""rb"") as f:
        inputs_meta = pickle.load(f)
        inputs = []
        for meta in inputs_meta:
            if len(meta) == 1:",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
f = torch.jit.freeze(f.eval()),"
        torch._C._jit_pass_remove_mutation(f.graph)

        f = torch.jit.freeze(f.eval())
        f = torch.jit.optimize_for_inference(f)
        if not any(isinstance(t, torch._subclasses.FakeTensor) for t in inps):
            f(*inps)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
f = torch.jit.freeze(f.eval()),"def simple_ts_compile(fx_g, _):
    strip_overloads(fx_g)
    f = torch.jit.script(fx_g)
    f = torch.jit.freeze(f.eval())
    return f

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return importlib.import_module(,"                # Use slow decomp whose backward will be in terms of index_put
                # importlib is required because the import cannot be top level
                # (cycle) and cannot be nested (TS doesn't support)
                return importlib.import_module(
                    ""torch._decomp.decompositions""
                )._upsample_linear_vec(input, output_size, align_corners, scale_factors)
        return torch._C._nn.upsample_bilinear2d(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return importlib.import_module(,"                # Use slow decomp whose backward will be in terms of index_put
                # importlib is required because the import cannot be top level
                # (cycle) and cannot be nested (TS doesn't support)
                return importlib.import_module(
                    ""torch._decomp.decompositions""
                )._upsample_linear_vec(input, output_size, align_corners, scale_factors)
        return torch._C._nn.upsample_trilinear3d(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return importlib.import_module(,"                # Use slow decomp whose backward will be in terms of index_put.
                # importlib is required because the import cannot be top level
                # (cycle) and cannot be nested (TS doesn't support)
                return importlib.import_module(
                    ""torch._decomp.decompositions""
                )._replication_pad(input, pad)
    return torch._C._nn.pad(input, pad, mode, value)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(fullgraph=True),"        return y


    @torch.compile(fullgraph=True)
    def all_reduce_wait_compiled(y):
        torch.ops.c10d_functional.wait_tensor(y)
        return y * y",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Note that this should be orthogonal to torch.compile(). But whether,"                # `unset_fake_temporarily()` will allow us to materialize the tensors
                # within `_mesh_resources`, which should not affect modling.
                #
                # Note that this should be orthogonal to torch.compile(). But whether
                # we can compile device_mesh `slicing` (no graph break) is not verified
                # yet and need a follow-up,
                # TODO: compiler + device_mesh slicing.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
`model.eval()` in addition to using this context manager.,"        `inference_mode` does NOT automatically set the model to evaluation mode.
        For proper inference behavior (e.g., disabling dropout, using running statistics
        in batch normalization), you must explicitly set your model to evaluation mode using
        `model.eval()` in addition to using this context manager.

    Args:
        mode (bool or function): Either a boolean flag to enable or disable",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(compile(src, key, ""exec""), globals)","
def _exec_with_source(src: str, globals: dict[str, Any], co_fields=None):
    key = _loader.cache(src, globals, co_fields)
    exec(compile(src, key, ""exec""), globals)


def _forward_from_src(src: str, globals: dict[str, Any], co_fields=None):",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(compile(src, key, ""exec""), globals)","
def _exec_with_source(src: str, globals: dict[str, Any], co_fields=None):
    key = _loader.cache(src, globals, co_fields)
    exec(compile(src, key, ""exec""), globals)


def _forward_from_src(src: str, globals: dict[str, Any], co_fields=None):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(compile(src, key, ""exec""), globals)","
def _exec_with_source(src: str, globals: dict[str, Any], co_fields=None):
    key = _loader.cache(src, globals, co_fields)
    exec(compile(src, key, ""exec""), globals)


def _forward_from_src(src: str, globals: dict[str, Any], co_fields=None):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(ts_type.annotation_str, _type_eval_globals)","    eval'ing the annotation_str. _type_eval_globals sets up expressions
    like ""List"" and ""Future"" to map to actual types (typing.List and jit.Future)
    """"""
    return eval(ts_type.annotation_str, _type_eval_globals)


def _torchscript_schema_to_signature_impl(",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(ts_type.annotation_str, _type_eval_globals)","    eval'ing the annotation_str. _type_eval_globals sets up expressions
    like ""List"" and ""Future"" to map to actual types (typing.List and jit.Future)
    """"""
    return eval(ts_type.annotation_str, _type_eval_globals)


def _torchscript_schema_to_signature_impl(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_snake_case_sub = functools.partial(re.compile(r""(?<=[a-z])([A-Z])"").sub, r""_\1"")","

# Replace occurrences where a lowercase letter is followed by an uppercase letter
_snake_case_sub = functools.partial(re.compile(r""(?<=[a-z])([A-Z])"").sub, r""_\1"")

# Find chars that can't be in a Python identifier
_illegal_char_regex = re.compile(""[^0-9a-zA-Z_]+"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_illegal_char_regex = re.compile(""[^0-9a-zA-Z_]+"")","_snake_case_sub = functools.partial(re.compile(r""(?<=[a-z])([A-Z])"").sub, r""_\1"")

# Find chars that can't be in a Python identifier
_illegal_char_regex = re.compile(""[^0-9a-zA-Z_]+"")

# Combined check for variable names:
# 1) Checks name is not empty",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_name_regex = re.compile(r""^([a-zA-Z_][0-9a-zA-Z_]*?)(?:_(\d+))?$"")","# 2) Checks first character is not a digit
# 3) Checks name has no illegal characters (_illegal_char_regex)
# 3) Splits off the number suffix (if present)
_name_regex = re.compile(r""^([a-zA-Z_][0-9a-zA-Z_]*?)(?:_(\d+))?$"")

# starts with torch but does not start with torch._dynamo. or torch._inductor.
_torch_but_not_dynamo = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
_torch_but_not_dynamo = re.compile(,"_name_regex = re.compile(r""^([a-zA-Z_][0-9a-zA-Z_]*?)(?:_(\d+))?$"")

# starts with torch but does not start with torch._dynamo. or torch._inductor.
_torch_but_not_dynamo = re.compile(
    r""^torch(?:\.(?!_dynamo\.|_inductor\.)[^.]+)*$""
).fullmatch
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pattern = re.compile(r""^File \""(.+)\"", line (\d+), in (.+)$"")","def _parse_stack_trace(stack_trace: str):
    if stack_trace is None:
        return None
    pattern = re.compile(r""^File \""(.+)\"", line (\d+), in (.+)$"")
    lines = stack_trace.strip().split(""\n"")
    # stacktrace should have innermost frame last, so we
    # iterate backwards to find the first line that starts",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_counter_regexp = re.compile(r""# COUNTER: (\d+)"")","    ""dim_green"": ""\033[2m\033[32m"",
}
_color_fns = {k: _make_color_fn(v) for k, v in _color_codes.items()}
_counter_regexp = re.compile(r""# COUNTER: (\d+)"")


reflectable_magic_methods = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"np = getattr(importlib.import_module(self.mod), self.name)","        self.name = name

    def unpickle(self, unpickle_state: _UnpickleState) -> Callable[..., object]:
        np = getattr(importlib.import_module(self.mod), self.name)
        return torch._dynamo.variables.misc.get_np_to_tnp_map()[np]

    @classmethod",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"assert np == getattr(importlib.import_module(mod), name)","        if not (name := getattr(np, ""__name__"", None)):
            return None

        assert np == getattr(importlib.import_module(mod), name)
        return cls(mod, name)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.loads(buf),"
    def recv(self):
        buf = self.recv_bytes()
        return pickle.loads(buf)

    def __getattr__(self, name):
        if ""conn"" in self.__dict__:",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
original_trace = pickle.load(fh),"                )

        with open(self.error_files[error_index], ""rb"") as fh:
            original_trace = pickle.load(fh)
        msg = f""\n\n-- Process {error_index:d} terminated with the following error:\n""
        msg += original_trace
        raise ProcessRaisedException(msg, error_index, failed_process.pid)",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"FACTORY_FUNCTION_REGEX = re.compile(""(new_.*|.*_like)"")","
# Note that this is only factories that take Tensor as input as they are
# the ones we care about.
FACTORY_FUNCTION_REGEX = re.compile(""(new_.*|.*_like)"")


class AccessType(enum.Enum):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
data = pickle.load(f),"            f = sys.stdin.buffer
        else:
            f = open(name, ""rb"")
        data = pickle.load(f)
        if isinstance(data, list):  # segments only...
            data = {""segments"": data, ""traces"": []}
        return data",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
# mod1 = torch.compile(...),"        # invocation. Triggering a backward pass typically doesn't lead to another torch.compile
        # invocation, making it less likely for the generation to increase between multiple
        # backward calls. The following use case is covered by this approach:
        # mod1 = torch.compile(...)
        # mod2 = torch.compile(...)
        # mod2(mod1(x)).sum().backward()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# mod2 = torch.compile(...),"        # invocation, making it less likely for the generation to increase between multiple
        # backward calls. The following use case is covered by this approach:
        # mod1 = torch.compile(...)
        # mod2 = torch.compile(...)
        # mod2(mod1(x)).sum().backward()

        self.running_forwards_with_pending_backwards = False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""To prevent overwriting, clone the tensor outside of torch.compile() ""","        return (
            ""Error: accessing tensor output of CUDAGraphs that has been overwritten by a subsequent run. ""
            f""Stack trace: {stack_trace}. ""
            ""To prevent overwriting, clone the tensor outside of torch.compile() ""
            ""or call torch.compiler.cudagraph_mark_step_begin() before each model invocation.""
        )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
tile_vals = eval(tile)  # type: ignore[arg-type],"        info = self.info_dict()
        # TODO(AlnisM): Does tile_shape always exist?
        tile = info[""tile_shape""]
        tile_vals = eval(tile)  # type: ignore[arg-type]
        BLOCK_M = tile_vals[0]
        BLOCK_K = tile_vals[1]
        BLOCK_N = tile_vals[2]",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
tile_vals = eval(tile)  # type: ignore[arg-type],"            info = choice.info_dict()
            tile = info[""tile_shape""]

            tile_vals = eval(tile)  # type: ignore[arg-type]
            BLOCK_M = tile_vals[0]
            BLOCK_K = tile_vals[1]
            BLOCK_N = tile_vals[2]",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
tile_vals = eval(tile)  # type: ignore[arg-type],"        info = self.info_dict()
        # TODO(AlnisM): Does tile_shape always exist?
        tile = info[""tile_shape""]
        tile_vals = eval(tile)  # type: ignore[arg-type]
        BLOCK_M = tile_vals[0]
        BLOCK_K = tile_vals[1]
        BLOCK_N = tile_vals[2]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
tile_vals = eval(tile)  # type: ignore[arg-type],"            info = choice.info_dict()
            tile = info[""tile_shape""]

            tile_vals = eval(tile)  # type: ignore[arg-type]
            BLOCK_M = tile_vals[0]
            BLOCK_K = tile_vals[1]
            BLOCK_N = tile_vals[2]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Return tuple as triton async compile (_worker_compile_triton),"            with restore_stdout_stderr():
                choice.precompile()
            elapsed_ns = time.time_ns() - start_ns
            # Return tuple as triton async compile (_worker_compile_triton)
            # returns tuple[CachingAutotuner, int]
            return None, elapsed_ns // 1000
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code.getvalue(), ctx)","                setattr(cls, target, cls._call_default(target))

        ctx: dict[str, Any] = {}
        exec(code.getvalue(), ctx)
        for target, impl in ctx.items():
            if target in OP_NAMES:
                setattr(cls, target, impl)",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code.getvalue(), ctx)","                setattr(cls, target, cls._call_default(target))

        ctx: dict[str, Any] = {}
        exec(code.getvalue(), ctx)
        for target, impl in ctx.items():
            if target in OP_NAMES:
                setattr(cls, target, impl)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_ignore_op_re = re.compile(r""_.*|paren"").fullmatch","        raise NotImplementedError


_ignore_op_re = re.compile(r""_.*|paren"").fullmatch


def list_ops(cls: type[Any]):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"yield pickle.loads(content), content","                    data = cache_data[""data""]
                    assert isinstance(data, (str, bytes))
                    content = base64.b64decode(data)
                    yield pickle.loads(content), content
            except Exception:
                log.warning(
                    ""%s unable to load compiled graph"", cls.__name__, exc_info=True",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"yield pickle.loads(content), content","                    try:
                        with open(os.path.join(subdir, path), ""rb"") as f:
                            content = f.read()
                            yield pickle.loads(content), content
                    except Exception:
                        log.warning(
                            ""fx graph cache unable to load compiled graph"",",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,"    """"""

    @classmethod
    def compile(
        cls,
        graph: GraphLowering,
        wrapper_code: str,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,"        return key, input_path

    @classmethod
    def compile(
        cls, source_code: str, dst_file_ext: str, extra_args: Optional[list[str]] = None
    ) -> tuple[str, str, str]:
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"obj_path, _, _ = cls.compile(source_code, ""o"", extra_args)","        """"""
        if dst_file_ext == ""so"":
            # Two-step compilation: first compile to .o, then link to .so
            obj_path, _, _ = cls.compile(source_code, ""o"", extra_args)
            key, input_path = cls.write(source_code, dst_file_ext)
            src_files, operation_name = [obj_path], ""Linking""
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"dst_file_path, hash_key, source_code_path = cls.compile(","                f""Only support loading a .so file for now. ""
                f""Requested file extension: {dst_file_ext}. Source code: {source_code}""
            )
        dst_file_path, hash_key, source_code_path = cls.compile(
            source_code, dst_file_ext
        )
        return (DLLWrapper(dst_file_path), hash_key, source_code_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,"        return key, input_path

    @classmethod
    def compile(
        cls, source_code: str, dst_file_ext: str, extra_args: Optional[list[str]] = None
    ) -> tuple[str, str, str]:
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"dst_file_path, hash_key, source_code_path = cls.compile(","                f""Only support loading a .so file for now. ""
                f""Requested file extension: {dst_file_ext}. Source code: {source_code}""
            )
        dst_file_path, hash_key, source_code_path = cls.compile(
            source_code, dst_file_ext
        )
        return (DLLWrapper(dst_file_path), hash_key, source_code_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
func = importlib.import_module(s),"    func = None
    for i, s in enumerate(op.split(""."")):
        if i == 0:
            func = importlib.import_module(s)
        func = getattr(func, s)

    assert callable(func), op + "" can not be loaded through custom_op_wrapper""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"os.path.join(os.environ[""HALIDE_LIB""], f""../include/{name}"")","                return path
        if ""HALIDE_LIB"" in os.environ:
            path = os.path.abspath(
                os.path.join(os.environ[""HALIDE_LIB""], f""../include/{name}"")
            )
            if os.path.exists(path):
                return path",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:PyTorch Core
"os.path.join(os.environ[""HALIDE_LIB""], f""../include/{name}"")","                return path
        if ""HALIDE_LIB"" in os.environ:
            path = os.path.abspath(
                os.path.join(os.environ[""HALIDE_LIB""], f""../include/{name}"")
            )
            if os.path.exists(path):
                return path",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:PyTorch Core
path = os.path.abspath(,"            if os.path.exists(path):
                return path
        if ""HALIDE_LIB"" in os.environ:
            path = os.path.abspath(
                os.path.join(os.environ[""HALIDE_LIB""], f""../include/{name}"")
            )
            if os.path.exists(path):",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:PyTorch Core
Torch.compile() is only work on MSVC with English language pack well.,"@functools.cache
def check_msvc_cl_language_id(compiler: str) -> None:
    """"""
    Torch.compile() is only work on MSVC with English language pack well.
    Check MSVC's language pack: https://github.com/pytorch/pytorch/issues/157673#issuecomment-3051682766
    """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Torch.compile() is only support MSVC with English language pack,""","    if lang_id != 1033:
        # MSVC English language id is 0x0409, and the DEC value is 1033.
        raise RuntimeError(
            ""Torch.compile() is only support MSVC with English language pack,""
            ""Please reinstall its language pack to English.""
        )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"os.path.join(os.path.dirname(torch.__file__), ""../third_party/cutlass/""),","    cutlass_dir = os.path.realpath(
        os.environ.get(
            ""TORCHINDUCTOR_CUTLASS_DIR"",
            os.path.join(os.path.dirname(torch.__file__), ""../third_party/cutlass/""),
        )
    )
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:PyTorch Core
size_hints_regex = re.compile(,"
_triton_kernel_metrics: Optional[dict[str, dict[str, Any]]] = None

size_hints_regex = re.compile(
    r""size_hints=(\{.*?\})"",
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"output_path, *_ = CUDACodeCache.compile(source_code, ""o"")","            if aot_compile:
                # We rely on JITInductor to compile the CUDA code,
                # so that we can load it into AOTInductor.
                output_path, *_ = CUDACodeCache.compile(source_code, ""o"")
                CUDACodeCache.aot_kernels_o.append(output_path)
            return CUDACodeCache.load(source_code, dst_file_ext)[0]
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"output_path, *_ = ROCmCodeCache.compile(source_code, dst_file_ext=""o"")","
        def task():
            if aot_compile:
                output_path, *_ = ROCmCodeCache.compile(source_code, dst_file_ext=""o"")
                ROCmCodeCache.aot_kernels_o.append(output_path)
            if config.rocm.generate_test_runner:
                _ = ROCmCodeCache.compile(source_code, dst_file_ext=""exe"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_ = ROCmCodeCache.compile(source_code, dst_file_ext=""exe"")","                output_path, *_ = ROCmCodeCache.compile(source_code, dst_file_ext=""o"")
                ROCmCodeCache.aot_kernels_o.append(output_path)
            if config.rocm.generate_test_runner:
                _ = ROCmCodeCache.compile(source_code, dst_file_ext=""exe"")
            return ROCmCodeCache.load(source_code, dst_file_ext)[0]

        return self.submit(task)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_mutation_op_re = re.compile(r""(?<!_)(_$|_[.]|(\b|_)(set|enter|exit|seed)(\b|_))(?!_)"")","
# match: copy_, relu_, _set_grad_enabled, manual_seed, _enter_autocast, etc
# doesn't match: __rshift__, etc
_mutation_op_re = re.compile(r""(?<!_)(_$|_[.]|(\b|_)(set|enter|exit|seed)(\b|_))(?!_)"")


def fixme_incorrect_inductor_schema_op(op: torch._ops.OpOverload) -> bool:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
m = importlib.import_module(,"        )
    else:
        pattern_name = search_fn.__name__
        m = importlib.import_module(
            f""torch._inductor.fx_passes.serialized_patterns.{pattern_name}""
        )
        if not m or not hasattr(m, unique_name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# e.g. if torch.compile(f)(x) if called on input-mutating,"                        # clone mutated Tensor inputs to avoid mutating them in
                        # the first pass of the CPP wrapper-based compilation, as
                        # this will lead to a side effect on the example inputs:
                        # e.g. if torch.compile(f)(x) if called on input-mutating
                        # f, the inputs x will be mutated twice in the process:
                        # once here, and again when running the compiled model;
                        # this will also lead to a numerically incorrect output",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
key = pickle.loads(key_bytes),"                ) from err

            try:
                key = pickle.loads(key_bytes)
            except pickle.UnpicklingError as err:
                raise CacheError(
                    f""Malformed key_bytes_repr {key_bytes_repr!r} in kv_pair {kv_pair!r}, not un-pickle-able.""",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
value = pickle.loads(value_bytes),"                    f""Malformed key_bytes_repr {key_bytes_repr!r} in kv_pair {kv_pair!r}, not un-pickle-able.""
                ) from err
            try:
                value = pickle.loads(value_bytes)
            except pickle.UnpicklingError as err:
                raise CacheError(
                    f""Malformed value_bytes_repr {value_bytes_repr!r} in kv_pair {kv_pair!r}, not un-pickle-able.""",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
cache._cache = pickle.load(fp),"
        try:
            with open(fpath, ""rb"") as fp:
                cache._cache = pickle.load(fp)
        except pickle.UnpicklingError as err:
            raise CacheError(
                f""Failed to create cache from file path {fpath}, file contents are un-pickle-able.""",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
value = pickle.loads(value_bytes),"                return None

            try:
                value = pickle.loads(value_bytes)
            except pickle.UnpicklingError as err:
                raise CacheError(
                    f""Failed to get key {key!r}, value is potentially corrupted (value is not un-pickle-able).""",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
def compile(,"log = logging.getLogger(__name__)


def compile(
    gm: torch.fx.GraphModule,
    example_inputs: list[InputType],
    options: Optional[dict[str, Any]] = None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
modes passed to `torch.compile()` performs.,"    mode: Optional[str] = None, dynamic: Optional[bool] = None
) -> dict[str, Any]:
    r""""""Returns a dictionary describing the optimizations that each of the available
    modes passed to `torch.compile()` performs.

    Args:
        mode (str, optional): The mode to return the optimizations for.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
that are available to `torch.compile()`.,"
def list_options() -> list[str]:
    r""""""Returns a dictionary describing the optimizations and debug configurations
    that are available to `torch.compile()`.

    The options are documented in `torch._inductor.config`.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_fn = AotCodeCompiler.compile(,"                                ""AotCodeCompiler.compile"", log_pt2_compile_event=True
                            ):
                                # Directly return the file path with the compiled code
                                compiled_fn = AotCodeCompiler.compile(
                                    graph,
                                    wrapper_code.value,
                                    kernel_code.value,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.load(read_pipe),"
    @staticmethod
    def recv(read_pipe: IO[bytes]) -> Any:
        return pickle.load(read_pipe)

    def __init__(self, device: Optional[int]):
        self.device = device",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"CUDACodeCache.compile(self.source_code, ""so"")","        This may happen in a separate thread pool.
        """"""
        autotuning_log.debug(""Precompiling %s"", self)
        CUDACodeCache.compile(self.source_code, ""so"")
        autotuning_log.debug(""Done precompiling %s"", self)

    def make_run_fn(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
graph = pickle.load(f),"                        assert os.path.exists(subdir)
                        for path in sorted(os.listdir(subdir)):
                            with open(os.path.join(subdir, path), ""rb"") as f:
                                graph = pickle.load(f)
                            output_file = graph.write_to_disk()
                            log.info(""Output code written to: %s"", output_file)
",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
exec(,"
    # wrapper is likely on the hot path, compile a specialized version of it
    ctx = {""fn"": fn}
    exec(
        f""""""\
        def {name}_cache_on_self(self):
            try:",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
exec(,"
    # wrapper is likely on the hot path, compile a specialized version of it
    ctx = {""fn"": fn}
    exec(
        f""""""\
        def {name}_cache_on_self(self):
            try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, value: sympy.Expr) -> Optional[sympy.Expr]:","    is_integer = True

    @classmethod
    def eval(cls, value: sympy.Expr) -> Optional[sympy.Expr]:
        if isinstance(value, (int, sympy.Integer)):
            return _align(int(value))
        if _is_aligned(value):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_triton_type_re = re.compile(r""^.*[.]"")","_torch_triton_mapping = {v: k for k, v in _triton_type_mapping.items()}


_triton_type_re = re.compile(r""^.*[.]"")


def triton_type(dtype: torch.dtype) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"args, kwargs = pickle.load(f)","    from torch._inductor.compile_fx import compile_fx_inner

    with open(path, ""rb"") as f:
        args, kwargs = pickle.load(f)

    def handle_tensor(x: Any) -> Any:
        if isinstance(x, TensorMetadataHolder):",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
state = pickle.load(f),"    def load_state(self, filename: str = ""fuzzer_state.pkl"") -> None:
        """"""Load fuzzer state from a file""""""
        with open(filename, ""rb"") as f:
            state = pickle.load(f)
            self.results = state[""results""]
            self.detailed_results = state.get(""detailed_results"", {})
",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"comp = torch.compile(test_model_fn2, backend=""inductor"")","        # try compilation
        try:
            test_model_fn2 = self.test_model_fn_factory()
            comp = torch.compile(test_model_fn2, backend=""inductor"")
        except Exception as exc:  # noqa: E722
            return handle_return(
                ""Exception compiling"", Status.FAILED_COMPILE, True, exc",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"importlib.import_module(""halide"")","def check_halide_import() -> bool:
    """"""checks if we have halide available""""""
    try:
        importlib.import_module(""halide"")
        return True
    except ModuleNotFoundError:
        return False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
fn = eval(py_kernel_name),"    # without cuda allocations.
    device = torch.device(f""cuda:{rank}"")

    fn = eval(py_kernel_name)
    args, kwargs = snode_args_kwargs(snode)

    # TODO(ivankobzarev): fix out variants snode_args_kwargs",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
fn = eval(py_kernel_name),"    # without cuda allocations.
    device = torch.device(f""cuda:{rank}"")

    fn = eval(py_kernel_name)
    args, kwargs = snode_args_kwargs(snode)

    # TODO(ivankobzarev): fix out variants snode_args_kwargs",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
config_data[key] = eval(value),"        config_data = {}
        for line in lines:
            key, value = line.strip().split(""="", 1)
            config_data[key] = eval(value)

        return config_data
",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
config_data[key] = eval(value),"        config_data = {}
        for line in lines:
            key, value = line.strip().split(""="", 1)
            config_data[key] = eval(value)

        return config_data
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"is_indirect = re.compile(r""indirect|tmp"").search","T = TypeVar(""T"")

log = logging.getLogger(__name__)
is_indirect = re.compile(r""indirect|tmp"").search


class Dep(abc.ABC):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(default_factory_module),"    default_factory_name = dumpable_context[""default_factory_name""]
    assert isinstance(default_factory_module, str)
    assert isinstance(default_factory_name, str)
    module = importlib.import_module(default_factory_module)
    default_factory = getattr(module, default_factory_name)

    dict_context = dumpable_context[""dict_context""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
mod = importlib.import_module(modname),"def enum_object_hook(obj: dict[str, Any]) -> Union[Enum, dict[str, Any]]:
    if ""__enum__"" in obj:
        modname, _, classname = obj[""fqn""].partition("":"")
        mod = importlib.import_module(modname)
        enum_cls = mod
        for attr in classname.split("".""):
            enum_cls = getattr(enum_cls, attr)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
fake_frame = eval(,"                        co_linetable=frame.f_code.co_linetable,  # type: ignore[attr-defined]
                        co_firstlineno=frame.f_code.co_firstlineno,  # type: ignore[attr-defined]
                    )
                fake_frame = eval(
                    code,
                    frame.f_globals,
                    {**frame.f_locals, ""__inspect_currentframe"": inspect.currentframe},",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
fake_frame = eval(,"                        co_linetable=frame.f_code.co_linetable,  # type: ignore[attr-defined]
                        co_firstlineno=frame.f_code.co_firstlineno,  # type: ignore[attr-defined]
                    )
                fake_frame = eval(
                    code,
                    frame.f_globals,
                    {**frame.f_locals, ""__inspect_currentframe"": inspect.currentframe},",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# This file contains utilities for ensuring dynamically compile()'d,"from typing import Optional


# This file contains utilities for ensuring dynamically compile()'d
# code fragments display their line numbers in backtraces.
#
# The constraints:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# - We don't want to create temporary files every time we compile(),"#   we cannot assume the linecache trick will work, c.f.
#   https://stackoverflow.com/q/50515651/23845 )
#
# - We don't want to create temporary files every time we compile()
#   some code; file creation should happen lazily only at exception
#   time.  Arguably, you *should* be willing to write out your
#   generated Python code to file system, but in some situations",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"code = compile(""__inspect_currentframe()"", f.name, ""eval"")","                # Create a frame.  Python doesn't let you construct
                # FrameType directly, so just make one with compile
                frame = tb.tb_frame
                code = compile(""__inspect_currentframe()"", f.name, ""eval"")
                code = code.replace(co_name=frame.f_code.co_name)
                # Python 3.11 only
                if hasattr(frame.f_code, ""co_linetable""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"uuid_regex = re.compile(r"" \(UUID: .+?\)"")","            return torch.cuda.get_device_name(None) + gcnArch
        return None
    smi = get_nvidia_smi()
    uuid_regex = re.compile(r"" \(UUID: .+?\)"")
    rc, out, _ = run_lambda(smi + "" -L"")
    if rc != 0:
        return None",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
config = pickle.loads(maybe_pickled_config),"    def load_config(self, maybe_pickled_config: Union[bytes, dict[str, Any]]) -> None:
        """"""Restore from a prior call to save_config() or shallow_copy_dict()""""""
        if not isinstance(maybe_pickled_config, dict):
            config = pickle.loads(maybe_pickled_config)
        else:
            config = maybe_pickled_config
        for k, v in config.items():",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(module_name),"            return None
        module_name, constant_name = alias.rsplit(""."", 1)
        try:
            module = importlib.import_module(module_name)
        except ImportError as e:
            raise AttributeError(f""config alias {alias} does not exist"") from e
        return module, constant_name",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
BUILT_FROM_SOURCE_VERSION_PATTERN = re.compile(r'\d+\.\d+\.\d+\w+\+\w+'),"# PyTorch releases have the version pattern major.minor.patch, whereas when
# PyTorch is built from source, we append the git commit hash, which gives
# it the below pattern.
BUILT_FROM_SOURCE_VERSION_PATTERN = re.compile(r'\d+\.\d+\.\d+\w+\+\w+')

COMMON_MSVC_FLAGS = ['/MD', '/wd4819', '/wd4251', '/wd4244', '/wd4267', '/wd4275', '/wd4018', '/wd4190', '/wd4624', '/wd4067', '/wd4068', '/EHsc']
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pattern = re.compile(""^COLLECT_GCC=(.*)$"", re.MULTILINE)","        version_string = subprocess.check_output([compiler, '--version'], stderr=subprocess.STDOUT, env=env).decode(*SUBPROCESS_DECODE_ARGS)
    if IS_LINUX:
        # Check for 'gcc' or 'g++' for sccache wrapper
        pattern = re.compile(""^COLLECT_GCC=(.*)$"", re.MULTILINE)
        results = re.findall(pattern, version_string)
        if len(results) != 1:
            # Clang is also a supported compiler on Linux",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
src_regex = re.compile('/T(p|c)(.*)'),"
            def spawn(cmd):
                # Using regex to match src, obj and include files
                src_regex = re.compile('/T(p|c)(.*)')
                src_list = [
                    m.group(2) for m in (src_regex.match(elem) for elem in cmd)
                    if m",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
obj_regex = re.compile('/Fo(.*)')  # codespell:ignore,"                    if m
                ]

                obj_regex = re.compile('/Fo(.*)')  # codespell:ignore
                obj_list = [
                    m.group(1) for m in (obj_regex.match(elem) for elem in cmd)
                    if m",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
include_regex = re.compile(r'((\-|\/)I.*)'),"                    if m
                ]

                include_regex = re.compile(r'((\-|\/)I.*)')
                include_list = [
                    m.group(1)
                    for m in (include_regex.match(elem) for elem in cmd) if m",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pattern = re.compile(""^COLLECT_GCC=(.*)$"", re.MULTILINE)","        except Exception:
            return False
    # Check for 'gcc' or 'g++' for sccache wrapper
    pattern = re.compile(""^COLLECT_GCC=(.*)$"", re.MULTILINE)
    results = re.findall(pattern, version_string)
    if len(results) != 1:
        return False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"subprocess.check_output(pch_cmd, shell=True, stderr=subprocess.STDOUT)","
    def build_precompile_header(pch_cmd):
        try:
            subprocess.check_output(pch_cmd, shell=True, stderr=subprocess.STDOUT)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f""Compile PreCompile Header fail, command: {pch_cmd}"") from e
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch.load(fn, weights_only=True)","        fn = os.path.join(self.loc, ""tensors"", name)
        if not os.path.exists(fn):
            raise FileNotFoundError(fn)
        return torch.load(fn, weights_only=True)

    def read_tensor(self, name: str, *, device=None) -> torch.Tensor:
        dtype, h, storage_offset, size, stride, metadata = self.read_tensor_metadata(",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
s = torch.load(,"            s = torch.UntypedStorage._new_with_weak_ptr(ws.cdata)
            if s is not None:
                return s
        s = torch.load(
            os.path.join(self.loc, ""storages"", h),
            weights_only=True,
            map_location=device,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"compiled_fn = torch.compile(fn, **compile_kwargs)","    def decorate_fn(fn):
        @functools.wraps(fn)
        def compile_hook(*args, **kwargs):
            compiled_fn = torch.compile(fn, **compile_kwargs)
            globals()[fn.__name__] = functools.wraps(fn)(compiled_fn)
            return compiled_fn(*args, **kwargs)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""message"": f""Operator {op_name} exists, remember to call eval() before ""","    for op_name in op_names:
        if ""dropout"" in op_name:
            lint_list.append({""name"": LintCode.DROPOUT.name,
                              ""message"": f""Operator {op_name} exists, remember to call eval() before ""
                              ""saving the module.and call torch.utils.mobile_optimizer.optimize_for_mobile to drop dropout ""
                              ""operator.""})
        if ""batch_norm"" in op_name:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""message"": f""Operator {op_name} exists, remember to call eval() before ""","                              ""operator.""})
        if ""batch_norm"" in op_name:
            lint_list.append({""name"": LintCode.BATCHNORM.name,
                              ""message"": f""Operator {op_name} exists, remember to call eval() before ""
                              ""saving the module and call torch.utils.mobile_optimizer.optimize_for_mobile to drop batch_norm ""
                              ""operator.""})
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(annotation_type, pf_globals, pf_locals)","
    def convert_type_string(annotation_type: str):
        try:
            return eval(annotation_type, pf_globals, pf_locals)
        except Exception:
            error_fn(
                f""Unsupported type annotation {annotation_type}. It is not a type.""",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(annotation_type, pf_globals, pf_locals)","
    def convert_type_string(annotation_type: str):
        try:
            return eval(annotation_type, pf_globals, pf_locals)
        except Exception:
            error_fn(
                f""Unsupported type annotation {annotation_type}. It is not a type.""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
">>> out = torch.compile(linear, fullgraph=True)(x, weight, bias)","            >>> weight = torch.randn(2, 2)
            >>> bias = torch.randn(2)
            >>> # xdoctest: +SKIP(""Requires Python <= 3.11"")
            >>> out = torch.compile(linear, fullgraph=True)(x, weight, bias)
            >>> # xdoctest: +SKIP(""Requires Python <= 3.11"")
            >>> assert torch.allclose(out, torch.nn.functional.linear(x, weight, bias))
            >>>",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
">>> out = torch.compile(nonzero, fullgraph=True)(x)","            >>>
            >>> x = torch.tensor([0, 1, 2, 0, 0, 1])
            >>> # xdoctest: +SKIP(""Requires Python <= 3.11"")
            >>> out = torch.compile(nonzero, fullgraph=True)(x)
            >>> # xdoctest: +SKIP(""Requires Python <= 3.11"")
            >>> assert torch.allclose(out, x.nonzero())
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model = Net().eval(),"    CPU Inference Example::

        # Creates model in default precision
        model = Net().eval()

        with torch.autocast(device_type=""cpu"", dtype=torch.bfloat16):
            for input in data:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"model = TestModel(input_size, num_classes).eval()","
        input_size = 2
        num_classes = 2
        model = TestModel(input_size, num_classes).eval()

        # For now, we suggest to disable the Jit Autocast Pass,
        # As the issue: https://github.com/pytorch/pytorch/issues/75956",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Allow torch.compile() to inline,"        fn.__doc__ = obj.__doc__
        fn.__name__ = ""ScriptFunction""
        fn.__qualname__ = ""torch.jit.ScriptFunction""
        # Allow torch.compile() to inline
        fn._torchdynamo_inline = obj  # type: ignore[attr-defined]
        _set_jit_function_cache(obj, fn)
        return fn",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"scripted_module = torch.jit.script(MyModule(2, 3).eval())","                output = self.linear(output)
                return output

        scripted_module = torch.jit.script(MyModule(2, 3).eval())
        frozen_module = torch.jit.freeze(scripted_module)
        # parameters have been removed and inlined into the Graph as constants
        assert len(list(frozen_module.named_parameters())) == 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
scripted_module = torch.jit.script(MyModule2().eval()),"                self.modified_tensor += 1
                return input + self.modified_tensor

        scripted_module = torch.jit.script(MyModule2().eval())
        frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=[""version""])
        # we've manually preserved `version`, so it still exists on the frozen module and can be modified
        assert frozen_module.version == 1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Please call .eval() on your module before freezing.""","    if mod.training:
        raise RuntimeError(
            ""Freezing is currently only implemented for modules in eval mode. ""
            ""Please call .eval() on your module before freezing.""
        )

    preserved_attrs = preserved_attrs if preserved_attrs is not None else []",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"frozen_mod = torch.jit.freeze(torch.jit.script(mod.eval()), optimize=False)","        bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)
        mod = torch.nn.Sequential(conv, bn)
        # set optimize to False here, by default freezing runs run_frozen_optimizations
        frozen_mod = torch.jit.freeze(torch.jit.script(mod.eval()), optimize=False)
        # inspect frozen mod
        assert ""batch_norm"" in str(frozen_mod.graph)
        torch.jit.run_frozen_optimizations(frozen_mod)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval())),"        )
        bn = torch.nn.BatchNorm2d(out_channels, eps=0.001)
        mod = torch.nn.Sequential(conv, bn)
        frozen_mod = torch.jit.optimize_for_inference(torch.jit.script(mod.eval()))
        assert ""batch_norm"" not in str(frozen_mod.graph)
        # if built with MKLDNN, convolution will be run with MKLDNN weights
        assert ""MKLDNN"" in frozen_mod.graph",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"mod = freeze(mod.eval(), preserved_attrs=other_methods)","        other_methods = []

    if hasattr(mod, ""training""):
        mod = freeze(mod.eval(), preserved_attrs=other_methods)

    torch._C._jit_pass_optimize_for_inference(mod._c, other_methods)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, glob, loc)","

def execWrapper(code, glob, loc):
    exec(code, glob, loc)


def _gen_unsupported_methods_properties():",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, glob, loc)","

def execWrapper(code, glob, loc):
    exec(code, glob, loc)


def _gen_unsupported_methods_properties():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(ignore_func_str, g)  # noqa: P204","{ast.unparse(ignore_function)}
""""""
    g = copy.copy(globals())
    exec(ignore_func_str, g)  # noqa: P204
    # registers the custom function in the global context
    globals()[ignore_function_name] = g[ignore_function_name]
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(ignore_func_str, g)  # noqa: P204","{ast.unparse(ignore_function)}
""""""
    g = copy.copy(globals())
    exec(ignore_func_str, g)  # noqa: P204
    # registers the custom function in the global context
    globals()[ignore_function_name] = g[ignore_function_name]
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Did you forget call .eval() on your model? Nodes:\n""","            if len(nondeterm_ops) > 0:
                nondeterministic_ops_warning = ""Trace had nondeterministic nodes. ""
                nondeterministic_ops_warning += (
                    ""Did you forget call .eval() on your model? Nodes:\n""
                )
                nondeterministic_ops_warning += ""\n"".join(
                    [indent(str(op)) for op in nondeterm_ops][:20]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Allow torch.compile() to inline,"                example_inputs_is_kwarg=isinstance(example_kwarg_inputs, dict),
            )

    # Allow torch.compile() to inline
    traced._torchdynamo_inline = func  # type: ignore[attr-defined]
    return traced
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(bytecode, glob, loc)  # type: ignore[arg-type] # noqa: P204","            raise RuntimeError(
                f""Type annotation should not contain calls, but '{stmt}' does""
            )
    return eval(bytecode, glob, loc)  # type: ignore[arg-type] # noqa: P204


def parse_type_line(type_line, rcb, loc):",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(bytecode, glob, loc)  # type: ignore[arg-type] # noqa: P204","            raise RuntimeError(
                f""Type annotation should not contain calls, but '{stmt}' does""
            )
    return eval(bytecode, glob, loc)  # type: ignore[arg-type] # noqa: P204


def parse_type_line(type_line, rcb, loc):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"bytecode = compile(stmt, """", mode=""eval"")","
def _eval_no_call(stmt, glob, loc):
    """"""Evaluate statement as long as it does not contain any method/function calls.""""""
    bytecode = compile(stmt, """", mode=""eval"")
    for insn in dis.get_instructions(bytecode):
        if ""CALL"" in insn.opname:
            raise RuntimeError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"type_pattern = re.compile(""# type:\\ ignore(\\[[a-zA-Z-]+\\])?$"")","
    # adding an extra backslash before the space, to avoid triggering
    # one of the checks in .github/workflows/lint.yml
    type_pattern = re.compile(""# type:\\ ignore(\\[[a-zA-Z-]+\\])?$"")
    type_lines = list(filter(lambda line: not type_pattern.search(line[1]), type_lines))

    if len(type_lines) == 0:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"wrong_type_pattern = re.compile(""#[\t ]*type[\t ]*(?!: ignore(\\[.*\\])?$):"")","
    if len(type_lines) == 0:
        # Catch common typo patterns like extra spaces, typo in 'ignore', etc.
        wrong_type_pattern = re.compile(""#[\t ]*type[\t ]*(?!: ignore(\\[.*\\])?$):"")
        wrong_type_lines = list(
            filter(lambda line: wrong_type_pattern.search(line[1]), lines)
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.loads(guards_state),"    except ImportError:
        ctx = nullcontext()  # type: ignore[assignment]
    with ctx:
        return pickle.loads(guards_state)


def load_guard_manager(",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
entry = pickle.loads(pickled_content),"        try:
            with open(os.path.join(path, ""entry""), ""rb"") as f:
                pickled_content = f.read()
                entry = pickle.loads(pickled_content)
                return entry
        except Exception as e:
            raise RuntimeError(f""Failed to load package from path {path}: {e}"") from e",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
m = importlib.import_module(code.module),"            dynamo.check_versions()
            if not ignore_inlined_sources:
                for code in dynamo.source_info.inlined_sources:
                    m = importlib.import_module(code.module)
                    checksum = _hash_sourcelines(m, code.firstlineno, code.lastlineno)
                    if checksum != code.checksum:
                        raise RuntimeError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"module, alias, importlib.import_module(module_name)","                module = sys.modules[entry.python_module]
                for alias, module_name in entry.import_sources.items():
                    self._install_global(
                        module, alias, importlib.import_module(module_name)
                    )
                target_code = code
                if entry.install_to_global:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pbar = tqdm(total=num_steps, desc=""torch.compile()"", delay=0)","        num_steps = 3
    except ImportError:
        num_steps = 2
    pbar = tqdm(total=num_steps, desc=""torch.compile()"", delay=0)


def get_step_logger(logger: logging.Logger) -> Callable[..., None]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Enables the Compiled Autograd engine to trace autograd calls made under torch.compile().,"
strict_precompile = os.environ.get(""TORCH_STRICT_PRECOMPILE"", ""0"") == ""1""

# Enables the Compiled Autograd engine to trace autograd calls made under torch.compile().
# Note: AOTAutograd will still trace and partition an AOT backward graph local to that
# compiled region. But AOTAutograd traces without knowledge of backward hooks which are
# coordinated by the Autograd engine, and under the hood, it uses the torch.autograd.grad",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Overrides torch.compile() kwargs for Compiled Autograd:,"# See https://github.com/pytorch/pytorch/issues/157452 for more context
graph_break_on_nn_param_ctor = True

# Overrides torch.compile() kwargs for Compiled Autograd:
compiled_autograd_kwargs_override: dict[str, Any] = {}

# Enables use of collectives *during* compilation to synchronize behavior",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(pycode, globals_for_guard_fn, out)","        out: dict[str, Any] = {}
        globals_for_guard_fn = {""G"": self.scope[""G""]}
        guards_log.debug(""Python shape guard function:\n%s"", pycode)
        exec(pycode, globals_for_guard_fn, out)
        guard_fn = out[""___make_guard_fn""](*closure_vars.values())
        if is_epilogue:
            # Epilogue guards are run after all the other guards have finished.",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ret = eval(name, self.scope, closure_vars)","                return self.source_get_cache[name]
        if closure_vars is None:
            closure_vars = _get_closure_vars()
        ret = eval(name, self.scope, closure_vars)
        if self.save_guards and "".__closure__"" in name:
            self.source_get_cache[name] = ret
        return ret",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"tensor_id = id(eval(tensor_source, global_scope, scope))","    ids_to_source = collections.defaultdict(list)
    for tensor_source in guard_manager.no_tensor_aliasing_sources:
        global_scope[""__compile_source__""] = tensor_source
        tensor_id = id(eval(tensor_source, global_scope, scope))
        ids_to_source[tensor_id].append(tensor_source)

    duplicate_tensors = [",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"eval(guard, guard_manager.global_scope, local_scope)","    local_scope = {""L"": f_locals, **guard_manager.closure_vars}
    for guard in guard_manager.code_parts:
        try:
            eval(guard, guard_manager.global_scope, local_scope)
        except:  # noqa: B001,E722
            print(f""Malformed guard:\n{guard}"")
",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"fail_reason = eval(part, global_scope, scope)","            global_scope[""__compile_source__""] = part
            with report_compile_source_on_error():
                try:
                    fail_reason = eval(part, global_scope, scope)
                except Exception:
                    if is_recompiles_verbose_enabled():
                        continue",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(pycode, globals_for_guard_fn, out)","        out: dict[str, Any] = {}
        globals_for_guard_fn = {""G"": self.scope[""G""]}
        guards_log.debug(""Python shape guard function:\n%s"", pycode)
        exec(pycode, globals_for_guard_fn, out)
        guard_fn = out[""___make_guard_fn""](*closure_vars.values())
        if is_epilogue:
            # Epilogue guards are run after all the other guards have finished.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ret = eval(name, self.scope, closure_vars)","                return self.source_get_cache[name]
        if closure_vars is None:
            closure_vars = _get_closure_vars()
        ret = eval(name, self.scope, closure_vars)
        if self.save_guards and "".__closure__"" in name:
            self.source_get_cache[name] = ret
        return ret",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"tensor_id = id(eval(tensor_source, global_scope, scope))","    ids_to_source = collections.defaultdict(list)
    for tensor_source in guard_manager.no_tensor_aliasing_sources:
        global_scope[""__compile_source__""] = tensor_source
        tensor_id = id(eval(tensor_source, global_scope, scope))
        ids_to_source[tensor_id].append(tensor_source)

    duplicate_tensors = [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"fail_reason = eval(part, global_scope, scope)","            global_scope[""__compile_source__""] = part
            with report_compile_source_on_error():
                try:
                    fail_reason = eval(part, global_scope, scope)
                except Exception:
                    if is_recompiles_verbose_enabled():
                        continue",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"eval(guard, guard_manager.global_scope, local_scope)","    local_scope = {""L"": f_locals, **guard_manager.closure_vars}
    for guard in guard_manager.code_parts:
        try:
            eval(guard, guard_manager.global_scope, local_scope)
        except:  # noqa: B001,E722
            print(f""Malformed guard:\n{guard}"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"+ ""into torch.compile() package since it's defined in local scope. ""","def raise_local_type_error(obj: Any) -> NoReturn:
    raise TypeError(
        f""Type {type(obj)} for object {obj} cannot be saved ""
        + ""into torch.compile() package since it's defined in local scope. ""
        + ""Please define the class at global scope (top level of a module).""
    )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"+ ""into torch.compile() package since it's defined in local scope. ""","        if type(obj).__qualname__ != type(obj).__name__:
            raise torch._dynamo.exc.PackageError(
                f""Type {type(obj)} for object {obj} cannot be saved ""
                + ""into torch.compile() package since it's defined in local scope. ""
                + ""Please define the class at global scope (top level of a module).""
            )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return importlib.import_module(alias),"
    @classmethod
    def _unpickle_python_module(cls, alias: str) -> types.ModuleType:
        return importlib.import_module(alias)

    @classmethod
    def _unpickle_dispatch_key_set(cls, raw_repr: int) -> torch._C.DispatchKeySet:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, {""__name__"": ""__main__"", ""__compile_source__"": code})","                        cwd = _as_posix_path(cwd)
                        os.chdir(cwd)
                    with patch(""sys.argv"", args), report_compile_source_on_error():
                        exec(code, {""__name__"": ""__main__"", ""__compile_source__"": code})
                    rc = 0
                except Exception:
                    rc = 1",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, {""__name__"": ""__main__"", ""__compile_source__"": code})","                        cwd = _as_posix_path(cwd)
                        os.chdir(cwd)
                    with patch(""sys.argv"", args), report_compile_source_on_error():
                        exec(code, {""__name__"": ""__main__"", ""__compile_source__"": code})
                    rc = 0
                except Exception:
                    rc = 1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
state = pickle.loads(data),"    def deserialize(cls, data: bytes) -> ""AOTCompiledFunction"":
        from torch._dynamo.package import SerializedCode

        state = pickle.loads(data)
        state[""bytecode""] = SerializedCode.to_code_object(state[""bytecode""])
        deserializer, compiled_fn_state = state[""compiled_fn""]
        state[""compiled_fn""] = deserializer(compiled_fn_state)",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
entry = pickle.loads(data),"            deserialize_bundled_cache_entry,
        )

        entry = pickle.loads(data)

        compiled_fn = deserialize_bundled_cache_entry(entry)
        return cls(compiled_fn)",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
results: list[bytes] = pickle.loads(data),"        from torch._dynamo.utils import get_metrics_context
        from torch._guards import compile_context, CompileContext

        results: list[bytes] = pickle.loads(data)
        compiled_results = []
        for result in results:
            with (",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
alias: importlib.import_module(module_name),"        self._artifacts.check_compatibility()

        import_sources = {
            alias: importlib.import_module(module_name)
            for alias, module_name in self._artifacts.import_sources.items()
        }
        f_globals = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Indicates whether we are tracing/compiling with torch.compile() or torch.export().,"
    def is_compiling() -> bool:
        """"""
        Indicates whether we are tracing/compiling with torch.compile() or torch.export().
        """"""
        # NOTE: With `@torch.compile(backend=""eager"")`, torch._dynamo.is_compiling() will get traced
        # and return true. torch.compiler.is_compiling() is skipped and will return false.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# NOTE: With `@torch.compile(backend=""eager"")`, torch._dynamo.is_compiling() will get traced","        """"""
        Indicates whether we are tracing/compiling with torch.compile() or torch.export().
        """"""
        # NOTE: With `@torch.compile(backend=""eager"")`, torch._dynamo.is_compiling() will get traced
        # and return true. torch.compiler.is_compiling() is skipped and will return false.
        return torch.compiler.is_compiling()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
importlib.import_module(need),"                return
        else:
            try:
                importlib.import_module(need)
            except ImportError:
                return
    run_tests()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
#     compiled_model = torch.compile(model),"    #
    # Example:
    #   def train(model, inputs, target):
    #     compiled_model = torch.compile(model)
    #     pred = compiled_model(data)
    #     loss = compute_loss(pred, target)
    #     loss.backward()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
`torch.compile()` with a different backend compiler arguments.,"            textwrap.dedent(
                """"""
                Must call `torch._dynamo.reset()` before changing backends.  Detected two calls to
                `torch.compile()` with a different backend compiler arguments.
                """"""
            )
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"fn = eval(f""lambda {','.join(vars)}: ({','.join(rotated)})"")","    assert n > 1
    vars = [f""v{i}"" for i in range(n)]
    rotated = reversed(vars[-1:] + vars[:-1])
    fn = eval(f""lambda {','.join(vars)}: ({','.join(rotated)})"")
    fn.__name__ = f""rot_{n}_helper""
    return fn
",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"fn = eval(f""lambda {','.join(vars)}: ({','.join(rotated)})"")","    assert n > 1
    vars = [f""v{i}"" for i in range(n)]
    rotated = reversed(vars[-1:] + vars[:-1])
    fn = eval(f""lambda {','.join(vars)}: ({','.join(rotated)})"")
    fn.__name__ = f""rot_{n}_helper""
    return fn
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch._inductor.compile(gm_, example_inputs_)","        def compiler_fn(gm: Any) -> Any:
            def inner_compiler(gm_: Any, example_inputs_: Any) -> Any:
                torch._dynamo.utils.counters[""compiled_autograd""][""compiles""] += 1
                return torch._inductor.compile(gm_, example_inputs_)

            return torch.compile(
                gm, backend=inner_compiler, fullgraph=fullgraph, dynamic=dynamic",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.compile(,"                torch._dynamo.utils.counters[""compiled_autograd""][""compiles""] += 1
                return torch._inductor.compile(gm_, example_inputs_)

            return torch.compile(
                gm, backend=inner_compiler, fullgraph=fullgraph, dynamic=dynamic
            )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ansi_escape = re.compile(r""\x1B[@-_][0-?]*[ -/]*[@-~]"")","
def strip_color_from_string(text: str) -> str:
    # This regular expression matches ANSI escape codes
    ansi_escape = re.compile(r""\x1B[@-_][0-?]*[ -/]*[@-~]"")
    return ansi_escape.sub("""", text)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"importlib.import_module(f""{mod.__name__}.{filename[:-3]}"")","    """"""
    for filename in sorted(os.listdir(os.path.dirname(cast(str, mod.__file__)))):
        if filename.endswith("".py"") and filename[0] != ""_"":
            importlib.import_module(f""{mod.__name__}.{filename[:-3]}"")


def object_has_getattribute(value: Any) -> bool:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"SKIP_DIRS_RE = re.compile(r""match nothing^"")","]
SKIP_DIRS.extend(map(_as_posix_path, filter(None, map(_module_dir, BUILTIN_SKIPLIST))))

SKIP_DIRS_RE = re.compile(r""match nothing^"")

# Skip fbcode paths(including torch.package paths) containing
# one of the following strings.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"FBCODE_SKIP_DIRS_RE = re.compile(f"".*({'|'.join(map(re.escape, FBCODE_SKIP_DIRS))})"")","# one of the following strings.
FBCODE_SKIP_DIRS: set[str] = set()

FBCODE_SKIP_DIRS_RE = re.compile(f"".*({'|'.join(map(re.escape, FBCODE_SKIP_DIRS))})"")

# Remove this after fbcode is fully migrated to tracing through torchrec.
FBCODE_SKIP_TORCHREC_DIRS = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
FBCODE_SKIP_TORCHREC_DIRS_RE = re.compile(,"    ""caffe2/torch/fb/sparsenn/pooled_embeddings_modules.py"",
}

FBCODE_SKIP_TORCHREC_DIRS_RE = re.compile(
    f"".*({'|'.join(re.escape(_as_posix_path(d)) for d in FBCODE_SKIP_TORCHREC_DIRS)})""
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
FBCODE_INLINE_FILES_IN_SKIPPED_DIRS_RE = re.compile(,"FBCODE_INLINE_FILES_IN_SKIPPED_DIRS = {
    ""torchrec/distributed/types.py"",
}
FBCODE_INLINE_FILES_IN_SKIPPED_DIRS_RE = re.compile(
    f"".*({'|'.join(re.escape(_as_posix_path(d)) for d in FBCODE_INLINE_FILES_IN_SKIPPED_DIRS)})""
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
SKIP_DIRS_RE = re.compile(,"
def _recompile_re() -> None:
    global SKIP_DIRS_RE
    SKIP_DIRS_RE = re.compile(
        rf""^[^\s<]*({'|'.join(re.escape(_as_posix_path(d)) for d in SKIP_DIRS)})""
    )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return getattr(importlib.import_module(module), obj_name)","
def _load_obj_from_str(fully_qualified_name: str) -> Any:
    module, obj_name = fully_qualified_name.rsplit(""."", maxsplit=1)
    return getattr(importlib.import_module(module), obj_name)


""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
through torch.compile() and related systems.,"- CatchErrorsWrapper: Provides error handling and suppression logic

The conversion process preserves program semantics while enabling optimizations
through torch.compile() and related systems.

NOTE: _torchdynamo_orig_backend is used for convert frame wrappers to identify the inner wrapped function.
By going down the _torchdynamo_orig_backend chain, one can recover the original unwrapped backend,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.compile(backend=extract_graph_backend, fullgraph=True)(fn)(*args, **kwargs)","        region_tracker = InstructionTranslator.current_tx().output.region_tracker
        return _gm

    torch.compile(backend=extract_graph_backend, fullgraph=True)(fn)(*args, **kwargs)
    return gm.graph, region_tracker  # type: ignore[union-attr]

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"path = os.path.join(os.path.dirname(__file__), ""../debug"")","

def debug_dir() -> str:
    path = os.path.join(os.path.dirname(__file__), ""../debug"")
    if not os.path.exists(path):
        os.mkdir(path)
    return path",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:PyTorch Core
code_state = pickle.loads(payload),"                    CompileEventLogger.pt2_compile(
                        event_name, cache_size_bytes=len(payload)
                    )
                code_state = pickle.loads(payload)
            except Exception:
                log.warning(
                    ""get_code_state failed parsing remote result on %s"",",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
_CODE_STATE = pickle.loads(content),"            with open(path, ""rb"") as f:
                try:
                    content = f.read()
                    _CODE_STATE = pickle.loads(content)
                    CompileEventLogger.pt2_compile(name, cache_size_bytes=f.tell())
                except Exception:
                    log.warning(",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"nn_modules_pattern = re.compile(r"".*torch/nn/modules.*"")","
    def is_co_filename_from_nn_modules(self) -> bool:
        filename = getattr(self.f_code, ""co_filename"", ""<unknown>"")
        nn_modules_pattern = re.compile(r"".*torch/nn/modules.*"")
        return nn_modules_pattern.match(filename) is not None

    def store_global_weakref_by_id(self, prefix: str, value: Any) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
value = __import__(,"            source = GlobalSource(recorded_name)
        else:
            try:
                value = __import__(
                    module_name,
                    fromlist=fromlist,
                    level=level,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Import the named module and cache the result. importlib.import_module(),"@functools.cache
def _import_module(name: str) -> types.ModuleType:
    """"""
    Import the named module and cache the result. importlib.import_module()
    seems to do some filesystem checking to validate the name so not caching
    this can be slow.
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return importlib.import_module(name),"    seems to do some filesystem checking to validate the name so not caching
    this can be slow.
    """"""
    return importlib.import_module(name)


@dataclasses.dataclass",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return ""__import__('torch')""","        install_guard(self.make_guard(GuardBuilder.ID_MATCH))

    def name(self) -> str:
        return ""__import__('torch')""

    def reconstruct(self, codegen: ""PyCodegen"") -> None:
        codegen.extend_output(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Using `torch.compile(module)` when there are global hooks on ""","    def __call__(self, *args: Any, **kwargs: Any) -> Any:
        if torch.nn.modules.module._has_any_global_hook():
            warnings.warn(
                ""Using `torch.compile(module)` when there are global hooks on ""
                ""modules (e.g., from `register_module_forward_hook`); this will""
                "" cause the hooks to fire an extra time for the ""
                ""`OptimizedModule` created by `torch.compile(module)`. If this """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""`OptimizedModule` created by `torch.compile(module)`. If this ""","                ""Using `torch.compile(module)` when there are global hooks on ""
                ""modules (e.g., from `register_module_forward_hook`); this will""
                "" cause the hooks to fire an extra time for the ""
                ""`OptimizedModule` created by `torch.compile(module)`. If this ""
                ""causes undesired behavior, please try using `module.compile()`""
                "", or use the per-module hooks instead"",
                stacklevel=2,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""causes undesired behavior, please try using `module.compile()`""","                ""modules (e.g., from `register_module_forward_hook`); this will""
                "" cause the hooks to fire an extra time for the ""
                ""`OptimizedModule` created by `torch.compile(module)`. If this ""
                ""causes undesired behavior, please try using `module.compile()`""
                "", or use the per-module hooks instead"",
                stacklevel=2,
            )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Graph breaks are not supported with aot compile. Please use torch.compile(fullgraph=True).""","            )
        if not self.dynamo_ctx.fullgraph:
            raise RuntimeError(
                ""Graph breaks are not supported with aot compile. Please use torch.compile(fullgraph=True).""
            )

        if not callable(self.dynamo_ctx.callback):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Graph breaks are not supported with aot compile. Please use torch.compile(fullgraph=True).""","
            if not self.fullgraph:
                raise RuntimeError(
                    ""Graph breaks are not supported with aot compile. Please use torch.compile(fullgraph=True).""
                )

            if not callable(self.callback):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# fact that the old callback from torch.compile() is still active and under,"
        # Under some circumstances (e.g. precompile) we can end up calling @disable
        # decorator in generated bytecode and trigger recompile. This is due to the
        # fact that the old callback from torch.compile() is still active and under
        # this circumstance we will trigger a failure with set_stance(""fail_on_recompile"").
        # Therefore we want to skip calling into any frame in this case.
        if self.wrapping:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"code_obj = compile('x = 1', '<string>', 'exec')","- Memory-leak prevention

Example usage:
    code_obj = compile('x = 1', '<string>', 'exec')

    # Store context
    context = code_context.get_context(code_obj)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
">>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)","        >>> import operator
        >>> operator.indexOf([1, 2, 3, 4, 5], 3)
        2
        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)
        Traceback (most recent call last):
        ...
        torch._dynamo.exc.Unsupported: ...",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
">>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)","        ...             return i
        ...     raise ValueError(""sequence.index(x): x not in sequence"")
        >>>
        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)
        2
    """"""
    if not is_function(original_fn) and not (",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return importlib.import_module(""."" + name, __name__)","    if name in __all__:
        import importlib

        return importlib.import_module(""."" + name, __name__)
    raise AttributeError(f""module {__name__!r} has no attribute {name!r}"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"raise NotImplementedError(""Calling eval() is not supported yet."")","            raise NotImplementedError(""Calling train() is not supported yet."")

        def _eval(self, mode: bool = True):
            raise NotImplementedError(""Calling eval() is not supported yet."")

        module.train = types.MethodType(_train, module)  # type: ignore[method-assign]
        module.eval = types.MethodType(_eval, module)  # type: ignore[method-assign]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(module_name),"    module_name, class_name = function_cls_name.rsplit(""."", 1)

    # Import the module and get the class
    module = importlib.import_module(module_name)
    function_cls = getattr(module, class_name)
    assert hasattr(function_cls, ""apply"")
    return function_cls.apply(*args, **kwargs)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code_str, namespace)","
    # populate namespace with sympy globals, materialize function (named `_`)
    namespace = {**SYMPY_INTERP}
    exec(code_str, namespace)

    # create and return a module whose forward is the materialized function
    # NOTE: we want Dynamo to trace through this module, to repopulate guards:",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code_str, namespace)","
    # populate namespace with sympy globals, materialize function (named `_`)
    namespace = {**SYMPY_INTERP}
    exec(code_str, namespace)

    # create and return a module whose forward is the materialized function
    # NOTE: we want Dynamo to trace through this module, to repopulate guards:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"eval(path, {""L"": {""self"": Path()}})","                                parts.append(str(idx))
                                return self

                        eval(path, {""L"": {""self"": Path()}})
                        return ""."".join(parts)
                    except Exception:  # TODO(zhxchen17) Remove this.
                        return path",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"eval(path, {""L"": {""self"": Path()}})","                                parts.append(str(idx))
                                return self

                        eval(path, {""L"": {""self"": Path()}})
                        return ""."".join(parts)
                    except Exception:  # TODO(zhxchen17) Remove this.
                        return path",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def compile(*args, **kwargs):","F = TypeVar(""F"", bound=FuncType)


def compile(*args, **kwargs):
    """"""
    See :func:`torch.compile` for details on the arguments for this function.
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return torch.compile(*args, **kwargs)","    """"""
    See :func:`torch.compile` for details on the arguments for this function.
    """"""
    return torch.compile(*args, **kwargs)


def reset() -> None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"It is recommended to call this function, especially after using operations like `torch.compile(...)`","def reset() -> None:
    """"""
    This function clears all compilation caches and restores the system to its initial state.
    It is recommended to call this function, especially after using operations like `torch.compile(...)`
    to ensure a clean state before another unrelated compilation
    """"""
    import torch._dynamo",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
@torch.compile(...),"        torch.compiler.allow_in_graph(my_custom_function)


        @torch.compile(...)
        def fn(x):
            x = torch.add(x, 1)
            x = my_custom_function(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
">>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)","        >>> import operator
        >>> operator.indexOf([1, 2, 3, 4, 5], 3)
        2
        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)
        ... # xdoctest: +SKIP(""Long tracebacks"")
        Traceback (most recent call last):
        ...",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
">>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)","        ...             return i
        ...     raise ValueError(""sequence.index(x): x not in sequence"")
        >>>
        >>> torch.compile(operator.indexOf, fullgraph=True)([1, 2, 3, 4, 5], 3)
        2
    """"""
    import torch._dynamo",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"Return valid strings that can be passed to `torch.compile(..., backend=""name"")`.","
def list_backends(exclude_tags=(""debug"", ""experimental"")) -> list[str]:
    """"""
    Return valid strings that can be passed to `torch.compile(..., backend=""name"")`.

    Args:
        exclude_tags(optional): A tuple of strings representing tags to exclude.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"@torch.compile(mode=""reduce-overhead"")","
    .. code-block:: python

        @torch.compile(mode=""reduce-overhead"")
        def rand_foo():
            return torch.rand([4], device=""cuda"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>>> @torch.compile(fullgraph=True),"
        >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
        >>> # Compile a NumPy function as a Tensor -> Tensor function
        >>> @torch.compile(fullgraph=True)
        >>> @torch.compiler.wrap_numpy
        >>> def fn(a: np.ndarray):
        >>>     return np.sum(a * a)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().,"
def is_compiling() -> bool:
    """"""
    Indicates whether a graph is executed/traced as part of torch.compile() or torch.export().

    Note that there are 2 other related flags that should deprecated eventually:
      * torch._dynamo.external_utils.is_compiling()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>> opt_mod = torch.compile(,"
    To use this API, use guard_filter_fn argument while calling torch.compile

    >> opt_mod = torch.compile(
    >>     mod,
    >>     options={""guard_filter_fn"": torch.compiler.skip_guard_on_all_nn_modules_unsafe},
    >> )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>> opt_mod = torch.compile(,"
    To use this API, use guard_filter_fn argument while calling torch.compile

    >> opt_mod = torch.compile(
    >>     mod,
    >>     options={""guard_filter_fn"": torch.compiler.skip_guard_on_all_nn_modules_unsafe},
    >> )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>> opt_mod = torch.compile(,"    can just keep the tensor guards.


    >> opt_mod = torch.compile(
    >>     mod,
    >>     options={""guard_filter_fn"": torch.compiler.keep_tensor_guards},
    >> )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>> opt_mod = torch.compile(,"    default. But if you don't expect any changes in the globals, you can just
    keep the tensor guards.

    >> opt_mod = torch.compile(
    >>     mod,
    >>     options={""guard_filter_fn"": torch.compiler.skip_guard_on_globals},
    >> )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return importlib.import_module(module_name),"    """"""An importer that implements the default behavior of Python.""""""

    def import_module(self, module_name: str):
        return importlib.import_module(module_name)

    def whichmodule(self, obj: Any, name: str) -> str:
        return _pickle_whichmodule(obj, name)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, ns)","            linecache.lazycache(mangled_filename, ns)

            code = self._compile_source(filename, mangled_filename)
            exec(code, ns)

        return module
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, ns)","            linecache.lazycache(mangled_filename, ns)

            code = self._compile_source(filename, mangled_filename)
            exec(code, ns)

        return module
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return compile(source, mangled_filename, ""exec"", dont_inherit=True)","    def _compile_source(self, fullpath: str, mangled_filename: str):
        source = self.zip_reader.get_record(fullpath)
        source = _normalize_line_endings(source)
        return compile(source, mangled_filename, ""exec"", dont_inherit=True)

    # note: named `get_source` so that linecache can find the source
    # when this is the __loader__ of a module.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def __import__(self, name, globals=None, locals=None, fromlist=(), level=0):","                        raise
        return module

    def __import__(self, name, globals=None, locals=None, fromlist=(), level=0):
        if level == 0:
            module = self._gcd_import(name)
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = self.modules[name] = importlib.import_module(name),"        for atom in name.split("".""):
            if not isinstance(cur, _PackageNode) or atom not in cur.children:
                if name in IMPLICIT_IMPORT_ALLOWLIST:
                    module = self.modules[name] = importlib.import_module(name)
                    return module
                raise ModuleNotFoundError(
                    f'No module named ""{name}"" in self-contained archive ""{self.filename}""'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = self.modules[name] = importlib.import_module(name),"                )
            cur = cur.children[atom]
            if isinstance(cur, _ExternNode):
                module = self.modules[name] = importlib.import_module(name)

                if compat_mapping := EXTERN_IMPORT_COMPAT_NAME_MAPPING.get(name):
                    for old_name, new_name in compat_mapping.items():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return re.compile(result),"                )

        result = """".join(component_to_re(c) for c in pattern.split(separator))
        return re.compile(result)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.compile() works). For more details, please refer to the document on .exporter() method.","    To export a model into an ExportPackage, users can use the exporter API provided by ExportPackage.
    Exporter is a decorator that takes a callable and returns a wrapper. The wrapper will export the
    function into an ExportPackage, when it's invoked with some sample inputs (similar to how
    torch.compile() works). For more details, please refer to the document on .exporter() method.

    This design allows users to decouple the exported callables from the actual sample inputs which can
    be helpful for use cases where the exported callable is hidden behind helper functions or when sample",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
state_dict[weight_fqn] = torch.load(,"                weight_bytes = archive_reader.read_bytes(
                    os.path.join(WEIGHTS_DIR, payload_meta.path_name)
                )
                state_dict[weight_fqn] = torch.load(
                    io.BytesIO(weight_bytes), weights_only=False
                )
            else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
constants[constant_fqn] = torch.load(,"                    constant_bytes = archive_reader.read_bytes(
                        os.path.join(CONSTANTS_DIR, path_name)
                    )
                    constants[constant_fqn] = torch.load(
                        io.BytesIO(constant_bytes), weights_only=False
                    )
                else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
loaded_weight = torch.load(io.BytesIO(weight_bytes)),"                    len(WEIGHTS_DIR) :
                ]  # remove data/weights/ prefix
                weight_bytes = archive_reader.read_bytes(file)
                loaded_weight = torch.load(io.BytesIO(weight_bytes))
                weights[weight_file_name] = loaded_weight

    if isinstance(f, (io.IOBase, IO)):",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"return importlib.import_module(""."" + name, __name__)","    if name in __all__:
        import importlib

        return importlib.import_module(""."" + name, __name__)
    raise AttributeError(f""module {__name__!r} has no attribute {name!r}"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"m = torch.nn.Sequential(torch.nn.Conv2d(1, 1, 1)).eval()","    import torch.ao.quantization.quantize_fx as quantize_fx
    import torch.ao.ns._numeric_suite_fx as ns

    m = torch.nn.Sequential(torch.nn.Conv2d(1, 1, 1)).eval()
    mp = quantize_fx.prepare_fx(m, {"""": torch.ao.quantization.default_qconfig})
    # We convert a copy because we need the original prepared model
    # to be available for comparisons, and `quantize_fx.convert_fx` is inplace.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>>> m = M().eval(),"    Examples::

            >>> # xdoctest: +SKIP
            >>> m = M().eval()
            >>> # m is a module containing the sub-modules below
            >>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]
            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
>>> m = M().eval(),"            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)
            >>> output = fused_m(input)

            >>> m = M().eval()
            >>> # Alternately provide a single list of modules to fuse
            >>> modules_to_fuse = ['conv1', 'bn1', 'relu1']
            >>> fused_m = torch.ao.quantization.fuse_modules(m, modules_to_fuse)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"        mapping = get_default_static_quant_module_mappings()
    if not inplace:
        model = copy.deepcopy(model)
    model.eval()
    prepare(model, inplace=True)
    run_fn(model, *run_args)
    convert(model, mapping, inplace=True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"
    if not inplace:
        model = copy.deepcopy(model)
    model.eval()
    propagate_qconfig_(model, qconfig_spec)
    convert(model, mapping, inplace=True)
    return model",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"    model, inplace=False, debug=False, quant_type=QuantType.STATIC, preserved_attrs=None
):
    _check_is_script_module(model)
    model.eval()
    model_c = model._c
    model_c = torch._C._jit_pass_insert_quant_dequant(
        model_c, ""forward"", inplace, debug, quant_type",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
float_model.eval(),"    from torch.ao.quantization import quantize_jit

    ts_model = torch.jit.script(
        float_model.eval()
    )  # or torch.jit.trace(float_model, input)
    qconfig = get_default_qconfig(""fbgemm"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"

    def calibrate(model, data_loader):
        model.eval()
        with torch.no_grad():
            for image, target in data_loader:
                model(image)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
float_model.eval(),"    from torch.ao.quantization import quantize_dynamic_jit

    ts_model = torch.jit.script(
        float_model.eval()
    )  # or torch.jit.trace(float_model, input)
    qconfig = get_default_qconfig(""fbgemm"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"

    def calibrate(model, data_loader):
        model.eval()
        with torch.no_grad():
            for image, target in data_loader:
                model(image)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
float_model.eval(),"    from torch.ao.quantization.quantize_jit import _quantize_ondevice_dynamic_jit

    ts_model = torch.jit.script(
        float_model.eval()
    )  # or torch.jit.trace(float_model, input)
    qconfig = get_default_qconfig(""fbgemm"")
    quant_ready_model = _quantize_ondevice_dynamic_jit(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
float_model = M().eval(),"               return self.linear(x)

        # initialize a floating point model
        float_model = M().eval()

        # define calibration function
        def calibrate(model, data_loader):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"
        # define calibration function
        def calibrate(model, data_loader):
            model.eval()
            with torch.no_grad():
                for image, target in data_loader:
                    model(image)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
float_model = M().eval(),"               return self.linear(x)

        # initialize a floating point model
        float_model = M().eval()

        # define the training loop for quantization aware training
        def train_loop(model, train_data):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
m = Model().eval(),"
        from torch.ao.quantization import fuse_fx

        m = Model().eval()
        m = fuse_fx(m)

    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
float_model = M().eval(),"                return x

        # initialize a floating point model
        float_model = M().eval()

        # define calibration function
        def calibrate(model, data_loader):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"
        # define calibration function
        def calibrate(model, data_loader):
            model.eval()
            with torch.no_grad():
                for image, target in data_loader:
                    model(image)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"saved_state = torch.load(path_to_model, map_location=device)","    print(""Running for norms - "", norms)

    orig_model = get_dlrm_model()
    saved_state = torch.load(path_to_model, map_location=device)
    orig_model.load_state_dict(saved_state[""state_dict""])

    orig_model = orig_model.to(device)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"model.load_state_dict(torch.load(unzip_path, map_location=device))","        unzip_path = model_path

    model = get_dlrm_model(sparse_dlrm=sparse_dlrm)
    model.load_state_dict(torch.load(unzip_path, map_location=device))
    model = model.to(device)
    model.eval()
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"    model = get_dlrm_model(sparse_dlrm=sparse_dlrm)
    model.load_state_dict(torch.load(unzip_path, map_location=device))
    model = model.to(device)
    model.eval()

    # If there was a zip file, clean up the unzipped files
    if zipfile.is_zipfile(model_path):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
self.sparsified = deepcopy(pl_module.model).eval(),"        self.sparsified: Optional[torch.nn.Module] = None

    def on_fit_end(self, trainer, pl_module) -> None:
        self.sparsified = deepcopy(pl_module.model).eval()
        self.data_sparsifier = self.data_sparsifier_class(**self.data_sparsifier_args)

        _attach_model_to_data_sparsifier(self.sparsified, self.data_sparsifier)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
ada_quantizer = ada_quantizer.eval(),"
        ada_quantizer.use_soft_rounding = True
        ada_quantizer.V.requires_grad = False
        ada_quantizer = ada_quantizer.eval()
        q_weight = ada_quantizer(q_module.weight)
        # At the end of optimization, we need to copy the adarounded weight back to the original module
        q_module.weight.data.copy_(q_weight)  # type: ignore[operator]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"however, calling `model.train()` or `model.eval()` does not automatically switch","    Switch dropout patterns in the model between train and eval modes.

    Dropout has different behavior in train vs eval mode. For exported models,
    however, calling `model.train()` or `model.eval()` does not automatically switch
    the dropout behavior between the two modes, so here we need to rewrite the aten
    dropout patterns manually to achieve the same effect.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"however, calling `model.train()` or `model.eval()` does not automatically switch","    Switch batchnorm patterns in the model between train and eval modes.

    Batchnorm has different behavior in train vs eval mode. For exported models,
    however, calling `model.train()` or `model.eval()` does not automatically switch
    the batchnorm behavior between the two modes, so here we need to rewrite the aten
    batchnorm patterns manually to achieve the same effect.
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"This is equivalent to model.eval() but only for certain special ops like dropout, batchnorm.","    """"""
    Move an exported GraphModule to eval mode.

    This is equivalent to model.eval() but only for certain special ops like dropout, batchnorm.
    QAT users should call this before performing inference on the model.

    This call is idempotent; if the model is already in eval mode, nothing will happen.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"Allow users to call `model.train()` and `model.eval()` on an exported model,","
def _allow_exported_model_train_eval(model: torch.fx.GraphModule):
    """"""
    Allow users to call `model.train()` and `model.eval()` on an exported model,
    but with the effect of changing behavior between the two modes limited to special
    ops only, which are currently dropout and batchnorm.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Note: This does not achieve the same effect as what `model.train()` and `model.eval()`,"    but with the effect of changing behavior between the two modes limited to special
    ops only, which are currently dropout and batchnorm.

    Note: This does not achieve the same effect as what `model.train()` and `model.eval()`
    does in eager models, but only provides an approximation. In particular, user code
    branching on `training` flag will not function correctly in general because the branch
    is already specialized at export time. Additionally, other ops beyond dropout and batchnorm",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Disallow calling `model.train()` or `model.eval()` on the given GraphModule.,"# in prepare and convert
def _disallow_eval_train(model: GraphModule):
    """"""
    Disallow calling `model.train()` or `model.eval()` on the given GraphModule.
    This is useful for exported models, where these methods don't actually behave as expected.
    """"""
    error_message = """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Calling train() or eval() is not supported for exported models.,"    This is useful for exported models, where these methods don't actually behave as expected.
    """"""
    error_message = """"""
        Calling train() or eval() is not supported for exported models.
        Please call `torch.ao.quantization.move_exported_model_to_train(model)` (or eval) instead.

        If you cannot replace the calls to `model.train()` and `model.eval()`, you may override",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"If you cannot replace the calls to `model.train()` and `model.eval()`, you may override","        Calling train() or eval() is not supported for exported models.
        Please call `torch.ao.quantization.move_exported_model_to_train(model)` (or eval) instead.

        If you cannot replace the calls to `model.train()` and `model.eval()`, you may override
        the behavior for these methods by calling `torch.ao.quantization.allow_exported_model_train_eval(model)`,
        which does the above automatically for you. Note that this has limited effect on switching
        behavior between train and eval modes, and should be used only for special ops such as dropout",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return importlib.import_module(""."" + name, __name__)","    if name in __all__:
        import importlib

        return importlib.import_module(""."" + name, __name__)
    raise AttributeError(f""module {__name__!r} has no attribute {name!r}"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
observed.eval(),"                observed.linear_V.bias = nn.Parameter(
                    other.in_proj_bias[(other.embed_dim * 2) :]
                )
        observed.eval()
        # Explicit prepare
        observed = torch.ao.quantization.prepare(observed, inplace=True)
        return observed",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
observed.eval(),"            observed.train()
            observed = torch.ao.quantization.prepare_qat(observed, inplace=True)
        else:
            observed.eval()
            observed = torch.ao.quantization.prepare(observed, inplace=True)
        return observed
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
backends that can be used with torch.compile(). It handles:,"This module implements TorchDynamo's backend registry system for managing compiler backends.

The registry provides a centralized way to register, discover and manage different compiler
backends that can be used with torch.compile(). It handles:

- Backend registration and discovery through decorators and entry points
- Lazy loading of backend implementations",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.compile(model, backend=""my_compiler"")","        return compiled_fn

    # Use registered backend
    torch.compile(model, backend=""my_compiler"")

The registry also supports discovering backends through setuptools entry points
in the ""torch_dynamo_backends"" group. Example:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"optimized_model = torch.compile(model, backend=""my_compiler"")","import torch

model = ...  # Your PyTorch model
optimized_model = torch.compile(model, backend=""my_compiler"")
```
""""""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.compile(..., backend=""name"")","    """"""
    Return valid strings that can be passed to:

        torch.compile(..., backend=""name"")
    """"""
    _lazy_import()
    exclude_tags_set = set(exclude_tags or ())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"optimized_fn = torch.compile(fn, backend=eb)","
        torch._dynamo.reset()
        eb = ExplainWithBackend(""inductor"")
        optimized_fn = torch.compile(fn, backend=eb)
        result = optimized_fn(torch.randn(5))
        print(eb.output())
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
The inductor backend can be used with torch.compile():,"into memory when it is not being used. This helps reduce memory overhead when using
other backends.

The inductor backend can be used with torch.compile():
    model = torch.compile(model, backend=""inductor"")
""""""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"model = torch.compile(model, backend=""inductor"")","other backends.

The inductor backend can be used with torch.compile():
    model = torch.compile(model, backend=""inductor"")
""""""

from typing import Any",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
The backend can be used with torch.compile():,"- Tensor conversion utilities between PyTorch and TVM formats
- Configurable optimization levels and tuning trials

The backend can be used with torch.compile():
    model = torch.compile(model, backend=""tvm"")
""""""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"model = torch.compile(model, backend=""tvm"")","- Configurable optimization levels and tuning trials

The backend can be used with torch.compile():
    model = torch.compile(model, backend=""tvm"")
""""""

import functools",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"importlib.import_module(""tvm"")","
def has_tvm() -> bool:
    try:
        importlib.import_module(""tvm"")
        return True
    except ImportError:
        return False",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_input_associated_real_value = eval(self.source.name(), scope)","            # eval(""super(L['mod'].model.model.encoder.embed_positions.forward__class__,
            # L['mod'].model.model.encoder.embed_positions)"", scope)
            # Which is incorrect, and violates the invariant that all sources should be eval()-able against the scope.
            _input_associated_real_value = eval(self.source.name(), scope)
        except Exception as exc:
            raise NotImplementedError from exc
",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_input_associated_real_value = eval(self.source.name(), scope)","        # For local source, we associate the real value. We use this real value
        scope = {""L"": tx.output.local_scope, ""G"": tx.output.global_scope}
        try:
            _input_associated_real_value = eval(self.source.name(), scope)
        except Exception as exc:
            unimplemented_v2(
                gb_type=""Error getting associated real value"",",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# eval(""super(L['mod'].model.model.encoder.embed_positions.forward__class__,","        try:
            # We raise in case we get a typerror bug w/ SuperSource.
            # SuperSource has bugs in it atm, and can produce code like
            # eval(""super(L['mod'].model.model.encoder.embed_positions.forward__class__,
            # L['mod'].model.model.encoder.embed_positions)"", scope)
            # Which is incorrect, and violates the invariant that all sources should be eval()-able against the scope.
            _input_associated_real_value = eval(self.source.name(), scope)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# Which is incorrect, and violates the invariant that all sources should be eval()-able against the scope.","            # SuperSource has bugs in it atm, and can produce code like
            # eval(""super(L['mod'].model.model.encoder.embed_positions.forward__class__,
            # L['mod'].model.model.encoder.embed_positions)"", scope)
            # Which is incorrect, and violates the invariant that all sources should be eval()-able against the scope.
            _input_associated_real_value = eval(self.source.name(), scope)
        except Exception as exc:
            raise NotImplementedError from exc",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_input_associated_real_value = eval(self.source.name(), scope)","            # eval(""super(L['mod'].model.model.encoder.embed_positions.forward__class__,
            # L['mod'].model.model.encoder.embed_positions)"", scope)
            # Which is incorrect, and violates the invariant that all sources should be eval()-able against the scope.
            _input_associated_real_value = eval(self.source.name(), scope)
        except Exception as exc:
            raise NotImplementedError from exc
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_input_associated_real_value = eval(self.source.name(), scope)","        # For local source, we associate the real value. We use this real value
        scope = {""L"": tx.output.local_scope, ""G"": tx.output.global_scope}
        try:
            _input_associated_real_value = eval(self.source.name(), scope)
        except Exception as exc:
            unimplemented_v2(
                gb_type=""Error getting associated real value"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""passed in from uncompiled to compiled regions (e.g. `torch.compile(fn)(enumerate(...))`). ""","                )
            hints.append(
                ""Dynamo does not fully support tracing builtin iterators (e.g. `map`, `zip`, `enumerate`) ""
                ""passed in from uncompiled to compiled regions (e.g. `torch.compile(fn)(enumerate(...))`). ""
                ""This can happen unintentionally if a previous graph break happens with a builtin iterator ""
                ""in the local scope.""
            )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
gm.eval(),"    compiler_fn = lookup_backend(compiler_name)  # type: ignore[arg-type]

    # Set the eval mode to remove randomness.
    gm.eval()

    # Check Accuracy
    if _accuracy_fails(gm, example_inputs, compiler_fn):  # type: ignore[arg-type]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
mod.eval(),"    opt_mod = torch._dynamo.optimize(options.backend)(mod)

    if options.accuracy != """":
        mod.eval()
        opt_mod.eval()  # type: ignore[union-attr]

        with torch.amp.autocast(""cuda"", enabled=options.autocast):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
opt_mod.eval()  # type: ignore[union-attr],"
    if options.accuracy != """":
        mod.eval()
        opt_mod.eval()  # type: ignore[union-attr]

        with torch.amp.autocast(""cuda"", enabled=options.autocast):
            # TODO: disable clone",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"importlib.import_module(f"".{submodule}"", package=polyfills.__name__)","    ""tensor"",
)
POLYFILLED_MODULES: tuple[""ModuleType"", ...] = tuple(
    importlib.import_module(f"".{submodule}"", package=polyfills.__name__)
    for submodule in POLYFILLED_MODULE_NAMES
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def eval(,"    # Automatic evaluation.
    # https://docs.sympy.org/latest/guides/custom-functions.html#best-practices-for-eval
    @classmethod
    def eval(
        cls, base: sympy.Integer, divisor: sympy.Integer
    ) -> Union[sympy.Basic, None]:
        # python test/test_dynamic_shapes.py -k TestDimConstraints.test_dim_constraints_solve_full",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def eval(,"    precedence: int = 35  # lower precedence than add

    @classmethod
    def eval(
        cls, base: sympy.Integer, divisor: sympy.Integer, modulus: sympy.Integer
    ) -> Optional[sympy.Basic]:
        if base == 0 or modulus == 1:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def eval(,"        return True if self.args[1].is_positive and self.args[2].is_positive else None  # type: ignore[attr-defined]

    @classmethod
    def eval(
        cls, c: sympy.Basic, p: sympy.Basic, q: sympy.Basic
    ) -> Optional[sympy.Basic]:
        if c == sympy.true:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, p: sympy.Expr, q: sympy.Expr) -> Optional[sympy.Expr]:","    is_integer: bool = True

    @classmethod
    def eval(cls, p: sympy.Expr, q: sympy.Expr) -> Optional[sympy.Expr]:
        # python test/dynamo/test_export.py -k ExportTests.test_trivial_constraint
        # Triggered by sympy.solvers.inequalities.reduce_inequalities
        # assert p.is_integer, p",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, p, q):","    is_nonnegative = True

    @classmethod
    def eval(cls, p, q):
        # This was adapted from: sympy/core/mod.py

        # Triggered by",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number):","    is_integer = True

    @classmethod
    def eval(cls, number):
        # assert number.is_integer is not True, number
        if number in (sympy.oo, int_oo):
            return int_oo",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number):","    is_integer = True

    @classmethod
    def eval(cls, number):
        if number in (sympy.oo, int_oo):
            return int_oo
        if number in (-sympy.oo, int_oo):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, base, shift):","    is_integer = True

    @classmethod
    def eval(cls, base, shift):
        if shift < 0:
            raise ValueError(""negative shift count"")
        return base * 2**shift",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, base, shift):","    is_integer = True

    @classmethod
    def eval(cls, base, shift):
        if shift < 0:
            raise ValueError(""negative shift count"")
        return FloorDiv(base, 2**shift)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, base, exp):","    precedence: int = 50  # precedence of mul

    @classmethod
    def eval(cls, base, exp):
        if isinstance(base, sympy.Integer) and isinstance(exp, sympy.Integer):
            r = safe_pow(base, exp)
            if r in (-int_oo, int_oo):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, base, exp):","    precedence: int = 60  # precedence of pow

    @classmethod
    def eval(cls, base, exp):
        # NB: These test sympy.Number, not sympy.Float, because:
        #   - Sometimes we may have sympy.oo or int_oo, and that's not a Float
        #     (but coerces to math.Inf)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, base, divisor):","    precedence: int = 35  # lower precedence than add

    @classmethod
    def eval(cls, base, divisor):
        # assert base.is_integer is not True, base
        # assert divisor.is_integer is not True, divisor
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, base, divisor):","    precedence: int = 35  # lower precedence than add

    @classmethod
    def eval(cls, base, divisor):
        if divisor.is_zero:
            raise ZeroDivisionError(""division by zero"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, *args):","    is_integer = True

    @classmethod
    def eval(cls, *args):
        assert len(args) % 2 == 0
        dim = len(args) // 2
        sizes = args[0:dim]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number):","    is_real = True

    @classmethod
    def eval(cls, number):
        # assert number.is_integer is not True, number
        if isinstance(number, sympy.Number):
            # NB: It is safe to use truncation to integer, which is what",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number):","    is_integer = True

    @classmethod
    def eval(cls, number):
        # assert number.is_integer is not True, number
        if number in (sympy.oo, int_oo):
            return int_oo",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number):","    is_integer = True

    @classmethod
    def eval(cls, number):
        # assert number.is_integer is not True, number

        if number is sympy.oo:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number, ndigits):","    is_real = True

    @classmethod
    def eval(cls, number, ndigits):
        # assert number.is_integer is not True, number

        if isinstance(number, sympy.Number) and isinstance(ndigits, sympy.Integer):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, number):","    is_real = True

    @classmethod
    def eval(cls, number):
        if number in [sympy.oo, -sympy.oo]:
            return number
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, a):","        _torch_unpickler = make_opaque_unary_fn

        @classmethod
        def eval(cls, a):
            if isinstance(a, (sympy.Integer, sympy.Float)):
                # Python converts to float64 before computing, c.f.
                # >>> math.sin(2**53+1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(cls, a, b):","        )

        @classmethod
        def eval(cls, a, b):
            if a.is_Boolean and b.is_Boolean:
                return getattr(operator, real_op_name)(a, b)
            if a.is_Boolean:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RE_KERNEL_LAUNCH = re.compile(r'([ ]+)(detail?)::[ ]+\\\n[ ]+'),"    return cuda_kernel


RE_KERNEL_LAUNCH = re.compile(r'([ ]+)(detail?)::[ ]+\\\n[ ]+')


def processKernelLaunches(string, stats):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_ASSERT = re.compile(r""\bassert[ ]*\("")","    return find_closure_group(input_string, start, group=[""("", "")""])


RE_ASSERT = re.compile(r""\bassert[ ]*\("")


def replace_math_functions(input_string):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_SYNCTHREADS = re.compile(r"":?:?\b(__syncthreads)\b(\w*\()"")","    return output_string


RE_SYNCTHREADS = re.compile(r"":?:?\b(__syncthreads)\b(\w*\()"")


def hip_header_magic(input_string):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_EXTERN_SHARED = re.compile(r""extern\s+([\w\(\)]+)?\s*__shared__\s+([\w:<>\s]+)\s+(\w+)\s*\[\s*\]\s*;"")","    return output_string


RE_EXTERN_SHARED = re.compile(r""extern\s+([\w\(\)]+)?\s*__shared__\s+([\w:<>\s]+)\s+(\w+)\s*\[\s*\]\s*;"")


def replace_extern_shared(input_string):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.export_to_regex()),"        if constants.API_PYTORCH not in meta_data and constants.API_SPECIAL not in meta_data:
            CAFFE2_TRIE.add(src)
            CAFFE2_MAP[src] = dst
RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.export_to_regex())
RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\W)({PYTORCH_TRIE.export_to_regex()})(?=\W)')

RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\W)({PYTORCH_TRIE.export_to_regex()})(?=\W)'),"            CAFFE2_TRIE.add(src)
            CAFFE2_MAP[src] = dst
RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.export_to_regex())
RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\W)({PYTORCH_TRIE.export_to_regex()})(?=\W)')

RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')
RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')","RE_CAFFE2_PREPROCESSOR = re.compile(CAFFE2_TRIE.export_to_regex())
RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\W)({PYTORCH_TRIE.export_to_regex()})(?=\W)')

RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')
RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')
RE_THC_GENERIC_FILE = re.compile(r'#define THC_GENERIC_FILE ""([^""]+)""')
RE_CU_SUFFIX = re.compile(r'\.cu\b')  # be careful not to pick up .cuh",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>'),"RE_PYTORCH_PREPROCESSOR = re.compile(fr'(?<=\W)({PYTORCH_TRIE.export_to_regex()})(?=\W)')

RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')
RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')
RE_THC_GENERIC_FILE = re.compile(r'#define THC_GENERIC_FILE ""([^""]+)""')
RE_CU_SUFFIX = re.compile(r'\.cu\b')  # be careful not to pick up .cuh
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_THC_GENERIC_FILE = re.compile(r'#define THC_GENERIC_FILE ""([^""]+)""')","
RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')
RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')
RE_THC_GENERIC_FILE = re.compile(r'#define THC_GENERIC_FILE ""([^""]+)""')
RE_CU_SUFFIX = re.compile(r'\.cu\b')  # be careful not to pick up .cuh

""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RE_CU_SUFFIX = re.compile(r'\.cu\b')  # be careful not to pick up .cuh,"RE_QUOTE_HEADER = re.compile(r'#include ""([^""]+)""')
RE_ANGLE_HEADER = re.compile(r'#include <([^>]+)>')
RE_THC_GENERIC_FILE = re.compile(r'#define THC_GENERIC_FILE ""([^""]+)""')
RE_CU_SUFFIX = re.compile(r'\.cu\b')  # be careful not to pick up .cuh

""""""
Returns a HipifyResult object with the following details:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_INCLUDE = re.compile(r""#include .*\n"")","    return in_txt


RE_INCLUDE = re.compile(r""#include .*\n"")


def extract_arguments(start, string):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
debug_info_t = pickle.loads(raw_debug),"
            # Parse debug info and add begin/end markers if not present
            # to ensure that we cover the entire source code.
            debug_info_t = pickle.loads(raw_debug)
            text_table = None

            if (len(debug_info_t) == 3 and",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"extra_files_json_pattern = re.compile(re.escape(path_prefix) + ""/extra/.*\\.json"")","                code_parts.append([text.decode(""utf-8""), intern(s_file), s_line, intern(s_text), s_start, s_end])
            code_files[zi.filename] = code_parts

        extra_files_json_pattern = re.compile(re.escape(path_prefix) + ""/extra/.*\\.json"")
        extra_files_jsons = {}
        for zi in zf.infolist():
            if not extra_files_json_pattern.fullmatch(zi.filename):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"np_str_obj_array_pattern = re.compile(r""[SaUO]"")","import torch


np_str_obj_array_pattern = re.compile(r""[SaUO]"")


def default_convert(data):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
self._datapipe = pickle.loads(value),"        if use_dill:
            self._datapipe = dill.loads(value)
        else:
            self._datapipe = pickle.loads(value)

    def __len__(self):
        try:",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"start_token, end_token = '(""', '"")'","def extract_method_name(line: str) -> str:
    """"""Extract method name from decorator in the form of ""@functional_datapipe({method_name})"".""""""
    if '(""' in line:
        start_token, end_token = '(""', '"")'
    elif ""('"" in line:
        start_token, end_token = ""('"", ""')""
    else:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:PyTorch Core
"start_token, end_token = ""('"", ""')""","    if '(""' in line:
        start_token, end_token = '(""', '"")'
    elif ""('"" in line:
        start_token, end_token = ""('"", ""')""
    else:
        raise RuntimeError(
            f""Unable to find appropriate method name within line:\n{line}""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:PyTorch Core
"start_token = ""class ""","
def extract_class_name(line: str) -> str:
    """"""Extract class name from class definition in the form of ""class {CLASS_NAME}({Type}):"".""""""
    start_token = ""class ""
    end_token = ""(""
    start, end = line.find(start_token) + len(start_token), line.find(end_token)
    return line[start:end]",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:PyTorch Core
"end_token = ""(""","def extract_class_name(line: str) -> str:
    """"""Extract class name from class definition in the form of ""class {CLASS_NAME}({Type}):"".""""""
    start_token = ""class ""
    end_token = ""(""
    start, end = line.find(start_token) + len(start_token), line.find(end_token)
    return line[start:end]
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.loads(data),"        return json.loads(data)

    if extension in ""pyd pickle"".split():
        return pickle.loads(data)

    if extension in ""pt"".split():
        stream = io.BytesIO(data)",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.load(stream),"
    if extension in ""pt"".split():
        stream = io.BytesIO(data)
        return torch.load(stream)

    # if extension in ""ten tb"".split():
    #     from . import tenbin",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(launcher_code, scope)","            f""    runner({', '.join(runner_args)})"",
        ]
        launcher_code = ""\n"".join(lines)
        exec(launcher_code, scope)
        return scope[""launcher""]

    def _get_arg_lists(",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""grid_0 = {self.x_grid}"", scope)","        scope = {**meta}
        for line in self.prefix:
            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)
        return scope[""grid_0""], scope[""grid_1""], scope[""grid_2""]",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""grid_1 = {self.y_grid}"", scope)","        for line in self.prefix:
            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)
        return scope[""grid_0""], scope[""grid_1""], scope[""grid_2""]
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""grid_2 = {self.z_grid}"", scope)","            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)
        return scope[""grid_0""], scope[""grid_1""], scope[""grid_2""]

",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(line, scope)","    def eval_slow(self, meta: dict[str, int]) -> tuple[int, int, int]:
        scope = {**meta}
        for line in self.prefix:
            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(launcher_code, scope)","            f""    runner({', '.join(runner_args)})"",
        ]
        launcher_code = ""\n"".join(lines)
        exec(launcher_code, scope)
        return scope[""launcher""]

    def _get_arg_lists(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(line, scope)","    def eval_slow(self, meta: dict[str, int]) -> tuple[int, int, int]:
        scope = {**meta}
        for line in self.prefix:
            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""grid_0 = {self.x_grid}"", scope)","        scope = {**meta}
        for line in self.prefix:
            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)
        return scope[""grid_0""], scope[""grid_1""], scope[""grid_2""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""grid_1 = {self.y_grid}"", scope)","        for line in self.prefix:
            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)
        return scope[""grid_0""], scope[""grid_1""], scope[""grid_2""]
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(f""grid_2 = {self.z_grid}"", scope)","            exec(line, scope)
        exec(f""grid_0 = {self.x_grid}"", scope)
        exec(f""grid_1 = {self.y_grid}"", scope)
        exec(f""grid_2 = {self.z_grid}"", scope)
        return scope[""grid_0""], scope[""grid_1""], scope[""grid_2""]

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"triton_name_sub = re.compile(r""^def [^(]+\("")","
log = logging.getLogger(__name__)

triton_name_sub = re.compile(r""^def [^(]+\("")


def generate_lookup_hash_from_source_code(size_hints_str: str, source_code: str) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"binary = triton.compile(*compile_args, **compile_kwargs)","        }

        try:
            binary = triton.compile(*compile_args, **compile_kwargs)
        except Exception:
            log.exception(
                ""Triton compilation failed: %s\n%s\nmetadata: %s"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, mod.__dict__, mod.__dict__)","        mod = ModuleType(f""{__name__}.{key}"")
        mod.__file__ = path
        mod.key = key  # type: ignore[attr-defined]
        exec(code, mod.__dict__, mod.__dict__)
        if set_sys_modules:
            sys.modules[mod.__name__] = mod
        return mod",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, mod.__dict__, mod.__dict__)","        mod = ModuleType(f""{__name__}.{key}"")
        mod.__file__ = path
        mod.key = key  # type: ignore[attr-defined]
        exec(code, mod.__dict__, mod.__dict__)
        if set_sys_modules:
            sys.modules[mod.__name__] = mod
        return mod",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"code = compile(f.read(), path, ""exec"", dont_inherit=True)",") -> ModuleType:
    with open(path) as f:
        try:
            code = compile(f.read(), path, ""exec"", dont_inherit=True)
        except Exception as e:
            raise RuntimeError(
                f""Failed to import {path}\n{type(e).__name__}: {e}""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_cpp_string_literal_pattern = re.compile(r'[""\\\n\t\r]')","    ""\t"": ""\\t"",
    ""\r"": ""\\r"",
}
_cpp_string_literal_pattern = re.compile(r'[""\\\n\t\r]')


def cpp_string_literal(s: str) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(tuning_code, scope)","            )
        # Execute the code to autotune kernels
        try:
            exec(tuning_code, scope)
        except Exception as e:
            raise RuntimeError(f""Failed to run autotuning code block: {e}"") from e
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(tuning_code, scope)","            )
        # Execute the code to autotune kernels
        try:
            exec(tuning_code, scope)
        except Exception as e:
            raise RuntimeError(f""Failed to run autotuning code block: {e}"") from e
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"RE_PYTORCH_PREPROCESSOR = re.compile(rf""({PYTORCH_TRIE.export_to_regex()})(?=\W)"")","
    # Note that lookahead (?=\W) is still needed to keep hipification idomponent, for example
    # we need to skip replacing ""getStreamFromExternal"" in ""getStreamFromExternalMasqueradingAsCUDA""
    RE_PYTORCH_PREPROCESSOR = re.compile(rf""({PYTORCH_TRIE.export_to_regex()})(?=\W)"")

    source_codes = RE_PYTORCH_PREPROCESSOR.sub(c2_repl, source_codes)  # type: ignore[arg-type]
    return source_codes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# as a global variable passed when calling exec(code, mod.__dict__, mod.__dict__).","        if V.graph.constants:
            # Append constants to the input args for cpp wrapper.
            # Python wrapper directly gets the value inside the wrapper call
            # as a global variable passed when calling exec(code, mod.__dict__, mod.__dict__).
            # For cpp wrapper, we need to pass this python value to the inductor_entry_impl function explicitly.
            assert all(
                isinstance(v, torch.Tensor) for v in list(V.graph.constants.values())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_RE_PAREN_NOT_NEEDED = re.compile(r""[a-z0-9_.]+|\([^)]*\)|"", flags=re.IGNORECASE)","        return ops.to_dtype(ops.round(a), dtype)


_RE_PAREN_NOT_NEEDED = re.compile(r""[a-z0-9_.]+|\([^)]*\)|"", flags=re.IGNORECASE)


def _all_in_parens(string: str) -> bool:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"undefined_re = re.compile(r""\b(tmp\d+)\[\?\]"")","

class HalideCSEVariable(CSEVariable):
    undefined_re = re.compile(r""\b(tmp\d+)\[\?\]"")

    def __init__(
        self,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return pickle.loads(data),"        return pickle.dumps(obj, pickle.HIGHEST_PROTOCOL)

    def loads(self, data: bytes) -> object:
        return pickle.loads(data)


class SubprocKind(Enum):",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
mod = importlib.import_module(pkg),"    that it's of the given type and then instantiate it.
    """"""
    pkg, name = qname.rsplit(""."", 1)
    mod = importlib.import_module(pkg)
    ty = getattr(mod, name)
    if not issubclass(ty, base):
        raise TypeError(f""Type {ty} is not a subtype of {base}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
package = importlib.import_module(package_name),") -> list[LearnedHeuristic]:
    instances = []

    package = importlib.import_module(package_name)
    for _, module_name, _ in pkgutil.walk_packages(
        package.__path__, package.__name__ + "".""
    ):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
module = importlib.import_module(module_name),"            if not module_basename.startswith(""_""):
                # learned heuristics start with an underscore
                continue
            module = importlib.import_module(module_name)

            # look for classes that are subclasses of base_class
            for _name, obj in inspect.getmembers(module):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# for torch.compile(dynamic=False),"            aten.reshape.default, pattern, KeywordArg(""out_shape_no_bias"")
        )

    # for torch.compile(dynamic=False)
    pattern_no_bias_1 = _with_outer_reshape(get_pattern_no_bias(expand_a_scale=False))
    pattern_with_bias_1 = CallFunction(
        aten.add.Tensor,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# for torch.compile(dynamic=True),"        pattern_no_bias_1,
        KeywordArg(""bias""),
    )
    # for torch.compile(dynamic=True)
    pattern_no_bias_2 = _with_outer_reshape(get_pattern_no_bias(expand_a_scale=True))
    pattern_with_bias_2 = CallFunction(
        aten.reshape.default,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pattern = re.compile(r""\s*struct\s(.*?)\s:"")","            op.epilogue_functor = cutlass_lib.EpilogueFunctor.LinearCombination

        op_def = emitter.emit(op)
        pattern = re.compile(r""\s*struct\s(.*?)\s:"")
        decl = [line for line in op_def.split(""\n"") if ""struct "" in line][-1]

        match = pattern.match(decl)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pattern = re.compile(r""\s*using\s(.*?)\s="")","        )
        if op.gemm_kind != cutlass_lib.GemmKind.Sparse:
            op_def = op_def.replace(""false,"", """")
        pattern = re.compile(r""\s*using\s(.*?)\s="")
        decl = op_def.split(""\n"")[2]

        match = pattern.match(decl)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ROCmCodeCache.compile(self.source_code, ""so"")","        # Prepopulate code cache
        # may happen in separate Threadpool
        log.debug(""Precompiling %s"", self)
        ROCmCodeCache.compile(self.source_code, ""so"")
        if config.rocm.generate_test_runner:
            ROCmCodeCache.compile(self.source_code, ""exe"")
        log.debug(""Done precompiling %s"", self)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ROCmCodeCache.compile(self.source_code, ""exe"")","        log.debug(""Precompiling %s"", self)
        ROCmCodeCache.compile(self.source_code, ""so"")
        if config.rocm.generate_test_runner:
            ROCmCodeCache.compile(self.source_code, ""exe"")
        log.debug(""Done precompiling %s"", self)

    def make_run_fn(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, globals_dict)","        code = compile(dest_ast, """", ""exec"")
        globals_dict = copy.copy(fn.__globals__)
        keys_before = set(globals_dict.keys())
        exec(code, globals_dict)
        new_keys = list(set(globals_dict.keys()) - keys_before)
        assert len(new_keys) == 1
        fn_compiled = globals_dict[new_keys[0]]",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(code, globals_dict)","        code = compile(dest_ast, """", ""exec"")
        globals_dict = copy.copy(fn.__globals__)
        keys_before = set(globals_dict.keys())
        exec(code, globals_dict)
        new_keys = list(set(globals_dict.keys()) - keys_before)
        assert len(new_keys) == 1
        fn_compiled = globals_dict[new_keys[0]]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"code = compile(dest_ast, """", ""exec"")","        dest_ast = ast.fix_missing_locations(self.visit(source_ast))

        # Pull out the compiled function from the newly-created Module
        code = compile(dest_ast, """", ""exec"")
        globals_dict = copy.copy(fn.__globals__)
        keys_before = set(globals_dict.keys())
        exec(code, globals_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
fn = torch.compile(f),"
    Example usage::

        fn = torch.compile(f)
        x = DynamicInt(4)
        fn(x)  # compiles x as a dynamic integer input; returns f(4)
    """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(code, SYMPY_INTERP, args)","        To be used by compile_fx to evaluate symexprs
        """"""
        args = {str(e): val for e, val in self.var_to_val.items()}
        return eval(code, SYMPY_INTERP, args)

    def deserialize_symexpr(self, code: str) -> Union[SymInt, SymFloat, SymBool]:
        """"""",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(code, SYMPY_INTERP, args)","            str(e): SymInt(SymNode(e, self, int, int(val), fx_node=None))
            for e, val in self.var_to_val.items()
        }
        return eval(code, SYMPY_INTERP, args)

    def evaluate_guards_expression(self, code: str, args: Sequence[object]) -> bool:
        """"""",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(code, SYMPY_INTERP, {""L"": dict(zip(arg_names, args))})","        generated by produce_guards_expression for the given concrete args.
        """"""
        arg_names = [f""t{i}"" for i in range(len(args))]
        return eval(code, SYMPY_INTERP, {""L"": dict(zip(arg_names, args))})

    def evaluate_guards_for_args(
        self,",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(code, SYMPY_INTERP, args)","        To be used by compile_fx to evaluate symexprs
        """"""
        args = {str(e): val for e, val in self.var_to_val.items()}
        return eval(code, SYMPY_INTERP, args)

    def deserialize_symexpr(self, code: str) -> Union[SymInt, SymFloat, SymBool]:
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(code, SYMPY_INTERP, args)","            str(e): SymInt(SymNode(e, self, int, int(val), fx_node=None))
            for e, val in self.var_to_val.items()
        }
        return eval(code, SYMPY_INTERP, args)

    def evaluate_guards_expression(self, code: str, args: Sequence[object]) -> bool:
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(code, SYMPY_INTERP, {""L"": dict(zip(arg_names, args))})","        generated by produce_guards_expression for the given concrete args.
        """"""
        arg_names = [f""t{i}"" for i in range(len(args))]
        return eval(code, SYMPY_INTERP, {""L"": dict(zip(arg_names, args))})

    def evaluate_guards_for_args(
        self,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# @torch.compile(),"    # ---------
    # Consider a function ""opt_f"" as shown below:

    # @torch.compile()
    # def opt_f(x: bool, y: Tensor):
    #   if x == True:
    #     return y + torch.randn([4])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(f""lambda {argnames}: fn({argnames})"", {""fn"": fn})","def fake_signature(fn: Callable[_P, R], nargs: int) -> Callable[_P, R]:
    """"""FX gets confused by varargs, de-confuse it""""""
    argnames = "","".join(f""arg{i}"" for i in range(nargs))
    return eval(f""lambda {argnames}: fn({argnames})"", {""fn"": fn})


@contextmanager",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return eval(f""lambda {argnames}: fn({argnames})"", {""fn"": fn})","def fake_signature(fn: Callable[_P, R], nargs: int) -> Callable[_P, R]:
    """"""FX gets confused by varargs, de-confuse it""""""
    argnames = "","".join(f""arg{i}"" for i in range(nargs))
    return eval(f""lambda {argnames}: fn({argnames})"", {""fn"": fn})


@contextmanager",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
split_mod.eval(),"
        self.tag(subgraphs)
        split_mod = self.split(remove_tag=True)
        split_mod.eval()

        if dump_graph:
            drawer = FxGraphDrawer(split_mod, ""preview"", ignore_getattr=True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"state_dict = torch.load(torch_save_path, weights_only=False)","        To avoid OOM, it's recommended to only run this function on a single rank.
    """"""

    state_dict = torch.load(torch_save_path, weights_only=False)
    # we don't need stateful behavior here because the expectation is anything loaded by
    # torch.load would not contain stateful objects.
    _save_state_dict(",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
torch_state_dict = torch.load(,"        # TODO: read on each host, instead of only the coordinator
        if self.is_coordinator:
            assert self.checkpoint_id is not None
            torch_state_dict = torch.load(
                self.checkpoint_id, map_location=""cpu"", weights_only=False
            )
            if planner.flatten_state_dict:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
metadata = pickle.load(metadata_file),"        rank = kwargs.get(""rank"", None)
        path = self._get_metadata_path(rank)
        with self.fs.create_stream(path, ""rb"") as metadata_file:
            metadata = pickle.load(metadata_file)

        if getattr(metadata, ""storage_meta"", None) is None:
            metadata.storage_meta = StorageMeta()",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
torch.load(,"
                        tensor = cast(
                            Tensor,
                            torch.load(
                                seekable,
                                map_location=""cpu"",
                                weights_only=True,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
meta: _StateDictMeta = pickle.loads(buf.cpu().numpy().tobytes()),"        buf = torch.empty(length, dtype=torch.uint8, device=self._device)
        self._pg.recv([buf], src_rank, tag=2).wait()

        meta: _StateDictMeta = pickle.loads(buf.cpu().numpy().tobytes())

        i: int = 0
        works: list[Work] = []",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
self.state_dict[read_item.dest_index.fqn] = torch.load(,"                torch.load(value, weights_only=False),
            )
        else:
            self.state_dict[read_item.dest_index.fqn] = torch.load(
                value, weights_only=False
            )
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.load(value, weights_only=False),","            set_element(
                self.original_state_dict,
                self.mappings[read_item.dest_index.fqn],
                torch.load(value, weights_only=False),
            )
        else:
            self.state_dict[read_item.dest_index.fqn] = torch.load(",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"def eval(self, *args, target=None, losses: Optional[list] = None, **kwargs):","        """"""
        raise NotImplementedError

    def eval(self, *args, target=None, losses: Optional[list] = None, **kwargs):
        """"""
        Run one iteration of the pipeline schedule with *whole-batch* input.
        Will chunk the input into microbatches automatically, and go through the",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Please call eval() instead.""","            raise RuntimeError(
                ""step() requires gradients to be enabled for backward computation; ""
                ""it should not be used under torch.no_grad() context. ""
                ""Please call eval() instead.""
            )

        # Set the same has_backward flag for stage object",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""Please call eval() instead.""","            raise RuntimeError(
                ""step() requires gradients to be enabled for backward computation; ""
                ""it should not be used under torch.no_grad() context. ""
                ""Please call eval() instead.""
            )

        # Set the same has_backward flag for stage object",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
_action_regex = re.compile(,"B = FULL_BACKWARD

# Helper to parse an action string like 1F0 into a tuple of (stage_index, computation_type, microbatch_index)
_action_regex = re.compile(
    r""(\d+)(F|I|B|W|UNSHARD|RESHARD|SEND_F|RECV_F|SEND_B|RECV_B)(\d*)""
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"obj = torch.load(buffer, map_location=device, weights_only=False)","        )
        dist.broadcast(data_recv_tensor, src=src_rank, group=group, async_op=False)
        buffer = io.BytesIO(data_recv_tensor.cpu().numpy())
        obj = torch.load(buffer, map_location=device, weights_only=False)
    return obj

",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
stats = pickle.load(f),"    def load(self, path: str) -> None:
        """"""Load the pickled memory stats to plot the traces or print the summary.""""""
        with open(path, ""rb"") as f:
            stats = pickle.load(f)

        self.memories_allocated = stats[""memories_allocated""]
        self.memories_active = stats[""memories_active""]",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
use ``torch.compile()``. Setting this to ``False`` exposes FSDP's,"            case, its data will be like a size-0 empty tensor. Users should not
            author programs relying on what data is present for a given
            original parameter in its sharded form. ``True`` is required to
            use ``torch.compile()``. Setting this to ``False`` exposes FSDP's
            internal :class:`FlatParameter` s to the user via
            :meth:`nn.Module.named_parameters`. (Default: ``False``)
        ignored_states (Optional[Iterable[torch.nn.Parameter]], Optional[Iterable[torch.nn.Module]]):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# exiting eval() mode.,"
        # We cast buffers back to full precision if we're forcing full precision. Disjointly, we check if buffers
        # are in full precision and if we should cast them back to lower precision, which happens when
        # exiting eval() mode.
        handle = state._handle
        if handle:
            should_cast_buffers_to_full_prec = handle._force_full_precision",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# (i.e. model.eval() + full precision in eval was configured), don't downcast gradient.","                not _low_precision_hook_enabled(state)
                and flat_param.grad.dtype != handle._reduce_dtype
                # If we are forcing full precision but communicating grads
                # (i.e. model.eval() + full precision in eval was configured), don't downcast gradient.
                and not handle._force_full_precision
            ):
                flat_param.grad.data = flat_param.grad.to(handle._reduce_dtype)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# Env var toggling whether when model is in .eval() mode, should we run in fp32","# pre-backward each iteration.
_FSDP_SKIP_WRITEBACK_CHECK = ""FSDP_SKIP_WRITEBACK_CHECK""

# Env var toggling whether when model is in .eval() mode, should we run in fp32
# or the reduced precision.
_FSDP_USE_FULL_PREC_IN_EVAL = ""FSDP_USE_FULL_PREC_IN_EVAL""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
output.append(torch.load(buffer)),"
    output = []
    buffer = io.BytesIO(np.asarray(input_tensor).tobytes())
    output.append(torch.load(buffer))
    return output

",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
compiled_create_block_mask = torch.compile(,"    """"""
    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE

    compiled_create_block_mask = torch.compile(
        create_block_mask, dynamic=False, fullgraph=True
    )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ansi_escape = re.compile(r""\x1B\[[0-?]*[ -/]*[@-~]"")","        """"""
        Alternative to console CommDebugMode output, writes to file specified by the user
        """"""
        ansi_escape = re.compile(r""\x1B\[[0-?]*[ -/]*[@-~]"")
        table = ansi_escape.sub("""", self.generate_comm_debug_tracing_table(noise_level))

        with open(file_name, ""w"") as log_file:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
self._state = pickle.loads(state_bits),"
        if state_bits is not None:
            try:
                self._state = pickle.loads(state_bits)
            except pickle.PickleError as exc:
                raise RendezvousStateError(
                    ""The rendezvous state is corrupt. See inner exception for details.""",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"generated_module = importlib.import_module(f""{generated_module_name}"")","    # you may need to call invalidate_caches() in order for the new module
    # to be noticed by the import system.
    importlib.invalidate_caches()
    generated_module = importlib.import_module(f""{generated_module_name}"")
    return generated_module

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def eval(self) -> Self:,"    def train(self, mode: bool = True) -> Self:
        return self.module_rref.rpc_sync().train()  # type: ignore[operator, union-attr]

    def eval(self) -> Self:
        return self.module_rref.rpc_sync().eval()  # type: ignore[operator, union-attr]

    def requires_grad_(self, requires_grad: bool = True) -> Self:  # type: ignore[return]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"return self.module_rref.rpc_sync().eval()  # type: ignore[operator, union-attr]","        return self.module_rref.rpc_sync().train()  # type: ignore[operator, union-attr]

    def eval(self) -> Self:
        return self.module_rref.rpc_sync().eval()  # type: ignore[operator, union-attr]

    def requires_grad_(self, requires_grad: bool = True) -> Self:  # type: ignore[return]
        _raise_not_supported(self.requires_grad_.__name__)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"metadata_dict = torch.load(file_path, map_location=map_location)","        """"""

        with FakeTensorMode():
            metadata_dict = torch.load(file_path, map_location=map_location)

        missing_keys = []
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"torch.load(file_path, map_location=map_location),","
        if state_dict is None:
            result: tuple[STATE_DICT, list[str]] = (
                torch.load(file_path, map_location=map_location),
                [],
            )
        else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"_regex = re.compile(r""^\s*"" + VERSION_PATTERN + r""\s*$"", re.VERBOSE | re.IGNORECASE)","    True
    """"""

    _regex = re.compile(r""^\s*"" + VERSION_PATTERN + r""\s*$"", re.VERBOSE | re.IGNORECASE)
    _key: CmpKey

    def __init__(self, version: str) -> None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_local_version_separators = re.compile(r""[\._-]"")","    return None


_local_version_separators = re.compile(r""[\._-]"")


def _parse_local_version(local: Optional[str]) -> Optional[LocalType]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"self._module = importlib.import_module(""."", self._name)","
    def __getattr__(self, attr: str) -> object:
        if self._module is None:
            self._module = importlib.import_module(""."", self._name)
        return getattr(self._module, attr)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"    if training == torch.onnx.TrainingMode.TRAINING:
        model.train()
    elif training == torch.onnx.TrainingMode.EVAL:
        model.eval()
    with torch.no_grad(), contextlib.ExitStack() as stack:
        model_f: str | io.BytesIO = io.BytesIO()
        if use_external_data:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"_ATTR_PATTERN = re.compile(""^(.+)_(([ifstgz])|(ty))$"")","from torch.onnx._internal.torchscript_exporter._globals import GLOBALS


_ATTR_PATTERN = re.compile(""^(.+)_(([ifstgz])|(ty))$"")
_SKIP_NODE_ATTRIBUTES = {""inplace"", ""aten""}

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
package = __import__(package_name),"        if isinstance(python_class_name, type):
            python_class_name = python_class_name.__module__
        package_name = python_class_name.split(""."")[0]
        package = __import__(package_name)
        version = getattr(package, ""__version__"", None)
        # TODO: Figure out how to retrieve commit hash.
        commit_hash = None",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def eval(,"            ) from e
        return node.outputs  # type: ignore[return-value]

    def eval(
        self,
        schema: onnx.defs.OpSchema,
        args: Sequence[AllowedArgType],  # type: ignore[override]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ansi_escape = re.compile(r""\x1B[@-_][0-?]*[ -/]*[@-~]"")","def _strip_color_from_string(text: str) -> str:
    # This regular expression matches ANSI escape codes
    # https://github.com/pytorch/pytorch/blob/9554a9af8788c57e1c5222c39076a5afcf0998ae/torch/_dynamo/utils.py#L2785-L2788
    ansi_escape = re.compile(r""\x1B[@-_][0-?]*[ -/]*[@-~]"")
    return ansi_escape.sub("""", text)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# If you have already wrapped flex_attention in torch.compile(), this flag has no effect","# in your score_mod/mask_mod functions during development.
#
# This flag only affects the internal compilation when flex_attention is called directly.
# If you have already wrapped flex_attention in torch.compile(), this flag has no effect
# and the user's compilation will still occur.
#
# Usage:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""_compile flag on create_block_mask was originally added to work around a torch.compile limitation. That limitation has since been addressed. So, to compile create_block_mask, we suggest doing torch.compile(create_block_mask). This still works for now, but will be removed in the future."",","
    if _compile:
        warnings.warn(
            ""_compile flag on create_block_mask was originally added to work around a torch.compile limitation. That limitation has since been addressed. So, to compile create_block_mask, we suggest doing torch.compile(create_block_mask). This still works for now, but will be removed in the future."",
            DeprecationWarning,
        )
        return torch.compile(create_block_mask)(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
return torch.compile(create_block_mask)(,"            ""_compile flag on create_block_mask was originally added to work around a torch.compile limitation. That limitation has since been addressed. So, to compile create_block_mask, we suggest doing torch.compile(create_block_mask). This still works for now, but will be removed in the future."",
            DeprecationWarning,
        )
        return torch.compile(create_block_mask)(
            mask_mod, B, H, Q_LEN, KV_LEN, device, BLOCK_SIZE
        )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n\n""","        _warn_once(
            warning_id=""flex_attention_performance"",
            message=(
                ""flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n\n""
                ""SOLUTION: Use torch.compile(flex_attention)(...)\n\n""
                ""If you want to debug your score_mod/mask_mod, you can set:\n""
                ""torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n\n""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""SOLUTION: Use torch.compile(flex_attention)(...)\n\n""","            warning_id=""flex_attention_performance"",
            message=(
                ""flex_attention called without torch.compile() - this will use an unfused implementation that materializes the full scores matrix instead of generating a fused kernel.\n\n""
                ""SOLUTION: Use torch.compile(flex_attention)(...)\n\n""
                ""If you want to debug your score_mod/mask_mod, you can set:\n""
                ""torch.nn.attention.flex_attention._FLEX_ATTENTION_DISABLE_COMPILE_DEBUG = True\n\n""
                ""This will allow you to use print statements or breakpoints. Note: This doesn't work with the backwards pass and may produce incorrect results.""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
flex_fn = torch.compile(,"                    if _FLEX_ATTENTION_DISABLE_COMPILE_DEBUG:
                        flex_fn = _flex_attention_hop_wrapper
                    else:
                        flex_fn = torch.compile(
                            _flex_attention_hop_wrapper, backend=backend, fullgraph=True
                        )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
package = importlib.import_module(new_module),"            # We are using the ""RuntimeWarning"" to make sure it is not
            # ignored by default.
            warnings.warn(warning_message, RuntimeWarning)
            package = importlib.import_module(new_module)
            return getattr(package, name)
        raise AttributeError(f""Module {new_module!r} has no attribute {name!r}."")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
synchronization is disabled when ``model.eval()`` is set or if,"
    .. note::
        Synchronization of batchnorm statistics occurs only while training, i.e.
        synchronization is disabled when ``model.eval()`` is set or if
        ``self.training`` is otherwise ``False``.

    Examples::",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
# Don't sync batchnorm stats in inference mode (model.eval()).,"            self.running_var if not self.training or self.track_running_stats else None
        )

        # Don't sync batchnorm stats in inference mode (model.eval()).
        need_sync = (
            bn_training
            and self.training",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
- training is disabled (using ``.eval()``),"    - self attention is being computed (i.e., ``query``, ``key``, and ``value`` are the same tensor).
    - inputs are batched (3D) with ``batch_first==True``
    - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor argument ``requires_grad``
    - training is disabled (using ``.eval()``)
    - ``add_bias_kv`` is ``False``
    - ``add_zero_attn`` is ``False``
    - ``kdim`` and ``vdim`` are equal to ``embed_dim``",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
- training is disabled (using ``.eval()``),"
        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor
          argument ``requires_grad``
        - training is disabled (using ``.eval()``)
        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)
        - activation is one of: ``""relu""``, ``""gelu""``, ``torch.functional.relu``, or ``torch.functional.gelu``
        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
def eval(self) -> Self:,"            module.train(mode)
        return self

    def eval(self) -> Self:
        r""""""Set the module in evaluation mode.

        This has an effect only on certain modules. See the documentation of",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
`.eval()` and several similar mechanisms that may be confused with it.,"        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.

        See :ref:`locally-disable-grad-doc` for a comparison between
        `.eval()` and several similar mechanisms that may be confused with it.

        Returns:
            Module: self",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"def compile(self, *args, **kwargs):","
        return replica

    def compile(self, *args, **kwargs):
        """"""
        Compile this Module's forward using :func:`torch.compile`.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"self._compiled_call_impl = torch.compile(self._call_impl, *args, **kwargs)","
        See :func:`torch.compile` for details on the arguments for this function.
        """"""
        self._compiled_call_impl = torch.compile(self._call_impl, *args, **kwargs)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"artifact = torch.load(buffer, weights_only=False)","    buffer = io.BytesIO(serialized)
    buffer.seek(0)
    # weights_only=False as we want to load custom objects here (e.g. ScriptObject)
    artifact = torch.load(buffer, weights_only=False)
    assert isinstance(artifact, (tuple, dict))
    return artifact
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"# e.g.., `exec('from torch.utils._sympy.functions import *', ...)`","            self.sympy_functions = {
                # all torch.utils._sympy.functions should go here
                # TODO(avik): find a better way to keep this collection in sync;
                # e.g.., `exec('from torch.utils._sympy.functions import *', ...)`
                # would work as long as the public API of that module is complete
                ""FloorDiv"": torch.utils._sympy.functions.FloorDiv,
                ""ModularIndexing"": torch.utils._sympy.functions.ModularIndexing,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
p = subprocess.run(,"
def test(cmd, limit):
    print(f""Testing PYTORCH_JIT_OPT_LIMIT=tensorexpr_fuser={limit} {cmd}"")
    p = subprocess.run(
        f""PYTORCH_JIT_OPT_LIMIT=tensorexpr_fuser={limit} {cmd}"",
        shell=True,
        capture_output=True,",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
p = subprocess.run(,"
def test(cmd, limit):
    print(f""Testing PYTORCH_JIT_OPT_LIMIT=tensorexpr_fuser={limit} {cmd}"")
    p = subprocess.run(
        f""PYTORCH_JIT_OPT_LIMIT=tensorexpr_fuser={limit} {cmd}"",
        shell=True,
        capture_output=True,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","    print(f""Testing PYTORCH_JIT_OPT_LIMIT=tensorexpr_fuser={limit} {cmd}"")
    p = subprocess.run(
        f""PYTORCH_JIT_OPT_LIMIT=tensorexpr_fuser={limit} {cmd}"",
        shell=True,
        capture_output=True,
        encoding=""utf-8"",
        check=False,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
x_pt2 = torch.compile(foo)(x),"        print(f""Testing smoke_test_compile for {device} and {dtype}"")
        x = torch.rand(3, 3, device=device).type(dtype)
        x_eager = foo(x)
        x_pt2 = torch.compile(foo)(x)
        torch.testing.assert_close(x_eager, x_pt2)

    # Check that SIMD were detected for the architecture",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"x_pt2 = torch.compile(model, mode=""max-autotune"")(x)","    print(f""Testing smoke_test_compile with mode 'max-autotune' for {dtype}"")
    x = torch.rand(64, 1, 28, 28, device=device).type(torch.float32)
    model = Net().to(device=device)
    x_pt2 = torch.compile(model, mode=""max-autotune"")(x)


def smoke_test_nvshmem() -> None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"imported_module = importlib.import_module(module[""name""])","
        if release_version and package == ""all"":
            for module in MODULES:
                imported_module = importlib.import_module(module[""name""])
                module_version = imported_module.__version__
                if not module_version.startswith(release_matrix[module[""name""]]):
                    raise RuntimeError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"imported_module = importlib.import_module(module[""name""])","
    if package == ""all"":
        for module in MODULES:
            imported_module = importlib.import_module(module[""name""])
            module_version = imported_module.__version__
            date_m_str = re.findall(""dev\\d+"", module_version)
            date_m_delta = datetime.now() - datetime.strptime(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"imported_module = importlib.import_module(module[""name""])","
    if package == ""all"" and is_cuda_system:
        for module in MODULES:
            imported_module = importlib.import_module(module[""name""])
            # TBD for vision move extension module to private so it will
            # be _extention.
            version = ""N/A""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","                    subprocess.check_output(
                        f""git clone --depth 1 {module['repo']}"",
                        stderr=subprocess.STDOUT,
                        shell=True,
                    )
                except subprocess.CalledProcessError as exc:
                    raise RuntimeError(",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"shell=True,","                output = subprocess.check_output(
                    smoke_test_command,
                    stderr=subprocess.STDOUT,
                    shell=True,
                    universal_newlines=True,
                )
            except subprocess.CalledProcessError as exc:",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
"STATICALLY_LINKED_CXX11_ABI = [re.compile(r"".*recursive_directory_iterator.*"")]",")

# Patterns for detecting statically linked libstdc++ symbols
STATICALLY_LINKED_CXX11_ABI = [re.compile(r"".*recursive_directory_iterator.*"")]


def _apply_libtorch_symbols(symbols):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"re.compile(f""{x}.*{y}"")","
def _apply_libtorch_symbols(symbols):
    return [
        re.compile(f""{x}.*{y}"")
        for (x, y) in itertools.product(LIBTORCH_NAMESPACE_LIST, symbols)
    ]
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"lines = check_output(f'nm ""{lib}""|c++filt', shell=True)","def get_symbols(lib: str) -> list[tuple[str, str, str]]:
    from subprocess import check_output

    lines = check_output(f'nm ""{lib}""|c++filt', shell=True)
    return [x.split("" "", 2) for x in lines.decode(""latin1"").split(""\n"")[:-1]]

",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
model.eval(),"

def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"opt_model = torch.compile(model, mode=""max-autotune"")","    test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)

    model = Net().to(device)
    opt_model = torch.compile(model, mode=""max-autotune"")
    optimizer = optim.Adadelta(opt_model.parameters(), lr=args.lr)

    scheduler = StepLR(optimizer, step_size=1, gamma=args.gamma)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"pattern = re.compile(rf""^({'|'.join(pkgs_to_remove)})\s*(==|@|>=)"", re.IGNORECASE)","    pkgs_to_add = []

    # Remove lines starting with the package names (==, @, >=)  case-insensitive
    pattern = re.compile(rf""^({'|'.join(pkgs_to_remove)})\s*(==|@|>=)"", re.IGNORECASE)
    kept_lines = [line for line in lines if not pattern.match(line)]

    # Get local installed torch/vision/audio from pip freeze",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"QUOTE_INCLUDE_RE = re.compile(r'^#include ""(.*)""')","import sys


QUOTE_INCLUDE_RE = re.compile(r'^#include ""(.*)""')
ANGLE_INCLUDE_RE = re.compile(r""^#include <(.*)>"")

# By default iwyu will pick the C include, but we prefer the C++ headers",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"ANGLE_INCLUDE_RE = re.compile(r""^#include <(.*)>"")","

QUOTE_INCLUDE_RE = re.compile(r'^#include ""(.*)""')
ANGLE_INCLUDE_RE = re.compile(r""^#include <(.*)>"")

# By default iwyu will pick the C include, but we prefer the C++ headers
STD_C_HEADER_MAP = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"re.compile(rf""^{pattern}$"") for pattern in _SKIP_PYTHON_BINDINGS","]

SKIP_PYTHON_BINDINGS = [
    re.compile(rf""^{pattern}$"") for pattern in _SKIP_PYTHON_BINDINGS
]

# These function signatures are not exposed to Python. Note that this signature",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"OPTIONAL_TYPE_PATTERN = re.compile(r""std::optional<(.+)>"")","from torchgen.utils import FileManager, mapMaybe


OPTIONAL_TYPE_PATTERN = re.compile(r""std::optional<(.+)>"")
TYPE_PATTERN = re.compile(r""(?:const\s+)?([A-Z]\w+)"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"TYPE_PATTERN = re.compile(r""(?:const\s+)?([A-Z]\w+)"")","

OPTIONAL_TYPE_PATTERN = re.compile(r""std::optional<(.+)>"")
TYPE_PATTERN = re.compile(r""(?:const\s+)?([A-Z]\w+)"")


# Add 'at::' to types defined in ATen namespace, e.g. Tensor, TensorList, IntArrayRef and etc.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(hints_source, hints_namespace)","        hints_source = f.read()

    hints_namespace: dict[str, Any] = {}
    exec(hints_source, hints_namespace)

    hint_constants = {
        name: value",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
Uses exec() to avoid import dependencies.,"def expand_hints(hints: list[str], dynamo_dir: Optional[str] = None) -> list[str]:
    """"""
    Expands hint references to their actual values from graph_break_hints.
    Uses exec() to avoid import dependencies.
    """"""
    if dynamo_dir is None:
        script_dir = Path(__file__).resolve().parent",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"exec(hints_source, hints_namespace)","        hints_source = f.read()

    hints_namespace: dict[str, Any] = {}
    exec(hints_source, hints_namespace)

    hint_constants = {
        name: value",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"sys.modules[""torch._torch_docs""] = importlib.import_module(""_torch_docs"")","        try:
            # manually import torch._torch_docs and torch._tensor_docs to trigger
            # the mocked _add_docstr and collect docstrings
            sys.modules[""torch._torch_docs""] = importlib.import_module(""_torch_docs"")
            sys.modules[""torch._tensor_docs""] = importlib.import_module(""_tensor_docs"")
        except ModuleNotFoundError:
            # Gracefully fail if these modules are not importable",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"sys.modules[""torch._tensor_docs""] = importlib.import_module(""_tensor_docs"")","            # manually import torch._torch_docs and torch._tensor_docs to trigger
            # the mocked _add_docstr and collect docstrings
            sys.modules[""torch._torch_docs""] = importlib.import_module(""_torch_docs"")
            sys.modules[""torch._tensor_docs""] = importlib.import_module(""_tensor_docs"")
        except ModuleNotFoundError:
            # Gracefully fail if these modules are not importable
            warn(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
ARTIFACT_REGEX = re.compile(,"ARTIFACTS = [
    ""test-reports"",
]
ARTIFACT_REGEX = re.compile(
    r""test-reports-test-(?P<name>[\w\-]+)-\d+-\d+-(?P<runner>[\w\.-]+)_(?P<job>\d+).zip""
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
match_filename_regex = re.compile(match_filename),"    head_branch: str,
    match_filename: str,
) -> list[dict[str, Any]]:
    match_filename_regex = re.compile(match_filename)
    perf_stats = []
    with TemporaryDirectory() as temp_dir:
        print(""Using temporary directory:"", temp_dir)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"binary_file, shell=True, cwd=get_oss_binary_folder(TestType.PY)","    # python test script
    try:
        subprocess.check_call(
            binary_file, shell=True, cwd=get_oss_binary_folder(TestType.PY)
        )
    except subprocess.CalledProcessError:
        print_error(f""Binary failed to run: {binary_file}"")",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:PyTorch Core
ranks = eval(ranks),"            _pg_guids[(pg_uid, global_rank)] = pg_guid
            if isinstance(ranks, str):
                # TODO Bug in FR data format? ranks is '[0, 1,...]'
                ranks = eval(ranks)

            if pg_guid not in _groups:
                groups.append(Group(id=pg_guid, desc=desc, size=len(ranks)))",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
ranks = eval(ranks),"            _pg_guids[(pg_uid, global_rank)] = pg_guid
            if isinstance(ranks, str):
                # TODO Bug in FR data format? ranks is '[0, 1,...]'
                ranks = eval(ranks)

            if pg_guid not in _groups:
                groups.append(Group(id=pg_guid, desc=desc, size=len(ranks)))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
dump = pickle.load(infile),"    host_name = f""host_rank{rank}""

    with open(filename, ""rb"") as infile:
        dump = pickle.load(infile)

    entries = dump[""entries""]
    version = dump[""version""]",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
"exp = re.compile(r""([\w\-\_]*?)(\d+)$"")","    }


exp = re.compile(r""([\w\-\_]*?)(\d+)$"")


def _determine_prefix(files: list[str]) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
r = yaml.load(contents),"
        yaml = ruamel.yaml.YAML()  # type: ignore[attr-defined]
        try:
            r = yaml.load(contents)
        except Exception as err:
            msg = LintMessage(
                path=None,",yaml.load,Unsafe YAML,HIGH,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
RESULTS_RE: re.Pattern[str] = re.compile(,"    description: str | None


RESULTS_RE: re.Pattern[str] = re.compile(
    r""""""(?mx)
    ^
    (?P<file>.*?):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RESULTS_RE: re.Pattern[str] = re.compile(,"

# CMakeLists.txt:901: Lines should be <= 80 characters long [linelength]
RESULTS_RE: re.Pattern[str] = re.compile(
    r""""""(?mx)
    ^
    (?P<file>.*?):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"version_match = re.compile(r""lintrunner (\d+)\.(\d+)\.(\d+)"").match(version_str)","
    import re

    version_match = re.compile(r""lintrunner (\d+)\.(\d+)\.(\d+)"").match(version_str)

    if not version_match:
        err_msg = LintMessage(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"SHA256_REGEX = re.compile(r""\s*sha256\s*=\s*['\""](?P<sha256>[a-zA-Z0-9]{64})['\""]\s*,"")","

LINTER_CODE = ""BAZEL_LINTER""
SHA256_REGEX = re.compile(r""\s*sha256\s*=\s*['\""](?P<sha256>[a-zA-Z0-9]{64})['\""]\s*,"")
DOMAINS_WITH_UNSTABLE_CHECKSUM = {""github.com""}

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RESULTS_RE: re.Pattern[str] = re.compile(,"# stdin:3:6: T484 Name 'foo' is not defined
# stdin:3:-100: W605 invalid escape sequence '\/'
# stdin:3:1: E302 expected 2 blank lines, found 1
RESULTS_RE: re.Pattern[str] = re.compile(
    r""""""(?mx)
    ^
    (?P<file>.*?):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"symbols_regex = re.compile(""|"".join(sorted(symbols.keys(), reverse=True)))","    # find_matched_symbols. For example, we want Float8_e5m2fnuz to match
    # before Float8_e5m2. Otherwise, both Float8_e5m2fnuz and Float8_e5m2 will
    # match Float8_e5m2
    symbols_regex = re.compile(""|"".join(sorted(symbols.keys(), reverse=True)))
    matched_symbols = find_matched_symbols(symbols_regex, test_globs)

    for s, lineno in symbols.items():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
r = yaml.load(contents),"    yaml.width = 1000  # type: ignore[assignment]
    yaml.boolean_representation = [""False"", ""True""]  # type: ignore[attr-defined]
    try:
        r = yaml.load(contents)
    except Exception as err:
        msg = LintMessage(
            path=None,",yaml.load,Unsafe YAML,HIGH,CWE-502,Python,ML/AI,1,ML/AI:PyTorch Core
RESULTS_RE: re.Pattern[str] = re.compile(,"

# c10/core/DispatchKey.cpp:281:26: error: 'k' used after it was moved [bugprone-use-after-move]
RESULTS_RE: re.Pattern[str] = re.compile(
    r""""""(?mx)
    ^
    (?P<file>.*?):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
CMAKE_MINIMUM_REQUIRED_PATTERN = re.compile(,"    )


CMAKE_MINIMUM_REQUIRED_PATTERN = re.compile(
    r""cmake_minimum_required\(VERSION\s+(?P<version>\d+\.\d+(\.\d+)?)\b.*\)"",
    flags=re.IGNORECASE,
)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
RESULTS_RE: re.Pattern[str] = re.compile(,"

# tools/linter/flake8_linter.py:15:13: error: Incompatibl...int"")  [assignment]
RESULTS_RE: re.Pattern[str] = re.compile(
    r""""""(?mx)
    ^
    (?P<file>.*?):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
INTERNAL_ERROR_RE: re.Pattern[str] = re.compile(,")

# torch/_dynamo/variables/tensor.py:363: error: INTERNAL ERROR
INTERNAL_ERROR_RE: re.Pattern[str] = re.compile(
    r""""""(?mx)
    ^
    (?P<file>.*?):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""compiled_program = torch.compile(fuzzed_program, fullgraph=True, dynamic=True)"",","            f""args = {args_tuple}"",
            ""result_original = fuzzed_program(*args)"",
            ""print(' eager success')"",
            ""compiled_program = torch.compile(fuzzed_program, fullgraph=True, dynamic=True)"",
            ""result_compiled = compiled_program(*args)"",
            ""print(' compile success')"",
        ]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"""compiled_program = torch.compile(fuzzed_program, fullgraph=True, dynamic=True)"",","            ""out_eager = fuzzed_program(*args)"",
            ""out_eager.sum().backward()"",
            ""print('Eager Success! ')"",
            ""compiled_program = torch.compile(fuzzed_program, fullgraph=True, dynamic=True)"",
            ""out_compiled = compiled_program(*args)"",
            ""out_compiled.sum().backward()"",
            ""print('Compile Success! ')"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
re.compile(,"
# List of regex patterns for ignore bucket
IGNORE_PATTERNS: list[re.Pattern] = [
    re.compile(
        r""Dynamo failed to run FX node with fake tensors: call_method fill_diagonal_""
    ),  # https://github.com/pytorch/pytorch/issues/163420
    re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
re.compile(,"    re.compile(
        r""Dynamo failed to run FX node with fake tensors: call_method fill_diagonal_""
    ),  # https://github.com/pytorch/pytorch/issues/163420
    re.compile(
        r""TypeError: unsupported operand type\(s\) for divmod\(\): 'SymInt' and 'int'""
    ),  # https://github.com/pytorch/pytorch/issues/163457
    re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
re.compile(,"    re.compile(
        r""TypeError: unsupported operand type\(s\) for divmod\(\): 'SymInt' and 'int'""
    ),  # https://github.com/pytorch/pytorch/issues/163457
    re.compile(
        r""RuntimeError: self\.stride\(-1\) must be 1 to view ComplexDouble as""
    ),  # https://github.com/pytorch/pytorch/issues/162561
    re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
re.compile(,"    re.compile(
        r""RuntimeError: self\.stride\(-1\) must be 1 to view ComplexDouble as""
    ),  # https://github.com/pytorch/pytorch/issues/162561
    re.compile(
        r""BooleanAtom not allowed in this context""
    ),  # https://github.com/pytorch/pytorch/issues/160726
    re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
re.compile(,"    re.compile(
        r""BooleanAtom not allowed in this context""
    ),  # https://github.com/pytorch/pytorch/issues/160726
    re.compile(
        r""TypeError\(\""unsupported operand type\(s\) for \*: 'SymBool' and 'FakeTensor'\""\)""
    ),  # https://github.com/pytorch/pytorch/issues/164684
    re.compile(r""KeyError: u\d+""),  # https://github.com/pytorch/pytorch/issues/164685",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"re.compile(r""KeyError: u\d+""),  # https://github.com/pytorch/pytorch/issues/164685","    re.compile(
        r""TypeError\(\""unsupported operand type\(s\) for \*: 'SymBool' and 'FakeTensor'\""\)""
    ),  # https://github.com/pytorch/pytorch/issues/164684
    re.compile(r""KeyError: u\d+""),  # https://github.com/pytorch/pytorch/issues/164685
    re.compile(
        r""torch\._inductor\.exc\.InductorError: CppCompileError: C\+\+ compile error""
    ),  # https://github.com/pytorch/pytorch/issues/164686",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
re.compile(,"        r""TypeError\(\""unsupported operand type\(s\) for \*: 'SymBool' and 'FakeTensor'\""\)""
    ),  # https://github.com/pytorch/pytorch/issues/164684
    re.compile(r""KeyError: u\d+""),  # https://github.com/pytorch/pytorch/issues/164685
    re.compile(
        r""torch\._inductor\.exc\.InductorError: CppCompileError: C\+\+ compile error""
    ),  # https://github.com/pytorch/pytorch/issues/164686
    # Add more patterns here as needed, e.g.:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"# re.compile(r""Some other error message""),","        r""torch\._inductor\.exc\.InductorError: CppCompileError: C\+\+ compile error""
    ),  # https://github.com/pytorch/pytorch/issues/164686
    # Add more patterns here as needed, e.g.:
    # re.compile(r""Some other error message""),
]

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:PyTorch Core
"self.model = AutoModelForCausalLM.from_pretrained(config.model_id, **model_init_kwargs).eval()","        model_init_kwargs = self.get_model_init_kwargs(config)
        model_init_kwargs.update({""generation_config"": gen_config})

        self.model = AutoModelForCausalLM.from_pretrained(config.model_id, **model_init_kwargs).eval()

        # Move model to target device
        self.logger.info(f""Moving model to device: {target_device}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)","
        # Perform torch.compile
        if config.compile_mode is not None:
            self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)
        else:
            self.compiled_model = torch.compile(model, **config.compile_options)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.compiled_model = torch.compile(model, **config.compile_options)","        if config.compile_mode is not None:
            self.compiled_model = torch.compile(model, mode=config.compile_mode, **config.compile_options)
        else:
            self.compiled_model = torch.compile(model, **config.compile_options)

        # Setup static cache for compiled mode if needed
        if config.use_cache and hasattr(self, ""inputs"") and self.inputs is not None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_args = re.compile(r""^\s*(Args?|Arguments?|Attributes?|Params?|Parameters?):\s*$"")","
OPTIONAL_KEYWORD = ""*optional*""
# Re pattern that catches args blocks in docstrings (with all variation around the name supported).
_re_args = re.compile(r""^\s*(Args?|Arguments?|Attributes?|Params?|Parameters?):\s*$"")
# Re pattern that parses the start of an arg block: catches <name> (<description>) in those lines.
_re_parse_arg = re.compile(r""^(\s*)(\S+)\s+\((.+)\)(?:\:|$)"")
# Re pattern that parses the end of a description of an arg (catches the default in *optional*, defaults to xxx).",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_parse_arg = re.compile(r""^(\s*)(\S+)\s+\((.+)\)(?:\:|$)"")","# Re pattern that catches args blocks in docstrings (with all variation around the name supported).
_re_args = re.compile(r""^\s*(Args?|Arguments?|Attributes?|Params?|Parameters?):\s*$"")
# Re pattern that parses the start of an arg block: catches <name> (<description>) in those lines.
_re_parse_arg = re.compile(r""^(\s*)(\S+)\s+\((.+)\)(?:\:|$)"")
# Re pattern that parses the end of a description of an arg (catches the default in *optional*, defaults to xxx).
_re_parse_description = re.compile(r""\*optional\*, defaults to (.*)$"")
# Args that are always overridden in the docstring, for clarity we don't want to remove them from the docstring",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_parse_description = re.compile(r""\*optional\*, defaults to (.*)$"")","# Re pattern that parses the start of an arg block: catches <name> (<description>) in those lines.
_re_parse_arg = re.compile(r""^(\s*)(\S+)\s+\((.+)\)(?:\:|$)"")
# Re pattern that parses the end of a description of an arg (catches the default in *optional*, defaults to xxx).
_re_parse_description = re.compile(r""\*optional\*, defaults to (.*)$"")
# Args that are always overridden in the docstring, for clarity we don't want to remove them from the docstring
ALWAYS_OVERRIDE = [""labels""]
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
test_module = importlib.import_module(test_module_path),"    """"""Get the module of a model test file.""""""
    test_module_path = get_module_path(test_file)
    try:
        test_module = importlib.import_module(test_module_path)
    except AttributeError as exc:
        # e.g. if you have a `tests` folder in `site-packages`, created by another package, when trying to import
        # `tests.models...`",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pattern = re.compile(r""((?:^\s*@.*?\n)*?)^\s*def\s+(test_[A-Za-z0-9_]+)\b"", re.MULTILINE)","    status and skip reason, e.g. {'test_forward': ('SKIPPED', 'too slow')}.
    """"""
    result: dict[str, tuple[str, str]] = {}
    pattern = re.compile(r""((?:^\s*@.*?\n)*?)^\s*def\s+(test_[A-Za-z0-9_]+)\b"", re.MULTILINE)
    for decorators_block, test_name in pattern.findall(file_content):
        if ""skip"" in decorators_block:
            result[test_name] = (""SKIPPED"", _extract_reason_from_decorators(decorators_block))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_pt_models = re.compile(r""(.*)(?:Model|Encoder|Decoder|ForConditionalGeneration|ForRetrieval)"")","

# Regexes that match model names
_re_pt_models = re.compile(r""(.*)(?:Model|Encoder|Decoder|ForConditionalGeneration|ForRetrieval)"")


# Fill this with tuples (pipeline_tag, model_mapping, auto_model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)","        ""https://api.github.com/repos/huggingface/transformers/actions/runners"",
    ]

    output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)
    o = output.stdout.decode(""utf-8"")
    status = json.loads(o)
",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:Transformers
"output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)","        ""https://api.github.com/repos/huggingface/transformers/actions/runners"",
    ]

    output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)
    o = output.stdout.decode(""utf-8"")
    status = json.loads(o)
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:Transformers
"output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)","        ""https://api.github.com/repos/huggingface/transformers/actions/runners"",
    ]

    output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)
    o = output.stdout.decode(""utf-8"")
    status = json.loads(o)
",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:Transformers
"""examples"": (re.compile(r'^check_min_version\(""[^""]+""\)\s*$', re.MULTILINE), 'check_min_version(""VERSION"")\n'),","# This maps a type of file to the pattern to look for when searching where the version is defined, as well as the
# template to follow when replacing it with the new version.
REPLACE_PATTERNS = {
    ""examples"": (re.compile(r'^check_min_version\(""[^""]+""\)\s*$', re.MULTILINE), 'check_min_version(""VERSION"")\n'),
    ""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),
    ""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),
    ""uv_script_release"": (",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),","# template to follow when replacing it with the new version.
REPLACE_PATTERNS = {
    ""examples"": (re.compile(r'^check_min_version\(""[^""]+""\)\s*$', re.MULTILINE), 'check_min_version(""VERSION"")\n'),
    ""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),
    ""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),
    ""uv_script_release"": (
        re.compile(r'^#     ""transformers(\[.+\])?.*$', re.MULTILINE),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),","REPLACE_PATTERNS = {
    ""examples"": (re.compile(r'^check_min_version\(""[^""]+""\)\s*$', re.MULTILINE), 'check_min_version(""VERSION"")\n'),
    ""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),
    ""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),
    ""uv_script_release"": (
        re.compile(r'^#     ""transformers(\[.+\])?.*$', re.MULTILINE),
        r'#     ""transformers\g<1>==VERSION"",',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re.compile(r'^#     ""transformers(\[.+\])?.*$', re.MULTILINE),","    ""init"": (re.compile(r'^__version__\s+=\s+""([^""]+)""\s*$', re.MULTILINE), '__version__ = ""VERSION""\n'),
    ""setup"": (re.compile(r'^(\s*)version\s*=\s*""[^""]+"",', re.MULTILINE), r'\1version=""VERSION"",'),
    ""uv_script_release"": (
        re.compile(r'^#     ""transformers(\[.+\])?.*$', re.MULTILINE),
        r'#     ""transformers\g<1>==VERSION"",',
    ),
    ""uv_script_dev"": (",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re.compile(r'^#     ""transformers(\[.+\])?.*$', re.MULTILINE),","        r'#     ""transformers\g<1>==VERSION"",',
    ),
    ""uv_script_dev"": (
        re.compile(r'^#     ""transformers(\[.+\])?.*$', re.MULTILINE),
        r'#     ""transformers\g<1> @ git+https://github.com/huggingface/transformers.git"",',
    ),
}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_1 = re.compile(r""src/transformers/(models/.*)/modeling_.*\.py"")","    pr_files = [item[""filename""] for item in pr_files if item[""status""] in [""added"", ""modified""]]

    # models or quantizers
    re_1 = re.compile(r""src/transformers/(models/.*)/modeling_.*\.py"")
    re_2 = re.compile(r""src/transformers/(quantizers/quantizer_.*)\.py"")

    # tests for models or quantizers",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_2 = re.compile(r""src/transformers/(quantizers/quantizer_.*)\.py"")","
    # models or quantizers
    re_1 = re.compile(r""src/transformers/(models/.*)/modeling_.*\.py"")
    re_2 = re.compile(r""src/transformers/(quantizers/quantizer_.*)\.py"")

    # tests for models or quantizers
    re_3 = re.compile(r""tests/(models/.*)/test_.*\.py"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_3 = re.compile(r""tests/(models/.*)/test_.*\.py"")","    re_2 = re.compile(r""src/transformers/(quantizers/quantizer_.*)\.py"")

    # tests for models or quantizers
    re_3 = re.compile(r""tests/(models/.*)/test_.*\.py"")
    re_4 = re.compile(r""tests/(quantization/.*)/test_.*\.py"")

    # files in a model directory but not necessary a modeling file",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_4 = re.compile(r""tests/(quantization/.*)/test_.*\.py"")","
    # tests for models or quantizers
    re_3 = re.compile(r""tests/(models/.*)/test_.*\.py"")
    re_4 = re.compile(r""tests/(quantization/.*)/test_.*\.py"")

    # files in a model directory but not necessary a modeling file
    re_5 = re.compile(r""src/transformers/(models/.*)/.*\.py"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_5 = re.compile(r""src/transformers/(models/.*)/.*\.py"")","    re_4 = re.compile(r""tests/(quantization/.*)/test_.*\.py"")

    # files in a model directory but not necessary a modeling file
    re_5 = re.compile(r""src/transformers/(models/.*)/.*\.py"")

    regexes = [re_1, re_2, re_3, re_4, re_5]
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"transformers_module = importlib.import_module(""transformers"")","    parser.add_argument(""--check_lib"", action=""store_true"", help=""Whether to check the build or the actual package."")
    args = parser.parse_args()
    if args.check_lib:
        transformers_module = importlib.import_module(""transformers"")
        transformers_path = Path(transformers_module.__file__).parent
    else:
        transformers_path = Path.cwd() / ""build/lib/transformers""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_single_line_relative_imports = re.compile(r""(?:^|\n)\s*from\s+(\.+\S+)\s+import\s+([^\n]+)(?=\n)"")","# \s*from\s+(\.+\S+)\s+import\s+([^\n]+) -> Line only contains from .xxx import yyy and we catch .xxx and yyy
# (?=\n) -> Look-ahead to a new line. We can't just put \n here or using find_all on this re will only catch every
#           other import.
_re_single_line_relative_imports = re.compile(r""(?:^|\n)\s*from\s+(\.+\S+)\s+import\s+([^\n]+)(?=\n)"")
# (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
# \s*from\s+(\.+\S+)\s+import\s+\(([^\)]+)\) -> Line continues with from .xxx import (yyy) and we catch .xxx and yyy
# yyy will take multiple lines otherwise there wouldn't be parenthesis.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_multi_line_relative_imports = re.compile(r""(?:^|\n)\s*from\s+(\.+\S+)\s+import\s+\(([^\)]+)\)"")","# (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
# \s*from\s+(\.+\S+)\s+import\s+\(([^\)]+)\) -> Line continues with from .xxx import (yyy) and we catch .xxx and yyy
# yyy will take multiple lines otherwise there wouldn't be parenthesis.
_re_multi_line_relative_imports = re.compile(r""(?:^|\n)\s*from\s+(\.+\S+)\s+import\s+\(([^\)]+)\)"")
# (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
# \s*from\s+transformers(\S*)\s+import\s+([^\n]+) -> Line only contains from transformers.xxx import yyy and we catch
#           .xxx and yyy",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_single_line_direct_imports = re.compile(r""(?:^|\n)\s*from\s+transformers(\S*)\s+import\s+([^\n]+)(?=\n)"")","#           .xxx and yyy
# (?=\n) -> Look-ahead to a new line. We can't just put \n here or using find_all on this re will only catch every
#           other import.
_re_single_line_direct_imports = re.compile(r""(?:^|\n)\s*from\s+transformers(\S*)\s+import\s+([^\n]+)(?=\n)"")
# (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
# \s*from\s+transformers(\S*)\s+import\s+\(([^\)]+)\) -> Line continues with from transformers.xxx import (yyy) and we
# catch .xxx and yyy. yyy will take multiple lines otherwise there wouldn't be parenthesis.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_multi_line_direct_imports = re.compile(r""(?:^|\n)\s*from\s+transformers(\S*)\s+import\s+\(([^\)]+)\)"")","# (:?^|\n) -> Non-catching group for the beginning of the doc or a new line.
# \s*from\s+transformers(\S*)\s+import\s+\(([^\)]+)\) -> Line continues with from transformers.xxx import (yyy) and we
# catch .xxx and yyy. yyy will take multiple lines otherwise there wouldn't be parenthesis.
_re_multi_line_direct_imports = re.compile(r""(?:^|\n)\s*from\s+transformers(\S*)\s+import\s+\(([^\)]+)\)"")


def extract_imports(module_fname: str, cache: Optional[dict[str, list[str]]] = None) -> list[str]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pr_number_re = re.compile(r""\(#(\d+)\)$"")","    ci_event = os.environ[""CI_EVENT""]

    # To find the PR number in a commit title, for example, `Add AwesomeFormer model (#99999)`
    pr_number_re = re.compile(r""\(#(\d+)\)$"")

    # Add Commit/PR title with a link for push CI
    ci_title = os.environ.get(""CI_TITLE"", """")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_backend = re.compile(r""is\_([a-z_]*)_available()"")","PATH_TO_TRANSFORMERS = ""src/transformers""

# Matches is_xxx_available()
_re_backend = re.compile(r""is\_([a-z_]*)_available()"")
# Matches from xxx import bla
_re_single_line_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")
# Matches if not is_xxx_available()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_single_line_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")","# Matches is_xxx_available()
_re_backend = re.compile(r""is\_([a-z_]*)_available()"")
# Matches from xxx import bla
_re_single_line_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")
# Matches if not is_xxx_available()
_re_test_backend = re.compile(r""^\s+if\s+not\s+\(?is\_[a-z_]*\_available\(\)"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_test_backend = re.compile(r""^\s+if\s+not\s+\(?is\_[a-z_]*\_available\(\)"")","# Matches from xxx import bla
_re_single_line_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")
# Matches if not is_xxx_available()
_re_test_backend = re.compile(r""^\s+if\s+not\s+\(?is\_[a-z_]*\_available\(\)"")


# Template for the dummy objects.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_checkpoint = re.compile(r""\[(.+?)\]\((https://huggingface\.co/.+?)\)"")","
# Regex pattern used to find the checkpoint mentioned in the docstring of `config_class`.
# For example, `[google-bert/bert-base-uncased](https://huggingface.co/google-bert/bert-base-uncased)`
_re_checkpoint = re.compile(r""\[(.+?)\]\((https://huggingface\.co/.+?)\)"")


CONFIG_CLASSES_TO_IGNORE_FOR_DOCSTRING_CHECKPOINT_CHECK = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_backend = re.compile(r""is\_([a-z_]*)_available()"")","

# Matches is_xxx_available()
_re_backend = re.compile(r""is\_([a-z_]*)_available()"")
# Catches a one-line _import_struct = {xxx}
_re_one_line_import_struct = re.compile(r""^_import_structure\s+=\s+\{([^\}]+)\}"")
# Catches a line with a key-values pattern: ""bla"": [""foo"", ""bar""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_one_line_import_struct = re.compile(r""^_import_structure\s+=\s+\{([^\}]+)\}"")","# Matches is_xxx_available()
_re_backend = re.compile(r""is\_([a-z_]*)_available()"")
# Catches a one-line _import_struct = {xxx}
_re_one_line_import_struct = re.compile(r""^_import_structure\s+=\s+\{([^\}]+)\}"")
# Catches a line with a key-values pattern: ""bla"": [""foo"", ""bar""]
_re_import_struct_key_value = re.compile(r'\s+""\S*"":\s+\[([^\]]*)\]')
# Catches a line if not is_foo_available",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_import_struct_key_value = re.compile(r'\s+""\S*"":\s+\[([^\]]*)\]')","# Catches a one-line _import_struct = {xxx}
_re_one_line_import_struct = re.compile(r""^_import_structure\s+=\s+\{([^\}]+)\}"")
# Catches a line with a key-values pattern: ""bla"": [""foo"", ""bar""]
_re_import_struct_key_value = re.compile(r'\s+""\S*"":\s+\[([^\]]*)\]')
# Catches a line if not is_foo_available
_re_test_backend = re.compile(r""^\s*if\s+not\s+is\_[a-z_]*\_available\(\)"")
# Catches a line _import_struct[""bla""].append(""foo"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_test_backend = re.compile(r""^\s*if\s+not\s+is\_[a-z_]*\_available\(\)"")","# Catches a line with a key-values pattern: ""bla"": [""foo"", ""bar""]
_re_import_struct_key_value = re.compile(r'\s+""\S*"":\s+\[([^\]]*)\]')
# Catches a line if not is_foo_available
_re_test_backend = re.compile(r""^\s*if\s+not\s+is\_[a-z_]*\_available\(\)"")
# Catches a line _import_struct[""bla""].append(""foo"")
_re_import_struct_add_one = re.compile(r'^\s*_import_structure\[""\S*""\]\.append\(""(\S*)""\)')
# Catches a line _import_struct[""bla""].extend([""foo"", ""bar""]) or _import_struct[""bla""] = [""foo"", ""bar""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_import_struct_add_one = re.compile(r'^\s*_import_structure\[""\S*""\]\.append\(""(\S*)""\)')","# Catches a line if not is_foo_available
_re_test_backend = re.compile(r""^\s*if\s+not\s+is\_[a-z_]*\_available\(\)"")
# Catches a line _import_struct[""bla""].append(""foo"")
_re_import_struct_add_one = re.compile(r'^\s*_import_structure\[""\S*""\]\.append\(""(\S*)""\)')
# Catches a line _import_struct[""bla""].extend([""foo"", ""bar""]) or _import_struct[""bla""] = [""foo"", ""bar""]
_re_import_struct_add_many = re.compile(r""^\s*_import_structure\[\S*\](?:\.extend\(|\s*=\s+)\[([^\]]*)\]"")
# Catches a line with an object between quotes and a comma:     ""MyModel"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_import_struct_add_many = re.compile(r""^\s*_import_structure\[\S*\](?:\.extend\(|\s*=\s+)\[([^\]]*)\]"")","# Catches a line _import_struct[""bla""].append(""foo"")
_re_import_struct_add_one = re.compile(r'^\s*_import_structure\[""\S*""\]\.append\(""(\S*)""\)')
# Catches a line _import_struct[""bla""].extend([""foo"", ""bar""]) or _import_struct[""bla""] = [""foo"", ""bar""]
_re_import_struct_add_many = re.compile(r""^\s*_import_structure\[\S*\](?:\.extend\(|\s*=\s+)\[([^\]]*)\]"")
# Catches a line with an object between quotes and a comma:     ""MyModel"",
_re_quote_object = re.compile(r'^\s+""([^""]+)"",')
# Catches a line with objects between brackets only:    [""foo"", ""bar""],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_quote_object = re.compile(r'^\s+""([^""]+)"",')","# Catches a line _import_struct[""bla""].extend([""foo"", ""bar""]) or _import_struct[""bla""] = [""foo"", ""bar""]
_re_import_struct_add_many = re.compile(r""^\s*_import_structure\[\S*\](?:\.extend\(|\s*=\s+)\[([^\]]*)\]"")
# Catches a line with an object between quotes and a comma:     ""MyModel"",
_re_quote_object = re.compile(r'^\s+""([^""]+)"",')
# Catches a line with objects between brackets only:    [""foo"", ""bar""],
_re_between_brackets = re.compile(r""^\s+\[([^\]]+)\]"")
# Catches a line with from foo import bar, bla, boo",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_between_brackets = re.compile(r""^\s+\[([^\]]+)\]"")","# Catches a line with an object between quotes and a comma:     ""MyModel"",
_re_quote_object = re.compile(r'^\s+""([^""]+)"",')
# Catches a line with objects between brackets only:    [""foo"", ""bar""],
_re_between_brackets = re.compile(r""^\s+\[([^\]]+)\]"")
# Catches a line with from foo import bar, bla, boo
_re_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")
# Catches a line with try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")","# Catches a line with objects between brackets only:    [""foo"", ""bar""],
_re_between_brackets = re.compile(r""^\s+\[([^\]]+)\]"")
# Catches a line with from foo import bar, bla, boo
_re_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")
# Catches a line with try:
_re_try = re.compile(r""^\s*try:"")
# Catches a line with else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_try = re.compile(r""^\s*try:"")","# Catches a line with from foo import bar, bla, boo
_re_import = re.compile(r""\s+from\s+\S*\s+import\s+([^\(\s].*)\n"")
# Catches a line with try:
_re_try = re.compile(r""^\s*try:"")
# Catches a line with else:
_re_else = re.compile(r""^\s*else:"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_else = re.compile(r""^\s*else:"")","# Catches a line with try:
_re_try = re.compile(r""^\s*try:"")
# Catches a line with else:
_re_else = re.compile(r""^\s*else:"")


def find_backend(line: str) -> Optional[str]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"r = re.compile(r""\s(is_\S+?_available\(\))\s"")","    # Extract `is_xxx_available()` from existing blocks: some models require specific libraries like `timm` and use
    # `is_timm_available()` instead of `is_torch_available()`.
    # Keep leading and trailing whitespaces
    r = re.compile(r""\s(is_\S+?_available\(\))\s"")
    for line in class_lines[start_idx : end_idx + 1]:
        backend_condition = r.search(line)
        if backend_condition is not None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.model = AutoModel.from_pretrained(EMBEDDING_MODEL, torch_dtype=""auto"", device_map=""auto"").eval()","        self.models_root = MODELS_ROOT
        self.hub_dataset = hub_dataset
        self.tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL)
        self.model = AutoModel.from_pretrained(EMBEDDING_MODEL, torch_dtype=""auto"", device_map=""auto"").eval()

        self.device = self.model.device
        self.index_dir: Path | None = None",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_RELEASE_RE = re.compile(,"        return output


_RELEASE_RE = re.compile(
    r""(?:^|[\*_`\s>])(?:this|the)\s+model\s+was\s+released\s+on\s+(\d{4}-\d{2}-\d{2})\b"", re.IGNORECASE
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"compiled_regex = re.compile(f""(?<![a-z0-9])({regex_pattern})(.|$)"", re.IGNORECASE | re.DOTALL)","def preserve_case_replace(text, patterns: dict, default_name: str):
    # Create a regex pattern to match all variations
    regex_pattern = ""|"".join(re.escape(key) for key in patterns)
    compiled_regex = re.compile(f""(?<![a-z0-9])({regex_pattern})(.|$)"", re.IGNORECASE | re.DOTALL)

    def replace(match):
        matched_pattern = match.group(1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_intro_mapping = re.compile(r""[A-Z_]+_MAPPING(\s+|_[A-Z_]+\s+)=\s+OrderedDict"")","
# re pattern that matches mapping introductions:
#    SUPER_MODEL_MAPPING_NAMES = OrderedDict or SUPER_MODEL_MAPPING = OrderedDict
_re_intro_mapping = re.compile(r""[A-Z_]+_MAPPING(\s+|_[A-Z_]+\s+)=\s+OrderedDict"")
# re pattern that matches identifiers in mappings
_re_identifier = re.compile(r'\s*\(\s*""(\S[^""]+)""')
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_identifier = re.compile(r'\s*\(\s*""(\S[^""]+)""')","#    SUPER_MODEL_MAPPING_NAMES = OrderedDict or SUPER_MODEL_MAPPING = OrderedDict
_re_intro_mapping = re.compile(r""[A-Z_]+_MAPPING(\s+|_[A-Z_]+\s+)=\s+OrderedDict"")
# re pattern that matches identifiers in mappings
_re_identifier = re.compile(r'\s*\(\s*""(\S[^""]+)""')


def sort_auto_mapping(fname: str, overwrite: bool = False) -> Optional[bool]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_copy_warning = re.compile(r""^(\s*)#\s*Copied from\s+transformers\.(\S+\.\S+)\s*($|\S.*$)"")","    return lines, code, code_splits


_re_copy_warning = re.compile(r""^(\s*)#\s*Copied from\s+transformers\.(\S+\.\S+)\s*($|\S.*$)"")
_re_copy_warning_for_test_file = re.compile(r""^(\s*)#\s*Copied from\s+tests\.(\S+\.\S+)\s*($|\S.*$)"")
_re_replace_pattern = re.compile(r""^\s*(\S+)->(\S+)(\s+.*|$)"")
_re_fill_pattern = re.compile(r""<FILL\s+[^>]*>"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_copy_warning_for_test_file = re.compile(r""^(\s*)#\s*Copied from\s+tests\.(\S+\.\S+)\s*($|\S.*$)"")","

_re_copy_warning = re.compile(r""^(\s*)#\s*Copied from\s+transformers\.(\S+\.\S+)\s*($|\S.*$)"")
_re_copy_warning_for_test_file = re.compile(r""^(\s*)#\s*Copied from\s+tests\.(\S+\.\S+)\s*($|\S.*$)"")
_re_replace_pattern = re.compile(r""^\s*(\S+)->(\S+)(\s+.*|$)"")
_re_fill_pattern = re.compile(r""<FILL\s+[^>]*>"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_replace_pattern = re.compile(r""^\s*(\S+)->(\S+)(\s+.*|$)"")","
_re_copy_warning = re.compile(r""^(\s*)#\s*Copied from\s+transformers\.(\S+\.\S+)\s*($|\S.*$)"")
_re_copy_warning_for_test_file = re.compile(r""^(\s*)#\s*Copied from\s+tests\.(\S+\.\S+)\s*($|\S.*$)"")
_re_replace_pattern = re.compile(r""^\s*(\S+)->(\S+)(\s+.*|$)"")
_re_fill_pattern = re.compile(r""<FILL\s+[^>]*>"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_fill_pattern = re.compile(r""<FILL\s+[^>]*>"")","_re_copy_warning = re.compile(r""^(\s*)#\s*Copied from\s+transformers\.(\S+\.\S+)\s*($|\S.*$)"")
_re_copy_warning_for_test_file = re.compile(r""^(\s*)#\s*Copied from\s+tests\.(\S+\.\S+)\s*($|\S.*$)"")
_re_replace_pattern = re.compile(r""^\s*(\S+)->(\S+)(\s+.*|$)"")
_re_fill_pattern = re.compile(r""<FILL\s+[^>]*>"")


def get_indent(code: str) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_class_match = re.compile(r""class\s+([^\(:]+)(?:\(|:)"")","    theoretical_code_header = theoretical_code.split(""\n"")[0]

    # Catch the function/class name: it is expected that those do not match.
    _re_class_match = re.compile(r""class\s+([^\(:]+)(?:\(|:)"")
    _re_func_match = re.compile(r""def\s+([^\(]+)\("")
    for re_pattern in [_re_class_match, _re_func_match]:
        if re_pattern.match(observed_code_header) is not None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_func_match = re.compile(r""def\s+([^\(]+)\("")","
    # Catch the function/class name: it is expected that those do not match.
    _re_class_match = re.compile(r""class\s+([^\(:]+)(?:\(|:)"")
    _re_func_match = re.compile(r""def\s+([^\(]+)\("")
    for re_pattern in [_re_class_match, _re_func_match]:
        if re_pattern.match(observed_code_header) is not None:
            try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_re_capture_meta = re.compile(,"    # This regex captures metadata from an English model description, including model title, model link,
    # affiliations of the paper, title of the paper, authors of the paper, and supplemental data (see DistilBERT for
    # example).
    _re_capture_meta = re.compile(
        r""\*\*\[([^\]]*)\]\(([^\)]*)\)\*\* \(from ([^)]*)\)[^\[]*([^\)]*\)).*?by (.*?[A-Za-z\*]{2,}?)\. (.*)$""
    )
    # This regex is used to synchronize title link.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_capture_title_link = re.compile(r""\*\*\[([^\]]*)\]\(([^\)]*)\)\*\*"")","        r""\*\*\[([^\]]*)\]\(([^\)]*)\)\*\* \(from ([^)]*)\)[^\[]*([^\)]*\)).*?by (.*?[A-Za-z\*]{2,}?)\. (.*)$""
    )
    # This regex is used to synchronize title link.
    _re_capture_title_link = re.compile(r""\*\*\[([^\]]*)\]\(([^\)]*)\)\*\*"")
    # This regex is used to synchronize paper title and link.
    _re_capture_paper_link = re.compile(r"" \[([^\]]*)\]\(([^\)]*)\)"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_capture_paper_link = re.compile(r"" \[([^\]]*)\]\(([^\)]*)\)"")","    # This regex is used to synchronize title link.
    _re_capture_title_link = re.compile(r""\*\*\[([^\]]*)\]\(([^\)]*)\)\*\*"")
    # This regex is used to synchronize paper title and link.
    _re_capture_paper_link = re.compile(r"" \[([^\]]*)\]\(([^\)]*)\)"")

    if len(localized_model_list) == 0:
        localized_model_index = {}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"reg = re.compile(r""src/transformers/models/(.*)/modeling_.*\.py"")","
def get_new_model(diff_with_last_commit=False):
    new_files = get_new_python_files(diff_with_last_commit)
    reg = re.compile(r""src/transformers/models/(.*)/modeling_.*\.py"")

    new_model = """"
    for x in new_files:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_indent = re.compile(r""^(\s*)\S"")","PATH_TO_TRANSFORMERS = ""src/transformers""

# Pattern that looks at the indentation in a line.
_re_indent = re.compile(r""^(\s*)\S"")
# Pattern that matches `""key"":"" and puts `key` in group 0.
_re_direct_key = re.compile(r'^\s*""([^""]+)"":')
# Pattern that matches `_import_structure[""key""]` and puts `key` in group 0.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_direct_key = re.compile(r'^\s*""([^""]+)"":')","# Pattern that looks at the indentation in a line.
_re_indent = re.compile(r""^(\s*)\S"")
# Pattern that matches `""key"":"" and puts `key` in group 0.
_re_direct_key = re.compile(r'^\s*""([^""]+)"":')
# Pattern that matches `_import_structure[""key""]` and puts `key` in group 0.
_re_indirect_key = re.compile(r'^\s*_import_structure\[""([^""]+)""\]')
# Pattern that matches `""key"",` and puts `key` in group 0.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_indirect_key = re.compile(r'^\s*_import_structure\[""([^""]+)""\]')","# Pattern that matches `""key"":"" and puts `key` in group 0.
_re_direct_key = re.compile(r'^\s*""([^""]+)"":')
# Pattern that matches `_import_structure[""key""]` and puts `key` in group 0.
_re_indirect_key = re.compile(r'^\s*_import_structure\[""([^""]+)""\]')
# Pattern that matches `""key"",` and puts `key` in group 0.
_re_strip_line = re.compile(r'^\s*""([^""]+)"",\s*$')
# Pattern that matches any `[stuff]` and puts `stuff` in group 0.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_strip_line = re.compile(r'^\s*""([^""]+)"",\s*$')","# Pattern that matches `_import_structure[""key""]` and puts `key` in group 0.
_re_indirect_key = re.compile(r'^\s*_import_structure\[""([^""]+)""\]')
# Pattern that matches `""key"",` and puts `key` in group 0.
_re_strip_line = re.compile(r'^\s*""([^""]+)"",\s*$')
# Pattern that matches any `[stuff]` and puts `stuff` in group 0.
_re_bracket_content = re.compile(r""\[([^\]]+)\]"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_bracket_content = re.compile(r""\[([^\]]+)\]"")","# Pattern that matches `""key"",` and puts `key` in group 0.
_re_strip_line = re.compile(r'^\s*""([^""]+)"",\s*$')
# Pattern that matches any `[stuff]` and puts `stuff` in group 0.
_re_bracket_content = re.compile(r""\[([^\]]+)\]"")


def get_indent(line: str) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_decorator = re.compile(r""^\s*@(\S+)\s+$"")","        raise Exception(f""There were {len(failures)} failures:\n"" + ""\n"".join(failures))


_re_decorator = re.compile(r""^\s*@(\S+)\s+$"")


def check_decorator_order(filename: str) -> list[int]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"`os.path.join(PATH_TO_TESTS, ...)` to access the files.","    Returns:
        `List[str]`: The list of test files. The returned files will NOT contain the `tests` (i.e. `PATH_TO_TESTS`
        defined in this script). They will be considered as paths relative to `tests`. A caller has to use
        `os.path.join(PATH_TO_TESTS, ...)` to access the files.
    """"""

    _ignore_files = [",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:Transformers
"regex = re.compile(rf""^({joined_dirs}).*?\.py$"")",")

joined_dirs = ""|"".join(sys.argv[1:])
regex = re.compile(rf""^({joined_dirs}).*?\.py$"")

relevant_modified_files = [x for x in modified_files if regex.match(x)]
print("" "".join(relevant_modified_files), end="""")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_tokenizer_file = re.compile(r""tokenizer\.(.*)\.json"")","
# Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file
FULL_TOKENIZER_FILE = ""tokenizer.json""
_re_tokenizer_file = re.compile(r""tokenizer\.(.*)\.json"")


class TruncationStrategy(ExplicitEnum):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_EXAMPLE_RE = re.compile(r''',"    # indentation of the first (PS1) line of the source code; and
    # `want` is the expected output (including leading indentation).
    # fmt: off
    _EXAMPLE_RE = re.compile(r'''
        # Source consists of a PS1 line followed by zero or more PS2 lines.
        (?P<source>
            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_ = importlib.import_module(backend),"    if ""TRANSFORMERS_TEST_BACKEND"" in os.environ:
        backend = os.environ[""TRANSFORMERS_TEST_BACKEND""]
        try:
            _ = importlib.import_module(backend)
        except ModuleNotFoundError as e:
            raise ModuleNotFoundError(
                f""Failed to import `TRANSFORMERS_TEST_BACKEND` '{backend}'! This should be the name of an installed module. The original error (look up to see its""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"mymodule = importlib.import_module(""mymodule"")","
    ```python
    with ExtendSysPath(""/path/to/dir""):
        mymodule = importlib.import_module(""mymodule"")
    ```
    """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
device_spec_module = importlib.import_module(import_name),"        except ValueError as e:
            raise ValueError(f""Provided device spec file was not a Python file! Received '{device_spec_path}"") from e

        device_spec_module = importlib.import_module(import_name)

        # Imported file must contain `DEVICE_NAME`. If it doesn't, terminate early.
        try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_checkpoint = re.compile(r""^"" + PREFIX_CHECKPOINT_DIR + r""\-(\d+)$"")","

PREFIX_CHECKPOINT_DIR = ""checkpoint""
_re_checkpoint = re.compile(r""^"" + PREFIX_CHECKPOINT_DIR + r""\-(\d+)$"")


def get_last_checkpoint(folder):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"tokenizer.enable_padding(pad_token=""<pad>"", pad_id=3)","            ]
        )
        tokenizer.add_tokens([AddedToken(""\n"", normalized=False, special=False)])
        tokenizer.enable_padding(pad_token=""<pad>"", pad_id=3)
        return tokenizer

    def vocab(self, proto):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"If compiling the prefill as well, e.g. calling `model.compile(...)` before `generate` with a static cache,","        If this is unwanted, one can call `early_initialization(...)` on the Cache directly, which will call this
        function ahead-of-time (this is required for `torch.export` for example). Note that for `compile`, as we
        internally don't compile the prefill, this is guaranteed to have been called already when compiling.
        If compiling the prefill as well, e.g. calling `model.compile(...)` before `generate` with a static cache,
        it is still supported in general, but without guarantees depending on the compilation options (e.g. cuda graphs,
        i.e. `mode=""reduce-overhead""` is known to fail). But it will in general work correctly, and prefill should
        not be compiled anyway for performances!",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
Static Cache class to be used with `torch.compile(model)` and `torch.export()`. It will check the `config`,"
class StaticCache(Cache):
    """"""
    Static Cache class to be used with `torch.compile(model)` and `torch.export()`. It will check the `config`
    for potential hybrid cache structure, and initialize each layer accordingly.

    See `Cache` for details on common methods that are implemented by all cache classes.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
importlib.import_module(imp),"    missing_packages = []
    for imp in imports:
        try:
            importlib.import_module(imp)
        except ImportError as exception:
            logger.warning(f""Encountered exception while importing {imp}: {exception}"")
            # Some packages can fail with an ImportError because of a dependency issue.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
return torch.load(,"        # mmap can only be used with files serialized with zipfile-based format.
        if isinstance(checkpoint_file, str) and map_location != ""meta"" and is_zipfile(checkpoint_file):
            extra_args = {""mmap"": True}
        return torch.load(
            checkpoint_file,
            map_location=map_location,
            weights_only=weights_only,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        r""""""
        Instantiate a pretrained pytorch model from a pre-trained model configuration.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you should first set it back in training mode with `model.train()`.

        The warning *Weights from XXX not initialized from pretrained model* means that the weights of XXX do not come",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"        model.tie_weights()

        # Set model in evaluation mode to deactivate DropOut modules by default
        model.eval()

        # check if using kernels
        if use_kernels:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
def eval(self):,"            self.kernelize()
        return out

    def eval(self):
        return self.train(False)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"reg = re.compile(r""(.*?)-\d{5}-of-\d{5}"")","
            # make sure that file to be deleted matches format of sharded file, e.g. pytorch_model-00001-of-00005
            filename_no_suffix = filename.replace("".bin"", """").replace("".safetensors"", """")
            reg = re.compile(r""(.*?)-\d{5}-of-\d{5}"")

            if (
                filename.startswith(weights_no_suffix)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"keep_in_fp32_regex = re.compile(""|"".join([rf""((^|\.){module}($|\.))"" for module in keep_in_fp32_modules]))","        keep_in_fp32_regex = None
        if keep_in_fp32_modules:
            # We need to match exact layers, so we add either `.` on each side, or start/end of string
            keep_in_fp32_regex = re.compile(""|"".join([rf""((^|\.){module}($|\.))"" for module in keep_in_fp32_modules]))

        if hf_quantizer is not None:
            hf_quantizer.preprocess_model(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self._compiled_call = torch.compile(self.__call__, **compile_config.to_dict())","            or getattr(self, ""_last_compile_config"", default_config) != compile_config
        ):
            self._last_compile_config = compile_config
            self._compiled_call = torch.compile(self.__call__, **compile_config.to_dict())
        return self._compiled_call

    @classmethod",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"ignore_missing_regex = re.compile(""|"".join(rf""({pattern})"" for pattern in missing_patterns))","        unexpected_patterns = (self._keys_to_ignore_on_load_unexpected or []) + additional_unexpected_patterns
        ignore_missing_regex, ignore_unexpected_regex = None, None
        if len(missing_patterns) > 0:
            ignore_missing_regex = re.compile(""|"".join(rf""({pattern})"" for pattern in missing_patterns))
        if len(unexpected_patterns) > 0:
            ignore_unexpected_regex = re.compile(""|"".join(rf""({pattern})"" for pattern in unexpected_patterns))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"ignore_unexpected_regex = re.compile(""|"".join(rf""({pattern})"" for pattern in unexpected_patterns))","        if len(missing_patterns) > 0:
            ignore_missing_regex = re.compile(""|"".join(rf""({pattern})"" for pattern in missing_patterns))
        if len(unexpected_patterns) > 0:
            ignore_unexpected_regex = re.compile(""|"".join(rf""({pattern})"" for pattern in unexpected_patterns))

        # Clean-up missing keys
        if ignore_missing_regex is not None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re.compile(""|"".join([re.escape(plan) for plan in tp_plan]))","
    tp_plan = getattr(model, ""_tp_plan"", []) or []
    tp_plan_regex = (
        re.compile(""|"".join([re.escape(plan) for plan in tp_plan]))
        if _torch_distributed_available and torch.distributed.is_initialized()
        else None
    )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"MEMORY_ADDRESS_REGEX = re.compile(r""object at 0x[0-9A-Fa-f]+"")","    return torch.distributed.get_rank() == 0


MEMORY_ADDRESS_REGEX = re.compile(r""object at 0x[0-9A-Fa-f]+"")


def _sanitize_repr_for_diff(x_str: str) -> str:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"LAYER_SUFFIX_RE = re.compile(r""(.*)\.(\d+)$"")  # should be generic enough, ends with a number","            prune_outputs_if_children(child)


LAYER_SUFFIX_RE = re.compile(r""(.*)\.(\d+)$"")  # should be generic enough, ends with a number


def is_layer_block(node):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
# with torch.load(weights_only=True). Starting from 2.6 weights_only=True becomes,"
def safe_globals():
    # Starting from version 2.4 PyTorch introduces a check for the objects loaded
    # with torch.load(weights_only=True). Starting from 2.6 weights_only=True becomes
    # a default and requires allowlisting of objects being loaded.
    # See: https://github.com/pytorch/pytorch/pull/137602
    # See: https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(weights_file, map_location=""cpu"", weights_only=True)","                    state_dict = safetensors.torch.load_file(safe_weights_file, device=""cpu"")
                else:
                    check_torch_load_is_safe()
                    state_dict = torch.load(weights_file, map_location=""cpu"", weights_only=True)

                # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963
                # which takes *args instead of **kwargs",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(best_model_path, map_location=""cpu"", weights_only=True)","                        state_dict = safetensors.torch.load_file(best_safe_model_path, device=""cpu"")
                    else:
                        check_torch_load_is_safe()
                        state_dict = torch.load(best_model_path, map_location=""cpu"", weights_only=True)

                    # If the model is on the GPU, it still works!
                    # workaround for FSDP bug https://github.com/pytorch/pytorch/issues/82963",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
jit_model.eval(),"            example_batch = self._prepare_inputs(example_batch)
            try:
                jit_model = copy.copy(model)
                jit_model.eval()
                original_forward = jit_model.__dict__.pop(""_original_forward"", None)
                # remove mixed precision hooks from the model
                if original_forward:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"        logger.info(f""  Batch size = {batch_size}"")

        if hasattr(model, ""eval"") and callable(model.eval):
            model.eval()
        if hasattr(self.optimizer, ""eval"") and callable(self.optimizer.eval):
            self.optimizer.eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.optimizer.eval(),"        if hasattr(model, ""eval"") and callable(model.eval):
            model.eval()
        if hasattr(self.optimizer, ""eval"") and callable(self.optimizer.eval):
            self.optimizer.eval()

        self.callback_handler.eval_dataloader = dataloader
        # Do this before wrapping.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"""You cannot fine-tune quantized model with `torch.compile()` make sure to pass a non-compiled model when fine-tuning a quantized model with PEFT""","        # Filter out quantized + compiled models
        if _is_quantized_and_base_model and hasattr(model, ""_orig_mod""):
            raise ValueError(
                ""You cannot fine-tune quantized model with `torch.compile()` make sure to pass a non-compiled model when fine-tuning a quantized model with PEFT""
            )

        # At this stage the model is already loaded",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
[re.compile(pattern) for pattern in forbidden_layer_names] if forbidden_layer_names is not None else [],"    Returns the names of the model parameters that are not inside a forbidden layer.
    """"""
    forbidden_layer_patterns = (
        [re.compile(pattern) for pattern in forbidden_layer_names] if forbidden_layer_names is not None else []
    )
    result = []
    for name, child in model.named_children():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"model = torch.compile(model, mode=""max-autotune"")","            from torchao import autoquant
            from torchao.quantization import ALL_AUTOQUANT_CLASS_LIST

            model = torch.compile(model, mode=""max-autotune"")
            model = autoquant(
                model,
                qtensor_class_list=ALL_AUTOQUANT_CLASS_LIST,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"IMAGE_TOKEN = ""<image>""","
logger = logging.get_logger(__name__)

IMAGE_TOKEN = ""<image>""


class ReturnType(enum.Enum):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"transformers_module = importlib.import_module(""transformers"")","        if config.architectures:
            classes = []
            for architecture in config.architectures:
                transformers_module = importlib.import_module(""transformers"")
                _class = getattr(transformers_module, architecture, None)
                if _class is not None:
                    classes.append(_class)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"words, mask, img_token=""<img>"", sliding_window=None, token_type_ids=None, image_seq_length=None","

def generate_attention_matrix_from_mask(
    words, mask, img_token=""<img>"", sliding_window=None, token_type_ids=None, image_seq_length=None
):
    """"""
    Generates an attention matrix from a given attention mask.",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<img>""","class AttentionMaskVisualizer:
    def __init__(self, model_name: str):
        config = AutoConfig.from_pretrained(model_name)
        self.image_token = ""<img>""
        if hasattr(config.get_text_config(), ""sliding_window""):
            self.sliding_window = getattr(config.get_text_config(), ""sliding_window"", None)
        try:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
Compatible with torch.compile (Dynamo tracing).,"def check_model_inputs(tie_last_hidden_states=True):
    """"""
    Decorator to intercept specific layer outputs without using hooks.
    Compatible with torch.compile (Dynamo tracing).

    Args:
        tie_last_hidden_states (`bool`, *optional*, defaults to `True`):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"description_re = re.compile(r""^(.*?)[\n\s]*(Args:|Returns:|Raises:|\Z)"", re.DOTALL)","
BASIC_TYPES = (int, float, str, bool, Any, type(None), ...)
# Extracts the initial segment of the docstring, containing the function description
description_re = re.compile(r""^(.*?)[\n\s]*(Args:|Returns:|Raises:|\Z)"", re.DOTALL)
# Extracts the Args: block from the docstring
args_re = re.compile(r""\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)"", re.DOTALL)
# Splits the Args: block into individual arguments",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"args_re = re.compile(r""\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)"", re.DOTALL)","# Extracts the initial segment of the docstring, containing the function description
description_re = re.compile(r""^(.*?)[\n\s]*(Args:|Returns:|Raises:|\Z)"", re.DOTALL)
# Extracts the Args: block from the docstring
args_re = re.compile(r""\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)"", re.DOTALL)
# Splits the Args: block into individual arguments
args_split_re = re.compile(
    r""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
args_split_re = re.compile(,"# Extracts the Args: block from the docstring
args_re = re.compile(r""\n\s*Args:\n\s*(.*?)[\n\s]*(Returns:|Raises:|\Z)"", re.DOTALL)
# Splits the Args: block into individual arguments
args_split_re = re.compile(
    r""""""
(?:^|\n)  # Match the start of the args block, or a newline
\s*(\w+):\s*  # Capture the argument name and strip spacing",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"returns_re = re.compile(r""\n\s*Returns:\n\s*(.*?)[\n\s]*(Raises:|\Z)"", re.DOTALL)","    re.DOTALL | re.VERBOSE,
)
# Extracts the Returns: block from the docstring, if present. Note that most chat templates ignore the return type/doc!
returns_re = re.compile(r""\n\s*Returns:\n\s*(.*?)[\n\s]*(Raises:|\Z)"", re.DOTALL)


class TypeHintParsingException(Exception):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_re_checkpoint = re.compile(r""\[(.+?)\]\((https://huggingface\.co/.+?)\)"")","    ""esmfold"": ""EsmConfig"",
}

_re_checkpoint = re.compile(r""\[(.+?)\]\((https://huggingface\.co/.+?)\)"")


class ImageProcessorArgs:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"shape_pattern = re.compile(r""(of shape\s*(?:`.*?`|\(.*?\)))"")","

def parse_shape(docstring):
    shape_pattern = re.compile(r""(of shape\s*(?:`.*?`|\(.*?\)))"")
    match = shape_pattern.search(docstring)
    if match:
        return "" "" + match.group(1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"default_pattern = re.compile(r""(defaults to \s*[^)]*)"")","

def parse_default(docstring):
    default_pattern = re.compile(r""(defaults to \s*[^)]*)"")
    match = default_pattern.search(docstring)
    if match:
        return "" "" + match.group(1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"args_pattern = re.compile(r""(?:Args:)(\n.*)?(\n)?$"", re.DOTALL)","        docstring = docstring[: match.start()]
    else:
        remainder_docstring = """"
    args_pattern = re.compile(r""(?:Args:)(\n.*)?(\n)?$"", re.DOTALL)

    args_match = args_pattern.search(docstring)
    # still try to find args description in the docstring, if args are not preceded by ""Args:""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
param_pattern = re.compile(,"    args_section = set_min_indent(args_section, 0)
    params = {}
    if args_section:
        param_pattern = re.compile(
            # |--- Group 1 ---|| Group 2 ||- Group 3 -||---------- Group 4 ----------|
            rf""^\s{{0,{max_indent_level}}}(\w+)\s*\(\s*([^, \)]*)(\s*.*?)\s*\)\s*:\s*((?:(?!\n^\s{{0,{max_indent_level}}}\w+\s*\().)*)"",
            re.DOTALL | re.MULTILINE,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
package = importlib.import_module(pkg_name),"        except importlib.metadata.PackageNotFoundError:
            # If we cannot find the metadata (because of editable install for example), try to import directly.
            # Note that this branch will almost never be run, so we do not import packages for nothing here
            package = importlib.import_module(pkg_name)
            package_version = getattr(package, ""__version__"", ""N/A"")
        logger.debug(f""Detected {pkg_name} version: {package_version}"")
    if return_version:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"return importlib.import_module(""."" + module_name, self.__name__)","
    def _get_module(self, module_name: str):
        try:
            return importlib.import_module(""."" + module_name, self.__name__)
        except Exception as e:
            raise e
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.eval(),"                logger.warning(err_msg)

        if peft_config.inference_mode:
            self.eval()

        # Re-dispatch model and hooks in case the model is offloaded to CPU / Disk.
        if (",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.vision_encoder.eval(),"
    def export_vision_encoder(self):
        """"""Export the vision encoder component.""""""
        self.vision_encoder.eval()

        # Create example input
        pixel_values = torch.randn(1, 3, 384, 384, dtype=torch.float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.connector.eval(),"
    def export_connector(self):
        """"""Export the connector component.""""""
        self.connector.eval()

        # Vision encoder output shape: [batch_size, num_patches, vision_hidden_size]
        vision_hidden_size = self.config.vision_config.hidden_size",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
wrapped_encoder = Seq2SeqLMEncoderExportableModule(self.encoder).to(self.full_model.device).eval(),"        self.exported_decoder = None

    def _export_encoder(self, encoder_input_ids):
        wrapped_encoder = Seq2SeqLMEncoderExportableModule(self.encoder).to(self.full_model.device).eval()

        # Define dynamic sequence length for encoder
        seq_len_dim = torch.export.Dim(""encoder_seq_length"", max=self.max_hidden_seq_length)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
.eval(),"                batch_size=self.generation_config.cache_config.get(""batch_size""),
            )
            .to(target_device)
            .eval()
        )

        # Move input tensors to the same device as the wrapped decoder",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
# Remove leading ../ from a relative path.,"        if self._volatile_checkpoints_dir is not None:
            consistent_checkpoint_path = os.path.join(self._volatile_checkpoints_dir, checkpoint)
            try:
                # Remove leading ../ from a relative path.
                cpkt_path = relative_path.replace("".."", """").lstrip(os.path.sep)
                copy_path = os.path.join(consistent_checkpoint_path, cpkt_path)
                shutil.copytree(relative_path, copy_path)",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:Transformers
""" only (`model.eval()`) or turn off the attention dropout in the respective config.""","    if kwargs.get(""dropout"", 0.0) > 0:
        raise ValueError(
            ""`flex_attention` does not support `dropout`. Please use it with inference""
            "" only (`model.eval()`) or turn off the attention dropout in the respective config.""
        )

    block_mask = None",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)","        if not self._is_flex_compiled or training != self.training:
            self.training = training
            if is_torch_less_or_equal(""2.5.1""):
                self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)
            # In PyTorch 2.6.0, there's a known issue with flex attention compilation which may
            # cause errors. The suggested fix is to compile with ""max-autotune-no-cudagraphs""
            # see https://github.com/pytorch/pytorch/issues/146260 for training",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self._compiled_flex_attention = torch.compile(,"            # cause errors. The suggested fix is to compile with ""max-autotune-no-cudagraphs""
            # see https://github.com/pytorch/pytorch/issues/146260 for training
            elif version.parse(get_torch_version()).base_version == ""2.6.0"" and training:
                self._compiled_flex_attention = torch.compile(
                    flex_attention, dynamic=False, mode=""max-autotune-no-cudagraphs""
                )
            # Fallback, usually the most recent torch 2.7.x+ versions",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self._compiled_flex_attention = torch.compile(flex_attention),"                )
            # Fallback, usually the most recent torch 2.7.x+ versions
            else:
                self._compiled_flex_attention = torch.compile(flex_attention)

            self._is_flex_compiled = True
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
block_regex = re.compile(,"        # we have an init file in the updated format
        # get the indented block after if TYPE_CHECKING: and before else:, append the new import, sort the imports and write the updated content
        # Step 1: Find the block
        block_regex = re.compile(
            r""if TYPE_CHECKING:\n(?P<if_block>.*?)(?=\s*else:)"",
            re.DOTALL,
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"regex = re.compile(pattern, re.VERBOSE | re.DOTALL)","                \((.*?)\)                         # 2. Multi-line imports (e.g., '(a, ... b)')
            )
        """"""
        regex = re.compile(pattern, re.VERBOSE | re.DOTALL)

        def replacement_function(match):
            # Extract existing imports",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"regex = re.compile(pattern, re.VERBOSE | re.DOTALL)","            \((.*?)\)                         # 2. Multi-line imports (e.g., '(a, ... b)')
        )
    """"""
    regex = re.compile(pattern, re.VERBOSE | re.DOTALL)

    def replacement_function(match):
        # Extract existing imports",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.model = model.eval(),"            max_queue_size: Maximum size of the request queue (0 = unlimited)
            streaming: Whether to stream tokens as they are generated
        """"""
        self.model = model.eval()
        if ""paged|"" not in model.config._attn_implementation:
            from ...integrations.hub_kernels import load_and_register_kernel
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"regex = re.compile(r""\b(a|an|the)\b"", re.UNICODE)","    """"""Lower text and remove punctuation, articles and extra whitespace.""""""

    def remove_articles(text):
        regex = re.compile(r""\b(a|an|the)\b"", re.UNICODE)
        return re.sub(regex, "" "", text)

    def white_space_fix(text):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.examples = pickle.load(handle),"            if os.path.exists(cached_features_file) and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, ""rb"") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f""Loading features from cached file {cached_features_file} [took %.3f s]"", time.time() - start
                )",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.examples = pickle.load(handle),"            if os.path.exists(cached_features_file) and not overwrite_cache:
                start = time.time()
                with open(cached_features_file, ""rb"") as handle:
                    self.examples = pickle.load(handle)
                logger.info(
                    f""Loading features from cached file {cached_features_file} [took %.3f s]"", time.time() - start
                )",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # Create model using the factory
    model, _, preprocess = create_model_and_transforms(model_name_with_class, pretrained=checkpoint_path, device=""cpu"")
    model.eval()
    return model, preprocess

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"shared_weights = torch.load(switch_checkpoint_path + ""-shared.pt"", weights_only=True)[""model""]","    save_path = os.path.join(
        dump_path, weights_name.replace("".bin"", f""-{len(sharded_state_dicts) + 1:05d}-of-???.bin"")
    )
    shared_weights = torch.load(switch_checkpoint_path + ""-shared.pt"", weights_only=True)[""model""]
    remove_ignore_keys_(shared_weights)
    shared_weights = rename_fairseq_keys(shared_weights, None)
    shared_weights[""shared.weight""] = shared_weights[""decoder.embed_tokens.weight""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"expert_state = torch.load(expert_path, weights_only=True)[""model""]","    for expert in range(num_experts):
        expert_path = switch_checkpoint_path + f""-rank-{expert}.pt""
        if os.path.isfile(expert_path):
            expert_state = torch.load(expert_path, weights_only=True)[""model""]
            remove_ignore_keys_(expert_state)
            expert_state = rename_fairseq_keys(expert_state, expert)
            save_path = os.path.join(",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
conv_layers = eval(fs_config.conv_feature_layers),"        fs_config = model.cfg

    config.conv_bias = fs_config.conv_bias
    conv_layers = eval(fs_config.conv_feature_layers)
    config.conv_dim = [x[0] for x in conv_layers]
    config.conv_kernel = [x[1] for x in conv_layers]
    config.conv_stride = [x[2] for x in conv_layers]",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
conv_layers = eval(fs_config.conv_feature_layers),"        fs_config = model.cfg

    config.conv_bias = fs_config.conv_bias
    conv_layers = eval(fs_config.conv_feature_layers)
    config.conv_dim = [x[0] for x in conv_layers]
    config.conv_kernel = [x[1] for x in conv_layers]
    config.conv_stride = [x[2] for x in conv_layers]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"        config = SEWDConfig.from_pretrained(config_path)
    else:
        config = convert_config(model[0], is_finetuned)
    model = model[0].eval()

    return_attention_mask = config.feat_extract_norm == ""layer""
    feature_extractor = Wav2Vec2FeatureExtractor(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","                pad_token=target_dict.pad_word,
                bos_token=target_dict.bos_word,
                eos_token=target_dict.eos_word,
                word_delimiter_token=""|"",
                do_lower_case=False,
            )
            processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]","def convert_edgetam_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub, run_sanity_check):
    config = get_config(model_name)

    state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]
    state_dict = replace_keys(state_dict)

    image_processor = Sam2ImageProcessorFast()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    video_processor = Sam2VideoVideoProcessor()
    processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)
    hf_model = EdgeTamVideoModel(config)
    hf_model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    )
    model = TableTransformerForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    # verify our conversion
    filename = ""example_pdf.png"" if ""detection"" in checkpoint_url else ""example_table.png""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    image_processor = DetrImageProcessor(format=""coco_detection"", size={""longest_edge"": 800})
    model = TableTransformerForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    # verify our conversion
    filename = ""example_pdf.png"" if ""detection"" in checkpoint_url else ""example_table.png""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)","    if not os.path.exists(ckpt_path):
        logger.info(f""{model_type} model not found, downloading into `{CACHE_DIR}`."")
        _download(model_info[""repo_id""], model_info[""file_name""])
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)
    # this is a hack
    model_args = checkpoint[""model_args""]
    if ""input_vocab_size"" not in model_args:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)","    if not os.path.exists(ckpt_path):
        logger.info(f""{model_type} model not found, downloading into `{CACHE_DIR}`."")
        _download(model_info[""repo_id""], model_info[""file_name""])
    checkpoint = torch.load(ckpt_path, map_location=device, weights_only=True)
    # this is a hack
    model_args = checkpoint[""model_args""]
    if ""input_vocab_size"" not in model_args:",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    n_params = model.num_parameters(exclude_embeddings=True)
    val_loss = checkpoint[""best_val_loss""].item()
    logger.info(f""model loaded: {round(n_params / 1e6, 1)}M params, {round(val_loss, 3)} loss"")
    model.eval()
    model.to(device)
    del checkpoint, state_dict
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = ConvNextV2ForImageClassification(config)
    model.load_state_dict(state_dict)
    model.eval()

    # Check outputs on an image, prepared by ConvNextImageProcessor
    preprocessor = convert_preprocessor(checkpoint_url)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"fake_image_token=""<|image|>"",","        tokenizer=None,
        patch_size: int = 14,
        pixel_shuffle_ratio: float = 0.5,
        fake_image_token=""<|image|>"",
        image_token=""<|image|>"",
        start_of_image_token=""<|image_start|>"",
        end_of_image_token=""<|image_end|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<|image|>"",","        patch_size: int = 14,
        pixel_shuffle_ratio: float = 0.5,
        fake_image_token=""<|image|>"",
        image_token=""<|image|>"",
        start_of_image_token=""<|image_start|>"",
        end_of_image_token=""<|image_end|>"",
        patch_token=""<|patch|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"start_of_image_token=""<|image_start|>"",","        pixel_shuffle_ratio: float = 0.5,
        fake_image_token=""<|image|>"",
        image_token=""<|image|>"",
        start_of_image_token=""<|image_start|>"",
        end_of_image_token=""<|image_end|>"",
        patch_token=""<|patch|>"",
        tile_x_separator_token=""<|tile_x_separator|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"end_of_image_token=""<|image_end|>"",","        fake_image_token=""<|image|>"",
        image_token=""<|image|>"",
        start_of_image_token=""<|image_start|>"",
        end_of_image_token=""<|image_end|>"",
        patch_token=""<|patch|>"",
        tile_x_separator_token=""<|tile_x_separator|>"",
        tile_y_separator_token=""<|tile_y_separator|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"patch_token=""<|patch|>"",","        image_token=""<|image|>"",
        start_of_image_token=""<|image_start|>"",
        end_of_image_token=""<|image_end|>"",
        patch_token=""<|patch|>"",
        tile_x_separator_token=""<|tile_x_separator|>"",
        tile_y_separator_token=""<|tile_y_separator|>"",
        chat_template=chat_template,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tile_x_separator_token=""<|tile_x_separator|>"",","        start_of_image_token=""<|image_start|>"",
        end_of_image_token=""<|image_end|>"",
        patch_token=""<|patch|>"",
        tile_x_separator_token=""<|tile_x_separator|>"",
        tile_y_separator_token=""<|tile_y_separator|>"",
        chat_template=chat_template,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tile_y_separator_token=""<|tile_y_separator|>"",","        end_of_image_token=""<|image_end|>"",
        patch_token=""<|patch|>"",
        tile_x_separator_token=""<|tile_x_separator|>"",
        tile_y_separator_token=""<|tile_y_separator|>"",
        chat_template=chat_template,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|begin_of_text|>"",","        pattern=O200K_PATTERN,
        special_tokens=special_tokens,
        chat_template=chat_template if args.instruct else None,
        bos_token=""<|begin_of_text|>"",
        eos_token=""<|end_of_text|>"" if not args.instruct else ""<|eot|>"",
        pad_token=""<|finetune_right_pad_id|>"",
        model_max_length=max_context_length(args.input_dir, args.instruct),",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|end_of_text|>"" if not args.instruct else ""<|eot|>"",","        special_tokens=special_tokens,
        chat_template=chat_template if args.instruct else None,
        bos_token=""<|begin_of_text|>"",
        eos_token=""<|end_of_text|>"" if not args.instruct else ""<|eot|>"",
        pad_token=""<|finetune_right_pad_id|>"",
        model_max_length=max_context_length(args.input_dir, args.instruct),
    )",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|finetune_right_pad_id|>"",","        chat_template=chat_template if args.instruct else None,
        bos_token=""<|begin_of_text|>"",
        eos_token=""<|end_of_text|>"" if not args.instruct else ""<|eot|>"",
        pad_token=""<|finetune_right_pad_id|>"",
        model_max_length=max_context_length(args.input_dir, args.instruct),
    )
    tokenizer = converter.converted_tokenizer",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"original_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""state_dict""]","    )

    print(""Converting model..."")
    original_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""state_dict""]
    all_keys = list(original_state_dict.keys())
    new_keys = convert_old_keys_to_new_keys(all_keys)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)[""img""]","    pixel_values = image_processor(images=image, boxes=boxes, return_tensors=""pt"").pixel_values

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""vitpose_batch_data.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)[""img""]
    # we allow for a small difference in the pixel values due to the original repository using cv2
    assert torch.allclose(pixel_values, original_pixel_values, atol=1e-1)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)[""img""]","    pixel_values = image_processor(images=image, boxes=boxes, return_tensors=""pt"").pixel_values

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""vitpose_batch_data.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)[""img""]
    # we allow for a small difference in the pixel values due to the original repository using cv2
    assert torch.allclose(pixel_values, original_pixel_values, atol=1e-1)
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    print(""Loading the checkpoint in a Vitpose model."")
    model = VitPoseForPoseEstimation(config)
    model.eval()
    model.load_state_dict(state_dict)
    print(""Checkpoint loaded successfully."")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, ""cpu"")","
    # Load original checkpoint
    print(f""Loading checkpoint from {checkpoint_path}..."")
    state_dict = torch.load(checkpoint_path, ""cpu"")

    # Flatten nested dictionary
    print(""Flattening nested dictionary..."")",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = MLCDVisionModel(config).eval(),"
    # load HuggingFace model
    print(""Loading HuggingFace model..."")
    model = MLCDVisionModel(config).eval()
    model.load_state_dict(new_state_dict)

    # Create processor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor, image_token=""[IMG]"")","    convert_mistral_model(args.input_dir, args.output_dir)
    tokenizer = convert_mistral_tokenizer(args.tokenizer_file)
    image_processor = PixtralImageProcessor()
    processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor, image_token=""[IMG]"")
    if args.chat_template_file:
        processor.chat_template = open(args.chat_template_file).read()
    processor.save_pretrained(args.output_dir)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token=""[IMG]"",  # set the default and let users change if they have peculiar special tokens in rare cases","        patch_size: int = 16,
        spatial_merge_size: int = 1,
        chat_template=None,
        image_token=""[IMG]"",  # set the default and let users change if they have peculiar special tokens in rare cases
        image_break_token=""[IMG_BREAK]"",
        image_end_token=""[IMG_END]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_break_token=""[IMG_BREAK]"",","        spatial_merge_size: int = 1,
        chat_template=None,
        image_token=""[IMG]"",  # set the default and let users change if they have peculiar special tokens in rare cases
        image_break_token=""[IMG_BREAK]"",
        image_end_token=""[IMG_END]"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_end_token=""[IMG_END]"",","        chat_template=None,
        image_token=""[IMG]"",  # set the default and let users change if they have peculiar special tokens in rare cases
        image_break_token=""[IMG_BREAK]"",
        image_end_token=""[IMG_END]"",
        **kwargs,
    ):
        self.patch_size = patch_size",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)","    logger.info(""Converting model..."")

    # load original state dict
    state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)

    # rename keys
    state_dict = rename_keys(state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # create HuggingFace model and load state dict
    model = GLPNForDepthEstimation(config)
    model.load_state_dict(state_dict)
    model.eval()

    # forward pass
    outputs = model(pixel_values)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
metadata = pickle.load(metadata_file),"        return state_dict

    with (Path(model_path) / "".metadata"").open(""rb"") as metadata_file:
        metadata = pickle.load(metadata_file)
        keys = [key for key in metadata.state_dict_metadata.keys() if key.startswith(""model."")]

    # keys = [""model.blocks.0.attention.w_q.weight""]",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"tensor = cast(torch.Tensor, torch.load(bytes, map_location=""cpu"", weights_only=False))","                planner.load_bytes(read_item, bytes)
            else:
                # NOTE: 'weights_only=False' needed to load torchao's float8 linear layer checkpoints
                tensor = cast(torch.Tensor, torch.load(bytes, map_location=""cpu"", weights_only=False))
                tensor = _narrow_tensor_by_index(tensor, read_item.storage_offsets, read_item.lengths)
                target_tensor = planner.resolve_tensor(read_item).detach()
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
metadata = pickle.load(metadata_file),"        if self._metadata is None:
            try:
                with (Path(self.path) / "".metadata"").open(""rb"") as metadata_file:
                    metadata = pickle.load(metadata_file)
            except FileNotFoundError as exc:
                msg = f""'{self.path}' is not a distributed checkpoint folder.""
                suggested_dir = os.path.join(self.path, ""model_and_optim"")",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.self.eval(),"        self.attention_type = value

        if not self.training:
            self.self.eval()

    def forward(
        self,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        merges_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,
    ):
        try:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"chkpt = torch.load(checkpoint_file, map_location=""cpu"", weights_only=True)","    checkpoint_file = os.path.join(biogpt_checkpoint_path, ""checkpoint.pt"")
    if not os.path.isfile(checkpoint_file):
        raise ValueError(f""path to the file {checkpoint_file} does not exist!"")
    chkpt = torch.load(checkpoint_file, map_location=""cpu"", weights_only=True)

    args = chkpt[""cfg""][""model""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","

def load_state_dict(checkpoint_path):
    sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    return sd

",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)","    state_dict_temp = ""pytorch_model-0000{i}-of-00002.bin""
    for shard in range(1, 3):
        state_dict_path = hf_hub_download(old_state_dict_id, state_dict_temp.format(i=shard))
        state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)
        state_dict = convert_state_dict_to_hf(state_dict)
        model.load_state_dict(state_dict, strict=False, assign=True)
        model_state_dict -= set(state_dict.keys())",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)","    state_dict_temp = ""pytorch_model-0000{i}-of-00002.bin""
    for shard in range(1, 3):
        state_dict_path = hf_hub_download(old_state_dict_id, state_dict_temp.format(i=shard))
        state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)
        state_dict = convert_state_dict_to_hf(state_dict)
        model.load_state_dict(state_dict, strict=False, assign=True)
        model_state_dict -= set(state_dict.keys())",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases","        tokenizer=None,
        patch_size=14,
        vision_feature_select_strategy=""default"",
        image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases
        video_token=""<video>"",
        chat_template=None,
        num_additional_image_tokens=1,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"video_token=""<video>"",","        patch_size=14,
        vision_feature_select_strategy=""default"",
        image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases
        video_token=""<video>"",
        chat_template=None,
        num_additional_image_tokens=1,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state),"        if add_prefix_space:
            pre_tok_state = pre_tok_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
            decoder_state = decoder_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
        self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state)
        self.backend_tokenizer.decoder = pickle.loads(decoder_state)

        self.add_prefix_space = add_prefix_space",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.backend_tokenizer.decoder = pickle.loads(decoder_state),"            pre_tok_state = pre_tok_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
            decoder_state = decoder_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
        self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state)
        self.backend_tokenizer.decoder = pickle.loads(decoder_state)

        self.add_prefix_space = add_prefix_space
",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<UNK>"",","        merges_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<UNK>"",
        bos_token=""<BOS_TOKEN>"",
        eos_token=""<|END_OF_TURN_TOKEN|>"",
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<BOS_TOKEN>"",","        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<UNK>"",
        bos_token=""<BOS_TOKEN>"",
        eos_token=""<|END_OF_TURN_TOKEN|>"",
        add_bos_token=True,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|END_OF_TURN_TOKEN|>"",","        clean_up_tokenization_spaces=False,
        unk_token=""<UNK>"",
        bos_token=""<BOS_TOKEN>"",
        eos_token=""<|END_OF_TURN_TOKEN|>"",
        add_bos_token=True,
        add_eos_token=False,
        use_default_system_prompt=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token_box=[1000, 1000, 1000, 1000],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        tokenizer_file=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],
        pad_token_label=-100,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","    def __init__(
        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        sep_token_box=[1000, 1000, 1000, 1000],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],
        pad_token_label=-100,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","
    # load original state dict
    checkpoint_path = name_to_checkpoint_path[model_name]
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    print(""Checkpoint path:"", checkpoint_path)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"input_ids = torch.load(filepath, weights_only=True)","    # autoregressive decoding with original input data
    print(""Testing generation with original inputs..."")
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""input_ids_udop.pt"", repo_type=""dataset"")
    input_ids = torch.load(filepath, weights_only=True)
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""bbox_udop.pt"", repo_type=""dataset"")
    bbox = torch.load(filepath, weights_only=True)
    pixel_values_filename = ""pixel_values_udop_512.pt"" if ""512"" in model_name else ""pixel_values_udop_224.pt""",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bbox = torch.load(filepath, weights_only=True)","    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""input_ids_udop.pt"", repo_type=""dataset"")
    input_ids = torch.load(filepath, weights_only=True)
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""bbox_udop.pt"", repo_type=""dataset"")
    bbox = torch.load(filepath, weights_only=True)
    pixel_values_filename = ""pixel_values_udop_512.pt"" if ""512"" in model_name else ""pixel_values_udop_224.pt""
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=pixel_values_filename, repo_type=""dataset"")
    pixel_values = torch.load(filepath, weights_only=True)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"pixel_values = torch.load(filepath, weights_only=True)","    bbox = torch.load(filepath, weights_only=True)
    pixel_values_filename = ""pixel_values_udop_512.pt"" if ""512"" in model_name else ""pixel_values_udop_224.pt""
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=pixel_values_filename, repo_type=""dataset"")
    pixel_values = torch.load(filepath, weights_only=True)

    print(""Decoded input ids:"", tokenizer.decode(input_ids[0], skip_special_tokens=True))
    print(""Bbox shape:"", bbox.shape)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"input_ids = torch.load(filepath, weights_only=True)","    # autoregressive decoding with original input data
    print(""Testing generation with original inputs..."")
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""input_ids_udop.pt"", repo_type=""dataset"")
    input_ids = torch.load(filepath, weights_only=True)
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""bbox_udop.pt"", repo_type=""dataset"")
    bbox = torch.load(filepath, weights_only=True)
    pixel_values_filename = ""pixel_values_udop_512.pt"" if ""512"" in model_name else ""pixel_values_udop_224.pt""",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bbox = torch.load(filepath, weights_only=True)","    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""input_ids_udop.pt"", repo_type=""dataset"")
    input_ids = torch.load(filepath, weights_only=True)
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""bbox_udop.pt"", repo_type=""dataset"")
    bbox = torch.load(filepath, weights_only=True)
    pixel_values_filename = ""pixel_values_udop_512.pt"" if ""512"" in model_name else ""pixel_values_udop_224.pt""
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=pixel_values_filename, repo_type=""dataset"")
    pixel_values = torch.load(filepath, weights_only=True)",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"pixel_values = torch.load(filepath, weights_only=True)","    bbox = torch.load(filepath, weights_only=True)
    pixel_values_filename = ""pixel_values_udop_512.pt"" if ""512"" in model_name else ""pixel_values_udop_224.pt""
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=pixel_values_filename, repo_type=""dataset"")
    pixel_values = torch.load(filepath, weights_only=True)

    print(""Decoded input ids:"", tokenizer.decode(input_ids[0], skip_special_tokens=True))
    print(""Bbox shape:"", bbox.shape)",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    image_size = 512 if ""512"" in model_name else 224
    config = UdopConfig(decoder_start_token_id=0, image_size=image_size)
    model = UdopForConditionalGeneration(config)
    model.eval()

    # rename keys
    state_dict = {k.replace(""cell2dembedding"", ""cell_2d_embedding""): v for k, v in state_dict.items()}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model = Blip2ForImageTextRetrieval(config).eval(),"    config, image_size = get_blip2_config(model_name, eos_token_id=eos_token_id)

    if ""itm"" in model_name:
        hf_model = Blip2ForImageTextRetrieval(config).eval()
    else:
        hf_model = Blip2ForConditionalGeneration(config).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model = Blip2ForConditionalGeneration(config).eval(),"    if ""itm"" in model_name:
        hf_model = Blip2ForImageTextRetrieval(config).eval()
    else:
        hf_model = Blip2ForConditionalGeneration(config).eval()

    model_name_to_original = {
        ""blip2-opt-2.7b"": (""blip2_opt"", ""pretrain_opt2.7b""),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"    original_model, vis_processors, _ = load_model_and_preprocess(
        name=name, model_type=type, is_eval=True, device=lavis_device
    )
    original_model.eval()
    print(""Done!"")

    # update state dict keys",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    codebook_state_dict = convert_dalle_checkpoint(codebook_path, None, save_checkpoint=False)

    if os.path.exists(checkpoint_path):
        state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    else:
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=""cpu"")
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    codebook_state_dict = convert_dalle_checkpoint(codebook_path, None, save_checkpoint=False)

    if os.path.exists(checkpoint_path):
        state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    else:
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_path, map_location=""cpu"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model = FlavaForPreTraining(config).eval(),"    else:
        config = FlavaConfig()

    hf_model = FlavaForPreTraining(config).eval()

    codebook_state_dict = convert_dalle_checkpoint(codebook_path, None, save_checkpoint=False)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"ckpt = torch.load(checkpoint_path, weights_only=True)","
    encoder = Encoder()
    if os.path.exists(checkpoint_path):
        ckpt = torch.load(checkpoint_path, weights_only=True)
    else:
        ckpt = torch.hub.load_state_dict_from_url(checkpoint_path)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"ckpt = torch.load(checkpoint_path, weights_only=True)","
    encoder = Encoder()
    if os.path.exists(checkpoint_path):
        ckpt = torch.load(checkpoint_path, weights_only=True)
    else:
        ckpt = torch.hub.load_state_dict_from_url(checkpoint_path)
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model = FlavaImageCodebook(config).eval(),"    else:
        config = FlavaImageCodebookConfig()

    hf_model = FlavaImageCodebook(config).eval()
    state_dict = encoder.state_dict()

    hf_state_dict = upgrade_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        do_lower_case=True,
        remove_space=True,
        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        remove_space=True,
        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it and",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        do_lower_case=True,
        remove_space=True,
        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        remove_space=True,
        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"files = torch.load(output, map_location=""cpu"", weights_only=True)","    # download original checkpoint, hosted on Google Drive
    output = ""pytorch_model.bin""
    gdown.cached_download(checkpoint_url, output, quiet=False)
    files = torch.load(output, map_location=""cpu"", weights_only=True)
    if ""model"" in files:
        state_dict = files[""model""]
    else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"files = torch.load(output, map_location=""cpu"", weights_only=True)","    # download original checkpoint, hosted on Google Drive
    output = ""pytorch_model.bin""
    gdown.cached_download(checkpoint_url, output, quiet=False)
    files = torch.load(output, map_location=""cpu"", weights_only=True)
    if ""model"" in files:
        state_dict = files[""model""]
    else:",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"inputs[""bool_masked_pos""] = torch.load(local_path, weights_only=True)","
    if ""finetuned"" not in model_name:
        local_path = hf_hub_download(repo_id=""hf-internal-testing/bool-masked-pos"", filename=""bool_masked_pos.pt"")
        inputs[""bool_masked_pos""] = torch.load(local_path, weights_only=True)

    outputs = model(**inputs)
    logits = outputs.logits",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_state_dict(state_dict, config)

    model.load_state_dict(new_state_dict)
    model.eval()

    # verify model on basic input
    image_processor = VideoMAEImageProcessor(image_mean=[0.5, 0.5, 0.5], image_std=[0.5, 0.5, 0.5])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|startoftext|>"",","            kwargs[""chat_template""] = chat_template
        self.tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token=""<|startoftext|>"",
            eos_token=""<|return|>"" if chat_template else ""<|endoftext|>"",
            pad_token=""<|endoftext|>"",
            model_input_names=[""input_ids"", ""attention_mask""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|return|>"" if chat_template else ""<|endoftext|>"",","        self.tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token=""<|startoftext|>"",
            eos_token=""<|return|>"" if chat_template else ""<|endoftext|>"",
            pad_token=""<|endoftext|>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            model_max_length=model_max_length,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|endoftext|>"",","            tokenizer_object=tokenizer,
            bos_token=""<|startoftext|>"",
            eos_token=""<|return|>"" if chat_template else ""<|endoftext|>"",
            pad_token=""<|endoftext|>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            model_max_length=model_max_length,
            **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"orig_checkpoint = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)","
    model.decoder.apply_weight_norm()

    orig_checkpoint = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)
    recursively_load_weights(orig_checkpoint[""model""], model)

    model.decoder.remove_weight_norm()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"non_roman_pattern = re.compile(r""[^\x00-\x7F]"")","
def has_non_roman_characters(input_string):
    # Find any character outside the ASCII range
    non_roman_pattern = re.compile(r""[^\x00-\x7F]"")

    # Search the input string for non-Roman characters
    match = non_roman_pattern.search(input_string)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","    def __init__(
        self,
        vocab_file,
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        language=None,
        add_blank=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        language=None,
        add_blank=True,
        normalize=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"        ""hidden_states"": OlmoeDecoderLayer,
        ""attentions"": OlmoeAttention,
    }
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(os.path.join(input_base_path, ""model.pt""), map_location=""cpu"", weights_only=True)","    print(f""Fetching all parameters from the checkpoint at {input_base_path}."")

    # Not sharded
    loaded = torch.load(os.path.join(input_base_path, ""model.pt""), map_location=""cpu"", weights_only=True)

    param_count = 0
    index_dict = {""weight_map"": {}}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"        ""hidden_states"": OlmoeDecoderLayer,
        ""attentions"": OlmoeAttention,
    }
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        spm_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        spm_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        do_upper_case=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        spm_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        do_upper_case=False,
        do_lower_case=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        do_upper_case=False,
        do_lower_case=False,
        tgt_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"m2m_100 = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","

def convert_fairseq_s2t_checkpoint_to_tfms(checkpoint_path, pytorch_dump_folder_path):
    m2m_100 = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    args = m2m_100[""args""]
    state_dict = m2m_100[""model""]
    lm_head_weights = state_dict[""decoder.output_projection.weight""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<bos>"",","        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<eos>"",","        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        add_bos_token=True,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<bos>"",","        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<eos>"",","        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        add_bos_token=True,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<bos>"",","        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<eos>"",","        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        add_bos_token=True,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<bos>"",
        eos_token=""<eos>"",
        pad_token=""<pad>"",
        add_bos_token=True,
        add_eos_token=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"model_state_dict = torch.load(input_base_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]","    head_dim = config.head_dim

    print(f""Fetching all parameters from the checkpoint at '{input_base_path}'"")
    model_state_dict = torch.load(input_base_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]
    model_state_dict.pop(""freqs_cis"")

    state_dict = {}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = TimeSeriesTransformerModel.from_pretrained(""huggingface/time-series-transformer-tourism-monthly"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = TimeSeriesTransformerForPrediction.from_pretrained(
        ...     ""huggingface/time-series-transformer-tourism-monthly""",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"loaded = [torch.load(path, map_location=""cpu"", mmap=True, weights_only=True)]","            path = os.path.join(input_base_path, ""consolidated.00.pth"")
        else:
            path = os.path.join(input_base_path, ""consolidated.pth"")
        loaded = [torch.load(path, map_location=""cpu"", mmap=True, weights_only=True)]
    else:
        loaded = [
            torch.load(",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"        loaded = [torch.load(path, map_location=""cpu"", mmap=True, weights_only=True)]
    else:
        loaded = [
            torch.load(
                os.path.join(input_base_path, f""consolidated.{i:02d}.pth""),
                map_location=""cpu"",
                mmap=True,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|begin_of_text|>"",","        special_tokens=special_tokens,
        model_max_length=model_max_length,
        chat_template=chat_template if instruct else None,
        bos_token=""<|begin_of_text|>"",
        eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",
        pad_token=""<|finetune_right_pad_id|>"",
    )",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",","        model_max_length=model_max_length,
        chat_template=chat_template if instruct else None,
        bos_token=""<|begin_of_text|>"",
        eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",
        pad_token=""<|finetune_right_pad_id|>"",
    )
    tokenizer = converter.tokenizer",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|finetune_right_pad_id|>"",","        chat_template=chat_template if instruct else None,
        bos_token=""<|begin_of_text|>"",
        eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",
        pad_token=""<|finetune_right_pad_id|>"",
    )
    tokenizer = converter.tokenizer
    tokenizer.save_pretrained(save_dir)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image|>""","
    def __init__(self, image_processor, tokenizer, chat_template=None):
        if not hasattr(tokenizer, ""image_token""):
            self.image_token = ""<|image|>""
            self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)
        else:
            self.image_token = tokenizer.image_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.python_token = ""<|python_tag|>""","            self.image_token = tokenizer.image_token
            self.image_token_id = tokenizer.image_token_id

        self.python_token = ""<|python_tag|>""
        self.python_token_id = tokenizer.convert_tokens_to_ids(self.python_token)
        self.bos_token = tokenizer.bos_token
        super().__init__(image_processor, tokenizer, chat_template=chat_template)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # Load HF model
    model = OmDetTurboForObjectDetection(config)
    model.eval()
    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    print(""Missing keys:"", missing_keys)
    print(""Unexpected keys:"", unexpected_keys)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"(re.compile(""\\b%s\\."" % x[0], re.IGNORECASE), x[1])","    def __init__(self):
        # List of (regular expression, replacement) pairs for abbreviations:
        self._abbreviations = [
            (re.compile(""\\b%s\\."" % x[0], re.IGNORECASE), x[1])
            for x in [
                (""mrs"", ""misess""),
                (""mr"", ""mister""),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"return re.sub(re.compile(r""\s+""), "" "", text)","        """"""
        Removes multiple whitespaces
        """"""
        return re.sub(re.compile(r""\s+""), "" "", text)

    def __call__(self, text):
        """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file,
        merges_file,
        errors=""replace"",
        unk_token=""[UNK]"",
        bos_token=""<|endoftext|>"",
        eos_token=""[STOP]"",
        pad_token=""[STOP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        merges_file,
        errors=""replace"",
        unk_token=""[UNK]"",
        bos_token=""<|endoftext|>"",
        eos_token=""[STOP]"",
        pad_token=""[STOP]"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[STOP]"",","        errors=""replace"",
        unk_token=""[UNK]"",
        bos_token=""<|endoftext|>"",
        eos_token=""[STOP]"",
        pad_token=""[STOP]"",
        add_prefix_space=False,
        add_bos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[STOP]"",","        unk_token=""[UNK]"",
        bos_token=""<|endoftext|>"",
        eos_token=""[STOP]"",
        pad_token=""[STOP]"",
        add_prefix_space=False,
        add_bos_token=False,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"clvp_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)","            _download(url=each_model_url, root=each_model_path)

        if each_model_name == ""clvp"":
            clvp_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)
        else:
            decoder_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"decoder_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)","        if each_model_name == ""clvp"":
            clvp_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)
        else:
            decoder_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)

    # Converting the weights
    converted_checkpoint.update(**convert_encoder_weights(clvp_checkpoint))",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"clvp_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)","            _download(url=each_model_url, root=each_model_path)

        if each_model_name == ""clvp"":
            clvp_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)
        else:
            decoder_checkpoint = torch.load(each_model_path, map_location=""cpu"", weights_only=True)
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"m2m_100 = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","

def convert_fairseq_m2m100_checkpoint_from_disk(checkpoint_path):
    m2m_100 = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    args = m2m_100[""args""] or m2m_100[""cfg""][""model""]
    state_dict = m2m_100[""model""]
    remove_ignore_keys_(state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        spm_file,
        src_lang=None,
        tgt_lang=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        src_lang=None,
        tgt_lang=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tgt_lang=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        language_codes=""m2m100"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        language_codes=""m2m100"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        language_codes=""m2m100"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        num_madeup_words=8,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(os.path.join(input_base_path, ""model.pt""), map_location=""cpu"", weights_only=True)","
    # Not sharded
    # (The sharded implementation would also work, but this is simpler.)
    loaded = torch.load(os.path.join(input_base_path, ""model.pt""), map_location=""cpu"", weights_only=True)

    param_count = 0
    index_dict: dict[str, Any] = {""weight_map"": {}}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.eval()  # Emu3's VQ model is frozen,"            config.embed_dim, config.latent_channels, kernel_size=(3, 1, 1), stride=(1, 1, 1)
        )
        self.spatial_scale_factor = 2 ** (len(config.channel_multiplier) - 1)
        self.eval()  # Emu3's VQ model is frozen

        self.post_init()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.eval()  # Emu3's VQ model is frozen,"            config.embed_dim, config.latent_channels, kernel_size=(3, 1, 1), stride=(1, 1, 1)
        )
        self.spatial_scale_factor = 2 ** (len(config.channel_multiplier) - 1)
        self.eval()  # Emu3's VQ model is frozen

        self.post_init()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pad_token = ""<|endoftext|>""","        ]
    )

    pad_token = ""<|endoftext|>""
    pad_token = AddedToken(pad_token, lstrip=False, rstrip=False, normalized=False, single_word=False)

    converter = GotOcr2Converter(",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        special_tokens=special_tokens,
        model_max_length=model_max_length,
        pad_token=pad_token,
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        clean_up_tokenization_spaces=True,
    )",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        model_max_length=model_max_length,
        pad_token=pad_token,
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        clean_up_tokenization_spaces=True,
    )
    tokenizer = converter.tokenizer",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.message_start_token = ""<|im_start|>""","    def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, chat_template=chat_template)

        self.message_start_token = ""<|im_start|>""
        self.message_end_token = ""<|im_end|>""
        self.img_start_token = ""<img>""
        self.img_end_token = ""</img>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.message_end_token = ""<|im_end|>""","        super().__init__(image_processor, tokenizer, chat_template=chat_template)

        self.message_start_token = ""<|im_start|>""
        self.message_end_token = ""<|im_end|>""
        self.img_start_token = ""<img>""
        self.img_end_token = ""</img>""
        self.img_pad_token = ""<imgpad>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.img_start_token = ""<img>""","
        self.message_start_token = ""<|im_start|>""
        self.message_end_token = ""<|im_end|>""
        self.img_start_token = ""<img>""
        self.img_end_token = ""</img>""
        self.img_pad_token = ""<imgpad>""
        self.image_token = ""<imgpad>""  # keep the above for BC, but we need to call it `image_token`",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.img_end_token = ""</img>""","        self.message_start_token = ""<|im_start|>""
        self.message_end_token = ""<|im_end|>""
        self.img_start_token = ""<img>""
        self.img_end_token = ""</img>""
        self.img_pad_token = ""<imgpad>""
        self.image_token = ""<imgpad>""  # keep the above for BC, but we need to call it `image_token`
        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.img_pad_token = ""<imgpad>""","        self.message_end_token = ""<|im_end|>""
        self.img_start_token = ""<img>""
        self.img_end_token = ""</img>""
        self.img_pad_token = ""<imgpad>""
        self.image_token = ""<imgpad>""  # keep the above for BC, but we need to call it `image_token`
        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)
        self.system_query = ""system\nYou should follow the instructions carefully and explain your answers in detail.""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<imgpad>""  # keep the above for BC, but we need to call it `image_token`","        self.img_start_token = ""<img>""
        self.img_end_token = ""</img>""
        self.img_pad_token = ""<imgpad>""
        self.image_token = ""<imgpad>""  # keep the above for BC, but we need to call it `image_token`
        self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)
        self.system_query = ""system\nYou should follow the instructions carefully and explain your answers in detail.""
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","

def convert_swin_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub):
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]

    config = get_swin_config(model_name)
    model = SwinForMaskedImageModeling(config)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_swin_config(model_name)
    model = SwinForMaskedImageModeling(config)
    model.eval()

    new_state_dict = convert_state_dict(state_dict, model)
    model.load_state_dict(new_state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
timm_model.eval(),"
def convert_swin_checkpoint(swin_name, pytorch_dump_folder_path):
    timm_model = timm.create_model(swin_name, pretrained=True)
    timm_model.eval()

    config = get_swin_config(swin_name)
    model = SwinForImageClassification(config)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_swin_config(swin_name)
    model = SwinForImageClassification(config)
    model.eval()

    new_state_dict = convert_state_dict(timm_model.state_dict(), model)
    model.load_state_dict(new_state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = MobileNetV1ForImageClassification(config).eval(),"    config = get_mobilenet_v1_config(model_name)

    # Load  model
    model = MobileNetV1ForImageClassification(config).eval()

    # Load weights from TensorFlow checkpoint
    load_tf_weights_in_mobilenet_v1(model, config, checkpoint_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = ConvNextForImageClassification(config)
    model.load_state_dict(state_dict)
    model.eval()

    # Check outputs on an image, prepared by ConvNextImageProcessor
    size = 224 if ""224"" in checkpoint_url else 384",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_bos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_bos_token=False,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)","    logger.info(f""Converting model {model_name}..."")

    # load original state dict
    state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)

    # rename keys
    state_dict = rename_keys(state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # create HuggingFace model and load state dict
    model = PoolFormerForImageClassification(config)
    model.load_state_dict(state_dict)
    model.eval()

    # Define image processor
    image_processor = PoolFormerImageProcessor(crop_pct=crop_pct)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"
    logger.info(f""model loaded: {round(n_params / 1e6, 1)}M params"")

    hf_model.eval()
    hf_model.to(device)
    del state_dict
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    """"""
    Copy/paste/tweak model's weights to transformers design.
    """"""
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    downstream_dict = checkpoint[""Downstream""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, weights_only=True)","@torch.no_grad()
def convert_wavlm_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_path=None):
    # load the pre-trained checkpoints
    checkpoint = torch.load(checkpoint_path, weights_only=True)
    cfg = WavLMConfigOrig(checkpoint[""cfg""])
    model = WavLMOrig(cfg)
    model.load_state_dict(checkpoint[""model""])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    cfg = WavLMConfigOrig(checkpoint[""cfg""])
    model = WavLMOrig(cfg)
    model.load_state_dict(checkpoint[""model""])
    model.eval()

    if config_path is not None:
        config = WavLMConfig.from_pretrained(config_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        target_vocab_file=None,
        source_lang=None,
        target_lang=None,
        unk_token=""<unk>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        model_max_length=512,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        source_lang=None,
        target_lang=None,
        unk_token=""<unk>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        model_max_length=512,
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        target_lang=None,
        unk_token=""<unk>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        model_max_length=512,
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        separate_vocabs=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"            state_dict.pop(k, None)

    # load state dict into HuggingFace model
    model.eval()
    if mlm_model:
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
        assert missing_keys == [""mlm_score.decoder.bias""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.tokenizer.pad_token = ""<|reserved_special_token_0|>""","
        super().__init__(protein_tokenizer, tokenizer)

        self.tokenizer.pad_token = ""<|reserved_special_token_0|>""
        self.protein_max_length = protein_max_length
        self.text_max_length = text_max_length
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
loaded = torch.load(,"        if num_shards == 1:
            # Not sharded
            # (The sharded implementation would also work, but this is simpler.)
            loaded = torch.load(
                os.path.join(input_base_path, ""consolidated.pth""),
                map_location=""cpu"",
                weights_only=True,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"            checkpoint_list = sorted([file for file in os.listdir(input_base_path) if file.endswith("".pth"")])
            print(""Loading in order:"", checkpoint_list)
            loaded = [
                torch.load(
                    os.path.join(input_base_path, file),
                    map_location=""cpu"",
                    weights_only=True,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|begin_of_text|>"",","
        self.converted_tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token=""<|begin_of_text|>"",
            eos_token=""<|eot_id|>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            model_max_length=context_length,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|eot_id|>"",","        self.converted_tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token=""<|begin_of_text|>"",
            eos_token=""<|eot_id|>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            model_max_length=context_length,
            clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]","def convert_edgetam_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub, run_sanity_check):
    config = get_config(model_name)

    state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]
    state_dict = replace_keys(state_dict)

    image_processor = Sam2ImageProcessorFast()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    image_processor = Sam2ImageProcessorFast()
    processor = Sam2Processor(image_processor=image_processor)
    hf_model = EdgeTamModel(config)
    hf_model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(model_files[""model_weights""], map_location=""cpu"", weights_only=True)","        else:
            raise ValueError(f""Key {key} not found in enocder_config_keys_mapping"")

    state_dict = torch.load(model_files[""model_weights""], map_location=""cpu"", weights_only=True)
    converted_state_dict = {}
    for key, value in state_dict.items():
        # Skip preprocessing weights (featurizer components)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
timm_model.eval(),"
    # load original model from timm
    timm_model = create_model(model_name, pretrained=True)
    timm_model.eval()

    # load state_dict of original model
    state_dict = timm_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # load HuggingFace model
    model = BitForImageClassification(config)
    model.eval()
    model.load_state_dict(state_dict)

    # create image processor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    config = LukeConfig(use_entity_aware_attention=True, **metadata[""model_config""])

    # Load in the weights from the checkpoint_path
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    # Load the entity vocab file
    entity_vocab = load_entity_vocab(entity_vocab_path)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = LukeModel(config=config).eval(),"    entity_emb = state_dict[""entity_embeddings.entity_embeddings.weight""]
    entity_emb[entity_vocab[""[MASK2]""]] = entity_emb[entity_vocab[""[MASK]""]]

    model = LukeModel(config=config).eval()

    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    if not (len(missing_keys) == 1 and missing_keys[0] == ""embeddings.position_ids""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        # we add 2 special tokens for downstream tasks
        # for more information about lstrip and rstrip, see https://github.com/huggingface/transformers/pull/2778",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"entity_unk_token=""[UNK]"",","        max_mention_length=30,
        entity_token_1=""<ent>"",
        entity_token_2=""<ent2>"",
        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_pad_token=""[PAD]"",","        entity_token_1=""<ent>"",
        entity_token_2=""<ent2>"",
        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        errors=""replace"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_mask_token=""[MASK]"",","        entity_token_2=""<ent2>"",
        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        errors=""replace"",
        bos_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_mask2_token=""[MASK2]"",","        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        entity_mask2_token=""[MASK2]"",
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        errors=""replace"",
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        merges_file,
        errors=""replace"",
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        errors=""replace"",
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        sep_token=""[SEP]"",
        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",
        add_prefix_space=False,
        add_bos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        cls_token=""[CLS]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        mask_token=""[MASK]"",
        add_prefix_space=False,
        add_bos_token=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"loaded_state_dict = torch.load(os.path.join(input_base_path, file), map_location=""cpu"", weights_only=True)","
        for file in files:
            print(file)
            loaded_state_dict = torch.load(os.path.join(input_base_path, file), map_location=""cpu"", weights_only=True)
            model_state_dict.update(loaded_state_dict)
    else:
        print(""Model does not seem to be sharded"")",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"model_state_dict = torch.load(input_base_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]","            model_state_dict.update(loaded_state_dict)
    else:
        print(""Model does not seem to be sharded"")
        model_state_dict = torch.load(input_base_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]
        model_state_dict.pop(""freqs_cis"")

    state_dict = {}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""block_sparse_moe.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"torch.load(os.path.join(input_base_path, f""consolidated.{i:02d}.pt""), map_location=""cpu"", weights_only=True)","    print(f""Fetching all parameters from the checkpoint at {input_base_path}."")
    # Load weights
    loaded = [
        torch.load(os.path.join(input_base_path, f""consolidated.{i:02d}.pt""), map_location=""cpu"", weights_only=True)
        for i in range(8)
    ]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"

class MixtralPreTrainedModel(MistralPreTrainedModel):
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""block_sparse_moe.gate"", index=0),
        ""hidden_states"": MixtralDecoderLayer,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"conditional_detr = torch.hub.load(""DeppMeng/ConditionalDETR"", model_name, pretrained=True).eval()","    logger.info(f""Converting model {model_name}..."")

    # load original model from torch hub
    conditional_detr = torch.hub.load(""DeppMeng/ConditionalDETR"", model_name, pretrained=True).eval()
    state_dict = conditional_detr.state_dict()
    # rename keys
    for src, dest in rename_keys:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = ConditionalDetrForSegmentation(config) if is_panoptic else ConditionalDetrForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()
    model.push_to_hub(repo_id=model_name, organization=""DepuMeng"", commit_message=""Add model"")
    # verify our conversion
    original_outputs = conditional_detr(pixel_values)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
converted_model.eval()  # Set to evaluation mode,"        dtype=torch.bfloat16,
        attn_implementation=""sdpa"",
    ).to(""cuda"" if torch.cuda.is_available() else ""cpu"")
    converted_model.eval()  # Set to evaluation mode

    # Create test inputs
    forecast_input = [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(pvt_v2_checkpoint, map_location=""cpu"", weights_only=True)","        )
    config = PvtV2Config.from_pretrained(config_path)
    # load original model from https://github.com/whai362/PVT
    state_dict = torch.load(pvt_v2_checkpoint, map_location=""cpu"", weights_only=True)

    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(pvt_v2_checkpoint, map_location=""cpu"", weights_only=True)","        )
    config = PvtV2Config.from_pretrained(config_path)
    # load original model from https://github.com/whai362/PVT
    state_dict = torch.load(pvt_v2_checkpoint, map_location=""cpu"", weights_only=True)

    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = PvtV2ForImageClassification(config).eval(),"    read_in_k_v(state_dict, config)

    # load HuggingFace model
    model = PvtV2ForImageClassification(config).eval()
    model.load_state_dict(state_dict)
    image_processor = PvtImageProcessor(size=config.image_size)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model = DonutModel.from_pretrained(model_name).eval(),"
def convert_donut_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):
    # load original model
    original_model = DonutModel.from_pretrained(model_name).eval()

    # load HuggingFace model
    encoder_config, decoder_config = get_configs(original_model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    encoder = DonutSwinModel(encoder_config)
    decoder = MBartForCausalLM(decoder_config)
    model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)
    model.eval()

    state_dict = original_model.state_dict()
    new_state_dict = convert_state_dict(state_dict, model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","def convert_sam_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub):
    config = get_config(model_name)

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    state_dict = replace_keys(state_dict)

    image_processor = SamImageProcessor()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    image_processor = SamImageProcessor()
    processor = SamProcessor(image_processor=image_processor)
    hf_model = SamModel(config)
    hf_model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        phone_delimiter_token="" "",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        phone_delimiter_token="" "",
        word_delimiter_token=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        phone_delimiter_token="" "",
        word_delimiter_token=None,
        do_phonemize=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"phone_delimiter_token="" "",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        phone_delimiter_token="" "",
        word_delimiter_token=None,
        do_phonemize=True,
        phonemizer_lang=""en-us"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
data = pickle.load(f),"
    # load original state_dict
    with open(checkpoint_path, ""rb"") as f:
        data = pickle.load(f)
    state_dict = data[""model""]

    # rename keys",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # load  model
    model = MaskFormerForInstanceSegmentation(config)
    model.eval()

    model.load_state_dict(state_dict)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model = original_model.eval(),"
def test(original_model, our_model: MaskFormerForInstanceSegmentation, image_processor: MaskFormerImageProcessor):
    with torch.no_grad():
        original_model = original_model.eval()
        our_model = our_model.eval()

        im = prepare_img()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = our_model.eval(),"def test(original_model, our_model: MaskFormerForInstanceSegmentation, image_processor: MaskFormerImageProcessor):
    with torch.no_grad():
        original_model = original_model.eval()
        our_model = our_model.eval()

        im = prepare_img()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model = OriginalMaskFormer(**mask_former_kwargs).eval(),"        original_config = setup_cfg(Args(config_file=config_file))
        mask_former_kwargs = OriginalMaskFormer.from_config(original_config)

        original_model = OriginalMaskFormer(**mask_former_kwargs).eval()

        DetectionCheckpointer(original_model).load(str(checkpoint_file))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
mask_former = MaskFormerModel(config=config).eval(),"
        config: MaskFormerConfig = OriginalMaskFormerConfigToOursConverter()(original_config)

        mask_former = MaskFormerModel(config=config).eval()

        converter = OriginalMaskFormerCheckpointToOursConverter(original_model, config)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
mask_former_for_instance_segmentation = MaskFormerForInstanceSegmentation(config=config).eval(),"
        maskformer = converter.convert(mask_former)

        mask_former_for_instance_segmentation = MaskFormerForInstanceSegmentation(config=config).eval()

        mask_former_for_instance_segmentation.model = mask_former
        mask_former_for_instance_segmentation = converter.convert_instance_segmentation(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
data = pickle.load(f),"
    # load original state_dict
    with open(checkpoint_path, ""rb"") as f:
        data = pickle.load(f)
    state_dict = data[""model""]

    # for name, param in state_dict.items():",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # load  model
    model = MaskFormerForInstanceSegmentation(config)
    model.eval()

    for name, param in model.named_parameters():
        print(name, param.shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        # additional properties
        self.max_depth = max_depth",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file,
        tags_dict,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tags_dict,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        max_depth=50,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        max_depth=50,
        max_width=1000,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        tags_dict,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        max_depth=50,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        max_depth=50,
        max_width=1000,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab, unk_token=""<unk>"", max_input_chars_per_word=200):","

class WordpieceTokenizer:
    def __init__(self, vocab, unk_token=""<unk>"", max_input_chars_per_word=200):
        self.vocab = vocab
        self.unk_token = unk_token
        self.max_input_chars_per_word = max_input_chars_per_word",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bod_token=""<d>"",","    def __init__(
        self,
        vocab_file,
        bod_token=""<d>"",
        eod_token=""</d>"",
        bos_token=""<s>"",
        eos_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eod_token=""</d>"",","        self,
        vocab_file,
        bod_token=""<d>"",
        eod_token=""</d>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        bod_token=""<d>"",
        eod_token=""</d>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        bod_token=""<d>"",
        eod_token=""</d>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        line_token=""</n>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eod_token=""</d>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        line_token=""</n>"",
        space_token=""</_>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        line_token=""</n>"",
        space_token=""</_>"",
        padding_side=""left"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"line_token=""</n>"",","        eos_token=""</s>"",
        pad_token=""<pad>"",
        unk_token=""<unk>"",
        line_token=""</n>"",
        space_token=""</_>"",
        padding_side=""left"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"space_token=""</_>"",","        pad_token=""<pad>"",
        unk_token=""<unk>"",
        line_token=""</n>"",
        space_token=""</_>"",
        padding_side=""left"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_encoder.eval(),"
    # load original model from torch hub
    original_encoder, original_predictor = torch.hub.load(HUB_REPO, ""vjepa2_"" + model_name, source=HUB_SOURCE)
    original_encoder.eval()
    original_predictor.eval()
    original_preprocessor = torch.hub.load(
        HUB_REPO, ""vjepa2_preprocessor"", source=HUB_SOURCE, crop_size=config.crop_size",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_predictor.eval(),"    # load original model from torch hub
    original_encoder, original_predictor = torch.hub.load(HUB_REPO, ""vjepa2_"" + model_name, source=HUB_SOURCE)
    original_encoder.eval()
    original_predictor.eval()
    original_preprocessor = torch.hub.load(
        HUB_REPO, ""vjepa2_preprocessor"", source=HUB_SOURCE, crop_size=config.crop_size
    )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = VJEPA2Model(config).eval(),"    encoder_state_dict = original_encoder.state_dict()
    decoder_state_dict = original_predictor.state_dict()

    model = VJEPA2Model(config).eval()
    state_dict = model.state_dict()

    og_encoder_sd = convert_encoder_keys(state_dict, encoder_state_dict, config)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.to(device).eval(),"        inputs = processor(video, return_tensors=""pt"").to(device)

        # run model
        model.to(device).eval()
        with torch.no_grad():
            outputs = model(**inputs)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(os.path.join(input_base_path, ""model.pt""), map_location=""cpu"", weights_only=True)","
    # Not sharded
    # (The sharded implementation would also work, but this is simpler.)
    loaded = torch.load(os.path.join(input_base_path, ""model.pt""), map_location=""cpu"", weights_only=True)

    param_count = 0
    index_dict = {""weight_map"": {}}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.eval()  # Chameleon's VQ model is frozen,"        self.quantize = ChameleonVQVAEVectorQuantizer(config)
        self.quant_conv = torch.nn.Conv2d(config.latent_channels, config.embed_dim, 1)
        self.post_quant_conv = torch.nn.Conv2d(config.embed_dim, config.latent_channels, 1)
        self.eval()  # Chameleon's VQ model is frozen

    def encode(self, pixel_values: torch.LongTensor):
        hidden_states = self.encoder(pixel_values)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"vqgan_state_dict = torch.load(vqgan_path, map_location=""cpu"", weights_only=True)[""state_dict""]","
    # Load VQGAN weights
    vqgan_path = os.path.join(input_base_path, ""tokenizer/vqgan.ckpt"")
    vqgan_state_dict = torch.load(vqgan_path, map_location=""cpu"", weights_only=True)[""state_dict""]
    for k, v in vqgan_state_dict.items():
        if ""decoder"" in k:
            continue  # we dont do image generation yet",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"    else:
        # Sharded
        loaded = [
            torch.load(
                os.path.join(input_model_path, f""consolidated.{i:02d}.pth""), map_location=""cpu"", weights_only=True
            )
            for i in range(num_shards)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(possible_path, map_location=""cpu"", weights_only=True)","        for possible_name in [""consolidated.pth"", ""consolidated.00.pth""]:
            possible_path = os.path.join(input_model_path, possible_name)
            if os.path.exists(possible_path):
                loaded = torch.load(possible_path, map_location=""cpu"", weights_only=True)
                break
        assert loaded is not None
    else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","        **kwargs,
    ):
        super().__init__(image_processor, tokenizer, chat_template=chat_template)
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token

        if visual_prompt_prefix is None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","    ):
        super().__init__(image_processor, tokenizer, chat_template=chat_template)
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token

        if visual_prompt_prefix is None:
            visual_prompt_prefix = ""<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"model = ColQwen2ForRetrieval(config=config).to(""cpu"").eval()","    config.is_composition = False

    # Load the untrained model
    model = ColQwen2ForRetrieval(config=config).to(""cpu"").eval()
    print(""Created model with new config and randomly initialized weights"")

    # NOTE: The new model was initialized with float32 weights. We need to convert it to the desired precision.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","        **kwargs,
    ):
        ProcessorMixin.__init__(self, image_processor, tokenizer, chat_template=chat_template)
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token

        if visual_prompt_prefix is None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","    ):
        ProcessorMixin.__init__(self, image_processor, tokenizer, chat_template=chat_template)
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token

        if visual_prompt_prefix is None:
            visual_prompt_prefix = ""<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"shard_dict = torch.load(shard_path, map_location=""cpu"")","        for shard_file in unique_shard_files:
            print(f""Loading shard {shard_file}..."")
            shard_path = os.path.join(input_path, shard_file)
            shard_dict = torch.load(shard_path, map_location=""cpu"")
            state_dict.update(shard_dict)

        return state_dict",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(single_file_path, map_location=""cpu"")","    # Single file model
    elif os.path.exists(single_file_path):
        print(""Loading single file model..."")
        return torch.load(single_file_path, map_location=""cpu"")

    else:
        raise ValueError(f""No model files found in {input_path}"")",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
esm.eval()  # disable dropout,"        esm = MODEL_MAPPING[model]()
    else:
        esm, alphabet = MODEL_MAPPING[model]()
    esm.eval()  # disable dropout

    if model.startswith(""esmfold""):
        embed_dim = esm.esm.embed_dim",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    else:
        model_class = EsmForMaskedLM
    model = model_class(config)
    model.eval()

    # Now let's copy all the weights.
    # Embeddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        vocab_file,
        unk_token=""<unk>"",
        cls_token=""<cls>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        self,
        vocab_file,
        unk_token=""<unk>"",
        cls_token=""<cls>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        eos_token=""<eos>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        vocab_file,
        unk_token=""<unk>"",
        cls_token=""<cls>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        eos_token=""<eos>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        unk_token=""<unk>"",
        cls_token=""<cls>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        eos_token=""<eos>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<eos>"",","        cls_token=""<cls>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        eos_token=""<eos>"",
        **kwargs,
    ):
        self.all_tokens = load_vocab_file(vocab_file)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tensors = torch.load(file, map_location=""cpu"")","    elif bin_files:
        bin_files = sorted(bin_files, key=lambda x: int(x.rsplit(""-"", 3)[1]))
        for file in bin_files:
            tensors = torch.load(file, map_location=""cpu"")
            all_weights.update(tensors)
        return all_weights
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file=None,
        tokenizer_file=None,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        eos_token=""</s>"",
        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        pad_token=""<pad>"",
        sep_token=""[SEP]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        **kwargs,
    ):
        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
self.self.eval(),"        attn_weights.key = self.self.key
        self.self = attn_weights
        if not self.training:
            self.self.eval()

    def forward(
        self,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are","
        List options

        The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are
        deactivated). To train the model, you should first set it back in training mode with `model.train()`

        Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"transformers_module = importlib.import_module(""transformers"")","        return getattr(module, attr)
    # Some of the mappings have entries model_type -> object of another model type. In that case we try to grab the
    # object at the top level.
    transformers_module = importlib.import_module(""transformers"")

    if module != transformers_module:
        try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self._modules[module_name] = importlib.import_module(f"".{module_name}"", ""transformers.models"")","    def _load_attr_from_module(self, model_type, attr):
        module_name = model_type_to_module_name(model_type)
        if module_name not in self._modules:
            self._modules[module_name] = importlib.import_module(f"".{module_name}"", ""transformers.models"")
        return getattribute_from_module(self._modules[module_name], attr)

    def keys(self) -> list[type[PreTrainedConfig]]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self._modules[module_name] = importlib.import_module(f"".{module_name}"", ""transformers.models"")","        value = self._mapping[key]
        module_name = model_type_to_module_name(key)
        if module_name not in self._modules:
            self._modules[module_name] = importlib.import_module(f"".{module_name}"", ""transformers.models"")
        if hasattr(self._modules[module_name], value):
            return getattr(self._modules[module_name], value)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"transformers_module = importlib.import_module(""transformers"")","
        # Some of the mappings have entries model_type -> config of another model type. In that case we try to grab the
        # object at the top level.
        transformers_module = importlib.import_module(""transformers"")
        return getattr(transformers_module, value)

    def keys(self) -> list[str]:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module(f"".{module_name}"", ""transformers.models"")","
        for model_type, map_name in self._mapping.items():
            module_name = model_type_to_module_name(model_type)
            module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            mapping = getattr(module, map_name)
            self._data.update(mapping)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module(f"".{module_name}"", ""transformers.models"")","        if class_name in extractors:
            module_name = model_type_to_module_name(module_name)

            module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            try:
                return getattr(module, class_name)
            except AttributeError:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"main_module = importlib.import_module(""transformers"")","
    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main
    # init and we return the proper dummy to get an appropriate error message.
    main_module = importlib.import_module(""transformers"")
    if hasattr(main_module, class_name):
        return getattr(main_module, class_name)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module(f"".{module_name}"", ""transformers.models"")","        if class_name in extractors:
            module_name = model_type_to_module_name(module_name)

            module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            try:
                return getattr(module, class_name)
            except AttributeError:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"main_module = importlib.import_module(""transformers"")","
    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main
    # init and we return the proper dummy to get an appropriate error message.
    main_module = importlib.import_module(""transformers"")
    if hasattr(main_module, class_name):
        return getattr(main_module, class_name)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module(f"".{module_name}"", ""transformers.models"")","        if class_name in processors:
            module_name = model_type_to_module_name(module_name)

            module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            try:
                return getattr(module, class_name)
            except AttributeError:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"main_module = importlib.import_module(""transformers"")","
    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main
    # init and we return the proper dummy to get an appropriate error message.
    main_module = importlib.import_module(""transformers"")
    if hasattr(main_module, class_name):
        return getattr(main_module, class_name)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module(f"".{module_name}"", ""transformers.models"")","        if class_name in extractors:
            module_name = model_type_to_module_name(module_name)

            module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            try:
                return getattr(module, class_name)
            except AttributeError:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"main_module = importlib.import_module(""transformers"")","
    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main
    # init and we return the proper dummy to get an appropriate error message.
    main_module = importlib.import_module(""transformers"")
    if hasattr(main_module, class_name):
        return getattr(main_module, class_name)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module("".tokenization_mistral_common"", ""transformers"")","        if class_name in tokenizers:
            module_name = model_type_to_module_name(module_name)
            if module_name in [""mistral"", ""mixtral"", ""ministral""] and class_name == ""MistralCommonTokenizer"":
                module = importlib.import_module("".tokenization_mistral_common"", ""transformers"")
            else:
                module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            try:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"module = importlib.import_module(f"".{module_name}"", ""transformers.models"")","            if module_name in [""mistral"", ""mixtral"", ""ministral""] and class_name == ""MistralCommonTokenizer"":
                module = importlib.import_module("".tokenization_mistral_common"", ""transformers"")
            else:
                module = importlib.import_module(f"".{module_name}"", ""transformers.models"")
            try:
                return getattr(module, class_name)
            except AttributeError:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"main_module = importlib.import_module(""transformers"")","
    # We did not fine the class, but maybe it's because a dep is missing. In that case, the class will be in the main
    # init and we return the proper dummy to get an appropriate error message.
    main_module = importlib.import_module(""transformers"")
    if hasattr(main_module, class_name):
        return getattr(main_module, class_name)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token=""<::::>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token=""<::::>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token=""<::::>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<::::>"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sep_token=""<::::>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    read_in_q_k_v(state_dict, config, base_model=True)

    model.load_state_dict(state_dict)
    model.eval()

    url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = RTDetrForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    # load image processor
    image_processor = RTDetrImageProcessor()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = DabDetrForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()
    logger.info(f""Saving PyTorch model to {pytorch_dump_folder_path}..."")
    Path(pytorch_dump_folder_path).mkdir(exist_ok=True)
    model.save_pretrained(pytorch_dump_folder_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
main_keys_pattern = re.compile(,"                block = next(blocks)
                if self.config.num_mem_blocks * len(self.config.hybrid_layer_ids) > 1:
                    prefix_pattern = rf""^layers\.{layer_id}\.shared_transformer\.""
                    main_keys_pattern = re.compile(
                        prefix_pattern
                        + r""(?:""
                        + r""self_attn\.(?:q_proj|k_proj|v_proj|o_proj)\.weight|""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
adapter_pattern = re.compile(,"                    adapter_id = 0
                    for _layer_type in self.layers_block_type:
                        if _layer_type == ""hybrid"" and adapter_id % self.config.num_mem_blocks == block.block_id:
                            adapter_pattern = re.compile(
                                r""^shared_transformer\.feed_forward\.gate_up_proj_adapter_list\.""
                                + str(adapter_id)
                                + r""\.(?:0|1)\.weight$""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
attn_adapter_pattern = re.compile(,"                        adapter_id = 0
                        for _layer_type in self.layers_block_type:
                            if _layer_type == ""hybrid"" and adapter_id % self.config.num_mem_blocks == block.block_id:
                                attn_adapter_pattern = re.compile(
                                    r""^shared_transformer\.self_attn\.""
                                    + r""(?:linear_q_adapter_list|linear_k_adapter_list|linear_v_adapter_list)\.""
                                    + str(adapter_id)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
main_keys_pattern = re.compile(,"                block = next(blocks)
                if self.config.num_mem_blocks * len(self.config.hybrid_layer_ids) > 1:
                    prefix_pattern = rf""^layers\.{layer_id}\.shared_transformer\.""
                    main_keys_pattern = re.compile(
                        prefix_pattern
                        + r""(?:""
                        + r""self_attn\.(?:q_proj|k_proj|v_proj|o_proj)\.weight|""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
adapter_pattern = re.compile(,"                    adapter_id = 0
                    for _layer_type in self.layers_block_type:
                        if _layer_type == ""hybrid"" and adapter_id % self.config.num_mem_blocks == block.block_id:
                            adapter_pattern = re.compile(
                                r""^shared_transformer\.feed_forward\.gate_up_proj_adapter_list\.""
                                + str(adapter_id)
                                + r""\.(?:0|1)\.weight$""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
attn_adapter_pattern = re.compile(,"                        adapter_id = 0
                        for _layer_type in self.layers_block_type:
                            if _layer_type == ""hybrid"" and adapter_id % self.config.num_mem_blocks == block.block_id:
                                attn_adapter_pattern = re.compile(
                                    r""^shared_transformer\.self_attn\.""
                                    + r""(?:linear_q_adapter_list|linear_k_adapter_list|linear_v_adapter_list)\.""
                                    + str(adapter_id)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""gate"", index=1),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model = CLIPModel(config).eval(),"    else:
        config = CLIPConfig(projection_dim=512, text_config={}, vision_config={})

    hf_model = CLIPModel(config).eval()

    pt_model, _ = load(checkpoint_path, device=""cpu"", jit=False)
    pt_model = pt_model.eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
pt_model = pt_model.eval(),"    hf_model = CLIPModel(config).eval()

    pt_model, _ = load(checkpoint_path, device=""cpu"", jit=False)
    pt_model = pt_model.eval()

    copy_text_model_and_projection(hf_model, pt_model)
    copy_vison_model_and_projection(hf_model, pt_model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.pat = re.compile(,"        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))
        self.cache = {""<|startoftext|>"": ""<|startoftext|>"", ""<|endoftext|>"": ""<|endoftext|>""}

        self.pat = re.compile(
            r""""""<\|startoftext\|>|<\|endoftext\|>|'s|'t|'re|'ve|'m|'ll|'d|[\p{L}]+|[\p{N}]|[^\s\p{L}\p{N}]+"""""",
            re.IGNORECASE,
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|startoftext|>"",","        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|endoftext|>"",  # hack to enable padding","        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding
        **kwargs,
    ):
        bos_token = AddedToken(bos_token, lstrip=False, rstrip=False) if isinstance(bos_token, str) else bos_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|startoftext|>"",","        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|endoftext|>"",  # hack to enable padding","        unk_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",  # hack to enable padding
        **kwargs,
    ):
        super().__init__(",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
self.pat = re.compile(PRETOKENIZE_REGEX),"        # GPT2Tokenizer has the same problem, so let's be consistent.
        self.cache = {}

        self.pat = re.compile(PRETOKENIZE_REGEX)

        if kwargs.get(""add_prefix_space"", False):
            logger.warning_once(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=None,
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=None,
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        clean_up_tokenization_spaces=False,
        split_special_tokens=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|endoftext|>"",","        unk_token=""<|endoftext|>"",
        bos_token=None,
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        clean_up_tokenization_spaces=False,
        split_special_tokens=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=None,
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=None,
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|endoftext|>"",","        unk_token=""<|endoftext|>"",
        bos_token=None,
        eos_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        **kwargs,
    ):
        # We need to at least pass vocab_file and merges_file to base class",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", **kwargs):","    vocab_files_names = VOCAB_FILES_NAMES
    control_codes = CONTROL_CODES

    def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", **kwargs):
        with open(vocab_file, encoding=""utf-8"") as vocab_handle:
            self.encoder = json.load(vocab_handle)
        self.decoder = {v: k for k, v in self.encoder.items()}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"WORD_RE = regex.compile(r""""""(%s)"""""" % ""|"".join(REGEXPS), regex.VERBOSE | regex.I | regex.UNICODE)","######################################################################
# This is the core tokenizing regex:

WORD_RE = regex.compile(r""""""(%s)"""""" % ""|"".join(REGEXPS), regex.VERBOSE | regex.I | regex.UNICODE)

# WORD_RE performs poorly on these patterns:
HANG_RE = regex.compile(r""([^a-zA-Z0-9])\1{3,}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"HANG_RE = regex.compile(r""([^a-zA-Z0-9])\1{3,}"")","WORD_RE = regex.compile(r""""""(%s)"""""" % ""|"".join(REGEXPS), regex.VERBOSE | regex.I | regex.UNICODE)

# WORD_RE performs poorly on these patterns:
HANG_RE = regex.compile(r""([^a-zA-Z0-9])\1{3,}"")

# The emoticon string gets its own regex so that we can preserve case for
# them as needed:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"EMOTICON_RE = regex.compile(EMOTICONS, regex.VERBOSE | regex.I | regex.UNICODE)","
# The emoticon string gets its own regex so that we can preserve case for
# them as needed:
EMOTICON_RE = regex.compile(EMOTICONS, regex.VERBOSE | regex.I | regex.UNICODE)

# These are for regularizing HTML entities to Unicode:
ENT_RE = regex.compile(r""&(#?(x?))([^&;\s]+);"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"ENT_RE = regex.compile(r""&(#?(x?))([^&;\s]+);"")","EMOTICON_RE = regex.compile(EMOTICONS, regex.VERBOSE | regex.I | regex.UNICODE)

# These are for regularizing HTML entities to Unicode:
ENT_RE = regex.compile(r""&(#?(x?))([^&;\s]+);"")


######################################################################",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pattern = regex.compile(r""(.)\1{2,}"")","    """"""
    Replace repeated character sequences of length 3 or greater with sequences of length 3.
    """"""
    pattern = regex.compile(r""(.)\1{2,}"")
    return pattern.sub(r""\1\1\1"", text)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
pattern = regex.compile(,"    """"""
    Remove Twitter username handles from text.
    """"""
    pattern = regex.compile(
        r""(?<![A-Za-z0-9_!@#\$%&*])@(([A-Za-z0-9_]){20}(?!@))|(?<![A-Za-z0-9_!@#\$%&*])@(([A-Za-z0-9_]){1,19})(?![A-Za-z0-9_]*@)""
    )
    # Substitute handles with ' ' to ensure that text on either side of removed handles are tokenized correctly",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        normalization=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        normalization=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        normalization=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):
        try:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
conv_layers = eval(fs_config.extractor_conv_feature_layers),"    config.apply_spec_augment = False
    config.attention_dropout = fs_config.attention_dropout
    config.conv_bias = False
    conv_layers = eval(fs_config.extractor_conv_feature_layers)
    config.conv_dim = [x[0] for x in conv_layers]
    config.conv_kernel = [x[1] for x in conv_layers]
    config.conv_stride = [x[2] for x in conv_layers]",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
conv_layers = eval(fs_config.extractor_conv_feature_layers),"    config.apply_spec_augment = False
    config.attention_dropout = fs_config.attention_dropout
    config.conv_bias = False
    conv_layers = eval(fs_config.extractor_conv_feature_layers)
    config.conv_dim = [x[0] for x in conv_layers]
    config.conv_kernel = [x[1] for x in conv_layers]
    config.conv_stride = [x[2] for x in conv_layers]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model.eval(),"        config = HubertConfig.from_pretrained(config_path)
    else:
        config = convert_config(model)
    model = model.eval()

    feature_extractor = Wav2Vec2FeatureExtractor(
        feature_size=1,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    """"""
    Copy/paste/tweak model's weights to transformers design.
    """"""
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    if checkpoint[""Config""][""downstream_expert""][""modelrc""][""select""] not in SUPPORTED_MODELS:
        raise NotImplementedError(f""The supported s3prl models are {SUPPORTED_MODELS}"")
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"    else:
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path])

    model = model[0].eval()

    recursively_load_weights(model, hf_wav2vec, is_finetuned)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","                pad_token=target_dict.pad_word,
                bos_token=target_dict.bos_word,
                eos_token=target_dict.eos_word,
                word_delimiter_token=""|"",
                do_lower_case=False,
            )
            return_attention_mask = config.feat_extract_norm == ""layer""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","def convert_sam_hq_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub, hub_path):
    config = get_config(model_name)

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    state_dict = replace_keys(state_dict)

    image_processor = SamImageProcessor()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    image_processor = SamImageProcessor()
    processor = SamHQProcessor(image_processor=image_processor)
    hf_model = SamHQModel(config)
    hf_model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    config = GroupViTConfig()
    model = GroupViTModel(config).eval()

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    new_state_dict = convert_state_dict(state_dict, config)
    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    assert missing_keys == [""text_model.embeddings.position_ids""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = GroupViTModel(config).eval(),"    Copy/paste/tweak model's weights to the Transformers design.
    """"""
    config = GroupViTConfig()
    model = GroupViTModel(config).eval()

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    new_state_dict = convert_state_dict(state_dict, config)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""hidden_states"": GraniteMoeDecoderLayer,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True

    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)

    def _init_weights(self, module):
        PreTrainedModel._init_weights(self, module)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(pvt_checkpoint, map_location=""cpu"", weights_only=True)","        raise ValueError(f""Available model's size: 'tiny', 'small', 'medium', 'large', but '{pvt_size}' was given"")
    config = PvtConfig(name_or_path=config_path)
    # load original model from https://github.com/whai362/PVT
    state_dict = torch.load(pvt_checkpoint, map_location=""cpu"", weights_only=True)

    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(pvt_checkpoint, map_location=""cpu"", weights_only=True)","        raise ValueError(f""Available model's size: 'tiny', 'small', 'medium', 'large', but '{pvt_size}' was given"")
    config = PvtConfig(name_or_path=config_path)
    # load original model from https://github.com/whai362/PVT
    state_dict = torch.load(pvt_checkpoint, map_location=""cpu"", weights_only=True)

    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = PvtForImageClassification(config).eval(),"    read_in_k_v(state_dict, config)

    # load HuggingFace model
    model = PvtForImageClassification(config).eval()
    model.load_state_dict(state_dict)

    # Check outputs on an image, prepared by PVTFeatureExtractor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""module""]","    config = LukeConfig(use_entity_aware_attention=True, **metadata[""model_config""])

    # Load in the weights from the checkpoint_path
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""module""]

    # Load the entity vocab file
    entity_vocab = load_original_entity_vocab(entity_vocab_path)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = LukeForMaskedLM(config=config).eval(),"    entity_mask_bias = entity_prediction_bias[entity_vocab[""[MASK]""]].unsqueeze(0)
    state_dict[""entity_predictions.bias""] = torch.cat([entity_prediction_bias, entity_mask_bias])

    model = LukeForMaskedLM(config=config).eval()

    state_dict.pop(""entity_predictions.decoder.weight"")
    state_dict.pop(""lm_head.decoder.weight"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        entity_vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        entity_vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        entity_vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        task=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        task=None,
        max_entity_length=32,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        task=None,
        max_entity_length=32,
        max_mention_length=30,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_unk_token=""[UNK]"",","        max_mention_length=30,
        entity_token_1=""<ent>"",
        entity_token_2=""<ent2>"",
        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_pad_token=""[PAD]"",","        entity_token_1=""<ent>"",
        entity_token_2=""<ent2>"",
        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_mask_token=""[MASK]"",","        entity_token_2=""<ent2>"",
        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"entity_mask2_token=""[MASK2]"",","        entity_unk_token=""[UNK]"",
        entity_pad_token=""[PAD]"",
        entity_mask_token=""[MASK]"",
        entity_mask2_token=""[MASK2]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # Load HuggingFace model
    model = SuperPointForKeypointDetection(config)
    model.load_state_dict(new_state_dict)
    model.eval()
    print(""Successfully loaded weights in the model"")

    # Check model outputs",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","    def __init__(
        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        additional_special_tokens=[],
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        additional_special_tokens=[],
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"model_weights = pickle.load(f)[""weights""]","    model = ReformerModelWithLMHead(config)

    with open(trax_model_pkl_path, ""rb"") as f:
        model_weights = pickle.load(f)[""weights""]

    set_model_weights_in_torch(model_weights, model, config.hidden_size)
",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        additional_special_tokens=[],
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        additional_special_tokens=[],
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
conv_layers = eval(fs_config.conv_feature_layers),"        fs_config = model.cfg

    config.conv_bias = fs_config.conv_bias
    conv_layers = eval(fs_config.conv_feature_layers)
    config.conv_dim = [x[0] for x in conv_layers]
    config.conv_kernel = [x[1] for x in conv_layers]
    config.conv_stride = [x[2] for x in conv_layers]",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
conv_layers = eval(fs_config.conv_feature_layers),"        fs_config = model.cfg

    config.conv_bias = fs_config.conv_bias
    conv_layers = eval(fs_config.conv_feature_layers)
    config.conv_dim = [x[0] for x in conv_layers]
    config.conv_kernel = [x[1] for x in conv_layers]
    config.conv_stride = [x[2] for x in conv_layers]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"        config = SEWConfig.from_pretrained(config_path)
    else:
        config = convert_config(model[0], is_finetuned)
    model = model[0].eval()

    return_attention_mask = config.feat_extract_norm == ""layer""
    feature_extractor = Wav2Vec2FeatureExtractor(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","                pad_token=target_dict.pad_word,
                bos_token=target_dict.bos_word,
                eos_token=target_dict.eos_word,
                word_delimiter_token=""|"",
                do_lower_case=False,
            )
            processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_model = original_model.eval(),"
    with torch.no_grad():
        tokenizer = CLIPTokenizer.from_pretrained(model_repo)
        original_model = original_model.eval()
        our_model = our_model.eval()

        im = prepare_img()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = our_model.eval(),"    with torch.no_grad():
        tokenizer = CLIPTokenizer.from_pretrained(model_repo)
        original_model = original_model.eval()
        our_model = our_model.eval()

        im = prepare_img()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model = OriginalOneFormer(**oneformer_kwargs).eval(),"        original_config = setup_cfg(Args(config_file=config_file))
        oneformer_kwargs = OriginalOneFormer.from_config(original_config)

        original_model = OriginalOneFormer(**oneformer_kwargs).eval()

        DetectionCheckpointer(original_model).load(str(checkpoint_file))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
oneformer = OneFormerModel(config=config).eval(),"
        config: OneFormerConfig = OriginalOneFormerConfigToOursConverter()(original_config, is_swin)

        oneformer = OneFormerModel(config=config).eval()

        converter = OriginalOneFormerCheckpointToOursConverter(original_model, config)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
oneformer_for_universal_segmentation = OneFormerForUniversalSegmentation(config=config).eval(),"
        oneformer = converter.convert(oneformer, is_swin)

        oneformer_for_universal_segmentation = OneFormerForUniversalSegmentation(config=config).eval()

        oneformer_for_universal_segmentation.model = oneformer
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"video_token=""<video>"",","        chat_template=None,
        patch_size=None,
        vision_feature_select_strategy=None,
        video_token=""<video>"",
        image_token=""<image>"",
        num_additional_image_tokens=0,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<image>"",","        patch_size=None,
        vision_feature_select_strategy=None,
        video_token=""<video>"",
        image_token=""<image>"",
        num_additional_image_tokens=0,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_focalnet_config(model_name)
    model = FocalNetForImageClassification(config)
    model.eval()

    # load state dict
    model.load_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"from_model = timm.create_model(name, pretrained=True).eval()","def convert_weight_and_push(name: str, config: ResNetConfig, save_directory: Path, push_to_hub: bool = True):
    print(f""Converting {name}..."")
    with torch.no_grad():
        from_model = timm.create_model(name, pretrained=True).eval()
        our_model = ResNetForImageClassification(config).eval()
        module_transfer = ModuleTransfer(src=from_model, dest=our_model)
        x = torch.randn((1, 3, 224, 224))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = ResNetForImageClassification(config).eval(),"    print(f""Converting {name}..."")
    with torch.no_grad():
        from_model = timm.create_model(name, pretrained=True).eval()
        our_model = ResNetForImageClassification(config).eval()
        module_transfer = ModuleTransfer(src=from_model, dest=our_model)
        x = torch.randn((1, 3, 224, 224))
        module_transfer(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model = original_model.eval(),"    tolerance: float,
):
    with torch.no_grad():
        original_model = original_model.eval()
        our_model = our_model.eval()

        im = prepare_img()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = our_model.eval(),"):
    with torch.no_grad():
        original_model = original_model.eval()
        our_model = our_model.eval()

        im = prepare_img()
        x = image_processor(images=im, return_tensors=""pt"")[""pixel_values""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model = OriginalMask2Former(**mask2former_kwargs).eval(),"
        original_config = setup_cfg(Args(config_file=config_file))
        mask2former_kwargs = OriginalMask2Former.from_config(original_config)
        original_model = OriginalMask2Former(**mask2former_kwargs).eval()

        DetectionCheckpointer(original_model).load(str(checkpoint_file))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
mask2former = Mask2FormerModel(config=config).eval(),"        DetectionCheckpointer(original_model).load(str(checkpoint_file))

        config: Mask2FormerConfig = OriginalMask2FormerConfigToOursConverter()(original_config)
        mask2former = Mask2FormerModel(config=config).eval()

        converter = OriginalMask2FormerCheckpointToOursConverter(original_model, config)
        mask2former = converter.convert(mask2former)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
mask2former_for_segmentation = Mask2FormerForUniversalSegmentation(config=config).eval(),"        converter = OriginalMask2FormerCheckpointToOursConverter(original_model, config)
        mask2former = converter.convert(mask2former)

        mask2former_for_segmentation = Mask2FormerForUniversalSegmentation(config=config).eval()
        mask2former_for_segmentation.model = mask2former

        mask2former_for_segmentation = converter.convert_universal_segmentation(mask2former_for_segmentation)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # Load HF model
    model = GroundingDinoForObjectDetection(config)
    model.eval()
    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    print(""Missing keys:"", missing_keys)
    print(""Unexpected keys:"", unexpected_keys)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"return pattern, mask, re.compile(""^"" + regex + ""$"")","        regex = regex.replace(field, field_regex)
    # Make sure we didn't miss any of the fields.
    assert ""%"" not in regex, regex
    return pattern, mask, re.compile(""^"" + regex + ""$"")


def _process_date_patterns():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"_NUMBER_PATTERN = re.compile(r""((^|\s)[+-])?((\.\d+)|(\d+(,\d\d\d)*(\.\d*)?))"")","
_ORDINAL_SUFFIXES = [""st"", ""nd"", ""rd"", ""th""]

_NUMBER_PATTERN = re.compile(r""((^|\s)[+-])?((\.\d+)|(\d+(,\d\d\d)*(\.\d*)?))"")

# Following DynSp:
# https://github.com/Microsoft/DynSP/blob/master/util.py#L293.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        empty_token=""[EMPTY]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        empty_token=""[EMPTY]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        empty_token=""[EMPTY]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"empty_token=""[EMPTY]"",","        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        empty_token=""[EMPTY]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        cell_trim_length: int = -1,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, weights_only=True)","        hf_wav2vec = Data2VecAudioModel(config)
        data2vec_checkpoint_dir = os.path.dirname(checkpoint_path)

        state_dict = torch.load(checkpoint_path, weights_only=True)
        state_dict[""model""][""final_proj.weight""] = state_dict[""model""].pop(""final_proj.0.weight"")
        state_dict[""model""][""final_proj.bias""] = state_dict[""model""].pop(""final_proj.0.bias"")
        converted_ckpt = os.path.join(data2vec_checkpoint_dir, ""converted.pt"")",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
return model[0].eval(),"
    def load_data2vec(path):
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([path])
        return model[0].eval()

    model = load_data2vec(converted_ckpt)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_wav2vec.eval(),"    #    input_values = inputs.input_values[:, :-1]
    #    attention_mask = inputs.attention_mask[:, :-1]

    hf_wav2vec.eval()
    model.eval()
    if is_finetuned:
        their_output = model(source=input_values, padding_mask=(1 - attention_mask), mask=False, features_only=True)[",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    #    attention_mask = inputs.attention_mask[:, :-1]

    hf_wav2vec.eval()
    model.eval()
    if is_finetuned:
        their_output = model(source=input_values, padding_mask=(1 - attention_mask), mask=False, features_only=True)[
            ""encoder_out""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(args.beit_checkpoint, map_location=""cpu"", weights_only=True)","    )
    patch_size = model.patch_embed.patch_size
    args.window_size = (args.input_size // patch_size[0], args.input_size // patch_size[1])
    checkpoint = torch.load(args.beit_checkpoint, map_location=""cpu"", weights_only=True)

    print(f""Load ckpt from {args.beit_checkpoint}"")
    checkpoint_model = None",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
orig_model.eval(),"
    # 2. Load Beit model
    orig_model = load_beit_model(args, is_finetuned, is_large)
    orig_model.eval()

    # 3. Forward Beit model
    image_processor = BeitImageProcessor(size=config.image_size, do_center_crop=False)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    # 4. Load HF Data2VecVision model
    if is_finetuned:
        hf_model = Data2VecVisionForImageClassification(config)
        hf_model.eval()
        has_lm_head = False
        hf_prefix = ""data2vec_vision.""
    else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"        hf_prefix = ""data2vec_vision.""
    else:
        hf_model = Data2VecVisionModel(config)
        hf_model.eval()
        has_lm_head = True
        hf_prefix = """"
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
data2vec.eval()  # disable dropout,"    data2vec = Data2VecTextModel.from_pretrained(
        data2vec_checkpoint_dir, checkpoint_file=data2vec_checkpoint_file_name
    )
    data2vec.eval()  # disable dropout
    data2vec_model = data2vec.models[0]
    data2vec_sent_encoder = data2vec_model.encoder.sentence_encoder
    config = Data2VecTextConfig(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    print(""Our BERT config:"", config)

    model = Data2VecTextForSequenceClassification(config) if classification_head else Data2VecTextForMaskedLM(config)
    model.eval()

    # Now let's copy all the weights.
    # Embeddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"original_state_dict = torch.load(mamba_checkpoint_path, map_location=""cpu"", weights_only=True)","        )
    logger.info(f""Loading model from {mamba_checkpoint_path} based on config from {config_json_file}"")
    # Load weights and config from paths
    original_state_dict = torch.load(mamba_checkpoint_path, map_location=""cpu"", weights_only=True)
    with open(config_json_file, ""r"", encoding=""utf-8"") as json_file:
        original_ssm_config_dict = json.load(json_file)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"    model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task(
        [checkpoint_path], arg_overrides={""data"": ""/"".join(dict_path.split(""/"")[:-1])}
    )
    model = model[0].eval()

    # set weights for wav2vec2 encoder
    hf_encoder = Wav2Vec2Model(encoder_config)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        checkpoints.


        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you need to first set it back in training mode with `model.train()`.

        Params:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"            ""load_pretrained_decoder_from"": None,
        },
    )
    model = model[0].eval()

    # load feature extractor
    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(encoder_config_path, token_token=True)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)","        filename=""zoedepth_pixel_values.pt"",
        repo_type=""dataset"",
    )
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)
    assert torch.allclose(pixel_values, original_pixel_values)

    # verify logits",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"cats_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)","        repo_type=""dataset"",
        revision=""1865dbb81984f01c89e83eec10f8d07efd10743d"",
    )
    cats_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)
    depth = model(cats_pixel_values).predicted_depth

    # Verify logits",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"    original_model = torch.hub.load(
        ""NielsRogge/ZoeDepth:understanding_zoedepth"", model_name, pretrained=True, force_reload=True
    )
    original_model.eval()
    state_dict = original_model.state_dict()

    print(""Original state dict:"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = ZoeDepthForDepthEstimation(config)
    model.load_state_dict(state_dict)
    model.eval()

    # verify image processor
    image = prepare_img()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        tokenizer_file=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        tokenizer_file=None,
        src_lang=""eng"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        tokenizer_file=None,
        src_lang=""eng"",
        tgt_lang=""fra"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        src_lang=""eng"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        src_lang=""eng"",
        tgt_lang=""fra"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        src_lang=""eng"",
        tgt_lang=""fra"",
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"
    logger.info(f""model loaded: {round(n_params / 1e6, 1)}M params"")

    hf_model.eval()
    hf_model.to(device)
    del state_dict
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        vocab_file,
        merges_file,
        tokenizer_file=None,
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        merges_file,
        tokenizer_file=None,
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        tokenizer_file=None,
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        bos_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        bos_token=""<s>"",
        do_lowercase_and_remove_accent=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        bos_token=""<s>"",
        do_lowercase_and_remove_accent=False,
        additional_special_tokens=[",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        bos_token=""<s>"",
        do_lowercase_and_remove_accent=False,
        additional_special_tokens=[
            ""<special0>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        merges_file=None,
        tokenizer_file=None,
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        tokenizer_file=None,
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sep_token=""</s>"",
        **kwargs,
    ):
        super().__init__(",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        cls_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        cls_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"original_state_dict = torch.load(state_dict_path, mmap=True)","
    model = DINOv3ViTModel(config).eval()
    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])
    original_state_dict = torch.load(state_dict_path, mmap=True)

    original_state_dict = split_qkv(original_state_dict)
    original_keys = list(original_state_dict.keys())",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_state_dict = torch.load(state_dict_path, mmap=True)","
    model = DINOv3ViTModel(config).eval()
    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])
    original_state_dict = torch.load(state_dict_path, mmap=True)

    original_state_dict = split_qkv(original_state_dict)
    original_keys = list(original_state_dict.keys())",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = DINOv3ViTModel(config).eval(),"    model_name = args.model_name
    config = get_dinov3_config(model_name)

    model = DINOv3ViTModel(config).eval()
    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])
    original_state_dict = torch.load(state_dict_path, mmap=True)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model.eval(),"        converted_state_dict[new_key] = weight_tensor

    model.load_state_dict(converted_state_dict, strict=True)
    model = model.eval()

    transform = get_transform()
    image_processor = get_image_processor()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""hidden_states"": GraniteMoeHybridDecoderLayer,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"    model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task(
        [checkpoint_path], arg_overrides={""data"": ""/"".join(dict_path.split(""/"")[:-1])}
    )
    model = model[0].eval()

    recursively_load_weights(model, hf_wav2vec)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    """"""
    Copy/paste/tweak model's weights to transformers design.
    """"""
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    downstream_dict = checkpoint[""Downstream""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","    def __init__(
        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=100,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=100,
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=100,
        additional_special_tokens=None,
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=100,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=100,
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        tokenizer_file=None,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=100,
        additional_special_tokens=None,
        add_prefix_space=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"audio_token=""<|audio|>"",","        self,
        audio_processor,
        tokenizer,
        audio_token=""<|audio|>"",
        chat_template=None,
    ):
        self.audio_token = tokenizer.audio_token if hasattr(tokenizer, ""audio_token"") else audio_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
state_dict = torch.load(,"
def load_states_from_checkpoint(model_file: str) -> CheckpointState:
    print(f""Reading saved model from {model_file}"")
    state_dict = torch.load(
        model_file, map_location=lambda s, l: default_restore_location(s, ""cpu""), weights_only=True
    )
    return CheckpointState(**state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"pattern = re.compile(""|"".join(map(re.escape, REPLACEMENT.keys())))","    state_dict = {}
    for k, v in model_state_dict.items():
        k = ""model."" + k
        pattern = re.compile(""|"".join(map(re.escape, REPLACEMENT.keys())))
        key = pattern.sub(lambda match: REPLACEMENT[match.group(0)], k)
        if ""conv_1d.weight"" in key:
            v = v[:, None, :].transpose(0, 2)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(model_file, map_location=""cpu"", weights_only=True)","
    # 3. Download model file then convert state_dict
    model_file = hf_hub_download(repo_id, checkpoint_file)
    state_dict = torch.load(model_file, map_location=""cpu"", weights_only=True)
    state_dict = convert_state_dict(state_dict)

    # 4. Split in shards and save",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(os.path.join(output_dir, shard_file), weights_only=True)","        gc.collect()

        for shard_file in shard_files:
            state_dict = torch.load(os.path.join(output_dir, shard_file), weights_only=True)
            torch.save({k: v.cpu().clone() for k, v in state_dict.items()}, os.path.join(output_dir, shard_file))

    del state_dict",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(model_file, map_location=""cpu"", weights_only=True)","
    # 3. Download model file then convert state_dict
    model_file = hf_hub_download(repo_id, checkpoint_file)
    state_dict = torch.load(model_file, map_location=""cpu"", weights_only=True)
    state_dict = convert_state_dict(state_dict)

    # 4. Split in shards and save",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        # additional properties
        self.cls_token_box = cls_token_box",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=True,
        cls_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=True,
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=True,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=True,
        trim_offsets=True,
        cls_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state),"        if add_prefix_space:
            pre_tok_state = pre_tok_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
            decoder_state = decoder_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
        self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state)
        self.backend_tokenizer.decoder = pickle.loads(decoder_state)

        self.add_prefix_space = add_prefix_space",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.backend_tokenizer.decoder = pickle.loads(decoder_state),"            pre_tok_state = pre_tok_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
            decoder_state = decoder_state.replace(b'""add_prefix_space"":false', b'""add_prefix_space"": true')
        self.backend_tokenizer.pre_tokenizer = pickle.loads(pre_tok_state)
        self.backend_tokenizer.decoder = pickle.loads(decoder_state)

        self.add_prefix_space = add_prefix_space
",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        add_prefix_space=False,
        clean_up_tokenization_spaces=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        add_prefix_space=False,
        clean_up_tokenization_spaces=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location=""cpu"", weights_only=True)","            for i in range(pretraining_tp):
                # load all TP files
                f_name = file.replace(""model_00"", f""model_0{i}"")
                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location=""cpu"", weights_only=True)

                # Rename keys in the transformers names
                keys = list(temp.keys())",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location=""cpu"", weights_only=True)","            for i in range(pretraining_tp):
                # load all TP files
                f_name = file.replace(""model_00"", f""model_0{i}"")
                temp = torch.load(os.path.join(bloom_checkpoint_path, f_name), map_location=""cpu"", weights_only=True)

                # Rename keys in the transformers names
                keys = list(temp.keys())",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenizer_file=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenizer_file=None,
        src_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenizer_file=None,
        src_lang=None,
        tgt_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        src_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        src_lang=None,
        tgt_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        src_lang=None,
        tgt_lang=None,
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    model = CLIPSegForImageSegmentation(config)
    model.eval()

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    # remove some keys
    for key in state_dict.copy():",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"def convert_clipseg_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_path, push_to_hub):
    config = get_clipseg_config(model_name)
    model = CLIPSegForImageSegmentation(config)
    model.eval()

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"prefix_token=""<PRE>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"middle_token=""<MID>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"suffix_token=""<SUF>"",","        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",
        suffix_first=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eot_token=""<EOT>"",","        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",
        suffix_first=False,
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"fill_token=""<FILL_ME>"",","        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",
        suffix_first=False,
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"prefix_token=""<PRE>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"middle_token=""<MID>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"suffix_token=""<SUF>"",","        eos_token=""</s>"",
        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eot_token=""<EOT>"",","        prefix_token=""<PRE>"",
        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",
        additional_special_tokens=None,
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"fill_token=""<FILL_ME>"",","        middle_token=""<MID>"",
        suffix_token=""<SUF>"",
        eot_token=""<EOT>"",
        fill_token=""<FILL_ME>"",
        additional_special_tokens=None,
        add_bos_token=True,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model = EfficientNetForImageClassification(config).eval(),"
    # Load HuggingFace model
    config = get_efficientnet_config(model_name)
    hf_model = EfficientNetForImageClassification(config).eval()
    hf_params = hf_model.state_dict()

    # Create src-to-dst parameter name mapping dictionary",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    inputs = preprocessor(images=prepare_img(), return_tensors=""pt"")

    # HF model inference
    hf_model.eval()
    with torch.no_grad():
        outputs = hf_model(**inputs)
    hf_logits = outputs.logits.detach().numpy()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):","
    vocab_files_names = VOCAB_FILES_NAMES

    def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):
        with open(vocab_file, encoding=""utf-8"") as vocab_handle:
            self.vocab = json.load(vocab_handle)
        self.decoder = {v: k for k, v in self.vocab.items()}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):","
    vocab_files_names = VOCAB_FILES_NAMES

    def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):
        with open(vocab_file, encoding=""utf-8"") as vocab_handle:
            self.vocab = json.load(vocab_handle)
        self.decoder = {v: k for k, v in self.vocab.items()}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):","
    vocab_files_names = VOCAB_FILES_NAMES

    def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):
        with open(vocab_file, encoding=""utf-8"") as vocab_handle:
            self.vocab = json.load(vocab_handle)
        self.decoder = {v: k for k, v in self.vocab.items()}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):","
    vocab_files_names = VOCAB_FILES_NAMES

    def __init__(self, vocab_file, unk_token=""[GO]"", bos_token=""[GO]"", eos_token=""[s]"", pad_token=""[GO]"", **kwargs):
        with open(vocab_file, encoding=""utf-8"") as vocab_handle:
            self.vocab = json.load(vocab_handle)
        self.decoder = {v: k for k, v in self.vocab.items()}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = InformerModel.from_pretrained(""huggingface/informer-tourism-monthly"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = InformerForPrediction.from_pretrained(
        ...     ""huggingface/informer-tourism-monthly""",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = InformerModel.from_pretrained(""huggingface/informer-tourism-monthly"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = InformerForPrediction.from_pretrained(
        ...     ""huggingface/informer-tourism-monthly""",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.eod_token = ""</doc>""","    def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwargs):
        tokenizer.return_token_type_ids = False

        self.eod_token = ""</doc>""

        self.boi_token = ""<image>""
        self.eoi_token = ""</image>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.boi_token = ""<image>""","
        self.eod_token = ""</doc>""

        self.boi_token = ""<image>""
        self.eoi_token = ""</image>""

        self.eoc_token = ""</chunk>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.eoi_token = ""</image>""","        self.eod_token = ""</doc>""

        self.boi_token = ""<image>""
        self.eoi_token = ""</image>""

        self.eoc_token = ""</chunk>""
        self.eol_token = ""</line>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.eoc_token = ""</chunk>""","        self.boi_token = ""<image>""
        self.eoi_token = ""</image>""

        self.eoc_token = ""</chunk>""
        self.eol_token = ""</line>""

        self.bop_token = ""<phrase>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.eol_token = ""</line>""","        self.eoi_token = ""</image>""

        self.eoc_token = ""</chunk>""
        self.eol_token = ""</line>""

        self.bop_token = ""<phrase>""
        self.eop_token = ""</phrase>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.bop_token = ""<phrase>""","        self.eoc_token = ""</chunk>""
        self.eol_token = ""</line>""

        self.bop_token = ""<phrase>""
        self.eop_token = ""</phrase>""

        self.boo_token = ""<object>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.eop_token = ""</phrase>""","        self.eol_token = ""</line>""

        self.bop_token = ""<phrase>""
        self.eop_token = ""</phrase>""

        self.boo_token = ""<object>""
        self.eoo_token = ""</object>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.boo_token = ""<object>""","        self.bop_token = ""<phrase>""
        self.eop_token = ""</phrase>""

        self.boo_token = ""<object>""
        self.eoo_token = ""</object>""

        self.dom_token = ""</delimiter_of_multi_objects/>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.eoo_token = ""</object>""","        self.eop_token = ""</phrase>""

        self.boo_token = ""<object>""
        self.eoo_token = ""</object>""

        self.dom_token = ""</delimiter_of_multi_objects/>""
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.dom_token = ""</delimiter_of_multi_objects/>""","        self.boo_token = ""<object>""
        self.eoo_token = ""</object>""

        self.dom_token = ""</delimiter_of_multi_objects/>""

        self.grd_token = ""<grounding>""
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.grd_token = ""<grounding>""","
        self.dom_token = ""</delimiter_of_multi_objects/>""

        self.grd_token = ""<grounding>""

        self.tag_tokens = [
            self.eod_token,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model = BlipForConditionalGeneration(config).eval(),"    else:
        config = BlipConfig(projection_dim=512, text_config={}, vision_config={})

    hf_model = BlipForConditionalGeneration(config).eval()

    model_url = ""https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
pt_model = pt_model.eval(),"    model_url = ""https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth""

    pt_model = blip_decoder(pretrained=model_url, image_size=384, vit=""base"")
    pt_model = pt_model.eval()

    modified_state_dict = pt_model.state_dict()
    for key in modified_state_dict.copy():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
vqa_model.eval(),"    )

    vqa_model = blip_vqa(pretrained=model_url, image_size=image_size, vit=""base"")
    vqa_model.eval()

    modified_state_dict = vqa_model.state_dict()
    for key in modified_state_dict.copy():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
itm_model.eval(),"    model_url = ""https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth""

    itm_model = blip_itm(pretrained=model_url, image_size=image_size, vit=""base"")
    itm_model.eval()

    modified_state_dict = itm_model.state_dict()
    for key in modified_state_dict.copy():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_itm_model.eval(),"    ).input_ids

    hf_itm_model.load_state_dict(modified_state_dict)
    hf_itm_model.eval()

    out_itm = hf_itm_model(question_input_ids, image, use_itm_head=True)
    out = hf_itm_model(question_input_ids, image, use_itm_head=False)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        Instantiates an question encoder and a generator from one or two base classes of the library from pretrained
        model checkpoints.

        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you need to first set it back in training mode with `model.train()`.

        Params:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
passages = pickle.load(passages_file),"                ""you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.""
            )
        with open(passages_path, ""rb"") as passages_file:
            passages = pickle.load(passages_file)
        return passages

    def _deserialize_index(self):",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.index_id_to_db_id = pickle.load(metadata_file),"                ""you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.""
            )
        with open(resolved_meta_path, ""rb"") as metadata_file:
            self.index_id_to_db_id = pickle.load(metadata_file)
        assert len(self.index_id_to_db_id) == self.index.ntotal, (
            ""Deserialized index_id_to_db_id should match faiss index size""
        )",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    config = get_mobilevitv2_config(task_name, orig_config_path)

    # load original state_dict
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    # load huggingface model
    if task_name.startswith(""ade20k_"") or task_name.startswith(""voc_""):",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = MobileViTV2ForSemanticSegmentation(config).eval(),"
    # load huggingface model
    if task_name.startswith(""ade20k_"") or task_name.startswith(""voc_""):
        model = MobileViTV2ForSemanticSegmentation(config).eval()
        base_model = False
    else:
        model = MobileViTV2ForImageClassification(config).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = MobileViTV2ForImageClassification(config).eval(),"        model = MobileViTV2ForSemanticSegmentation(config).eval()
        base_model = False
    else:
        model = MobileViTV2ForImageClassification(config).eval()
        base_model = False

    # remove and rename some keys of load the original model",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)
        self.drop = nn.Dropout(config.embedding_dropout)

    @torch.compile(dynamic=True)
    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:
        return self.drop(self.norm(self.tok_embeddings(input_ids)))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"        self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)
        self.mlp = ModernBertMLP(config)

    @torch.compile(dynamic=True)
    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.mlp(self.mlp_norm(hidden_states))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"    def set_output_embeddings(self, new_embeddings: nn.Linear):
        self.decoder = new_embeddings

    @torch.compile(dynamic=True)
    def compiled_head(self, output: torch.Tensor) -> torch.Tensor:
        return self.decoder(self.head(output))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)
        self.drop = nn.Dropout(config.embedding_dropout)

    @torch.compile(dynamic=True)
    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:
        return self.drop(self.norm(self.tok_embeddings(input_ids)))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"        self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)
        self.mlp = ModernBertMLP(config)

    @torch.compile(dynamic=True)
    def compiled_mlp(self, hidden_states: torch.Tensor) -> torch.Tensor:
        return self.mlp(self.mlp_norm(hidden_states))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"    def set_output_embeddings(self, new_embeddings: nn.Linear):
        self.decoder = new_embeddings

    @torch.compile(dynamic=True)
    def compiled_head(self, output: torch.Tensor) -> torch.Tensor:
        return self.decoder(self.head(output))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")
        super().__init__(
            errors=errors,
            unk_token=unk_token,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"terminals = [re.compile(pattern, re.MULTILINE) for pattern in truncate_before_pattern]","            m = pattern.search(string, start_pos)
            return m.start() if m else -1

        terminals = [re.compile(pattern, re.MULTILINE) for pattern in truncate_before_pattern]

        prints = list(re.finditer(""^print"", completion, re.MULTILINE))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_prefix_space=False,
        add_bos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"terminals = [re.compile(pattern, re.MULTILINE) for pattern in truncate_before_pattern]","            m = pattern.search(string, start_pos)
            return m.start() if m else -1

        terminals = [re.compile(pattern, re.MULTILINE) for pattern in truncate_before_pattern]

        prints = list(re.finditer(""^print"", completion, re.MULTILINE))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        return_token_type_ids=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        return_token_type_ids=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
timm_model.eval(),"
    # load original model from timm
    timm_model = timm.create_model(deit_name, pretrained=True)
    timm_model.eval()

    # load state_dict of original model, remove and rename some keys
    state_dict = timm_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = DeiTForImageClassificationWithTeacher(config).eval(),"    read_in_q_k_v(state_dict, config, base_model)

    # load HuggingFace model
    model = DeiTForImageClassificationWithTeacher(config).eval()
    model.load_state_dict(state_dict)

    # Check outputs on an image, prepared by DeiTImageProcessor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    print(""Unexpected keys:"", unexpected_keys)
    assert missing_keys == []
    # assert unexpected_keys == [""pretrained.model.fc_norm.weight"", ""pretrained.model.fc_norm.bias""]
    model.eval()

    # Check outputs on an image
    # We set `keep_aspect_ratio=False` as our current BEiT does not support arbitrary window sizes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_url, map_location=""cpu"", weights_only=True)","    config, expected_shape = get_dpt_config(checkpoint_url)
    # load original state_dict from URL
    # state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=""cpu"")
    state_dict = torch.load(checkpoint_url, map_location=""cpu"", weights_only=True)
    # remove certain keys
    remove_ignore_keys_(state_dict)
    # rename keys",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_url, map_location=""cpu"", weights_only=True)","    config, expected_shape = get_dpt_config(checkpoint_url)
    # load original state_dict from URL
    # state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=""cpu"")
    state_dict = torch.load(checkpoint_url, map_location=""cpu"", weights_only=True)
    # remove certain keys
    remove_ignore_keys_(state_dict)
    # rename keys",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = DPTForSemanticSegmentation(config) if ""ade"" in checkpoint_url else DPTForDepthEstimation(config)
    model.load_state_dict(state_dict)
    model.eval()

    # Check outputs on an image
    size = 480 if ""ade"" in checkpoint_url else 384",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    print(""Missing keys:"", missing_keys)
    print(""Unexpected keys:"", unexpected_keys)
    model.eval()

    # Check outputs on an image
    processor = DPTImageProcessor(size={""height"": image_size, ""width"": image_size})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"        original_model = torch.hub.load(""facebookresearch/dinov2"", ""dinov2_vitg14"")
    else:
        raise NotImplementedError(""To do"")
    original_model.eval()
    backbone_state_dict = original_model.state_dict()

    # rename keys",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"        ""neck.fusion_stage.layers.0.residual_layer1.convolution1.weight"",
        ""neck.fusion_stage.layers.0.residual_layer1.convolution2.weight"",
    ]
    model.eval()

    # Verify image processor
    processor = DPTImageProcessor(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = DPTForSemanticSegmentation(config) if ""ade"" in checkpoint_url else DPTForDepthEstimation(config)
    model.load_state_dict(state_dict)
    model.eval()

    # Check outputs on an image
    size = 480 if ""ade"" in checkpoint_url else 384",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path], task=task)

    model = model[0].eval()

    recursively_load_weights(model, hf_wav2vec, not is_finetuned)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","                pad_token=target_dict.pad_word,
                bos_token=target_dict.bos_word,
                eos_token=target_dict.eos_word,
                word_delimiter_token=""|"",
                do_lower_case=False,
            )
            return_attention_mask = config.feat_extract_norm == ""layer""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    """"""
    Copy/paste/tweak model's weights to transformers design.
    """"""
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    downstream_dict = checkpoint[""Downstream""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",
        replace_word_delimiter_char="" "",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",
        replace_word_delimiter_char="" "",
        do_lower_case=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",
        replace_word_delimiter_char="" "",
        do_lower_case=False,
        target_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",
        do_lower_case=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",
        do_lower_case=False,
        do_normalize=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        word_delimiter_token=""|"",
        do_lower_case=False,
        do_normalize=False,
        return_attention_mask=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"espnet_checkpoint = torch.load(checkpoint_path, weights_only=True)","    # Prepare the model
    model = FastSpeech2ConformerModel(config)

    espnet_checkpoint = torch.load(checkpoint_path, weights_only=True)
    hf_compatible_state_dict = convert_espnet_state_dict_to_hf(espnet_checkpoint)

    model.load_state_dict(hf_compatible_state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<sos/eos>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<sos/eos>"",
        eos_token=""<sos/eos>"",
        pad_token=""<blank>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<sos/eos>"",","        self,
        vocab_file,
        bos_token=""<sos/eos>"",
        eos_token=""<sos/eos>"",
        pad_token=""<blank>"",
        unk_token=""<unk>"",
        should_strip_spaces=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<blank>"",","        vocab_file,
        bos_token=""<sos/eos>"",
        eos_token=""<sos/eos>"",
        pad_token=""<blank>"",
        unk_token=""<unk>"",
        should_strip_spaces=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        bos_token=""<sos/eos>"",
        eos_token=""<sos/eos>"",
        pad_token=""<blank>"",
        unk_token=""<unk>"",
        should_strip_spaces=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"espnet_checkpoint = torch.load(checkpoint_path, weights_only=True)","
    model = FastSpeech2ConformerModel(model_config)

    espnet_checkpoint = torch.load(checkpoint_path, weights_only=True)
    hf_compatible_state_dict = convert_espnet_state_dict_to_hf(espnet_checkpoint)
    model.load_state_dict(hf_compatible_state_dict)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"orig_checkpoint = torch.load(checkpoint_path, weights_only=True)","
    model = FastSpeech2ConformerHifiGan(config)

    orig_checkpoint = torch.load(checkpoint_path, weights_only=True)
    load_weights(orig_checkpoint, model, config)

    model.save_pretrained(pytorch_dump_folder_path)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, weights_only=True)","            raise ValueError(""Image size not supported"")

        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=filename, repo_type=""dataset"")
        original_pixel_values = torch.load(filepath, weights_only=True)
        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""siglip_input_ids.pt"", repo_type=""dataset"")
        original_input_ids = torch.load(filepath, weights_only=True)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, weights_only=True)","        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=filename, repo_type=""dataset"")
        original_pixel_values = torch.load(filepath, weights_only=True)
        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""siglip_input_ids.pt"", repo_type=""dataset"")
        original_input_ids = torch.load(filepath, weights_only=True)

        if ""i18n"" not in model_name:
            assert inputs.input_ids.tolist() == original_input_ids.tolist()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, weights_only=True)","            raise ValueError(""Image size not supported"")

        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=filename, repo_type=""dataset"")
        original_pixel_values = torch.load(filepath, weights_only=True)
        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""siglip_input_ids.pt"", repo_type=""dataset"")
        original_input_ids = torch.load(filepath, weights_only=True)
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, weights_only=True)","        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=filename, repo_type=""dataset"")
        original_pixel_values = torch.load(filepath, weights_only=True)
        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""siglip_input_ids.pt"", repo_type=""dataset"")
        original_input_ids = torch.load(filepath, weights_only=True)

        if ""i18n"" not in model_name:
            assert inputs.input_ids.tolist() == original_input_ids.tolist()",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = SiglipModel(config).eval(),"    read_in_q_k_v_head(state_dict, config)

    # Load HuggingFace model
    model = SiglipModel(config).eval()
    model.load_state_dict(state_dict)

    # Create processor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","    def __init__(
        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""</s>"",
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""</s>"",
        additional_special_tokens=None,
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""</s>"",","        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""</s>"",
        additional_special_tokens=None,
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        model_max_length=64,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"IMAGE_TOKEN = ""<image>""","    }


IMAGE_TOKEN = ""<image>""
EXTRA_TOKENS = [f""<loc{i:0>4}>"" for i in range(1024)] + [f""<seg{i:0>3}>"" for i in range(128)]

",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<im>"",","        prompt=""Prefix str""
        bos_token=""<s>"",
        image_seq_len=3,
        image_token=""<im>"",
    )
    The output will be:
    ""<im><im><im><s>Initial str""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"model = ColPaliForRetrieval(config=config).to(""cpu"").eval()","    config.is_composition = False

    # Load the untrained model
    model = ColPaliForRetrieval(config=config).to(""cpu"").eval()
    print(""Created model with new config and randomly initialized weights"")

    # NOTE: The model was initialized with float32 weights. We need to convert it to the desired precision.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",
        mask_token_sent=""<mask_1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        tokenizer_file=None,
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",
        mask_token_sent=""<mask_1>"",
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask_2>"",","        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",
        mask_token_sent=""<mask_1>"",
        additional_special_tokens=None,
        offset=103,  # entries 2 - 104 are only used for pretraining",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","    def __init__(
        self,
        vocab_file,
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",
        mask_token_sent=""<mask_1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",
        mask_token_sent=""<mask_1>"",
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask_2>"",","        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        mask_token=""<mask_2>"",
        mask_token_sent=""<mask_1>"",
        additional_special_tokens=None,
        offset=103,  # entries 2 - 104 are only used for pretraining",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","    tokenizer_class = (""Qwen2Tokenizer"", ""Qwen2TokenizerFast"")

    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","
    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, ""image_token_id"", None)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","
def load_xsum_checkpoint(checkpoint_path):
    """"""Checkpoint path should end in model.pt""""""
    sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    hub_interface = torch.hub.load(""pytorch/fairseq"", ""bart.large.cnn"").eval()
    hub_interface.model.load_state_dict(sd[""model""])
    return hub_interface",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"hub_interface = torch.hub.load(""pytorch/fairseq"", ""bart.large.cnn"").eval()","def load_xsum_checkpoint(checkpoint_path):
    """"""Checkpoint path should end in model.pt""""""
    sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    hub_interface = torch.hub.load(""pytorch/fairseq"", ""bart.large.cnn"").eval()
    hub_interface.model.load_state_dict(sd[""model""])
    return hub_interface
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bart = torch.hub.load(""pytorch/fairseq"", checkpoint_path).eval()","    Copy/paste/tweak model's weights to our BERT structure.
    """"""
    if not os.path.exists(checkpoint_path):
        bart = torch.hub.load(""pytorch/fairseq"", checkpoint_path).eval()
    else:
        bart = load_xsum_checkpoint(checkpoint_path)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = BartForSequenceClassification(config).eval(),"        state_dict[""model.shared.weight""] = state_dict[""model.decoder.embed_tokens.weight""]
        for src, dest in mnli_rename_keys:
            rename_key(state_dict, src, dest)
        model = BartForSequenceClassification(config).eval()
        model.load_state_dict(state_dict)
        fairseq_output = bart.predict(""mnli"", tokens, return_logits=True)
        new_model_outputs = model(tokens)[0]  # logits",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = BartModel(config).eval(),"        state_dict[""shared.weight""] = state_dict[""decoder.embed_tokens.weight""]
        fairseq_output = bart.extract_features(tokens)
        if hf_checkpoint_name == ""facebook/bart-large"":
            model = BartModel(config).eval()
            model.load_state_dict(state_dict)
            new_model_outputs = model(tokens).model[0]
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = BartForConditionalGeneration(config).eval()  # an existing summarization ckpt,"            model.load_state_dict(state_dict)
            new_model_outputs = model(tokens).model[0]
        else:
            model = BartForConditionalGeneration(config).eval()  # an existing summarization ckpt
            model.model.load_state_dict(state_dict)
            if hasattr(model, ""lm_head""):
                model.lm_head = make_linear_from_emb(model.model.shared)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]","

def convert_yoso_checkpoint(checkpoint_path, yoso_config_file, pytorch_dump_path):
    orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]
    config = YosoConfig.from_json_file(yoso_config_file)
    model = YosoForMaskedLM(config)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_checkpoint_helper(config.max_position_embeddings, orig_state_dict)

    print(model.load_state_dict(new_state_dict))
    model.eval()
    model.save_pretrained(pytorch_dump_path)

    print(f""Checkpoint successfully converted. Model saved at {pytorch_dump_path}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
>>> model = AutoModel.from_pretrained(checkpoint).eval(),"        >>> # Load model and image processor
        >>> checkpoint = ""timm/resnet50.a1_in1k""
        >>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
        >>> model = AutoModel.from_pretrained(checkpoint).eval()

        >>> # Preprocess image
        >>> inputs = image_processor(image)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
>>> model = AutoModelForImageClassification.from_pretrained(checkpoint).eval(),"        >>> # Load model and image processor
        >>> checkpoint = ""timm/resnet50.a1_in1k""
        >>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
        >>> model = AutoModelForImageClassification.from_pretrained(checkpoint).eval()

        >>> # Preprocess image
        >>> inputs = image_processor(image)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.eval()  # Janus's VQ model is frozen,"        self.quantize = JanusVQVAEVectorQuantizer(config)
        self.quant_conv = torch.nn.Conv2d(config.latent_channels, config.embed_dim, 1)
        self.post_quant_conv = torch.nn.Conv2d(config.embed_dim, config.latent_channels, 1)
        self.eval()  # Janus's VQ model is frozen
        self.decoder = JanusVQVAEDecoder(config)
        self.gradient_checkpointing = False
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"shard_dict = torch.load(shard_path, map_location=""cpu"")","        for shard_file in unique_shard_files:
            print(f""Loading shard {shard_file}..."")
            shard_path = os.path.join(input_path, shard_file)
            shard_dict = torch.load(shard_path, map_location=""cpu"")
            state_dict.update(shard_dict)

        return state_dict",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(single_file_path, map_location=""cpu"")","    # Single file model
    elif os.path.exists(single_file_path):
        print(""Loading single file model..."")
        return torch.load(single_file_path, map_location=""cpu"")

    else:
        raise ValueError(f""No model files found in {input_path}"")",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(path.join(mamba2_checkpoint_path, ckpt_name), map_location=""cpu"", weights_only=True)","

def load_state_dict_from_torch(mamba2_checkpoint_path: str, ckpt_name: str) -> dict[str, torch.Tensor]:
    return torch.load(path.join(mamba2_checkpoint_path, ckpt_name), map_location=""cpu"", weights_only=True)


def convert_ssm_config_to_hf_config(config_ssm: dict, mamba2_model_dict: dict) -> Mamba2Config:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"d = torch.load(checkpoint_path, weights_only=True)","

def convert_dialogpt_checkpoint(checkpoint_path: str, pytorch_dump_folder_path: str):
    d = torch.load(checkpoint_path, weights_only=True)
    d[NEW_KEY] = d.pop(OLD_KEY)
    os.makedirs(pytorch_dump_folder_path, exist_ok=True)
    torch.save(d, os.path.join(pytorch_dump_folder_path, WEIGHTS_NAME))",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = AutoformerModel.from_pretrained(""huggingface/autoformer-tourism-monthly"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = AutoformerForPrediction.from_pretrained(""huggingface/autoformer-tourism-monthly"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/tourism-monthly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> # check number of static real features
        >>> num_static_real_features = batch[""static_real_features""].shape[-1]",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<s>NOTUSED"", ""</s>NOTUSED"", ""<unk>NOTUSED""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<s>NOTUSED"", ""</s>NOTUSED"", ""<unk>NOTUSED""],
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<s>NOTUSED"", ""</s>NOTUSED"", ""<unk>NOTUSED""],
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<s>NOTUSED"", ""</s>NOTUSED"", ""<unk>NOTUSED""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<s>NOTUSED"", ""</s>NOTUSED"", ""<unk>NOTUSED""],
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<s>NOTUSED"", ""</s>NOTUSED"", ""<unk>NOTUSED""],
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
re_pattern = re.compile(re_str),") -> Iterable[tuple[str, np.ndarray]]:
    def generate_base_path(path: str, block_type: str) -> tuple[str, tuple[int, int]]:
        re_str = rf""{block_type}(\d+)/""
        re_pattern = re.compile(re_str)
        match = re.search(re_pattern, path).group(1)
        idx = abs(int(match)) - 1
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"files = torch.load(output, map_location=""cpu"", weights_only=True)","    # download original checkpoint, hosted on Google Drive
    output = ""pytorch_model.bin""
    gdown.cached_download(checkpoint_url, output, quiet=False)
    files = torch.load(output, map_location=""cpu"", weights_only=True)
    if ""model"" in files:
        state_dict = files[""model""]
    elif ""module"" in files:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"files = torch.load(output, map_location=""cpu"", weights_only=True)","    # download original checkpoint, hosted on Google Drive
    output = ""pytorch_model.bin""
    gdown.cached_download(checkpoint_url, output, quiet=False)
    files = torch.load(output, map_location=""cpu"", weights_only=True)
    if ""model"" in files:
        state_dict = files[""model""]
    elif ""module"" in files:",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_state_dict(state_dict, config)

    model.load_state_dict(new_state_dict)
    model.eval()

    # verify model on basic input
    image_processor = VideoMAEImageProcessor(image_mean=[0.5, 0.5, 0.5], image_std=[0.5, 0.5, 0.5])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    logger.info(""Converting model..."")

    # load original state dict
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    # rename keys
    for key in state_dict.copy():
        val = state_dict.pop(key)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = DeformableDetrForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    model.to(device)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(file_path, weights_only=True)","
    # download and load state_dict from hf repo
    file_path = hf_hub_download(hf_repo_id, ""depth_pro.pt"")
    loaded = torch.load(file_path, weights_only=True)

    print(""Converting model..."")
    all_keys = list(loaded.keys())",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(file_path, weights_only=True)","
    # download and load state_dict from hf repo
    file_path = hf_hub_download(hf_repo_id, ""depth_pro.pt"")
    loaded = torch.load(file_path, weights_only=True)

    print(""Converting model..."")
    all_keys = list(loaded.keys())",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model = AlignModel(config).eval(),"
    # Load HuggingFace model
    config = get_align_config()
    hf_model = AlignModel(config).eval()
    hf_params = hf_model.state_dict()

    # Create src-to-dst parameter name mapping dictionary",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    )

    # HF model inference
    hf_model.eval()
    with torch.no_grad():
        outputs = hf_model(**inputs)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.compile(),"    seq_length = 64
    tok = Tokenizer(seq_length)
    original_model = align.Align(""efficientnet-b7"", ""bert-base"", 640, seq_length, tok.get_vocab_size())
    original_model.compile()
    original_model.load_weights(checkpoint_path)

    tf_params = original_model.trainable_variables",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
@torch.compile(dynamic=True),"        self.norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)
        self.drop = nn.Dropout(config.embedding_dropout)

    @torch.compile(dynamic=True)
    def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:
        return self.drop(self.norm(self.tok_embeddings(input_ids)))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"model = torch.load(directory_path, map_location=""cpu"")[""model""]","    directory_path = hf_hub_download(repo_id=repo_id, filename=f""{model_name}.pth"")

    original_state_dict = {}
    model = torch.load(directory_path, map_location=""cpu"")[""model""]
    for key in model:
        original_state_dict[key] = model[key]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"model = torch.load(directory_path, map_location=""cpu"")[""model""]","    directory_path = hf_hub_download(repo_id=repo_id, filename=f""{model_name}.pth"")

    original_state_dict = {}
    model = torch.load(directory_path, map_location=""cpu"")[""model""]
    for key in model:
        original_state_dict[key] = model[key]
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # finally, create HuggingFace model and load state dict
    model.load_state_dict(state_dict)
    model.eval()

    # load image processor
    image_processor = RTDetrImageProcessor()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
timm_model.eval(),"
    # load original model from timm
    timm_model = timm.create_model(vit_name, pretrained=True)
    timm_model.eval()

    # detect unsupported ViT models in transformers
    # fc_norm is present",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"model = ViTModel(config, add_pooling_layer=False).eval()","
    # load HuggingFace model
    if base_model:
        model = ViTModel(config, add_pooling_layer=False).eval()
    else:
        model = ViTForImageClassification(config).eval()
    model.load_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = ViTForImageClassification(config).eval(),"    if base_model:
        model = ViTModel(config, add_pooling_layer=False).eval()
    else:
        model = ViTForImageClassification(config).eval()
    model.load_state_dict(state_dict)

    # Check outputs on an image, prepared by ViTImageProcessor/DeiTImageProcessor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"
    # load original model from torch hub
    original_model = torch.hub.load(""facebookresearch/dino:main"", model_name)
    original_model.eval()

    # load state_dict of original model, remove and rename some keys
    state_dict = original_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"model = ViTModel(config, add_pooling_layer=False).eval()","
    # load HuggingFace model
    if base_model:
        model = ViTModel(config, add_pooling_layer=False).eval()
    else:
        model = ViTForImageClassification(config).eval()
    model.load_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = ViTForImageClassification(config).eval(),"    if base_model:
        model = ViTModel(config, add_pooling_layer=False).eval()
    else:
        model = ViTForImageClassification(config).eval()
    model.load_state_dict(state_dict)

    # Check outputs on an image, prepared by ViTImageProcessor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)","
    # load original state dict
    if encoder_only:
        state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)
    else:
        state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)[""state_dict""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)[""state_dict""]","    if encoder_only:
        state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)
    else:
        state_dict = torch.load(checkpoint_path, map_location=torch.device(""cpu""), weights_only=True)[""state_dict""]

    # rename keys
    state_dict = rename_keys(state_dict, encoder_only=encoder_only)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    else:
        model = SegformerForSemanticSegmentation(config)
    model.load_state_dict(state_dict)
    model.eval()

    # forward pass
    outputs = model(pixel_values)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)","
    filename = model_name_to_filename[model_name]
    filepath = hf_hub_download(repo_id=""nielsr/vitmatte-checkpoints"", filename=filename, repo_type=""model"")
    state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)

    # rename keys
    for key in state_dict.copy():",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)","
    filename = model_name_to_filename[model_name]
    filepath = hf_hub_download(repo_id=""nielsr/vitmatte-checkpoints"", filename=filename, repo_type=""model"")
    state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)

    # rename keys
    for key in state_dict.copy():",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # create model
    processor = VitMatteImageProcessor()
    model = VitMatteForImageMatting(config)
    model.eval()

    # load state dict
    model.load_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = PaliGemmaForConditionalGeneration(config).to(device).eval(),"        state_dict_transformers = slice_state_dict(state_dict, config)
        del state_dict
        del config.hidden_size  # this key is unused
        model = PaliGemmaForConditionalGeneration(config).to(device).eval()
        model.load_state_dict(state_dict_transformers)
        del state_dict_transformers
        model.config.text_config._attn_implementation = ""sdpa""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
.eval(),"        model = (
            PaliGemmaForConditionalGeneration.from_pretrained(pytorch_dump_folder_path, attn_implementation=""sdpa"")
            .to(device)
            .eval()
        )

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"IMAGE_TOKEN = ""<image>""","
logger = logging.get_logger(__name__)

IMAGE_TOKEN = ""<image>""
EXTRA_TOKENS = [f""<loc{i:0>4}>"" for i in range(1024)] + [f""<seg{i:0>3}>"" for i in range(128)]

",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<im>"",","        prompt=""Prefix str""
        bos_token=""<s>"",
        image_seq_len=3,
        image_token=""<im>"",
    )
    The output will be:
    ""<im><im><im><s>Initial str""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model = PaliGemmaForConditionalGeneration(config).to(device).eval(),"        state_dict_transformers = slice_state_dict(state_dict, config)
        del state_dict

        model = PaliGemmaForConditionalGeneration(config).to(device).eval()
        model.load_state_dict(state_dict_transformers)
        del state_dict_transformers
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
.eval(),"        model = (
            PaliGemmaForConditionalGeneration.from_pretrained(pytorch_dump_folder_path, attn_implementation=""sdpa"")
            .to(device)
            .eval()
        )
    model.config.text_config._attn_implementation = ""sdpa""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_attention_backend = True
    _supports_flash_attn = True
    _supports_sdpa = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _can_record_outputs = {
        ""hidden_states"": DbrxBlock,
        ""attentions"": DbrxAttention,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_attention_backend = True
    _supports_flash_attn = True
    _supports_sdpa = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _can_record_outputs = {
        ""hidden_states"": DbrxBlock,
        ""attentions"": DbrxAttention,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        checkpoints.


        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you need to first set it back in training mode with `model.train()`.

        Params:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        merges_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        merges_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",
        additional_special_tokens=[",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""</s>"",","        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",
        additional_special_tokens=[
            ""<special0>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<special1>"",","        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",
        additional_special_tokens=[
            ""<special0>"",
            ""<special1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"chkpt = torch.load(xlm_checkpoint_path, map_location=""cpu"", weights_only=True)","
def convert_xlm_checkpoint_to_pytorch(xlm_checkpoint_path, pytorch_dump_folder_path):
    # Load checkpoint
    chkpt = torch.load(xlm_checkpoint_path, map_location=""cpu"", weights_only=True)

    state_dict = chkpt[""model""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
print(new_model.eval()),"
    # Integration test - should load without any errors ;)
    new_model = BertForMaskedLM.from_pretrained(pytorch_dump_path)
    print(new_model.eval())

    print(""Model conversion was done successfully!"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token=""[IMG]"",","    processor = PixtralProcessor(
        tokenizer=tokenizer,
        image_processor=image_processor,
        image_token=""[IMG]"",
        patch_size=patch_size,
        chat_template=chat_template,
        spatial_merge_size=spatial_merge_size,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        library from pretrained model checkpoints.


        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you need to first set it back in training mode with `model.train()`.

        Params:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
decoder = MusicgenForCausalLM(decoder_config).eval(),"
    text_encoder = T5EncoderModel.from_pretrained(""google-t5/t5-base"")
    audio_encoder = EncodecModel.from_pretrained(""facebook/encodec_32khz"")
    decoder = MusicgenForCausalLM(decoder_config).eval()

    # load all decoder weights - expect that we'll be missing embeddings and enc-dec projection
    missing_keys, unexpected_keys = decoder.load_state_dict(decoder_state_dict, strict=False)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","def convert_fairseq_plbart_checkpoint_from_disk(
    checkpoint_path, hf_config_path=""uclanlp/plbart-base"", finetuned=False, classification=False
):
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    remove_ignore_keys_(state_dict)
    vocab_size = state_dict[""encoder.embed_tokens.weight""].shape[0]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        language_codes=""base"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        language_codes=""base"",
        tokenizer_file=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        language_codes=""base"",
        tokenizer_file=None,
        src_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
).eval(),"        model_name_or_path,
        dtype=torch.bfloat16,
        trust_remote_code=True,
    ).eval()

    return model.state_dict()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
.eval(),"            args.save_dir,
            dtype=torch.bfloat16,
        )
        .eval()
        .to(""cuda:0"")
    )
    processor = AutoProcessor.from_pretrained(args.save_dir)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<image>"",","        image_processor=None,
        tokenizer=None,
        chat_template=None,
        image_token=""<image>"",
        image_seq_length=256,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path], task=task)

    model = model[0].eval()

    recursively_load_weights(model, hf_wav2vec, not is_finetuned)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","                pad_token=target_dict.pad_word,
                bos_token=target_dict.bos_word,
                eos_token=target_dict.eos_word,
                word_delimiter_token=""|"",
                do_lower_case=False,
            )
            return_attention_mask = config.feat_extract_norm == ""layer""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing),"class AriaPreTrainedModel(LlamaPreTrainedModel):
    config: AriaConfig
    base_model_prefix = """"
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing)
    _supports_attention_backend = True

    def _init_weights(self, module):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (dynamic slicing)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""hidden_states"": AriaTextDecoderLayer,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
torch_model.eval(),"
def convert_clip_backbone(flax_params, torch_config):
    torch_model = CLIP(**torch_config)
    torch_model.eval()
    torch_clip_params = torch_model.state_dict()

    flax_clip_params = flatten_nested_dict(flax_params[""backbone""][""clip""])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_backbone = OwlViTModel(config).eval(),"    else:
        config = OwlViTConfig()

    hf_backbone = OwlViTModel(config).eval()
    hf_model = OwlViTForObjectDetection(config).eval()

    copy_text_model_and_projection(hf_backbone, pt_backbone)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model = OwlViTForObjectDetection(config).eval(),"        config = OwlViTConfig()

    hf_backbone = OwlViTModel(config).eval()
    hf_model = OwlViTForObjectDetection(config).eval()

    copy_text_model_and_projection(hf_backbone, pt_backbone)
    copy_vision_model_and_projection(hf_backbone, pt_backbone)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"tokenizer = CLIPTokenizer.from_pretrained(""openai/clip-vit-base-patch32"", pad_token=""!"", model_max_length=16)","        size=config.vision_config.image_size, crop_size=config.vision_config.image_size
    )
    # Initialize tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(""openai/clip-vit-base-patch32"", pad_token=""!"", model_max_length=16)

    # Initialize processor
    processor = OwlViTProcessor(image_processor=image_processor, tokenizer=tokenizer)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)","
    state_dict_path = hf_hub_download(old_state_dict_id, ""model_state_dict_7b.bin"")

    state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)
    state_dict = convert_state_dict_to_hf(state_dict)

    model.load_state_dict(state_dict, strict=True, assign=True)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)","
    state_dict_path = hf_hub_download(old_state_dict_id, ""model_state_dict_7b.bin"")

    state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)
    state_dict = convert_state_dict_to_hf(state_dict)

    model.load_state_dict(state_dict, strict=True, assign=True)",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"model = IJepaModel(config, add_pooling_layer=False).eval()","    read_in_q_k_v(state_dict, config)

    # load HuggingFace model
    model = IJepaModel(config, add_pooling_layer=False).eval()
    model.load_state_dict(state_dict)
    size = {""height"": config.image_size, ""width"": config.image_size}
    image_processor = ViTImageProcessor(size=size)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self._regex_to_remove_extra_special_tokens = re.compile(r""(\n?<global-img>\n?|<row_\d+_col_\d+>\n?)+"")","
        # This regex matches one or more occurrences of <global-img> tags (optionally surrounded by newline characters)
        # or <row_x_col_y> tags (where x and y are digits, also optionally surrounded by newline characters).
        self._regex_to_remove_extra_special_tokens = re.compile(r""(\n?<global-img>\n?|<row_\d+_col_\d+>\n?)+"")

        tokens_to_add = {
            ""additional_special_tokens"": [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/etth1-hourly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> model = PatchTSTModel.from_pretrained(""namctin/patchtst_etth1_pretrain"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/etth1-hourly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> # Config for random mask pretraining
        >>> config = PatchTSTConfig(",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
>>> batch = torch.load(file),"        >>> file = hf_hub_download(
        ...     repo_id=""hf-internal-testing/etth1-hourly-batch"", filename=""train-batch.pt"", repo_type=""dataset""
        ... )
        >>> batch = torch.load(file)

        >>> # Prediction task with 7 input channels and prediction length is 96
        >>> model = PatchTSTForPrediction.from_pretrained(""namctin/patchtst_etth1_forecast"")",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"model = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    """"""
    Copy/paste/tweak model's weights to our BERT structure.
    """"""
    model = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    sd = model[""model""]
    cfg = BlenderbotConfig.from_json_file(config_json_path)
    m = BlenderbotForConditionalGeneration(cfg)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
decoder = MusicgenMelodyForCausalLM(decoder_config).eval(),"
    text_encoder = T5EncoderModel.from_pretrained(""t5-base"")
    audio_encoder = EncodecModel.from_pretrained(""facebook/encodec_32khz"")
    decoder = MusicgenMelodyForCausalLM(decoder_config).eval()

    # load all decoder weights - expect that we'll be missing embeddings and enc-dec projection
    missing_keys, unexpected_keys = decoder.load_state_dict(decoder_state_dict, strict=False)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        library from pretrained model checkpoints.


        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you need to first set it back in training mode with `model.train()`.

        Params:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
clap_model.eval(),"def convert_clap_checkpoint(checkpoint_path, pytorch_dump_folder_path, config_path, model_type, enable_fusion=False):
    clap_model = init_clap(checkpoint_path, model_type, enable_fusion=enable_fusion)

    clap_model.eval()
    state_dict = clap_model.model.state_dict()
    state_dict = rename_state_dict(state_dict)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","    config = get_mobilevit_config(mobilevit_name)

    # load original state_dict
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    # load  model
    if mobilevit_name.startswith(""deeplabv3_""):",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = MobileViTForSemanticSegmentation(config).eval(),"
    # load  model
    if mobilevit_name.startswith(""deeplabv3_""):
        model = MobileViTForSemanticSegmentation(config).eval()
    else:
        model = MobileViTForImageClassification(config).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = MobileViTForImageClassification(config).eval(),"    if mobilevit_name.startswith(""deeplabv3_""):
        model = MobileViTForSemanticSegmentation(config).eval()
    else:
        model = MobileViTForImageClassification(config).eval()

    new_state_dict = convert_state_dict(state_dict, model)
    model.load_state_dict(new_state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"IMAGE_TOKEN = ""<image>""","    import torch


IMAGE_TOKEN = ""<image>""


class IdeficsTextKwargs(TextKwargs, total=False):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"fake_token = ""<fake_token_around_image>""","        if not any(isinstance(i, (list, tuple)) for i in prompts):
            prompts = [prompts]

        fake_token = ""<fake_token_around_image>""
        image_token = ""<image>""
        end_of_utterance_token = ""<end_of_utterance>""
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"image_token = ""<image>""","            prompts = [prompts]

        fake_token = ""<fake_token_around_image>""
        image_token = ""<image>""
        end_of_utterance_token = ""<end_of_utterance>""

        def image_tokens(last_was_image):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"end_of_utterance_token = ""<end_of_utterance>""","
        fake_token = ""<fake_token_around_image>""
        image_token = ""<image>""
        end_of_utterance_token = ""<end_of_utterance>""

        def image_tokens(last_was_image):
            if last_was_image:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"
    logger.info(f""model loaded: {round(n_params / 1e6, 1)}M params"")

    hf_model.eval()
    hf_model.to(device)
    del state_dict
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            chat_template=None,
            unk_token=""<unk>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            clean_up_tokenization_spaces=False,
            bos_token_id=original_tokenizer.bos_id(),",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"        checkpoint=original_checkpoint_name,
    )

    original_model.eval()
    original_state_dict = original_model.state_dict()
    # Don't need to remove head for MAE because original implementation doesn't have it on MAE
    if base_model:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    else:
        model = HieraForImageClassification(config)

    model.eval()

    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    print(""Missing keys:"", missing_keys)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = MobileNetV2ForSemanticSegmentation(config).eval(),"
    # Load  model
    if model_name.startswith(""deeplabv3_""):
        model = MobileNetV2ForSemanticSegmentation(config).eval()
    else:
        model = MobileNetV2ForImageClassification(config).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = MobileNetV2ForImageClassification(config).eval(),"    if model_name.startswith(""deeplabv3_""):
        model = MobileNetV2ForSemanticSegmentation(config).eval()
    else:
        model = MobileNetV2ForImageClassification(config).eval()

    # Load weights from TensorFlow checkpoint
    load_tf_weights_in_mobilenet_v2(model, config, checkpoint_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train,"        checkpoints.


        The model is set in evaluation mode by default using `model.eval()` (Dropout modules are deactivated). To train
        the model, you need to first set it back in training mode with `model.train()`.

        Params:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pt_weights = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""state_dict""]","
    hf_model = ChineseCLIPModel(config).eval()

    pt_weights = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""state_dict""]
    pt_weights = {(name.removeprefix(""module."")): value for name, value in pt_weights.items()}

    copy_text_model_and_projection(hf_model, pt_weights)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model = ChineseCLIPModel(config).eval(),"    assert config_path is not None, ""Please specify the ChineseCLIP model config of the corresponding model size.""
    config = ChineseCLIPConfig.from_pretrained(config_path)

    hf_model = ChineseCLIPModel(config).eval()

    pt_weights = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""state_dict""]
    pt_weights = {(name.removeprefix(""module."")): value for name, value in pt_weights.items()}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    model = VivitForVideoClassification(config)
    model.load_state_dict(new_state)
    model.eval()

    extractor = get_processor()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"loaded = torch.load(model_path, map_location=""cpu"")","        model_name,
    )
    print(f""Fetching all parameters from the checkpoint at {model_path}..."")
    loaded = torch.load(model_path, map_location=""cpu"")

    print(""Converting model..."")
    state_dict = {}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.audio_token = ""<|AUDIO|>""","        chat_template=None,
    ):
        if not hasattr(tokenizer, ""audio_token""):
            self.audio_token = ""<|AUDIO|>""
            self.audio_token_id = tokenizer.convert_tokens_to_ids(self.audio_token)
        else:
            self.audio_token = tokenizer.audio_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.audio_eos_token = ""<|audio_eos|>""","            self.audio_token_id = tokenizer.audio_token_id

        if not hasattr(tokenizer, ""audio_eos_token""):
            self.audio_eos_token = ""<|audio_eos|>""
            self.audio_eos_token_id = tokenizer.convert_tokens_to_ids(self.audio_eos_token)
        else:
            self.audio_eos_token = tokenizer.audio_eos_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"layer_re = re.compile(r""layers\.(\d+)\.([a-z0-9_.]+)\.([a-z]+)"")","    pp_size = megatron_args.pipeline_model_parallel_size
    dtype = torch.float32
    # The regex to extract layer names.
    layer_re = re.compile(r""layers\.(\d+)\.([a-z0-9_.]+)\.([a-z]+)"")

    # Convert.
    print(""Converting"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"layer_re = re.compile(r""transformer.h\.(\d+)\.([a-z0-9_.]+)\.([a-z]+)"")","
    num_layers = config.num_hidden_layers // args.target_pipeline_model_parallel_size

    layer_re = re.compile(r""transformer.h\.(\d+)\.([a-z0-9_.]+)\.([a-z]+)"")
    # The number of heads.
    heads = config.n_head
    # The hidden_size per head.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"input_state_dict = torch.load(args.path_to_checkpoint, map_location=""cpu"", weights_only=False)","            with checkpoint.open(""release/mp_rank_00/model_optim_rng.pt"") as pytorch_dict:
                input_state_dict = torch.load(pytorch_dict, map_location=""cpu"", weights_only=True)
    else:
        input_state_dict = torch.load(args.path_to_checkpoint, map_location=""cpu"", weights_only=False)

    ds_args = input_state_dict.get(""args"", None)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"input_state_dict = torch.load(pytorch_dict, map_location=""cpu"", weights_only=True)","    if args.path_to_checkpoint.endswith("".zip""):
        with zipfile.ZipFile(args.path_to_checkpoint, ""r"") as checkpoint:
            with checkpoint.open(""release/mp_rank_00/model_optim_rng.pt"") as pytorch_dict:
                input_state_dict = torch.load(pytorch_dict, map_location=""cpu"", weights_only=True)
    else:
        input_state_dict = torch.load(args.path_to_checkpoint, map_location=""cpu"", weights_only=False)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"layer_re = re.compile(r""layers\.(\d+)\.([a-z0-9_.]+)\.([a-z0-9_]+)"")","    transformer = lm[""transformer""] if ""transformer"" in lm else lm[""encoder""]

    # The regex to extract layer names.
    layer_re = re.compile(r""layers\.(\d+)\.([a-z0-9_.]+)\.([a-z0-9_]+)"")

    # The simple map of names for ""automated"" rules.
    megatron_to_transformers = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_upernet_config(model_name)
    model = UperNetForSemanticSegmentation(config)
    model.eval()

    # replace ""bn"" => ""batch_norm""
    for key in state_dict.copy():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_upernet_config(model_name)
    model = UperNetForSemanticSegmentation(config)
    model.eval()

    # replace ""bn"" => ""batch_norm""
    for key in state_dict.copy():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    encoder = ViTModel(encoder_config, add_pooling_layer=False)
    decoder = TrOCRForCausalLM(decoder_config)
    model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)
    model.eval()

    # load state_dict of original model, rename some keys
    state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=""cpu"", check_hash=True)[""model""]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.patcher.eval(),"        self.encoder_hash_tok_embedding = nn.Embedding(total_vocab_size, config.encoder_config.hidden_size)
        if self.config.patch_in_forward:
            self.patcher = BltPatcher(config.patcher_config)
            self.patcher.eval()
            for param in self.patcher.parameters():
                param.requires_grad = False
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.patcher.eval(),"        self.encoder_hash_tok_embedding = nn.Embedding(total_vocab_size, config.encoder_config.hidden_size)
        if self.config.patch_in_forward:
            self.patcher = BltPatcher(config.patcher_config)
            self.patcher.eval()
            for param in self.patcher.parameters():
                param.requires_grad = False
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"entropy_weights = torch.load(entropy_weights_path, map_location=""cpu"", weights_only=True)","def merge_weights(weights_path: str, entropy_weights_path: str) -> dict[str, torch.Tensor]:
    main_weights = load_file(weights_path)

    entropy_weights = torch.load(entropy_weights_path, map_location=""cpu"", weights_only=True)

    if ""model"" in entropy_weights:
        entropy_weights = entropy_weights[""model""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_state_dict = torch.load(checkpoint_path, weights_only=True, map_location=""cpu"")[""state_dict""]","
    print(f""Fetching all parameters from the checkpoint at {model_repo}/{file_name}..."")
    checkpoint_path = hf_hub_download(repo_id=model_repo, filename=file_name)
    original_state_dict = torch.load(checkpoint_path, weights_only=True, map_location=""cpu"")[""state_dict""]

    print(""Converting model..."")
    all_keys = list(original_state_dict.keys())",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_state_dict = torch.load(checkpoint_path, weights_only=True, map_location=""cpu"")[""state_dict""]","
    print(f""Fetching all parameters from the checkpoint at {model_repo}/{file_name}..."")
    checkpoint_path = hf_hub_download(repo_id=model_repo, filename=file_name)
    original_state_dict = torch.load(checkpoint_path, weights_only=True, map_location=""cpu"")[""state_dict""]

    print(""Converting model..."")
    all_keys = list(original_state_dict.keys())",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    preprocessor = EfficientLoFTRImageProcessor()
    inputs = preprocessor(images=images, return_tensors=""pt"").to(device)
    model.to(device)
    model.eval()
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"shard = torch.load(path, map_location=""cpu"", weights_only=True)","    """"""
    Load only the tensor objects from a checkpoint, skipping any BytesIO
    """"""
    shard = torch.load(path, map_location=""cpu"", weights_only=True)
    return {k: v for k, v in shard.items() if not isinstance(v, io.BytesIO)}

",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
self.semantic_model = AutoModel.from_config(config.semantic_model_config).eval(),"        self._adjust_dac_decoder(self.acoustic_decoder)
        self.encoder_semantic = SemanticEncoder(config)
        self.decoder_semantic = SemanticDecoder(config)
        self.semantic_model = AutoModel.from_config(config.semantic_model_config).eval()
        self.fc = nn.Linear(config.hidden_size, config.hidden_size)
        self.fc1 = nn.Linear(config.hidden_size, config.semantic_model_config.hidden_size)
        self.fc2 = nn.Linear(config.hidden_size, config.acoustic_model_config.hidden_size)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model[0].eval(),"    else:
        model, _, _ = fairseq.checkpoint_utils.load_model_ensemble_and_task([checkpoint_path])

    model = model[0].eval()

    recursively_load_weights(model, hf_unispeech, is_finetuned)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"word_delimiter_token=""|"",","                pad_token=target_dict.pad_word,
                bos_token=target_dict.bos_word,
                eos_token=target_dict.eos_word,
                word_delimiter_token=""|"",
                do_lower_case=False,
            )
            return_attention_mask = config.feat_extract_norm == ""layer""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
roberta.eval()  # disable dropout,"    Copy/paste/tweak roberta's weights to our BERT structure.
    """"""
    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)
    roberta.eval()  # disable dropout
    roberta_sent_encoder = roberta.model.encoder.sentence_encoder
    config = XLMRobertaConfig(
        vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    print(""Our RoBERTa config:"", config)

    model = XLMRobertaXLForSequenceClassification(config) if classification_head else XLMRobertaXLForMaskedLM(config)
    model.eval()

    # Now let's copy all the weights.
    # Embeddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases","        patch_size=None,
        vision_feature_select_strategy=None,
        chat_template=None,
        image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases
        num_additional_image_tokens=0,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)","
    # verify inputs
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""llava_1_6_pixel_values.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)
    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())

    if model_id == ""liuhaotian/llava-v1.6-mistral-7b"":",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)","
    if model_id == ""liuhaotian/llava-v1.6-mistral-7b"":
        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""llava_1_6_input_ids.pt"", repo_type=""dataset"")
        original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)
        # replace -200 by image_token_id (since we use token ID = 32000 for the image token)
        original_input_ids[original_input_ids == -200] = image_token_id
        assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)","        filepath = hf_hub_download(
            repo_id=""nielsr/test-image"", filename=""llava_1_6_34b_input_ids.pt"", repo_type=""dataset""
        )
        original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)
        # replace -200 by image_token_id
        original_input_ids[original_input_ids == -200] = image_token_id
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)","
    # verify inputs
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""llava_1_6_pixel_values.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)
    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())

    if model_id == ""liuhaotian/llava-v1.6-mistral-7b"":",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)","
    if model_id == ""liuhaotian/llava-v1.6-mistral-7b"":
        filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""llava_1_6_input_ids.pt"", repo_type=""dataset"")
        original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)
        # replace -200 by image_token_id (since we use token ID = 32000 for the image token)
        original_input_ids[original_input_ids == -200] = image_token_id
        assert original_input_ids[0].tolist() == inputs.input_ids[0].tolist()",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)","        filepath = hf_hub_download(
            repo_id=""nielsr/test-image"", filename=""llava_1_6_34b_input_ids.pt"", repo_type=""dataset""
        )
        original_input_ids = torch.load(filepath, map_location=""cpu"", weights_only=True)
        # replace -200 by image_token_id
        original_input_ids[original_input_ids == -200] = image_token_id
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    state_dict = load_original_state_dict(model_id)
    state_dict = convert_state_dict_to_hf(state_dict)
    model.load_state_dict(state_dict, assign=True)
    model.eval()

    pre_expansion_embeddings = model.language_model.model.embed_tokens.weight.data
    mu = torch.mean(pre_expansion_embeddings, dim=0).float()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","
    def __init__(
        self,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=125,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=125,
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        self,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=125,
        additional_special_tokens=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"question_token=""[QUESTION]"",","        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"question_token=""[QUESTION]"",","        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        question_token=""[QUESTION]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"ckpt = torch.load(longformer_question_answering_ckpt_path, map_location=torch.device(""cpu""), weights_only=True)","    longformer = LongformerModel.from_pretrained(longformer_model)
    lightning_model = LightningModel(longformer)

    ckpt = torch.load(longformer_question_answering_ckpt_path, map_location=torch.device(""cpu""), weights_only=True)
    lightning_model.load_state_dict(ckpt[""state_dict""])

    # init longformer question answering model",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
longformer_for_qa.eval(),"    # transfer weights
    longformer_for_qa.longformer.load_state_dict(lightning_model.model.state_dict())
    longformer_for_qa.qa_outputs.load_state_dict(lightning_model.qa_outputs.state_dict())
    longformer_for_qa.eval()

    # save model
    longformer_for_qa.save_pretrained(pytorch_dump_folder_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        cls_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        cls_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        cls_token_box=[0, 0, 0, 0],
        sep_token_box=[1000, 1000, 1000, 1000],
        pad_token_box=[0, 0, 0, 0],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # load HuggingFace model
    model = BeitForMaskedImageModeling(config) if has_lm_head else BeitForImageClassification(config)
    model.eval()
    model.load_state_dict(state_dict)

    # Check outputs on an image",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    config = get_yolos_config(yolos_name)

    # load original state_dict
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]

    # load  model
    model = YolosForObjectDetection(config)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # load  model
    model = YolosForObjectDetection(config)
    model.eval()
    new_state_dict = convert_state_dict(state_dict, model)
    model.load_state_dict(new_state_dict)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","
def load_checkpoint(checkpoint_path):
    """"""Checkpoint path should end in model.pt""""""
    sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    if ""model"" in sd:
        sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    """"""Checkpoint path should end in model.pt""""""
    sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    if ""model"" in sd:
        sd = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]

    # pop unnecessary weights
    keys_to_delete = [",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = OPTModel(config).half().eval(),"    else:
        config = OPTConfig()

    model = OPTModel(config).half().eval()
    model.load_state_dict(state_dict)

    # Check results",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]","

def convert_nystromformer_checkpoint(checkpoint_path, nystromformer_config_file, pytorch_dump_path):
    orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]
    config = NystromformerConfig.from_json_file(nystromformer_config_file)
    model = NystromformerForMaskedLM(config)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_checkpoint_helper(config, orig_state_dict)

    model.load_state_dict(new_state_dict)
    model.eval()
    model.save_pretrained(pytorch_dump_path)

    print(f""Checkpoint successfully converted. Model saved at {pytorch_dump_path}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_state_dict = torch.load(state_dict_path),"
    model = DINOv3ConvNextModel(config).eval()
    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])
    original_state_dict = torch.load(state_dict_path)
    original_keys = list(original_state_dict.keys())
    new_keys = convert_old_keys_to_new_keys(original_keys)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
original_state_dict = torch.load(state_dict_path),"
    model = DINOv3ConvNextModel(config).eval()
    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])
    original_state_dict = torch.load(state_dict_path)
    original_keys = list(original_state_dict.keys())
    new_keys = convert_old_keys_to_new_keys(original_keys)
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model = DINOv3ConvNextModel(config).eval(),"    config = get_dinov3_config(model_name)
    # print(config)

    model = DINOv3ConvNextModel(config).eval()
    state_dict_path = hf_hub_download(repo_id=HUB_MODELS[model_name], filename=HUB_CHECKPOINTS[model_name])
    original_state_dict = torch.load(state_dict_path)
    original_keys = list(original_state_dict.keys())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = model.eval(),"            continue
        converted_state_dict[new_key] = weight_tensor
    model.load_state_dict(converted_state_dict, strict=True)
    model = model.eval()

    transform = get_transform()
    image_processor = get_image_processor()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
>>> model.eval(),"        >>> model = DecisionTransformerModel.from_pretrained(""edbeeching/decision-transformer-gym-hopper-medium"")
        >>> # evaluation
        >>> model = model.to(device)
        >>> model.eval()

        >>> env = gym.make(""Hopper-v3"")
        >>> state_dim = env.observation_space.shape[0]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"        model = BeitForSemanticSegmentation(config)
    else:
        model = BeitForImageClassification(config)
    model.eval()
    model.load_state_dict(state_dict)

    # Check outputs on an image",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
state_dict = torch.load(,"    hf_config.save_pretrained(output_dir)

    # Load state dict of the original model and transfer to hf model
    state_dict = torch.load(
        path.join(mamba_ssm_checkpoint_path, ""pytorch_model.bin""),
        map_location=""cpu"",
        weights_only=True,",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(output, map_location=""cpu"", weights_only=True)[""model""]","    if ""drive"" in checkpoint_url:
        output = ""pytorch_model.bin""
        gdown.cached_download(checkpoint_url, output, quiet=False)
        state_dict = torch.load(output, map_location=""cpu"", weights_only=True)[""model""]
    else:
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)[""model""]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(output, map_location=""cpu"", weights_only=True)[""model""]","    if ""drive"" in checkpoint_url:
        output = ""pytorch_model.bin""
        gdown.cached_download(checkpoint_url, output, quiet=False)
        state_dict = torch.load(output, map_location=""cpu"", weights_only=True)[""model""]
    else:
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)[""model""]
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_xclip_config(model_name, num_frames)
    model = XCLIPModel(config)
    model.eval()

    if ""drive"" in checkpoint_url:
        output = ""pytorch_model.bin""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    model = XCLIPModel(config)
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == [""text_model.embeddings.position_ids"", ""vision_model.embeddings.position_ids""]
    model.eval()

    size = 336 if model_name == ""xclip-large-patch14-16-frames"" else 224
    image_processor = VideoMAEImageProcessor(size=size)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"original_weights = torch.load(cvt_file_name, map_location=torch.device(""cpu""), weights_only=True)","    model = CvtForImageClassification(config)
    image_processor = AutoImageProcessor.from_pretrained(""facebook/convnext-base-224-22k-1k"")
    image_processor.size[""shortest_edge""] = image_size
    original_weights = torch.load(cvt_file_name, map_location=torch.device(""cpu""), weights_only=True)

    huggingface_weights = OrderedDict()
    list_of_state_dict = []",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<sep>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<sep>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    model = DepthAnythingForDepthEstimation(config)
    model.load_state_dict(converted_state_dict)
    model.eval()

    processor = DPTImageProcessor(
        do_resize=True,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)","        filename=f""{filename}"",
    )

    state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)
    # rename keys
    rename_keys = create_rename_keys(config)
    for src, dest in rename_keys:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = DepthAnythingForDepthEstimation(config)
    model.load_state_dict(state_dict)
    model.eval()

    processor = DPTImageProcessor(
        do_resize=True,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = Siglip2Model(config).eval(),"
    # load HuggingFace model
    print(""Loading HuggingFace model..."")
    model = Siglip2Model(config).eval()
    model.load_state_dict(new_state_dict)

    # Create processor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        merges_file,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
roberta.eval()  # disable dropout,"    Copy/paste/tweak roberta's weights to our BERT structure.
    """"""
    roberta = FairseqRobertaModel.from_pretrained(roberta_checkpoint_path)
    roberta.eval()  # disable dropout
    roberta_sent_encoder = roberta.model.encoder.sentence_encoder
    config = RobertaConfig(
        vocab_size=roberta_sent_encoder.embed_tokens.num_embeddings,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    print(""Our BERT config:"", config)

    model = RobertaForSequenceClassification(config) if classification_head else RobertaForMaskedLM(config)
    model.eval()

    # Now let's copy all the weights.
    # Embeddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)[""state_dict""]","        filename=f""{filename}"",
    )

    state_dict = torch.load(filepath, map_location=""cpu"", weights_only=True)[""state_dict""]
    state_dict = {key[9:]: state_dict[key] for key in state_dict}

    # Convert state dict using mappings",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = PromptDepthAnythingForDepthEstimation(config)
    model.load_state_dict(new_state_dict, strict=False)
    model.eval()

    processor = PromptDepthAnythingImageProcessor(
        do_resize=True,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"img_line_break_token=""<|IMG_LINE_BREAK|>"",","        start_of_img_token=""<|START_OF_IMG|>"",
        end_of_img_token=""<|END_OF_IMG|>"",
        img_patch_token=""<|IMG_PATCH|>"",
        img_line_break_token=""<|IMG_LINE_BREAK|>"",
        tile_token=""TILE"",
        tile_global_token=""TILE_GLOBAL"",
        chat_template=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tile_token=""TILE"",","        end_of_img_token=""<|END_OF_IMG|>"",
        img_patch_token=""<|IMG_PATCH|>"",
        img_line_break_token=""<|IMG_LINE_BREAK|>"",
        tile_token=""TILE"",
        tile_global_token=""TILE_GLOBAL"",
        chat_template=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tile_global_token=""TILE_GLOBAL"",","        img_patch_token=""<|IMG_PATCH|>"",
        img_line_break_token=""<|IMG_LINE_BREAK|>"",
        tile_token=""TILE"",
        tile_global_token=""TILE_GLOBAL"",
        chat_template=None,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # load  model
    model = ASTForAudioClassification(config)
    model.eval()

    model.load_state_dict(new_state_dict)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
regex_pattern = re.compile(,"    """"""
    # Wherever, we notice the [TOKEN_OPEN_STRING, TOKEN_CLOSE_STRING], we split the prompt
    prompt_text_list: list = []
    regex_pattern = re.compile(
        f""({TOKEN_BBOX_OPEN_STRING}|{TOKEN_BBOX_CLOSE_STRING}|{TOKEN_POINT_OPEN_STRING}|{TOKEN_POINT_CLOSE_STRING})""
    )
    # Split by the regex pattern",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")","        safe_serialization=args.safe_serialization,
        ada_lib_path=args.ada_lib_path,
    )
    tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")
    tokenizer.save_pretrained(args.output_dir)

",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")","        safe_serialization=args.safe_serialization,
        ada_lib_path=args.ada_lib_path,
    )
    tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")
    tokenizer.save_pretrained(args.output_dir)

",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"model_dict = torch.load(checkpoint_path, ""cpu"", weights_only=True)","    sample_rate=16000,
    repo_id=None,
):
    model_dict = torch.load(checkpoint_path, ""cpu"", weights_only=True)

    config = DacConfig()
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
regex = re.compile(key),"    for name, value in orig_dict.items():
        is_used = False
        for key, mapped_key in MAPPING.items():
            regex = re.compile(key)
            if regex.search(name):
                if len(mapped_key) == 1:
                    if mapped_key[0][0] == ""q"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"original_checkpoint = torch.load(checkpoint_path, weights_only=True)","    )
    feature_extractor.save_pretrained(pytorch_dump_folder_path)

    original_checkpoint = torch.load(checkpoint_path, weights_only=True)
    if ""best_state"" in original_checkpoint:
        # we might have a training state saved, in which case discard the yaml results and just retain the weights
        original_checkpoint = original_checkpoint[""best_state""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        vocab_file,
        do_lower_case=False,
        split_by_punct=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        do_lower_case=False,
        split_by_punct=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        split_by_punct=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        tokenizer_file=None,
        do_lower_case=False,
        split_by_punct=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        do_lower_case=False,
        split_by_punct=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        split_by_punct=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ) -> None:
        super().__init__(",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"fairseq_checkpoint = torch.load(checkpoint_path, weights_only=True)","    processor = SpeechT5Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)
    processor.save_pretrained(pytorch_dump_folder_path)

    fairseq_checkpoint = torch.load(checkpoint_path, weights_only=True)
    recursively_load_weights(fairseq_checkpoint[""model""], model, task)

    model.save_pretrained(pytorch_dump_folder_path)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        normalize=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        normalize=False,
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        normalize=False,
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"orig_checkpoint = torch.load(checkpoint_path, weights_only=True)","
    model = SpeechT5HifiGan(config)

    orig_checkpoint = torch.load(checkpoint_path, weights_only=True)
    load_weights(orig_checkpoint[""model""][""generator""], model, config)

    stats = np.load(stats_path)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","    tokenizer_class = (""Qwen2Tokenizer"", ""Qwen2TokenizerFast"")

    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","
    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, ""image_token_id"", None)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model = MMGroundingDinoForObjectDetection(hf_cfg).eval(),"    print(""Creating model..."")
    hf_cfg = get_mm_grounding_dino_config(model_name)
    hf_state = convert_mm_to_hf_state(mm_state, hf_cfg)
    hf_model = MMGroundingDinoForObjectDetection(hf_cfg).eval()
    hf_model.load_state_dict(hf_state)
    hf_processor = get_mm_grounding_dino_processor()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.non_printing_characters_re = re.compile(,"        # fmt : on

        # Regular expression to remove non-printing characters (e.g. some unicode control chars) in preprocessing
        self.non_printing_characters_re = re.compile(
            f""[{''.join(map(chr, list(range(0, 9)) + list(range(11, 32)) + list(range(127, 160)) + [160, 173, 8203]))}]""
        )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pad_token = ""<pad>"" if pad_token is None else pad_token","            pad_token = unk_token if pad_token is None else pad_token
            bos_token = eos_token if bos_token is None else bos_token
        else:
            pad_token = ""<pad>"" if pad_token is None else pad_token
            bos_token = ""<s>"" if bos_token is None else bos_token

        self.do_lower_case = do_lower_case",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token = ""<s>"" if bos_token is None else bos_token","            bos_token = eos_token if bos_token is None else bos_token
        else:
            pad_token = ""<pad>"" if pad_token is None else pad_token
            bos_token = ""<s>"" if bos_token is None else bos_token

        self.do_lower_case = do_lower_case
        self.remove_space = remove_space",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","        raise FileNotFoundError(f""ERROR! could not find file {checkpoint_path}"")

    # Load the model.
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)

    # Load the config.
    config_megatron = checkpoint[""hyper_parameters""][""cfg""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(original_ckpt, map_location=""cpu"", weights_only=True)","        if original_ckpt.startswith(""https""):
            checkpoint = torch.hub.load_state_dict_from_url(original_ckpt, map_location=""cpu"", check_hash=True)
        else:
            checkpoint = torch.load(original_ckpt, map_location=""cpu"", weights_only=True)
    state_dict = checkpoint

    rename_keys = create_rename_keys(state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(original_ckpt, map_location=""cpu"", weights_only=True)","        if original_ckpt.startswith(""https""):
            checkpoint = torch.hub.load_state_dict_from_url(original_ckpt, map_location=""cpu"", check_hash=True)
        else:
            checkpoint = torch.load(original_ckpt, map_location=""cpu"", weights_only=True)
    state_dict = checkpoint

    rename_keys = create_rename_keys(state_dict)",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model = SwiftFormerForImageClassification(config).eval(),"        rename_key(state_dict, rename_key_src, rename_key_dest)

    # load HuggingFace model
    hf_model = SwiftFormerForImageClassification(config).eval()
    hf_model.load_state_dict(state_dict)

    # prepare test inputs",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,
    ):
        super().__init__(",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"    # load original model
    checkpoint_path = get_checkpoint(None, model_tag)
    original_model = NougatModel.from_pretrained(checkpoint_path)
    original_model.eval()

    # load HuggingFace model
    encoder_config, decoder_config = get_configs(original_model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    encoder = DonutSwinModel(encoder_config)
    decoder = MBartForCausalLM(decoder_config)
    model = VisionEncoderDecoderModel(encoder=encoder, decoder=decoder)
    model.eval()

    state_dict = original_model.state_dict()
    new_state_dict = convert_state_dict(state_dict, model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"tokenizer.pad_token = ""<pad>""","
    tokenizer_file = checkpoint_path / ""tokenizer.json""
    tokenizer = NougatTokenizerFast(tokenizer_file=str(tokenizer_file))
    tokenizer.pad_token = ""<pad>""
    tokenizer.bos_token = ""<s>""
    tokenizer.eos_token = ""</s>""
    tokenizer.unk_token = ""<unk>""",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tokenizer.bos_token = ""<s>""","    tokenizer_file = checkpoint_path / ""tokenizer.json""
    tokenizer = NougatTokenizerFast(tokenizer_file=str(tokenizer_file))
    tokenizer.pad_token = ""<pad>""
    tokenizer.bos_token = ""<s>""
    tokenizer.eos_token = ""</s>""
    tokenizer.unk_token = ""<unk>""
    tokenizer.model_max_length = original_model.config.max_length",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tokenizer.eos_token = ""</s>""","    tokenizer = NougatTokenizerFast(tokenizer_file=str(tokenizer_file))
    tokenizer.pad_token = ""<pad>""
    tokenizer.bos_token = ""<s>""
    tokenizer.eos_token = ""</s>""
    tokenizer.unk_token = ""<unk>""
    tokenizer.model_max_length = original_model.config.max_length
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tokenizer.unk_token = ""<unk>""","    tokenizer.pad_token = ""<pad>""
    tokenizer.bos_token = ""<s>""
    tokenizer.eos_token = ""</s>""
    tokenizer.unk_token = ""<unk>""
    tokenizer.model_max_length = original_model.config.max_length

    size = {""height"": original_model.config.input_size[0], ""width"": original_model.config.input_size[1]}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
timm_model.eval(),"
def convert_swinv2_checkpoint(swinv2_name, pytorch_dump_folder_path):
    timm_model = timm.create_model(swinv2_name, pretrained=True)
    timm_model.eval()

    config = get_swinv2_config(swinv2_name)
    model = Swinv2ForImageClassification(config)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    config = get_swinv2_config(swinv2_name)
    model = Swinv2ForImageClassification(config)
    model.eval()

    new_state_dict = convert_state_dict(timm_model.state_dict(), model)
    model.load_state_dict(new_state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"val = partial(lambda: (timm.create_model(x, pretrained=True).eval(), None))","        # default to timm!
        if x not in self:
            x = self.convert_name_to_timm(x)
            val = partial(lambda: (timm.create_model(x, pretrained=True).eval(), None))

        else:
            val = super().__getitem__(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = our_model_func(config).eval(),"    print(f""Converting {name}..."")
    with torch.no_grad():
        from_model, from_state_dict = from_model_func()
        our_model = our_model_func(config).eval()
        module_transfer = ModuleTransfer(src=from_model, dest=our_model, raise_if_mismatch=False)
        x = torch.randn((1, 3, 224, 224))
        module_transfer(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"return model.eval(), model_state_dict[""heads""]","        model_state_dict = files[""classy_state_dict""][""base_model""][""model""]
        state_dict = model_state_dict[""trunk""]
        model.load_state_dict(state_dict)
        return model.eval(), model_state_dict[""heads""]

    # pretrained
    names_to_from_model_map[""regnet-y-320-seer""] = partial(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
from_model = from_model.eval(),"    )

    with torch.no_grad():
        from_model = from_model.eval()
        our_model = our_model.eval()

        x = torch.randn((1, 3, 32, 32))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = our_model.eval(),"
    with torch.no_grad():
        from_model = from_model.eval()
        our_model = our_model.eval()

        x = torch.randn((1, 3, 32, 32))
        # trace both",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""-1"",","        vocab,
        default_velocity=77,
        num_bars=2,
        unk_token=""-1"",
        eos_token=""1"",
        pad_token=""0"",
        bos_token=""2"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""1"",","        default_velocity=77,
        num_bars=2,
        unk_token=""-1"",
        eos_token=""1"",
        pad_token=""0"",
        bos_token=""2"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""0"",","        num_bars=2,
        unk_token=""-1"",
        eos_token=""1"",
        pad_token=""0"",
        bos_token=""2"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""2"",","        unk_token=""-1"",
        eos_token=""1"",
        pad_token=""0"",
        bos_token=""2"",
        **kwargs,
    ):
        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"official_weights = torch.load(""./model-1999-val_0.67311615.ckpt"", weights_only=True)","
# This weights were downloaded from the official pop2piano repository
# https://huggingface.co/sweetcocoa/pop2piano/blob/main/model-1999-val_0.67311615.ckpt
official_weights = torch.load(""./model-1999-val_0.67311615.ckpt"", weights_only=True)
state_dict = {}

",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"official_weights = torch.load(""./model-1999-val_0.67311615.ckpt"", weights_only=True)","
# This weights were downloaded from the official pop2piano repository
# https://huggingface.co/sweetcocoa/pop2piano/blob/main/model-1999-val_0.67311615.ckpt
official_weights = torch.load(""./model-1999-val_0.67311615.ckpt"", weights_only=True)
state_dict = {}

",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        src_lang=None,
        tgt_lang=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        src_lang=None,
        tgt_lang=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        tgt_lang=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        src_lang=None,
        tgt_lang=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tgt_lang=None,
        tokenizer_file=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        tokenizer_file=None,
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_state_dict = torch.load(,"    )

    # convert state_dict
    original_state_dict = torch.load(
        hf_hub_download(repo_id=checkpoint_repo, filename=""pytorch_model.bin""), weights_only=True
    )
    state_dict = {}",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
original_state_dict = torch.load(,"    )

    # convert state_dict
    original_state_dict = torch.load(
        hf_hub_download(repo_id=checkpoint_repo, filename=""pytorch_model.bin""), weights_only=True
    )
    state_dict = {}",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","    def __init__(
        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=125,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        self,
        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=125,
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        vocab_file,
        eos_token=""</s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        extra_ids=125,
        additional_special_tokens=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"
    logger.info(f""model loaded: {round(n_params / 1e6, 1)}M params"")

    hf_model.eval()
    del state_dict

    return hf_model",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    hf_wav2vec = Wav2Vec2BertModel(config)

    model = load_conformer_shaw_model(checkpoint_path, dtype=torch.float32)
    model.eval()

    hf_wav2vec = _convert_model(model, hf_wav2vec, wav2vec_convert_list)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_wav2vec.eval(),"            seqs, padding_mask = model.encoder_frontend(seqs, padding_mask)
            original_output, padding_mask = model.encoder(seqs, padding_mask)

        hf_wav2vec.eval()

        inputs = fe(waveform, return_tensors=""pt"", padding=True)
        with torch.no_grad():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
from_model.eval(),"        if hidden_sizes == 384:
            from_model = timm.create_model(""levit_384"", pretrained=True)

        from_model.eval()
        our_model = LevitForImageClassificationWithTeacher(config).eval()
        huggingface_weights = OrderedDict()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = LevitForImageClassificationWithTeacher(config).eval(),"            from_model = timm.create_model(""levit_384"", pretrained=True)

        from_model.eval()
        our_model = LevitForImageClassificationWithTeacher(config).eval()
        huggingface_weights = OrderedDict()

        weights = from_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"
    logger.info(f""model loaded: {round(n_params / 1e6, 1)}M params"")

    hf_model.eval()
    hf_model.to(device)
    del state_dict
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"
    # load original model from torch hub
    original_model = torch.hub.load(""facebookresearch/dinov2"", model_name.replace(""_1layer"", """"))
    original_model.eval()

    # load state_dict of original model, remove and rename some keys
    state_dict = original_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = Dinov2WithRegistersForImageClassification(config).eval(),"
    # load HuggingFace model
    if image_classifier:
        model = Dinov2WithRegistersForImageClassification(config).eval()
        model.dinov2_with_registers.load_state_dict(state_dict)
        model_name_to_classifier_dict_url = {
            ""dinov2_vits14_reg_1layer"": ""https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_reg4_linear_head.pth"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = Dinov2WithRegistersModel(config).eval(),"        model.classifier.weight = nn.Parameter(classifier_state_dict[""weight""])
        model.classifier.bias = nn.Parameter(classifier_state_dict[""bias""])
    else:
        model = Dinov2WithRegistersModel(config).eval()
        model.load_state_dict(state_dict)

    # load image",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<sep>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        remove_space=True,
        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<sep>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""</s>"",
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        additional_special_tokens=[""<eop>"", ""<eod>""],
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        do_basic_tokenize=True,
        never_split=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        never_split=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        tokenizer_file=None,
        do_lower_case=True,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        do_lower_case=True,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""[UNK]"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        merges_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        merges_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        merges_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        **kwargs,
    ):
        self.vocab_file = vocab_file",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model = InstructBlipForConditionalGeneration(config).eval(),"        tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})

    config, image_size = get_blip2_config(model_name)
    hf_model = InstructBlipForConditionalGeneration(config).eval()

    model_name_to_original = {
        ""instructblip-vicuna-7b"": (""blip2_vicuna_instruct"", ""vicuna7b""),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"    original_model, vis_processors, _ = load_model_and_preprocess(
        name=name, model_type=type, is_eval=True, device=lavis_device
    )
    original_model.eval()
    print(""Done!"")

    # update state dict keys",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""","        # tokenizer.add_special_tokens({""eos_token"": ""</s>""})
        # tokenizer.add_special_tokens({""unk_token"": ""</s>""})
        tokenizer = LlamaTokenizerFast.from_pretrained(
            ""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""
        )
        tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""","        # tokenizer.add_special_tokens({""eos_token"": ""</s>""})
        # tokenizer.add_special_tokens({""unk_token"": ""</s>""})
        tokenizer = LlamaTokenizerFast.from_pretrained(
            ""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""
        )
        tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_model = bros.BrosModel.from_pretrained(model_name).eval(),"
def convert_bros_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_hub=False):
    # load original model
    original_model = bros.BrosModel.from_pretrained(model_name).eval()

    # load HuggingFace Model
    bros_config = get_configs(model_name)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace Model
    bros_config = get_configs(model_name)
    model = BrosModel.from_pretrained(model_name, config=bros_config)
    model.eval()

    state_dict = original_model.state_dict()
    new_state_dict = convert_state_dict(state_dict, model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"detr = torch.hub.load(""facebookresearch/detr"", model_name, pretrained=True).eval()","    logger.info(f""Converting model {model_name}..."")

    # load original model from torch hub
    detr = torch.hub.load(""facebookresearch/detr"", model_name, pretrained=True).eval()
    state_dict = detr.state_dict()
    # rename keys
    for src, dest in rename_keys:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = DetrForSegmentation(config) if is_panoptic else DetrForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()
    # verify our conversion
    original_outputs = detr(pixel_values)
    outputs = model(pixel_values)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"detr = torch.hub.load(""facebookresearch/detr"", model_name_to_original_name[model_name], pretrained=True).eval()","        ""detr-resnet-101"": ""detr_resnet101"",
    }
    logger.info(f""Converting model {model_name}..."")
    detr = torch.hub.load(""facebookresearch/detr"", model_name_to_original_name[model_name], pretrained=True).eval()
    state_dict = detr.state_dict()
    # rename keys
    for src, dest in create_rename_keys(config):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = DetrForSegmentation(config) if is_panoptic else DetrForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    # verify our conversion on an image
    format = ""coco_panoptic"" if is_panoptic else ""coco_detection""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"tokenizer.image_token = ""<image>""","    image_processor = hf_processor.image_processor

    tokenizer = hf_processor.tokenizer
    tokenizer.image_token = ""<image>""
    tokenizer.add_tokens(AddedToken(tokenizer.image_token, special=True, normalized=False), special_tokens=True)
    tokenizer.image_token_id = tokenizer.encode(tokenizer.image_token, add_special_tokens=False)[0]
    tokenizer.extra_special_tokens = {""image_token"": ""<image>""}",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        monolingual_vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        monolingual_vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        monolingual_vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]","def convert_sam2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub):
    config = get_config(model_name)

    state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]
    state_dict = replace_keys(state_dict, config)

    image_processor = Sam2ImageProcessorFast()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    video_processor = Sam2VideoVideoProcessor()
    processor = Sam2VideoProcessor(image_processor=image_processor, video_processor=video_processor)
    hf_model = Sam2VideoModel(config)
    hf_model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]","

def convert_mra_checkpoint(checkpoint_path, mra_config_file, pytorch_dump_path):
    orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model_state_dict""]
    config = MraConfig.from_json_file(mra_config_file)
    model = MraForMaskedLM(config)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_checkpoint_helper(config.max_position_embeddings, orig_state_dict)

    print(model.load_state_dict(new_state_dict))
    model.eval()
    model.save_pretrained(pytorch_dump_path)

    print(f""Checkpoint successfully converted. Model saved at {pytorch_dump_path}"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
).eval(),"        dtype=torch.bfloat16,
        use_flash_attn=False,
        trust_remote_code=True,
    ).eval()

    return model.state_dict()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"#     ""OpenGVLab/InternVL2_5-2B-MPO"", pad_token=""</s>"", legacy=False, from_slow=True","    else:
        # Obtained with:
        # tokenizer_llama_fast = LlamaTokenizerFast.from_pretrained(
        #     ""OpenGVLab/InternVL2_5-2B-MPO"", pad_token=""</s>"", legacy=False, from_slow=True
        # )
        # tokenizer_llama_fast._tokenizer.pre_tokenizer.prepend_scheme = ""never""
        # Then manually modifying `added_tokens_decoder` indices to match the original tokenizer",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)","    # Some llava variants like microsoft/llava-med-v1.5-mistral-7b use safetensors to store weights
    if file_exists(old_state_dict_id, ""model_state_dict.bin""):
        state_dict_path = hf_hub_download(old_state_dict_id, ""model_state_dict.bin"")
        state_dict = torch.load(state_dict_path, map_location=""cpu"", weights_only=True)
    else:
        state_dict = load_original_state_dict(old_state_dict_id)
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases","        patch_size=None,
        vision_feature_select_strategy=None,
        chat_template=None,
        image_token=""<image>"",  # set the default and let users change if they have peculiar special tokens in rare cases
        num_additional_image_tokens=0,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
checkpoint = pickle.loads(f.read()),"
    # load parameters as FlatMapping data structure
    with open(pickle_file, ""rb"") as f:
        checkpoint = pickle.loads(f.read())

    state = None
    if isinstance(checkpoint, dict) and architecture in [",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"        config.label2id = {v: k for k, v in id2label.items()}
    else:
        raise ValueError(f""Architecture {architecture} not supported"")
    model.eval()

    # load weights
    model.load_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","
    def __init__(
        self,
        pad_token=""[PAD]"",
        bos_token=""[BOS]"",
        eos_token=""[EOS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[BOS]"",","    def __init__(
        self,
        pad_token=""[PAD]"",
        bos_token=""[BOS]"",
        eos_token=""[EOS]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[EOS]"",","        self,
        pad_token=""[PAD]"",
        bos_token=""[BOS]"",
        eos_token=""[EOS]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        pad_token=""[PAD]"",
        bos_token=""[BOS]"",
        eos_token=""[EOS]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sep_token=""[SEP]"",
        model_max_length=2048,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        bos_token=""[BOS]"",
        eos_token=""[EOS]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sep_token=""[SEP]"",
        model_max_length=2048,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        eos_token=""[EOS]"",
        mask_token=""[MASK]"",
        cls_token=""[CLS]"",
        sep_token=""[SEP]"",
        model_max_length=2048,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
hf_model = InstructBlipVideoForConditionalGeneration(config).eval(),"        tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})

    config, image_size = get_blip2_config(model_name)
    hf_model = InstructBlipVideoForConditionalGeneration(config).eval()

    model_name_to_original = {
        ""instructblipvideo-vicuna-7b"": (""blip2_vicuna_instruct"", ""vicuna7b""),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"    original_model, vis_processors, _ = load_model_and_preprocess(
        name=name, model_type=type, is_eval=True, device=lavis_device
    )
    original_model.eval()
    print(""Done!"")

    # update state dict keys",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""","        # tokenizer.add_special_tokens({""eos_token"": ""</s>""})
        # tokenizer.add_special_tokens({""unk_token"": ""</s>""})
        tokenizer = LlamaTokenizerFast.from_pretrained(
            ""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""
        )
        tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""","        # tokenizer.add_special_tokens({""eos_token"": ""</s>""})
        # tokenizer.add_special_tokens({""unk_token"": ""</s>""})
        tokenizer = LlamaTokenizerFast.from_pretrained(
            ""huggyllama/llama-7b"", truncation_side=""left"", bos_token=""</s>"", unk_token=""</s>""
        )
        tokenizer.add_special_tokens({""pad_token"": ""[PAD]""})
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.add_prefix_space = add_prefix_space

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        super().__init__(
            errors=errors,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file,
        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        merges_file,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_prefix_space=False,
        add_bos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        vocab_file=None,
        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        merges_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tensors = torch.load(file, map_location=""cpu"", weights_only=True)","    elif bin_files:
        bin_files = sorted(bin_files, key=lambda x: int(x.rsplit(""-"", 3)[1]))
        for file in bin_files:
            tensors = torch.load(file, map_location=""cpu"", weights_only=True)
            all_weights.update(tensors)
        return all_weights
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_sdpa = True
    _supports_flex_attn = True

    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""hidden_states"": GraniteMoeSharedDecoderLayer,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file,
        merges_file,
        do_lowercase=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file,
        do_lowercase=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        do_lowercase=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",
        additional_special_tokens=[",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""</s>"",","        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",
        additional_special_tokens=[
            ""<special0>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<special1>"",","        sep_token=""</s>"",
        pad_token=""<pad>"",
        cls_token=""</s>"",
        mask_token=""<special1>"",
        additional_special_tokens=[
            ""<special0>"",
            ""<special1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_state_dict(state_dict, config)

    model.load_state_dict(new_state_dict)
    model.eval()

    url = ""https://user-images.githubusercontent.com/11435359/147738734-196fd92f-9260-48d5-ba7e-bf103d29364d.jpg""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","
    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)
        self.image_token = ""<|image|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)
        self.image_token = ""<|image|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, ""image_token_id"", None)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","
    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)
        self.image_token = ""<|image|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token

    def __call__(",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)
        self.image_token = ""<|image|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token

    def __call__(
        self,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sd = torch.load(weight_path, map_location=""cpu"", pickle_module=pickle)","    for tp_rank in range(tp_size):
        print(f""Loading TP shard {tp_rank}..."")
        weight_path = Path(model_path) / f""mp_rank_{tp_rank:02d}"" / ""model_optim_rng.pt""
        sd = torch.load(weight_path, map_location=""cpu"", pickle_module=pickle)

        for k in list(sd.keys()):
            if ""_extra_state"" in k or ""dummy_parameter"" in k:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"    print(f""TP and PP INFO: original_tp: {original_tp}, original_pp:{original_pp}, target_tp: {target_tp}"")
    mgt_sd = [
        [
            torch.load(
                Path(model_path)
                / (f""mp_rank_{j:02d}_{i:03d}"" if original_pp_enabled else f""mp_rank_{j:02d}"")
                / ""model_optim_rng.pt"",",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"            new_key = re.sub(pattern, adjust_stage, new_key)
            state_dict_changed[new_key] = val
    model.load_state_dict(state_dict_changed)
    model.eval()

    url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
    image = Image.open(requests.get(url, stream=True).raw).convert(""RGB"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        self,
        vocab_file=None,
        merges_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        vocab_file=None,
        merges_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        trim_offsets=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        merges_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        trim_offsets=True,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""__start__"",","        self,
        vocab_file,
        merges_file,
        bos_token=""__start__"",
        eos_token=""__end__"",
        unk_token=""__unk__"",
        pad_token=""__null__"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""__end__"",","        vocab_file,
        merges_file,
        bos_token=""__start__"",
        eos_token=""__end__"",
        unk_token=""__unk__"",
        pad_token=""__null__"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""__unk__"",","        merges_file,
        bos_token=""__start__"",
        eos_token=""__end__"",
        unk_token=""__unk__"",
        pad_token=""__null__"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""__null__"",","        bos_token=""__start__"",
        eos_token=""__end__"",
        unk_token=""__unk__"",
        pad_token=""__null__"",
        **kwargs,
    ):
        with open(vocab_file, encoding=""utf-8"") as vocab_handle:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"original_processor.tokenizer.image_token = ""<|image|>""","    original_processor = AutoProcessor.from_pretrained(input_dir, trust_remote_code=True)
    original_processor.tokenizer.extra_special_tokens = {""image_token"": ""<|image|>"", ""audio_token"": ""<|audio|>""}
    # We need to add those temporarily to instantiate the processor
    original_processor.tokenizer.image_token = ""<|image|>""
    original_processor.tokenizer.audio_token = ""<|audio|>""
    original_processor.tokenizer.image_token_id = 200010
    original_processor.tokenizer.audio_token_id = 200011",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"original_processor.tokenizer.audio_token = ""<|audio|>""","    original_processor.tokenizer.extra_special_tokens = {""image_token"": ""<|image|>"", ""audio_token"": ""<|audio|>""}
    # We need to add those temporarily to instantiate the processor
    original_processor.tokenizer.image_token = ""<|image|>""
    original_processor.tokenizer.audio_token = ""<|audio|>""
    original_processor.tokenizer.image_token_id = 200010
    original_processor.tokenizer.audio_token_id = 200011
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"input_state_dict = torch.load(args.path_to_checkpoint, map_location=""cpu"", weights_only=True)","            with checkpoint.open(""release/mp_rank_00/model_optim_rng.pt"") as pytorch_dict:
                input_state_dict = torch.load(pytorch_dict, map_location=""cpu"", weights_only=True)
    else:
        input_state_dict = torch.load(args.path_to_checkpoint, map_location=""cpu"", weights_only=True)

    if args.config_file == """":
        # Default config of megatron-bert 345m",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"input_state_dict = torch.load(pytorch_dict, map_location=""cpu"", weights_only=True)","    if args.path_to_checkpoint.endswith("".zip""):
        with zipfile.ZipFile(args.path_to_checkpoint, ""r"") as checkpoint:
            with checkpoint.open(""release/mp_rank_00/model_optim_rng.pt"") as pytorch_dict:
                input_state_dict = torch.load(pytorch_dict, map_location=""cpu"", weights_only=True)
    else:
        input_state_dict = torch.load(args.path_to_checkpoint, map_location=""cpu"", weights_only=True)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"layer_re = re.compile(r""layers\.(\d+)\.([a-z0-9_.]+)\.([a-z]+)"")","    transformer = lm[""transformer""] if ""transformer"" in lm else lm[""encoder""]

    # The regex to extract layer names.
    layer_re = re.compile(r""layers\.(\d+)\.([a-z0-9_.]+)\.([a-z]+)"")

    # The simple map of names for ""automated"" rules.
    megatron_to_transformers = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<sep>"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        do_lower_case=True,
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        clean_text=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        clean_text=True,
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        clean_text=True,
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<sep>"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        never_split=None,
        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<cls>"",","        unk_token=""<unk>"",
        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        sep_token=""<sep>"",
        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        pad_token=""<pad>"",
        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        cls_token=""<cls>"",
        mask_token=""<mask>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        word_tokenizer_type=""basic"",
        subword_tokenizer_type=""wordpiece"",
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        subword_tokenizer_type=""wordpiece"",
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        mecab_kwargs=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        mecab_kwargs=None,
        sudachi_kwargs=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        mecab_kwargs=None,
        sudachi_kwargs=None,
        jumanpp_kwargs=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        tgt_vocab_file=None,
        merges_file=None,
        do_lower_case=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file=None,
        do_lower_case=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        do_lower_case=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        unk_token=""<unk>"",
        bos_token=""<s>"",
        sep_token=""</s>"",
        pad_token=""<pad>"",
        **kwargs,
    ):
        try:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
xmod.eval()  # disable dropout,"        sentencepiece_model=str(Path(xmod_checkpoint_path).parent / ""sentencepiece.bpe.model""),
        src_dict=str(data_dir / ""dict.txt""),
    )
    xmod.eval()  # disable dropout
    print(xmod)

    xmod_sent_encoder = xmod.model.encoder.sentence_encoder",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    print(""Our X-MOD config:"", config)

    model = XmodForSequenceClassification(config) if classification_head else XmodForMaskedLM(config)
    model.eval()

    # Now let's copy all the weights.
    # Embeddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]","def convert_sam2_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub):
    config = get_config(model_name)

    state_dict = torch.load(checkpoint_path, map_location=""cpu"")[""model""]
    state_dict = replace_keys(state_dict)

    image_processor = Sam2ImageProcessorFast()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
hf_model.eval(),"    image_processor = Sam2ImageProcessorFast()
    processor = Sam2Processor(image_processor=image_processor)
    hf_model = Sam2Model(config)
    hf_model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"model_state_dict_base = torch.load(pt_model_path, map_location=""cpu"", weights_only=True)","    import sys

    sys.path.insert(0, ada_lib_path)
    model_state_dict_base = torch.load(pt_model_path, map_location=""cpu"", weights_only=True)
    state_dict = flatdict.FlatDict(model_state_dict_base[""model""], ""."")
    state_dict = rename_state_dict(state_dict)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")","        safe_serialization=args.safe_serialization,
        ada_lib_path=args.ada_lib_path,
    )
    tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")
    tokenizer.save_pretrained(args.output_dir)

",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")","        safe_serialization=args.safe_serialization,
        ada_lib_path=args.ada_lib_path,
    )
    tokenizer = tokenizer_class(spm_path, bos_token=""|ENDOFTEXT|"", eos_token=""|ENDOFTEXT|"")
    tokenizer.save_pretrained(args.output_dir)

",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = RTDetrV2ForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    # load image processor
    image_processor = RTDetrImageProcessor()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")","        self.ids_to_tokens = ids_to_tokens  # same as bpe
        self.emoji = emoji
        self.maxlen = np.max([len(w) for w in self.vocab])
        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")","        self.emoji = emoji
        self.maxlen = np.max([len(w) for w in self.vocab])
        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")","        self.maxlen = np.max([len(w) for w in self.vocab])
        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter4 = re.compile(,"        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )
        self.content_repatter5 = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter5 = re.compile(,"        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )
        self.content_repatter5 = re.compile(
            r""(|||||||||\u32ff)\d{1,2}(0?[1-9]|1[0-2])(0?[1-9]|[12][0-9]|3[01])(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )
        # The original version of this regex displays catastrophic backtracking behaviour. We avoid this using",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter6 = re.compile(,"        # possessive quantifiers in Py >= 3.11. In versions below this, we avoid the vulnerability using a slightly
        # different regex that should generally have the same behaviour in most non-pathological cases.
        if sys.version_info >= (3, 11):
            self.content_repatter6 = re.compile(
                r""(?:\d,\d{3}|[\d])*+""
                r""(?:\d,\d{3}|[\d])*+""
                r""(?:\d,\d{3}|[\d])*+""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter6 = re.compile(,"                r""(?:\(\)|\(\)|\+tax)*""
            )
        else:
            self.content_repatter6 = re.compile(
                r""(?:\d,\d{3}|[\d])*""
                r""(?:|||||||||||)+""
                r""(?:\(\)|\(\)|\+tax)*""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        self,
        vocab_file,
        emoji_file,
        unk_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|endoftext|>"",","        vocab_file,
        emoji_file,
        unk_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        do_clean_text=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|startoftext|>"",","        emoji_file,
        unk_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        do_clean_text=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        unk_token=""<|endoftext|>"",
        pad_token=""<|endoftext|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        do_clean_text=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)","
    # Verify pixel_values and input_ids
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlvit_pixel_values_960.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlv2_input_ids.pt"", repo_type=""dataset"")
    original_input_ids = torch.load(filepath, weights_only=True).squeeze()",torch.load.permute,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, weights_only=True).squeeze()","    original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlv2_input_ids.pt"", repo_type=""dataset"")
    original_input_ids = torch.load(filepath, weights_only=True).squeeze()

    filepath = hf_hub_download(repo_id=""adirik/OWL-ViT"", repo_type=""space"", filename=""assets/astronaut.png"")
    image = Image.open(filepath)",torch.load.squeeze,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)","
    # Verify pixel_values and input_ids
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlvit_pixel_values_960.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlv2_input_ids.pt"", repo_type=""dataset"")
    original_input_ids = torch.load(filepath, weights_only=True).squeeze()",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, weights_only=True).squeeze()","    original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlv2_input_ids.pt"", repo_type=""dataset"")
    original_input_ids = torch.load(filepath, weights_only=True).squeeze()

    filepath = hf_hub_download(repo_id=""adirik/OWL-ViT"", repo_type=""space"", filename=""assets/astronaut.png"")
    image = Image.open(filepath)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)","
    # Verify pixel_values and input_ids
    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlvit_pixel_values_960.pt"", repo_type=""dataset"")
    original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlv2_input_ids.pt"", repo_type=""dataset"")
    original_input_ids = torch.load(filepath, weights_only=True).squeeze()",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_input_ids = torch.load(filepath, weights_only=True).squeeze()","    original_pixel_values = torch.load(filepath, weights_only=True).permute(0, 3, 1, 2)

    filepath = hf_hub_download(repo_id=""nielsr/test-image"", filename=""owlv2_input_ids.pt"", repo_type=""dataset"")
    original_input_ids = torch.load(filepath, weights_only=True).squeeze()

    filepath = hf_hub_download(repo_id=""adirik/OWL-ViT"", repo_type=""space"", filename=""assets/astronaut.png"")
    image = Image.open(filepath)",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    assert missing_keys == [""owlv2.visual_projection.weight""]
    assert unexpected_keys == []
    model.eval()

    # Initialize image processor
    size = {""height"": config.vision_config.image_size, ""width"": config.vision_config.image_size}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"tokenizer = CLIPTokenizer.from_pretrained(""openai/clip-vit-base-patch32"", pad_token=""!"", model_max_length=16)","    size = {""height"": config.vision_config.image_size, ""width"": config.vision_config.image_size}
    image_processor = Owlv2ImageProcessor(size=size)
    # Initialize tokenizer
    tokenizer = CLIPTokenizer.from_pretrained(""openai/clip-vit-base-patch32"", pad_token=""!"", model_max_length=16)
    # Initialize processor
    processor = Owlv2Processor(image_processor=image_processor, tokenizer=tokenizer)
",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"DEFAULT_CHAT_TEMPLATE = '{%- if not add_generation_prompt is defined -%}\n    {%- set add_generation_prompt = true -%}\n{%- endif -%}\n{%- if not cls_token is defined -%}\n    {%- set cls_token = ""<|begin_of_sentence|>"" -%}\n{%- endif -%}\n{%- if not sep_token is defined -%}\n    {%- set sep_token = ""<|end_of_sentence|>"" -%}\n{%- endif -%}\n{{- cls_token -}}\n{%- for message in messages -%}\n    {%- if message[""role""] == ""user"" -%}\n        {{- ""User: "" + message[""content""] + ""\n"" -}}\n    {%- elif message[""role""] == ""assistant"" -%}\n        {{- ""Assistant: "" + message[""content""] + sep_token -}}\n    {%- elif message[""role""] == ""system"" -%}\n        {{- message[""content""] + ""\n"" -}}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{- ""Assistant: "" -}}\n{%- endif -%}'","from transformers import LlamaTokenizer, LlamaTokenizerFast


DEFAULT_CHAT_TEMPLATE = '{%- if not add_generation_prompt is defined -%}\n    {%- set add_generation_prompt = true -%}\n{%- endif -%}\n{%- if not cls_token is defined -%}\n    {%- set cls_token = ""<|begin_of_sentence|>"" -%}\n{%- endif -%}\n{%- if not sep_token is defined -%}\n    {%- set sep_token = ""<|end_of_sentence|>"" -%}\n{%- endif -%}\n{{- cls_token -}}\n{%- for message in messages -%}\n    {%- if message[""role""] == ""user"" -%}\n        {{- ""User: "" + message[""content""] + ""\n"" -}}\n    {%- elif message[""role""] == ""assistant"" -%}\n        {{- ""Assistant: "" + message[""content""] + sep_token -}}\n    {%- elif message[""role""] == ""system"" -%}\n        {{- message[""content""] + ""\n"" -}}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{- ""Assistant: "" -}}\n{%- endif -%}'
DEFAULT_TEXT_ADD_TOKENS = [
    ""<mask:4>"",
    ""<mask:5>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"DEFAULT_CHAT_TEMPLATE = '{%- if not add_generation_prompt is defined -%}\n    {%- set add_generation_prompt = true -%}\n{%- endif -%}\n{%- if not cls_token is defined -%}\n    {%- set cls_token = ""<|begin_of_sentence|>"" -%}\n{%- endif -%}\n{%- if not sep_token is defined -%}\n    {%- set sep_token = ""<|end_of_sentence|>"" -%}\n{%- endif -%}\n{{- cls_token -}}\n{%- for message in messages -%}\n    {%- if message[""role""] == ""user"" -%}\n        {{- ""User: "" + message[""content""] + ""\n"" -}}\n    {%- elif message[""role""] == ""assistant"" -%}\n        {{- ""Assistant: "" + message[""content""] + sep_token -}}\n    {%- elif message[""role""] == ""system"" -%}\n        {{- message[""content""] + ""\n"" -}}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{- ""Assistant: "" -}}\n{%- endif -%}'","from transformers import LlamaTokenizer, LlamaTokenizerFast


DEFAULT_CHAT_TEMPLATE = '{%- if not add_generation_prompt is defined -%}\n    {%- set add_generation_prompt = true -%}\n{%- endif -%}\n{%- if not cls_token is defined -%}\n    {%- set cls_token = ""<|begin_of_sentence|>"" -%}\n{%- endif -%}\n{%- if not sep_token is defined -%}\n    {%- set sep_token = ""<|end_of_sentence|>"" -%}\n{%- endif -%}\n{{- cls_token -}}\n{%- for message in messages -%}\n    {%- if message[""role""] == ""user"" -%}\n        {{- ""User: "" + message[""content""] + ""\n"" -}}\n    {%- elif message[""role""] == ""assistant"" -%}\n        {{- ""Assistant: "" + message[""content""] + sep_token -}}\n    {%- elif message[""role""] == ""system"" -%}\n        {{- message[""content""] + ""\n"" -}}\n    {%- endif -%}\n{%- endfor -%}\n{%- if add_generation_prompt -%}\n    {{- ""Assistant: "" -}}\n{%- endif -%}'
DEFAULT_TEXT_ADD_TOKENS = [
    ""<mask:4>"",
    ""<mask:5>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<unk>"",","
    hf_tok = LlamaTokenizer.from_pretrained(
        args.repo_name,
        pad_token=""<unk>"",
        cls_token=""<|begin_of_sentence|>"",
        sep_token=""<|end_of_sentence|>"",
        mask_token=""<mask:1>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<|begin_of_sentence|>"",","    hf_tok = LlamaTokenizer.from_pretrained(
        args.repo_name,
        pad_token=""<unk>"",
        cls_token=""<|begin_of_sentence|>"",
        sep_token=""<|end_of_sentence|>"",
        mask_token=""<mask:1>"",
        add_bos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<|end_of_sentence|>"",","        args.repo_name,
        pad_token=""<unk>"",
        cls_token=""<|begin_of_sentence|>"",
        sep_token=""<|end_of_sentence|>"",
        mask_token=""<mask:1>"",
        add_bos_token=False,
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask:1>"",","        pad_token=""<unk>"",
        cls_token=""<|begin_of_sentence|>"",
        sep_token=""<|end_of_sentence|>"",
        mask_token=""<mask:1>"",
        add_bos_token=False,
        add_prefix_space=False,
        chat_template=DEFAULT_CHAT_TEMPLATE,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    tokenizer = PreTrainedTokenizerFast(
        tokenizer_object=tokenizer,
        chat_template=None,
        unk_token=""<unk>"",
        model_input_names=[""input_ids"", ""attention_mask""],
        clean_up_tokenization_spaces=False,
        bos_token_id=original_tokenizer.bos_id(),",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    if ""large"" in model_name and not is_video and ""large-r"" not in model_name:
        # large checkpoints take way too long to download
        checkpoint_path = model_name_to_path[model_name]
        state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    else:
        checkpoint_url = model_name_to_url[model_name]
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=""cpu"", file_name=model_name)[",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    if ""large"" in model_name and not is_video and ""large-r"" not in model_name:
        # large checkpoints take way too long to download
        checkpoint_path = model_name_to_path[model_name]
        state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    else:
        checkpoint_url = model_name_to_url[model_name]
        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=""cpu"", file_name=model_name)[",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # load HuggingFace model
    model = GitForCausalLM(config)
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    model.eval()

    print(""Missing keys:"", missing_keys)
    print(""Unexpected keys:"", unexpected_keys)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"torch.load(os.path.join(input_dir, file), map_location=""cpu"", weights_only=True) for file in shards","        shards = [file for file in os.listdir(input_dir) if re.match(r""consolidated.\d+.pth"", file)]
        shards = sorted(shards, key=lambda x: int(x.split(""."")[1]))
        loaded_shards = [
            torch.load(os.path.join(input_dir, file), map_location=""cpu"", weights_only=True) for file in shards
        ]
        full_state_dict = convert_state_dict_sharded(loaded_shards, config)
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=True,
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        remove_space=True,
        keep_accents=True,
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        keep_accents=True,
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=True,
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        remove_space=True,
        keep_accents=True,
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        keep_accents=True,
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it and",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # Initialize PyTorch model
    config = CanineConfig()
    model = CanineModel(config)
    model.eval()

    print(f""Building PyTorch model from configuration: {config}"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", **kwargs):","    vocab_files_names = VOCAB_FILES_NAMES
    model_input_names = [""input_ids"", ""attention_mask""]

    def __init__(self, vocab_file, merges_file, unk_token=""<unk>"", **kwargs):
        try:
            import ftfy
            from spacy.lang.en import English",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"def __init__(self, vocab_file=None, merges_file=None, tokenizer_file=None, unk_token=""<unk>"", **kwargs):","    model_input_names = [""input_ids"", ""attention_mask""]
    slow_tokenizer_class = OpenAIGPTTokenizer

    def __init__(self, vocab_file=None, merges_file=None, tokenizer_file=None, unk_token=""<unk>"", **kwargs):
        super().__init__(vocab_file, merges_file, tokenizer_file=tokenizer_file, unk_token=unk_token, **kwargs)

    @property",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
original_model.eval(),"
    # load original model from torch hub
    original_model = torch.hub.load(""facebookresearch/dinov2"", model_name.replace(""_1layer"", """"))
    original_model.eval()

    # load state_dict of original model, remove and rename some keys
    state_dict = original_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = Dinov2ForImageClassification(config).eval(),"
    # load HuggingFace model
    if image_classifier:
        model = Dinov2ForImageClassification(config).eval()
        model.dinov2.load_state_dict(state_dict)
        model_name_to_classifier_dict_url = {
            ""dinov2_vits14_1layer"": ""https://dl.fbaipublicfiles.com/dinov2/dinov2_vits14/dinov2_vits14_linear_head.pth"",",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = Dinov2Model(config).eval(),"        model.classifier.weight = nn.Parameter(classifier_state_dict[""weight""])
        model.classifier.bias = nn.Parameter(classifier_state_dict[""bias""])
    else:
        model = Dinov2Model(config).eval()
        model.load_state_dict(state_dict)

    # load image",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","    def __init__(
        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=None,
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        pad_token=None,
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
loaded = torch.load(,"        if num_shards == 1:
            # Not sharded
            # (The sharded implementation would also work, but this is simpler.)
            loaded = torch.load(
                os.path.join(input_base_path, ""consolidated.00.pth""), map_location=""cpu"", weights_only=True
            )
        else:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"torch.load(os.path.join(input_base_path, file), map_location=""cpu"", weights_only=True)","            checkpoint_list = sorted([file for file in os.listdir(input_base_path) if file.endswith("".pth"")])
            print(""Loading in order:"", checkpoint_list)
            loaded = [
                torch.load(os.path.join(input_base_path, file), map_location=""cpu"", weights_only=True)
                for file in checkpoint_list
            ]
        param_count = 0",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|begin_of_text|>"",","
        self.converted_tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token=""<|begin_of_text|>"",
            eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            model_max_length=CONTEXT_LENGTH_FOR_VERSION[llama_version],",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",","        self.converted_tokenizer = PreTrainedTokenizerFast(
            tokenizer_object=tokenizer,
            bos_token=""<|begin_of_text|>"",
            eos_token=""<|end_of_text|>"" if not instruct else ""<|eot_id|>"",
            model_input_names=[""input_ids"", ""attention_mask""],
            model_max_length=CONTEXT_LENGTH_FOR_VERSION[llama_version],
            clean_up_tokenization_spaces=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        add_bos_token=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        tokenizer_file=None,
        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        add_bos_token=True,
        add_eos_token=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        clean_up_tokenization_spaces=False,
        unk_token=""<unk>"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        add_bos_token=True,
        add_eos_token=False,
        use_default_system_prompt=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.timestamp_pat = re.compile(r""<\|(\d+\.\d+)\|>"")","        else:
            self.english_spelling_normalizer = None

        self.timestamp_pat = re.compile(r""<\|(\d+\.\d+)\|>"")

        self.language = language
        self.task = task",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        merges_file=None,
        normalizer_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        normalizer_file=None,
        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        language=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        tokenizer_file=None,
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        add_prefix_space=False,
        language=None,
        task=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","            self.english_spelling_normalizer = None

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")
        self.timestamp_pat = re.compile(r""<\|(\d+\.\d+)\|>"")

        self.language = language",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.timestamp_pat = re.compile(r""<\|(\d+\.\d+)\|>"")","
        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")
        self.timestamp_pat = re.compile(r""<\|(\d+\.\d+)\|>"")

        self.language = language
        super().__init__(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        merges_file,
        normalizer_file=None,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|endoftext|>"",","        normalizer_file=None,
        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        errors=""replace"",
        unk_token=""<|endoftext|>"",
        bos_token=""<|endoftext|>"",
        eos_token=""<|endoftext|>"",
        pad_token=None,
        add_prefix_space=False,
        language=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(io.BytesIO(model_bytes), weights_only=True)","            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )

    return torch.load(io.BytesIO(model_bytes), weights_only=True)


def convert_openai_whisper_to_tfms(",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","        original_checkpoint = _download(_MODELS[checkpoint_path], root)
        openai_version = checkpoint_path
    else:
        original_checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
        openai_version = None

    dimensions = original_checkpoint[""dims""]",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(io.BytesIO(model_bytes), weights_only=True)","    if os.path.isfile(download_target):
        model_bytes = open(download_target, ""rb"").read()
        if insecure_hashlib.sha256(model_bytes).hexdigest() == expected_sha256:
            return torch.load(io.BytesIO(model_bytes), weights_only=True)
        else:
            warnings.warn(f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file"")
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(io.BytesIO(model_bytes), weights_only=True)","    if os.path.isfile(download_target):
        model_bytes = open(download_target, ""rb"").read()
        if insecure_hashlib.sha256(model_bytes).hexdigest() == expected_sha256:
            return torch.load(io.BytesIO(model_bytes), weights_only=True)
        else:
            warnings.warn(f""{download_target} exists, but the SHA256 checksum does not match; re-downloading the file"")
",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"return torch.load(io.BytesIO(model_bytes), weights_only=True)","            ""Model has been downloaded but the SHA256 checksum does not match. Please retry loading the model.""
        )

    return torch.load(io.BytesIO(model_bytes), weights_only=True)


def convert_openai_whisper_to_tfms(",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","        original_checkpoint = _download(_MODELS[checkpoint_path], root)
        openai_version = checkpoint_path
    else:
        original_checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
        openai_version = None

    dimensions = original_checkpoint[""dims""]",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)","    filepath = hf_hub_download(
        repo_id=""RaushanTurganbay/test-image"", filename=""llava_onevision_pixel_values.pt"", repo_type=""dataset""
    )
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)
    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())

    image_sizes = torch.tensor([[899, 1024]])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)","    filepath = hf_hub_download(
        repo_id=""RaushanTurganbay/test-image"", filename=""llava_onevision_pixel_values.pt"", repo_type=""dataset""
    )
    original_pixel_values = torch.load(filepath, map_location=""cpu"", weights_only=True)
    assert torch.allclose(original_pixel_values, inputs.pixel_values.half())

    image_sizes = torch.tensor([[899, 1024]])",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    state_dict = load_original_state_dict(model_id)
    state_dict = convert_state_dict_to_hf(state_dict)
    model.load_state_dict(state_dict, assign=True)
    model.eval()

    pre_expansion_embeddings = model.language_model.model.embed_tokens.weight.data
    mu = torch.mean(pre_expansion_embeddings, dim=0).float()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"image_token=""<image>"",","        num_image_tokens=None,
        vision_feature_select_strategy=None,
        chat_template=None,
        image_token=""<image>"",
        video_token=""<video>"",
        vision_aspect_ratio=""anyres_max_9"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"video_token=""<video>"",","        vision_feature_select_strategy=None,
        chat_template=None,
        image_token=""<image>"",
        video_token=""<video>"",
        vision_aspect_ratio=""anyres_max_9"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"audio_token=""<|AUDIO|>"",","        feature_extractor=None,
        tokenizer=None,
        chat_template=None,
        audio_token=""<|AUDIO|>"",
        audio_bos_token=""<|audio_bos|>"",
        audio_eos_token=""<|audio_eos|>"",
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"audio_bos_token=""<|audio_bos|>"",","        tokenizer=None,
        chat_template=None,
        audio_token=""<|AUDIO|>"",
        audio_bos_token=""<|audio_bos|>"",
        audio_eos_token=""<|audio_eos|>"",
    ):
        if chat_template is None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"audio_eos_token=""<|audio_eos|>"",","        chat_template=None,
        audio_token=""<|AUDIO|>"",
        audio_bos_token=""<|audio_bos|>"",
        audio_eos_token=""<|audio_eos|>"",
    ):
        if chat_template is None:
            chat_template = self.default_chat_template",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        do_lower_case=True,
        remove_space=True,
        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        remove_space=True,
        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        keep_accents=False,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        eos_token=""[SEP]"",
        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""<unk>"",
        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""<pad>"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[CLS]"",","        do_lower_case=False,
        remove_space=True,
        keep_accents=True,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        remove_space=True,
        keep_accents=True,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        keep_accents=True,
        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        bos_token=""[CLS]"",
        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        eos_token=""[SEP]"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        **kwargs,
    ):
        # Mask token behave like a normal word, i.e. include the space before it",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
_can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported),"    _supports_flash_attn = True
    _supports_sdpa = True
    _supports_flex_attn = True
    _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)
    _supports_attention_backend = True
    _can_record_outputs = {
        ""router_logits"": OutputRecorder(nn.Linear, layer_name=""mlp.gate"", index=0),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"
    # Load HF model
    model = SegGptForImageSegmentation(config)
    model.eval()
    missing_keys, unexpected_keys = model.load_state_dict(new_state_dict, strict=False)
    print(""Missing keys:"", missing_keys)
    print(""Unexpected keys:"", unexpected_keys)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"def convert_swin2sr_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub):
    config = get_config(checkpoint_url)
    model = Swin2SRForImageSuperResolution(config)
    model.eval()

    state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=""cpu"")
    new_state_dict = convert_state_dict(state_dict, config)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token","
    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token","    def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):
        super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)
        self.image_token = ""<|image_pad|>"" if not hasattr(tokenizer, ""image_token"") else tokenizer.image_token
        self.video_token = ""<|video_pad|>"" if not hasattr(tokenizer, ""video_token"") else tokenizer.video_token
        self.image_token_id = (
            tokenizer.image_token_id
            if getattr(tokenizer, ""image_token_id"", None)",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenizer_file=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenizer_file=None,
        src_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        tokenizer_file=None,
        src_lang=None,
        tgt_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        src_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        src_lang=None,
        tgt_lang=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        src_lang=None,
        tgt_lang=None,
        additional_special_tokens=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","def convert_fairseq_mbart_checkpoint_from_disk(
    checkpoint_path, hf_config_path=""facebook/mbart-large-en-ro"", finetuned=False, mbart_50=False
):
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    remove_ignore_keys_(state_dict)
    vocab_size = state_dict[""encoder.embed_tokens.weight""].shape[0]
",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        self,
        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file=None,
        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        tokenizer_file=None,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        **kwargs,
    ):
        # Compatibility with the original tokenizer",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)","

def convert_fairseq_xglm_checkpoint_from_disk(checkpoint_path):
    checkpoint = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)
    args = Namespace(**checkpoint[""cfg""][""model""])
    state_dict = checkpoint[""model""]
    remove_ignore_keys_(state_dict)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
>>> model.eval(),"        ...     ""CarlCochet/trajectory-transformer-halfcheetah-medium-v2""
        ... )
        >>> model.to(device)
        >>> model.eval()

        >>> observations_dim, action_dim, batch_size = 17, 6, 256
        >>> seq_length = observations_dim + action_dim + 1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""[SEP]"",","    def __init__(
        self,
        vocab_file,
        bos_token=""[SEP]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        unk_token=""[UNK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""[SEP]"",","        self,
        vocab_file,
        bos_token=""[SEP]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        vocab_file,
        bos_token=""[SEP]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        bos_token=""[SEP]"",
        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        eos_token=""[SEP]"",
        sep_token=""[SEP]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        sep_token=""[SEP]"",
        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        unk_token=""[UNK]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"corpus = pickle.load(fp, encoding=""latin1"")","    if transfo_xl_dataset_file:
        # Convert a pre-processed corpus (see original TensorFlow repo)
        with open(transfo_xl_dataset_file, ""rb"") as fp:
            corpus = pickle.load(fp, encoding=""latin1"")
        # Save vocabulary and dataset cache as Dictionaries (should be better than pickles for the long-term)
        pytorch_vocab_dump_path = pytorch_dump_folder_path + ""/"" + VOCAB_FILES_NAMES[""pretrained_vocab_file""]
        print(f""Save vocabulary to {pytorch_vocab_dump_path}"")",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
corpus = pickle.load(fp),"                ""you can set the environment variable `TRUST_REMOTE_CODE` to `True` to allow it.""
            )
        with open(fn, ""rb"") as fp:
            corpus = pickle.load(fp)
    else:
        logger.info(f""Producing dataset {dataset}..."")
        kwargs = {}",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
vocab_dict = pickle.load(f),"                        ""`TRUST_REMOTE_CODE` to `True` to allow it.""
                    )
                with open(pretrained_vocab_file, ""rb"") as f:
                    vocab_dict = pickle.load(f)

                # Loading a torch-saved transfo-xl vocab dict with pickle results in an integer
                # Entering this if statement means that we tried to load a torch-saved file with pickle, and we failed.",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"self.punction_without_space_before_pattern = re.compile(rf""[^\s][{self.punctuation_symbols}]"")","        self.delimiter = delimiter
        self.vocab_file = vocab_file
        self.punctuation_symbols = '!""#$%&()*+,-./\\:;<=>?@[\\]^_`{|}~'
        self.punction_without_space_before_pattern = re.compile(rf""[^\s][{self.punctuation_symbols}]"")
        self.punctuation_with_space_around_pattern = self._compile_space_around_punctuation_pattern()
        self.language = language
        self.moses_punct_normalizer = sm.MosesPunctNormalizer(language)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"return re.compile(r"""" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)","    def _compile_space_around_punctuation_pattern(self):
        look_ahead_for_special_token = f""(?=[{self.punctuation_symbols}])""
        look_ahead_to_match_all_except_space = r""(?=[^\s])""
        return re.compile(r"""" + look_ahead_for_special_token + look_ahead_to_match_all_except_space)

    def count_file(self, path, verbose=False, add_eos=False):
        if verbose:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        vocab_file=None,
        pretrained_vocab_file: Optional[str] = None,
        never_split=None,
        unk_token=""<unk>"",
        eos_token=""<eos>"",
        additional_special_tokens=[""<formula>""],
        language=""en"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<eos>"",","        pretrained_vocab_file: Optional[str] = None,
        never_split=None,
        unk_token=""<unk>"",
        eos_token=""<eos>"",
        additional_special_tokens=[""<formula>""],
        language=""en"",
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        do_lower_case=False,
        encoding=""utf8"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_lower_case=False,
        encoding=""utf8"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        encoding=""utf8"",
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        sp_model_kwargs: Optional[dict[str, Any]] = None,
        **kwargs,
    ) -> None:",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"from_state_dict = torch.load(checkpoint_path, weights_only=True)[""state_dict""]","    print(f""Downloading weights for {name}..."")
    checkpoint_path = cached_download(checkpoint)
    print(f""Converting {name}..."")
    from_state_dict = torch.load(checkpoint_path, weights_only=True)[""state_dict""]
    from_model.load_state_dict(from_state_dict)
    from_model.eval()
    with torch.no_grad():",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"from_state_dict = torch.load(checkpoint_path, weights_only=True)[""state_dict""]","    print(f""Downloading weights for {name}..."")
    checkpoint_path = cached_download(checkpoint)
    print(f""Converting {name}..."")
    from_state_dict = torch.load(checkpoint_path, weights_only=True)[""state_dict""]
    from_model.load_state_dict(from_state_dict)
    from_model.eval()
    with torch.no_grad():",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
from_model.eval(),"    print(f""Converting {name}..."")
    from_state_dict = torch.load(checkpoint_path, weights_only=True)[""state_dict""]
    from_model.load_state_dict(from_state_dict)
    from_model.eval()
    with torch.no_grad():
        our_model = VanForImageClassification(config).eval()
        module_transfer = ModuleTransfer(src=from_model, dest=our_model)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
our_model = VanForImageClassification(config).eval(),"    from_model.load_state_dict(from_state_dict)
    from_model.eval()
    with torch.no_grad():
        our_model = VanForImageClassification(config).eval()
        module_transfer = ModuleTransfer(src=from_model, dest=our_model)
        x = torch.randn((1, 3, 224, 224))
        module_transfer(x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_bort_model.eval(),"
    hf_bort_config = BertConfig.from_dict(hf_bort_config_json)
    hf_bort_model = BertForMaskedLM(hf_bort_config)
    hf_bort_model.eval()

    # Parameter mapping table (Gluonnlp to Transformers)
    # * denotes layer index",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_bort_model.eval(),"    # Get Transformer output (save and reload model again)
    hf_bort_model.save_pretrained(pytorch_dump_folder_path)
    hf_bort_model = BertModel.from_pretrained(pytorch_dump_folder_path)
    hf_bort_model.eval()

    input_ids = tokenizer.encode_plus(SAMPLE_TEXT, return_tensors=""pt"")
    output_hf = hf_bort_model(**input_ids)[0]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"    print(
        ""Original Mega encoder:"",
        original_mlm.mega.load_state_dict(
            torch.load(
                os.path.join(pretrained_checkpoint_path, ""encoder_weights.pt""), map_location=""cpu"", weights_only=True
            )
        ),",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"    print(
        ""Original Mega MLM layer:"",
        original_mlm.mlm_head.load_state_dict(
            torch.load(
                os.path.join(pretrained_checkpoint_path, ""mlm_head_weights.pt""), map_location=""cpu"", weights_only=True
            )
        ),",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
torch.load(,"    print(
        ""HF Mega MLM layer:"",
        hf_mlm.mlm_head.load_state_dict(
            torch.load(
                os.path.join(pretrained_checkpoint_path, ""mlm_head_weights.pt""), map_location=""cpu"", weights_only=True
            )
        ),",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval(),"        mega_original_args = pkl.load(f)

    # load the original encoder
    original_mlm = OriginalMegaForMaskedLM(**mega_original_args).eval()

    # load its weights
    print(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
hf_mlm = MegaForMaskedLM(hf_config).eval(),"        add_lm_hidden_dense_layer=False,
    )

    hf_mlm = MegaForMaskedLM(hf_config).eval()

    # the original checkpoint just uses nn.Embedding for the word embeddings
    # we use a wrapper module for embeddings to add support for positional embeddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"old_dic = torch.load(f""{pytorch_dump_folder_path}/{dict_name.split('/')[-1]}"", weights_only=True)[""model""]","    weight_dict = []
    mapping = {}
    for i, dict_name in enumerate(model_to_convert):
        old_dic = torch.load(f""{pytorch_dump_folder_path}/{dict_name.split('/')[-1]}"", weights_only=True)[""model""]

        new_dic = {}
        for k in old_dic:",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"re_encoder_block_conv_in = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")","    new_dict = {}
    import re

    re_encoder_block_conv_in = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")
    re_encoder_block_resnet = re.compile(
        r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
re_encoder_block_resnet = re.compile(,"    import re

    re_encoder_block_conv_in = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")
    re_encoder_block_resnet = re.compile(
        r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )
    re_encoder_block_proj_out = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_encoder_block_proj_out = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")","    re_encoder_block_resnet = re.compile(
        r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )
    re_encoder_block_proj_out = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")

    re_decoder_block_conv_out = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")
    re_decoder_block_resnet = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_decoder_block_conv_out = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")","    )
    re_encoder_block_proj_out = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")

    re_decoder_block_conv_out = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")
    re_decoder_block_resnet = re.compile(
        r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
re_decoder_block_resnet = re.compile(,"    re_encoder_block_proj_out = re.compile(r""encoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")

    re_decoder_block_conv_out = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).(bias|weight)"")
    re_decoder_block_resnet = re.compile(
        r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )
    re_decoder_block_proj_in = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_decoder_block_proj_in = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")","    re_decoder_block_resnet = re.compile(
        r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )
    re_decoder_block_proj_in = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")

    re_prior_cond_conv_out = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).(bias|weight)"")
    re_prior_cond_resnet = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_prior_cond_conv_out = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).(bias|weight)"")","    )
    re_decoder_block_proj_in = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")

    re_prior_cond_conv_out = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).(bias|weight)"")
    re_prior_cond_resnet = re.compile(
        r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
re_prior_cond_resnet = re.compile(,"    re_decoder_block_proj_in = re.compile(r""decoders.(\d*).level_blocks.(\d*).model.(\d*).(bias|weight)"")

    re_prior_cond_conv_out = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).(bias|weight)"")
    re_prior_cond_resnet = re.compile(
        r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )
    re_prior_cond_proj_in = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(bias|weight)"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"re_prior_cond_proj_in = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(bias|weight)"")","    re_prior_cond_resnet = re.compile(
        r""conditioner_blocks.(\d*).cond.model.(\d*).(\d).model.(\d*).model.(\d*).(bias|weight)""
    )
    re_prior_cond_proj_in = re.compile(r""conditioner_blocks.(\d*).cond.model.(\d*).(bias|weight)"")

    for original_key, value in state_dict.items():
        # rename vqvae.encoder keys",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.out_of_vocab = regex.compile(oov),"        if len(self.lyrics_encoder) == 79:
            oov = oov.replace(r""\-'"", r""\-+'"")

        self.out_of_vocab = regex.compile(oov)
        self.artists_decoder = {v: k for k, v in self.artists_encoder.items()}
        self.genres_decoder = {v: k for k, v in self.genres_encoder.items()}
        self.lyrics_decoder = {v: k for k, v in self.lyrics_encoder.items()}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.out_of_vocab = regex.compile(r""[^A-Za-z0-9.,:;!?\-'\""()\[\] \t\n]+"")","                ]  # split is for the full dictionary with combined genres

        if self.version[0] == ""v2"":
            self.out_of_vocab = regex.compile(r""[^A-Za-z0-9.,:;!?\-'\""()\[\] \t\n]+"")
            vocab = ""ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789.,:;!?-+'\""()[] \t\n""
            self.vocab = {vocab[index]: index + 1 for index in range(len(vocab))}
            self.vocab[""<unk>""] = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.out_of_vocab = regex.compile(r""[^A-Za-z0-9.,:;!?\-+'\""()\[\] \t\n]+"")","            self.lyrics_decoder = {v: k for k, v in self.vocab.items()}
            self.lyrics_decoder[0] = """"
        else:
            self.out_of_vocab = regex.compile(r""[^A-Za-z0-9.,:;!?\-+'\""()\[\] \t\n]+"")

        lyrics = self._run_strip_accents(lyrics)
        lyrics = lyrics.replace(""\\"", ""\n"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"pattern = re.compile(r""_+"")","            + ["".""]
        )
        accepted = frozenset(accepted)
        pattern = re.compile(r""_+"")
        text = """".join([c if c in accepted else ""_"" for c in text.lower()])
        text = pattern.sub(""_"", text).strip(""_"")
        return text",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|endoftext|>"",","        version=[""v3"", ""v2"", ""v2""],
        max_n_lyric_tokens=512,
        n_genres=5,
        unk_token=""<|endoftext|>"",
        **kwargs,
    ):
        unk_token = AddedToken(unk_token, lstrip=False, rstrip=False) if isinstance(unk_token, str) else unk_token",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
">>> model = JukeboxVQVAE.from_pretrained(""openai/jukebox-1b-lyrics"").eval()","        >>> from transformers import JukeboxVQVAE, set_seed
        >>> import torch

        >>> model = JukeboxVQVAE.from_pretrained(""openai/jukebox-1b-lyrics"").eval()
        >>> set_seed(0)
        >>> zs = [torch.randint(100, (4, 1))]
        >>> model.decode(zs).shape",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
">>> model = JukeboxModel.from_pretrained(""openai/jukebox-1b-lyrics"", min_duration=0).eval()","
        >>> metas = dict(artist=""Zac Brown Band"", genres=""Country"", lyrics=""I met a traveller from an antique land"")
        >>> tokenizer = AutoTokenizer.from_pretrained(""openai/jukebox-1b-lyrics"")
        >>> model = JukeboxModel.from_pretrained(""openai/jukebox-1b-lyrics"", min_duration=0).eval()

        >>> labels = tokenizer(**metas)[""input_ids""]
        >>> set_seed(0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
">>> model = JukeboxModel.from_pretrained(""openai/jukebox-1b-lyrics"", min_duration=0).eval()","        ```python
        >>> from transformers import AutoTokenizer, JukeboxModel, set_seed

        >>> model = JukeboxModel.from_pretrained(""openai/jukebox-1b-lyrics"", min_duration=0).eval()
        >>> tokenizer = AutoTokenizer.from_pretrained(""openai/jukebox-1b-lyrics"")

        >>> lyrics = ""Hey, are you awake? Can you talk to me?""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")","        self.do_lower_case = do_lower_case

        # Should have added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions
        self.pat = re.compile(r""""""'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""""")

        # additional properties
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","        merges_file,
        do_lower_case=True,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        do_lower_case=True,
        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""</s>"",","        errors=""replace"",
        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""<s>"",","        bos_token=""<s>"",
        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        eos_token=""</s>"",
        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        sep_token=""</s>"",
        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        max_cell_length=15,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""<mask>"",","        cls_token=""<s>"",
        unk_token=""<unk>"",
        pad_token=""<pad>"",
        mask_token=""<mask>"",
        add_prefix_space=False,
        max_cell_length=15,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
timm_model.eval(),"
    # load original model from timm
    timm_model = timm.create_model(vit_name, pretrained=True)
    timm_model.eval()

    # load state_dict of original model, remove and rename some keys
    state_dict = timm_model.state_dict()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = ViTHybridModel(config).eval(),"
    # load HuggingFace model
    if vit_name[-5:] == ""in21k"":
        model = ViTHybridModel(config).eval()
    else:
        model = ViTHybridForImageClassification(config).eval()
    model.load_state_dict(state_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
model = ViTHybridForImageClassification(config).eval(),"    if vit_name[-5:] == ""in21k"":
        model = ViTHybridModel(config).eval()
    else:
        model = ViTHybridForImageClassification(config).eval()
    model.load_state_dict(state_dict)

    # create image processor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    else:
        raise ValueError(f""Model name {model_name} not supported"")
    checkpoint_path = hf_hub_download(repo_id=""nielsr/deta-checkpoints"", filename=filename)
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]

    # rename keys
    rename_keys = create_rename_keys(config)",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    else:
        raise ValueError(f""Model name {model_name} not supported"")
    checkpoint_path = hf_hub_download(repo_id=""nielsr/deta-checkpoints"", filename=filename)
    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]

    # rename keys
    rename_keys = create_rename_keys(config)",pickle_load,ML-pickle_load,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = DetaForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    model.to(device)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","    else:
        raise ValueError(f""Model name {model_name} not supported"")

    state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]

    # original state dict
    for name, param in state_dict.items():",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    # finally, create HuggingFace model and load state dict
    model = DetaForObjectDetection(config)
    model.load_state_dict(state_dict)
    model.eval()

    device = ""cuda"" if torch.cuda.is_available() else ""cpu""
    model.to(device)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<s>"",","    def __init__(
        self,
        vocab_file,
        bos_token=""<s>"",
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<pad>"",","        self,
        vocab_file,
        bos_token=""<s>"",
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        do_lower_case=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""</s>"",","        vocab_file,
        bos_token=""<s>"",
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        do_lower_case=False,
        merges_file=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<unk>"",","        bos_token=""<s>"",
        pad_token=""<pad>"",
        eos_token=""</s>"",
        unk_token=""<unk>"",
        do_lower_case=False,
        merges_file=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]","def convert_efficientformer_checkpoint(
    checkpoint_path: Path, efficientformer_config_file: Path, pytorch_dump_path: Path, push_to_hub: bool
):
    orig_state_dict = torch.load(checkpoint_path, map_location=""cpu"", weights_only=True)[""model""]
    config = EfficientFormerConfig.from_json_file(efficientformer_config_file)
    model = EfficientFormerForImageClassificationWithTeacher(config)
    model_name = ""_"".join(checkpoint_path.split(""/"")[-1].split(""."")[0].split(""_"")[:-1])",torch.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"    new_state_dict = convert_torch_checkpoint(orig_state_dict, num_meta4D_last_stage)

    model.load_state_dict(new_state_dict)
    model.eval()

    pillow_resamplings = {
        ""bilinear"": PILImageResampling.BILINEAR,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        do_lower_case=True,
        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        do_basic_tokenize=True,
        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        never_split=None,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""[UNK]"",","        vocab_file=None,
        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""[SEP]"",","        tokenizer_file=None,
        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""[PAD]"",","        do_lower_case=True,
        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"cls_token=""[CLS]"",","        unk_token=""[UNK]"",
        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"mask_token=""[MASK]"",","        sep_token=""[SEP]"",
        pad_token=""[PAD]"",
        cls_token=""[CLS]"",
        mask_token=""[MASK]"",
        tokenize_chinese_chars=True,
        strip_accents=None,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")","        self.ids_to_tokens = ids_to_tokens  # same as bpe
        self.emoji = emoji
        self.maxlen = np.max([len(w) for w in self.vocab])
        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")","        self.emoji = emoji
        self.maxlen = np.max([len(w) for w in self.vocab])
        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")","        self.maxlen = np.max([len(w) for w in self.vocab])
        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter4 = re.compile(,"        self.content_repatter1 = re.compile(r""(https?|ftp)(:\/\/[-_\.!~*\'()a-zA-Z0-9;\/?:\@&=\+$,%#]+)"")
        self.content_repatter2 = re.compile(r""[A-Za-z0-9\._+]*@[\-_0-9A-Za-z]+(\.[A-Za-z]+)*"")
        self.content_repatter3 = re.compile(r""[\(]{0,1}[0-9]{2,4}[\)\-\(]{0,1}[0-9]{2,4}[\)\-]{0,1}[0-9]{3,4}"")
        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )
        self.content_repatter5 = re.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter5 = re.compile(,"        self.content_repatter4 = re.compile(
            r""([12]\d{3}[/\-])*(0?[1-9]|1[0-2])[/\-]((0?[1-9]|[12][0-9]|3[01])?)*(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )
        self.content_repatter5 = re.compile(
            r""(|||||||||\u32ff)\d{1,2}(0?[1-9]|1[0-2])(0?[1-9]|[12][0-9]|3[01])(\d{1,2}|:|\d{1,2}|\d{1,2}|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|\(\)|||||||)*""
        )
        # The original version of this regex displays catastrophic backtracking behaviour. We avoid this using",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter6 = re.compile(,"        # possessive quantifiers in Py >= 3.11. In versions below this, we avoid the vulnerability using a slightly
        # different regex that should generally have the same behaviour in most non-pathological cases.
        if sys.version_info >= (3, 11):
            self.content_repatter6 = re.compile(
                r""(?:\d,\d{3}|[\d])*+""
                r""(?:\d,\d{3}|[\d])*+""
                r""(?:\d,\d{3}|[\d])*+""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
self.content_repatter6 = re.compile(,"                r""(?:\(\)|\(\)|\+tax)*""
            )
        else:
            self.content_repatter6 = re.compile(
                r""(?:\d,\d{3}|[\d])*""
                r""(?:|||||||||||)+""
                r""(?:\(\)|\(\)|\+tax)*""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"unk_token=""<|nottoken|>"",","        self,
        vocab_file,
        emoji_file,
        unk_token=""<|nottoken|>"",
        pad_token=""<|separator|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"pad_token=""<|separator|>"",","        vocab_file,
        emoji_file,
        unk_token=""<|nottoken|>"",
        pad_token=""<|separator|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        sep_token=""<|segmenter|>"",",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"bos_token=""<|startoftext|>"",","        emoji_file,
        unk_token=""<|nottoken|>"",
        pad_token=""<|separator|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        sep_token=""<|segmenter|>"",
        do_clean_text=False,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"eos_token=""<|endoftext|>"",","        unk_token=""<|nottoken|>"",
        pad_token=""<|separator|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        sep_token=""<|segmenter|>"",
        do_clean_text=False,
        **kwargs,",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
"sep_token=""<|segmenter|>"",","        pad_token=""<|separator|>"",
        bos_token=""<|startoftext|>"",
        eos_token=""<|endoftext|>"",
        sep_token=""<|segmenter|>"",
        do_clean_text=False,
        **kwargs,
    ):",credentials,ML-credentials,HIGH,CWE-798,Python,ML/AI,1,ML/AI:Transformers
model.eval(),"            if completed_steps >= args.max_train_steps:
                break

        model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                outputs = model(**batch)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:Transformers
"importlib.import_module(""tf_keras.src.optimizers"")","# See b/196254385 for more details.
try:
  if _tf_uses_legacy_keras:
    importlib.import_module(""tf_keras.src.optimizers"")
  else:
    importlib.import_module(""keras.src.optimizers"")
except (ImportError, AttributeError):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"importlib.import_module(""keras.src.optimizers"")","  if _tf_uses_legacy_keras:
    importlib.import_module(""tf_keras.src.optimizers"")
  else:
    importlib.import_module(""keras.src.optimizers"")
except (ImportError, AttributeError):
  pass
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"raise RuntimeError(""Gradient computation of graph in xla.compile() is ""","@ops.RegisterGradient(""XlaClusterOutput"")
def _XlaClusterOutputGrad(_, grad):
  del grad  # unused
  raise RuntimeError(""Gradient computation of graph in xla.compile() is ""
                     ""prohibited because it can cause performance degradation.""
                     ""Please move gradient computation inside xla.compile()."")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"""Please move gradient computation inside xla.compile()."")","  del grad  # unused
  raise RuntimeError(""Gradient computation of graph in xla.compile() is ""
                     ""prohibited because it can cause performance degradation.""
                     ""Please move gradient computation inside xla.compile()."")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
compiled_regex = re.compile(,"
def _substitute_for_loop_template(module: str) -> str:
  """"""Substitutes the for loop templates in the given module.""""""
  compiled_regex = re.compile(
      r'^\s*for\s(.*?)\sin\s(\[.*?\])\s\{(.*?)\}\s//\send\sfor\n',
      re.MULTILINE | re.DOTALL)
  while True:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
compiled_regex = re.compile(,"
def _substitute_parameterization_template(module: str) -> str:
  """"""Substitutes all the function templates in the given module.""""""
  compiled_regex = re.compile(
      r'^\s*parameters(\[.*?\])\n?(^\s*(?:func\.)+func.*?\{.*?(?:func\.)+return.*?\}\n)',
      re.MULTILINE | re.DOTALL)
  while True:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
compiled_regex = re.compile(r'GenerateImplFunctionName\(([\w\s]+)\)'),"
def _substitute_impl_function_name_template(module: str) -> str:
  """"""Generates the op-specific implementation function name.""""""
  compiled_regex = re.compile(r'GenerateImplFunctionName\(([\w\s]+)\)')
  while True:
    func_match = re.search(compiled_regex, module)
    if func_match is None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
compiled_regex = re.compile(,"
def _substitute_quantized_function_name_template(module: str) -> str:
  """"""Generates the quantized function name.""""""
  compiled_regex = re.compile(
      r'GenerateQuantizedFunctionName(\([\w\s\'\""\[\],]+\))')
  while True:
    func_match = re.search(compiled_regex, module)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tensorlike_value_1 = tensorlike_value_1.eval(session=sess),"      tensorlike_value_2: A TensorLike value.
    """"""
    if isinstance(tensorlike_value_1, core.Tensor):
      tensorlike_value_1 = tensorlike_value_1.eval(session=sess)

    if isinstance(tensorlike_value_2, core.Tensor):
      tensorlike_value_2 = tensorlike_value_2.eval(session=sess)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tensorlike_value_2 = tensorlike_value_2.eval(session=sess),"      tensorlike_value_1 = tensorlike_value_1.eval(session=sess)

    if isinstance(tensorlike_value_2, core.Tensor):
      tensorlike_value_2 = tensorlike_value_2.eval(session=sess)

    self.assertAllClose(tensorlike_value_1, tensorlike_value_2)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
input_tensor_data = input_tensor.eval(),"      feed_dict = repr_dataset.create_feed_dict_from_input_data(
          sample, signature_def
      )
      input_tensor_data = input_tensor.eval()

    self.assertLen(feed_dict, 1)
    self.assertIn('input:0', feed_dict)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
input_data = input_data.eval(session=sess),"    for input_key, input_data in sample.items():
      # Evaluate the Tensor to get the actual value.
      if isinstance(input_data, core.Tensor):
        input_data = input_data.eval(session=sess)

      new_sample[input_key] = input_data
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
value = input_value.eval(),"    value = input_value
    if isinstance(input_value, core.Tensor):
      # Take the data out of the tensor.
      value = input_value.eval()

    feed_dict[input_tensor_name] = value
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
ret = ret.eval(),"      try:
        ret = ret.numpy()
      except AttributeError:
        ret = ret.eval()
      return ret

    min_max_mse = get_mean_square_error(original_output, min_max_output)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
compiled_regex = re.compile(node_name),"    def match_node_name(name):
      if not node_name:
        return True
      compiled_regex = re.compile(node_name)
      match = re.fullmatch(compiled_regex, name)
      return match is not None
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mse')","  x_train = tf.constant([1, 2, 3, 4, 5], dtype=tf.float32)
  y_train = tf.constant([2, 3, 4, 5, 6], dtype=tf.float32)

  model.compile(optimizer='sgd', loss='mse')
  model.fit(x_train, y_train, epochs=1)

  path = tempdir + '/add_one_model'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"image_tensor = image_ops.decode_image(file_data, channels=channels).eval()","    with session.Session():
      file_data = io_ops.read_file(filepath)
      channels = 1 if want_grayscale else 3
      image_tensor = image_ops.decode_image(file_data, channels=channels).eval()
      resized_tensor = image_ops.resize_images_v2(image_tensor,
                                                  (height, width)).eval()
  return resized_tensor",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"(height, width)).eval()","      channels = 1 if want_grayscale else 3
      image_tensor = image_ops.decode_image(file_data, channels=channels).eval()
      resized_tensor = image_ops.resize_images_v2(image_tensor,
                                                  (height, width)).eval()
  return resized_tensor

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"pattern = re.compile(r'\W*(0x[0-9a-fA-F,x ]+).*')","    A bytearray corresponding to the input cc file array.
  """"""
  # Match hex values in the string with comma as separator
  pattern = re.compile(r'\W*(0x[0-9a-fA-F,x ]+).*')

  model_bytearray = bytearray()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.call(command, shell=True)","    input_cc_file = os.path.join(tmp_dir, 'model.cc')

    command = 'xxd -i {} > {}'.format(model_filename, input_cc_file)
    subprocess.call(command, shell=True)

    # 4. VALIDATE
    final_bytes = flatbuffer_utils.xxd_output_to_bytes(input_cc_file)",subprocess.call,Command Injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.call(command, shell=True)","    input_cc_file = os.path.join(tmp_dir, 'model.cc')

    command = 'xxd -i {} > {}'.format(model_filename, input_cc_file)
    subprocess.call(command, shell=True)

    # 4. VALIDATE
    final_bytes = flatbuffer_utils.xxd_output_to_bytes(input_cc_file)",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.call(command, shell=True)","    input_cc_file = os.path.join(tmp_dir, 'model.cc')

    command = 'xxd -i {} > {}'.format(model_filename, input_cc_file)
    subprocess.call(command, shell=True)

    # 4. VALIDATE
    final_bytes = flatbuffer_utils.xxd_output_to_bytes(input_cc_file)",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"html_text, re.compile(r'%s' % model_filename, re.MULTILINE | re.DOTALL))","    # It's hard to test debug output without doing a full HTML parse,
    # but at least sanity check that expected identifiers are present.
    self.assertRegex(
        html_text, re.compile(r'%s' % model_filename, re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text,
                     re.compile(r'input_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile(r'input_tensor', re.MULTILINE | re.DOTALL))","    self.assertRegex(
        html_text, re.compile(r'%s' % model_filename, re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text,
                     re.compile(r'input_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text,
                     re.compile(r'constant_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text, re.compile(r'ADD', re.MULTILINE | re.DOTALL))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile(r'constant_tensor', re.MULTILINE | re.DOTALL))","    self.assertRegex(html_text,
                     re.compile(r'input_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text,
                     re.compile(r'constant_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text, re.compile(r'ADD', re.MULTILINE | re.DOTALL))

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertRegex(html_text, re.compile(r'ADD', re.MULTILINE | re.DOTALL))","                     re.compile(r'input_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text,
                     re.compile(r'constant_tensor', re.MULTILINE | re.DOTALL))
    self.assertRegex(html_text, re.compile(r'ADD', re.MULTILINE | re.DOTALL))


if __name__ == '__main__':",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"      model.add(MyAddLayer(1.0))
    model.add(keras.layers.RepeatVector(3))
    model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))
    model.compile(
        loss=keras.losses.MSE,
        optimizer='sgd',
        metrics=[keras.metrics.categorical_accuracy],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"      output = keras.layers.Dense(3)(x)

      model = keras.models.Model(inputs, output)
      model.compile(
          loss=keras.losses.MSE,
          optimizer='sgd',
          metrics=[keras.metrics.categorical_accuracy])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"    e = keras.layers.Dropout(0.5, name='dropout')(c)

    model = keras.models.Model([a, b], [d, e])
    model.compile(
        loss=keras.losses.MSE,
        optimizer='sgd',
        metrics=[keras.metrics.mae],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"    model.add(keras.layers.TimeDistributed(keras.layers.Dense(3)))
    model = keras.models.Model(model.input, model.output)

    model.compile(
        loss=keras.losses.MSE,
        optimizer='sgd',
        metrics=[keras.metrics.categorical_accuracy],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
'../tflite_mobilenet_ssd_quant_protobuf/tflite_graph.pb'),"    if not os.path.exists(filename):
      filename = os.path.join(
          resource_loader.get_root_dir_with_all_resources(),
          '../tflite_mobilenet_ssd_quant_protobuf/tflite_graph.pb')
      if not os.path.exists(filename):
        raise IOError(""File '{0}' does not exist."".format(filename))
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"    model.add(tf.keras.layers.Lambda(lambda x: -x))

  # Train
  model.compile(
      optimizer=""adam"",
      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
      metrics=[""accuracy""])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(optimizer='sgd', loss='mean_squared_error')","  ...           tf.keras.layers.Dropout(0.2),
  ...           tf.keras.layers.Dense(units=1, input_shape=[1])
  ...         ])
  >>> model.compile(optimizer='sgd', loss='mean_squared_error')
  >>> model.fit(x, y, epochs=1)
  >>> converter = tf.lite.TFLiteConverter.from_keras_model(model)
  >>> tflite_model = converter.convert()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, x):","
      @tf.function(
          input_signature=[tf.TensorSpec(shape=[1, 4], dtype=tf.float32)])
      def eval(self, x):
        # Control flow is needed to generate ""FlexReadVariableOp"".
        if tf.reduce_mean(x) > 1.0:
          self.v.assign_add([[1.0, 1.0, 1.0, 1.0]])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(1),
    ])
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=1)

    save_dir = os.path.join(self.get_temp_dir(), 'saved_model')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(1),
    ])
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=1)

    export_dir = os.path.join(self.get_temp_dir(), 'exported_model')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(),"            batch_input_shape=(1, 1, 10, 10, 1),
        )
    ])
    model.compile()

    # Export the keras model to saved model.
    saved_model_dir = os.path.join(self.get_temp_dir(), 'conv_lstm_2d')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"    )(input_tensor)

    model = tf.keras.Model(inputs=[input_tensor], outputs=output)
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"    x = _FakeQuantArgsLayer()(x)
    model = tf.keras.Model(input_tensor, x)

    model.compile(
        optimizer='adam', loss='mean_squared_error', metrics=['accuracy']
    )
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","        tf.keras.layers.Dropout(0.2),
        tf.keras.layers.Dense(units=1, input_shape=[1]),
    ])
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=1)

    # Convert model and ensure model is not None.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","    model = tf.keras.models.Model(
        inputs=[input_a, input_b], outputs=[output_c, output_d]
    )
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit([input_a_np, input_b_np], [output_c_np, output_d_np], epochs=1)

    # Convert model and ensure model is not None.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","    model = tf.keras.models.Sequential(
        [tf.keras.layers.Dense(units=1, input_shape=[1])]
    )
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=1)
    converter = lite.TFLiteConverterV2.from_keras_model(model)
    converter.convert()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","
    # Building the model.
    model = Model()
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(input_data, input_data, epochs=1)

    # Convert model.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","        keras.layers.Dropout(0.2, input_shape=(1,)),
        keras.layers.Dense(1),
    ])
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=1)

    keras_file = self._getFilepath('model.h5')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
'../tflite_mobilenet_ssd_quant_protobuf/tflite_graph.pb'),"    if not os.path.exists(filename):
      filename = os.path.join(
          resource_loader.get_root_dir_with_all_resources(),
          '../tflite_mobilenet_ssd_quant_protobuf/tflite_graph.pb')
      if not os.path.exists(filename):
        raise IOError(""File '{0}' does not exist."".format(filename))
",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mean_squared_error')","    y = [-3, -1, 1, 3, 5, 7]
    model = tf.keras.models.Sequential(
        [tf.keras.layers.Dense(units=1, input_shape=[1])])
    model.compile(optimizer='sgd', loss='mean_squared_error')
    model.fit(x, y, epochs=1)
    converter = lite.TFLiteConverterV2.from_keras_model(model)
    mock_metrics = mock.create_autospec(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"    output = tf.keras.layers.Add()([input_tensor1, input_tensor2])
    model = tf.keras.Model(
        inputs=[input_tensor1, input_tensor2], outputs=output)
    model.compile(
        optimizer='adam',
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy'])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, x):","      @tf.function(input_signature=[
          tf.TensorSpec(shape=[None], dtype=tf.float32)
      ])
      def eval(self, x):
        return tf.cosh(x)

    m = Model()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = m.eval(tf.constant([0.0])),"        return tf.cosh(x)

    m = Model()
    result = m.eval(tf.constant([0.0]))
    log_messages = m.eval.get_compatibility_log()

    self.assertEqual(result, tf.constant([1.0]))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, x):","      @tf.function(input_signature=[
          tf.TensorSpec(shape=[None], dtype=tf.float32)
      ])
      def eval(self, x):
        return tf.cos(x)

    m = Model()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = m.eval(tf.constant([0.0])),"        return tf.cos(x)

    m = Model()
    result = m.eval(tf.constant([0.0]))
    self.assertEqual(result, tf.constant([1.0]))

    # Check if the decorator keeps __name__ attribute.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
config = {'normalizer': backend.eval(n) if is_tensor_or_variable(n) else n},"
  def get_config(self):
    n = self.normalizer
    config = {'normalizer': backend.eval(n) if is_tensor_or_variable(n) else n}
    base_config = super(MeanRelativeError, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
config[k] = backend.eval(v) if is_tensor_or_variable(v) else v,"      config['fn'] = self._fn

    for k, v in self._fn_kwargs.items():
      config[k] = backend.eval(v) if is_tensor_or_variable(v) else v
    base_config = super(MeanMetricWrapper, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
label_weights = backend.eval(self.label_weights),"
  def get_config(self):
    if is_tensor_or_variable(self.label_weights):
      label_weights = backend.eval(self.label_weights)
    else:
      label_weights = self.label_weights
    config = {",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
config[k] = backend.eval(v) if is_tensor_or_variable(v) else v,"  def get_config(self):
    config = {}
    for k, v in self._fn_kwargs.items():
      config[k] = backend.eval(v) if is_tensor_or_variable(v) else v
    base_config = super(SumOverBatchSizeMetricWrapper, self).get_config()
    return dict(list(base_config.items()) + list(config.items()))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  print('Final result: ', m.result().numpy())
  ```

  Usage with `compile()` API:

  ```python
  model = tf.keras.Sequential()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),","  model.add(tf.keras.layers.Dense(64, activation='relu'))
  model.add(tf.keras.layers.Dense(10, activation='softmax'))

  model.compile(optimizer=tf.keras.optimizers.RMSprop(0.01),
                loss=tf.keras.losses.CategoricalCrossentropy(),
                metrics=[tf.keras.metrics.CategoricalAccuracy()])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  16.0

  Usage with `compile()` API:

  ```python
  model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mse')","
  ```python
  model.add_metric(tf.keras.metrics.Sum(name='sum_1')(outputs))
  model.compile(optimizer='sgd', loss='mse')
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  2.0

  Usage with `compile()` API:

  ```python
  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mse')","
  ```python
  model.add_metric(tf.keras.metrics.Mean(name='mean_1')(outputs))
  model.compile(optimizer='sgd', loss='mse')
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.25

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.MeanRelativeError(normalizer=[1, 3])])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"keras_model.compile(..., metrics=accuracy_metric)","
  accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

  keras_model.compile(..., metrics=accuracy_metric)
  ```

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.5

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.Accuracy()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.5

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.BinaryAccuracy()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.3

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.CategoricalAccuracy()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.3

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.3

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.TopKCategoricalAccuracy()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.3

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SparseTopKCategoricalAccuracy()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.FalsePositives()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.FalseNegatives()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.TrueNegatives()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.TruePositives()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.5

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.Precision()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.Recall()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.333333

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.SensitivityAtSpecificity()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.5

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.SpecificityAtSensitivity()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.33333333

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.PrecisionAtRecall(recall=0.8)])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.RecallAtPrecision(precision=0.8)])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.0

  Usage with `compile()` API:

  ```python
  # Reports the AUC of a model outputting a probability.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","
  ```python
  # Reports the AUC of a model outputting a probability.
  model.compile(optimizer='sgd',
                loss=tf.keras.losses.BinaryCrossentropy(),
                metrics=[tf.keras.metrics.AUC()])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","                metrics=[tf.keras.metrics.AUC()])

  # Reports the AUC of a model outputting a logit.
  model.compile(optimizer='sgd',
                loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                metrics=[tf.keras.metrics.AUC(from_logits=True)])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.6999999

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.CosineSimilarity(axis=1)])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.5

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.MeanAbsoluteError()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  500000000.0

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.5

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.MeanSquaredError()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.24022643

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.MeanSquaredLogarithmicError()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.1

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss='mse', metrics=[tf.keras.metrics.Hinge()])
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.46

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.SquaredHinge()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.2

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.CategoricalHinge()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.70710677

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.RootMeanSquaredError()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.21689045

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.LogCoshError()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.99999994

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.Poisson()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.9162892

  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.KLDivergence()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.23809525

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.MeanIoU(num_classes=2)])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  0.9162905

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
      optimizer='sgd',
      loss='mse',
      metrics=[tf.keras.metrics.BinaryCrossentropy()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.6271976

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.CategoricalCrossentropy()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with `compile()` API:,"  >>> m.result().numpy()
  1.6271976

  Usage with `compile()` API:

  ```python
  model.compile(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  Usage with `compile()` API:

  ```python
  model.compile(
    optimizer='sgd',
    loss='mse',
    metrics=[tf.keras.metrics.SparseCategoricalCrossentropy()])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
def eval(x):,"

@doc_controls.do_not_generate_docs
def eval(x):
  """"""Evaluates the value of a variable.

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar),"
  >>> kvar = tf.keras.backend.variable(np.array([[1, 2], [3, 4]]),
  ...                                  dtype='float32')
  >>> tf.keras.backend.eval(kvar)
  array([[1.,  2.],
         [3.,  4.]], dtype=float32)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar),"  Example:

  >>> kvar = tf.keras.backend.zeros((3,4))
  >>> tf.keras.backend.eval(kvar)
  array([[0.,  0.,  0.,  0.],
         [0.,  0.,  0.,  0.],
         [0.,  0.,  0.,  0.]], dtype=float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar2),"         [0.,  0.,  0.,  0.]], dtype=float32)
  >>> A = tf.constant([1,2,3])
  >>> kvar2 = tf.keras.backend.zeros(A.shape) # [0., 0., 0.]
  >>> tf.keras.backend.eval(kvar2)
  array([0., 0., 0.], dtype=float32)
  >>> kvar3 = tf.keras.backend.zeros(A.shape,dtype=tf.int32)
  >>> tf.keras.backend.eval(kvar3)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar3),"  >>> tf.keras.backend.eval(kvar2)
  array([0., 0., 0.], dtype=float32)
  >>> kvar3 = tf.keras.backend.zeros(A.shape,dtype=tf.int32)
  >>> tf.keras.backend.eval(kvar3)
  array([0, 0, 0], dtype=int32)
  >>> kvar4 = tf.keras.backend.zeros([2,3])
  >>> tf.keras.backend.eval(kvar4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar4),"  >>> tf.keras.backend.eval(kvar3)
  array([0, 0, 0], dtype=int32)
  >>> kvar4 = tf.keras.backend.zeros([2,3])
  >>> tf.keras.backend.eval(kvar4)
  array([[0., 0., 0.],
         [0., 0., 0.]], dtype=float32)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar),"

  >>> kvar = tf.keras.backend.ones((3,4))
  >>> tf.keras.backend.eval(kvar)
  array([[1.,  1.,  1.,  1.],
         [1.,  1.,  1.,  1.],
         [1.,  1.,  1.,  1.]], dtype=float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar),"

  >>> kvar = tf.keras.backend.eye(3)
  >>> tf.keras.backend.eval(kvar)
  array([[1.,  0.,  0.],
         [0.,  1.,  0.],
         [0.,  0.,  1.]], dtype=float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
K.eval(kvar_zeros),"  from tensorflow.keras import backend as K
  kvar = K.variable(np.random.random((2,3)))
  kvar_zeros = K.zeros_like(kvar)
  K.eval(kvar_zeros)
  # array([[ 0.,  0.,  0.], [ 0.,  0.,  0.]], dtype=float32)
  ```
  """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar_ones),"
  >>> kvar = tf.keras.backend.variable(np.random.random((2,3)))
  >>> kvar_ones = tf.keras.backend.ones_like(kvar)
  >>> tf.keras.backend.eval(kvar_ones)
  array([[1.,  1.,  1.],
         [1.,  1.,  1.]], dtype=float32)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(kvar),"  >>> kvar = tf.keras.backend.zeros((2,3))
  >>> tf.keras.backend.count_params(kvar)
  6
  >>> tf.keras.backend.eval(kvar)
  array([[0.,  0.,  0.],
         [0.,  0.,  0.]], dtype=float32)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(var),"  Examples:

  >>> var = tf.keras.backend.variable([[1, 2, 3], [4, 5, 6]])
  >>> tf.keras.backend.eval(var)
  array([[1.,  2.,  3.],
         [4.,  5.,  6.]], dtype=float32)
  >>> var_transposed = tf.keras.backend.transpose(var)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(var_transposed),"  array([[1.,  2.,  3.],
         [4.,  5.,  6.]], dtype=float32)
  >>> var_transposed = tf.keras.backend.transpose(var)
  >>> tf.keras.backend.eval(var_transposed)
  array([[1.,  4.],
         [2.,  5.],
         [3.,  6.]], dtype=float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(var),"  Examples:

  >>> var = tf.keras.backend.variable([[1, 2, 3], [4, 5, 6]])
  >>> tf.keras.backend.eval(var)
  array([[1., 2., 3.],
         [4., 5., 6.]], dtype=float32)
  >>> var_gathered = tf.keras.backend.gather(var, [0])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(var_gathered),"  array([[1., 2., 3.],
         [4., 5., 6.]], dtype=float32)
  >>> var_gathered = tf.keras.backend.gather(var, [0])
  >>> tf.keras.backend.eval(var_gathered)
  array([[1., 2., 3.]], dtype=float32)
  >>> var_gathered = tf.keras.backend.gather(var, [1])
  >>> tf.keras.backend.eval(var_gathered)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(var_gathered),"  >>> tf.keras.backend.eval(var_gathered)
  array([[1., 2., 3.]], dtype=float32)
  >>> var_gathered = tf.keras.backend.gather(var, [1])
  >>> tf.keras.backend.eval(var_gathered)
  array([[4., 5., 6.]], dtype=float32)
  >>> var_gathered = tf.keras.backend.gather(var, [0,1,0])
  >>> tf.keras.backend.eval(var_gathered)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> tf.keras.backend.eval(var_gathered),"  >>> tf.keras.backend.eval(var_gathered)
  array([[4., 5., 6.]], dtype=float32)
  >>> var_gathered = tf.keras.backend.gather(var, [0,1,0])
  >>> tf.keras.backend.eval(var_gathered)
  array([[1., 2., 3.],
         [4., 5., 6.],
         [1., 2., 3.]], dtype=float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"return x.eval(session=get_session((x,)))","      return x.numpy()

  with x.graph.as_default():
    return x.eval(session=get_session((x,)))


@dispatch.add_dispatch_support",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
`feed_dict` is passed once in the constructor (called in `model.compile()`),"  In particular additional operations via `fetches` argument and additional
  tensor substitutions via `feed_dict` arguments. Note that given
  substitutions are merged with substitutions from `inputs`. Even though
  `feed_dict` is passed once in the constructor (called in `model.compile()`)
  we can modify the values in the dictionary. Through this feed_dict we can
  provide additional substitutions besides Keras inputs.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
clone.compile(**compile_args),"        compile_args['metrics'])
    compile_args['weighted_metrics'] = metrics_module.clone_metrics(
        compile_args['weighted_metrics'])
    clone.compile(**compile_args)

  return clone
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  if validate_training:
    model = models.Model(x, layer(x))
    if _thread_local_data.run_eagerly is not None:
      model.compile(
          'rmsprop',
          'mse',
          weighted_metrics=['acc'],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile('rmsprop', 'mse', weighted_metrics=['acc'])","          weighted_metrics=['acc'],
          run_eagerly=should_run_eagerly())
    else:
      model.compile('rmsprop', 'mse', weighted_metrics=['acc'])
    model.train_on_batch(input_data, actual_output)

  # test as first layer in Sequential API",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> model.compile(loss='mean_squared_error'),"  ...     global training_finished
  ...     training_finished = True
  >>> model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1,))])
  >>> model.compile(loss='mean_squared_error')
  >>> model.fit(tf.constant([[1.0]]), tf.constant([[1.0]]),
  ...           callbacks=[MyCallback()])
  >>> assert training_finished == True",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(tf.keras.optimizers.SGD(), loss='mse')","  Example:

  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')
  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
  ...                     epochs=10, verbose=1)
  >>> print(history.params)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(loss=..., optimizer=...,","  Example:

  ```python
  model.compile(loss=..., optimizer=...,
                metrics=['accuracy'])

  EPOCHS = 10",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(tf.keras.optimizers.SGD(), loss='mse')","  >>> callback = tf.keras.callbacks.experimental.BackupAndRestore(
  ... backup_dir=""/tmp/backup"")
  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')
  >>> try:
  ...   model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
  ...             batch_size=1, callbacks=[callback, InterruptingCallback()],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"To make it so, pass the loss or metrics at `model.compile()`.","  `model.stop_training` is marked True and the training terminates.

  The quantity to be monitored needs to be available in `logs` dict.
  To make it so, pass the loss or metrics at `model.compile()`.

  Args:
    monitor: Quantity to be monitored.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(tf.keras.optimizers.SGD(), loss='mse')","  >>> # This callback will stop the training when there is no improvement in
  >>> # the loss for three consecutive epochs.
  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')
  >>> history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
  ...                     epochs=10, batch_size=1, callbacks=[callback],
  ...                     verbose=0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(tf.keras.optimizers.SGD(), loss='mse')","  ...     return lr * tf.math.exp(-0.1)
  >>>
  >>> model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
  >>> model.compile(tf.keras.optimizers.SGD(), loss='mse')
  >>> round(model.optimizer.lr.numpy(), 5)
  0.01
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile('sgd', 'mse')","      return outputs

  model = MyModel()
  model.compile('sgd', 'mse')

  # Make sure to set `update_freq=N` to log a batch-level summary every N batches.
  # In addition to any `tf.summary` contained in `Model.call`, metrics added in",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile('sgd', 'mse')","  x = tf.keras.layers.Dense(10)(inputs)
  outputs = tf.keras.layers.Lambda(my_summary)(x)
  model = tf.keras.Model(inputs, outputs)
  model.compile('sgd', 'mse')

  # Make sure to set `update_freq=N` to log a batch-level summary every N batches.
  # In addition to any `tf.summary` contained in `Model.call`, metrics added in",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
config[k] = backend.eval(v) if tf_utils.is_tensor_or_variable(v) else v,"  def get_config(self):
    config = {}
    for k, v in self._fn_kwargs.items():
      config[k] = backend.eval(v) if tf_utils.is_tensor_or_variable(v) else v
    base_config = super().get_config()
    return dict(list(base_config.items()) + list(config.items()))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> mse(y_true, y_pred).numpy()
  array([0.5, 0.5], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanSquaredError())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> mae(y_true, y_pred).numpy()
  array([0.5, 0.5], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.MeanAbsoluteError())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> mape(y_true, y_pred).numpy()
  array([25., 75.], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss=tf.keras.losses.MeanAbsolutePercentageError())
  ```
  """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> msle(y_true, y_pred).numpy()
  array([0.240, 0.240], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss=tf.keras.losses.MeanSquaredLogarithmicError())
  ```
  """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  With `tf.keras` API:

  ```python
  model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
    ....
  )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> cce(y_true, y_pred).numpy()
  array([0.0513, 2.303], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalCrossentropy())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> scce(y_true, y_pred).numpy()
  array([0.0513, 2.303], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd',
                loss=tf.keras.losses.SparseCategoricalCrossentropy())
  ```
  """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> h(y_true, y_pred).numpy()
  array([1.1, 1.5], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.Hinge())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> h(y_true, y_pred).numpy()
  array([1.46, 2.26], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.SquaredHinge())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> h(y_true, y_pred).numpy()
  array([1.2, 1.6], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.CategoricalHinge())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> p(y_true, y_pred).numpy()
  array([0.999, 0.], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.Poisson())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> l(y_true, y_pred).numpy()
  array([0.217, 0.], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.LogCosh())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> kl(y_true, y_pred).numpy()
  array([0.916, -3.08e-06], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.KLDivergence())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> h(y_true, y_pred).numpy()
  array([0.18, 0.13], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.Huber())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Usage with the `compile()` API:,"  >>> cosine_loss(y_true, y_pred).numpy()
  array([-0., -0.999], dtype=float32)

  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))","  Usage with the `compile()` API:

  ```python
  model.compile(optimizer='sgd', loss=tf.keras.losses.CosineSimilarity(axis=1))
  ```

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(loss='mse', optimizer='sgd', metrics=['acc'])","      model = keras.models.Sequential()
      model.add(keras.layers.Dense(2, input_shape=(3,)))
      model.add(keras.layers.Dense(3))
      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

      keras.models.save_model(model, saved_model_dir, save_format=save_format)
      model = keras.models.load_model(saved_model_dir)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(loss='mse', optimizer='sgd', metrics=['acc'])","      model = keras.models.Sequential()
      model.add(keras.layers.Dense(2, input_shape=(3,)))
      model.add(keras.layers.Dense(3))
      model.compile(loss='mse', optimizer='sgd', metrics=['acc'])

      keras.models.save_model(model, saved_model_dir, save_format=save_format)
      model = tf.keras.models.load_model(saved_model_dir)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer, loss, metrics=metrics)","      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(optimizer, loss, metrics=metrics)

      inputs = np.zeros((10, 3))
      targets = np.zeros((10, 4))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer, loss, metrics=metrics)","      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(optimizer, loss, metrics=metrics)

      inputs = np.zeros((10, 3))
      targets = np.zeros((10, 4))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"      optimizer = RMSPropOptimizer(learning_rate=0.001)
      loss = 'mse'
      metrics = ['mae']
      model.compile(
          optimizer, loss, metrics=metrics,
          run_eagerly=testing_utils.should_run_eagerly())
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(command, shell=True, check=True)","    subprocess.run(mkdir_command, shell=True, check=True)
    print(""Created directory ~/.config/systemd/user/"")
  command = f""mv {service_name} ~/.config/systemd/user/{service_name}""
  subprocess.run(command, shell=True, check=True)
  print(f""Service file moved to ~/.config/systemd/user/{service_name}"")

",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(mkdir_command, shell=True, check=True)","def move_file_to_systemd(service_name):
  if not os.path.exists(""~/.config/systemd/user/""):
    mkdir_command = ""mkdir -p ~/.config/systemd/user""
    subprocess.run(mkdir_command, shell=True, check=True)
    print(""Created directory ~/.config/systemd/user/"")
  command = f""mv {service_name} ~/.config/systemd/user/{service_name}""
  subprocess.run(command, shell=True, check=True)",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(command, shell=True, check=True)","      f""systemctl --user start {service_name}"",
  ]
  for command in commands:
    subprocess.run(command, shell=True, check=True)
    print(f""Executed: {command}"")

",subprocess.run,Command Injection,MEDIUM,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(mkdir_command, shell=True, check=True)","def move_file_to_systemd(service_name):
  if not os.path.exists(""~/.config/systemd/user/""):
    mkdir_command = ""mkdir -p ~/.config/systemd/user""
    subprocess.run(mkdir_command, shell=True, check=True)
    print(""Created directory ~/.config/systemd/user/"")
  command = f""mv {service_name} ~/.config/systemd/user/{service_name}""
  subprocess.run(command, shell=True, check=True)",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(command, shell=True, check=True)","    subprocess.run(mkdir_command, shell=True, check=True)
    print(""Created directory ~/.config/systemd/user/"")
  command = f""mv {service_name} ~/.config/systemd/user/{service_name}""
  subprocess.run(command, shell=True, check=True)
  print(f""Service file moved to ~/.config/systemd/user/{service_name}"")

",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(command, shell=True, check=True)","      f""systemctl --user start {service_name}"",
  ]
  for command in commands:
    subprocess.run(command, shell=True, check=True)
    print(f""Executed: {command}"")

",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(mkdir_command, shell=True, check=True)","def move_file_to_systemd(service_name):
  if not os.path.exists(""~/.config/systemd/user/""):
    mkdir_command = ""mkdir -p ~/.config/systemd/user""
    subprocess.run(mkdir_command, shell=True, check=True)
    print(""Created directory ~/.config/systemd/user/"")
  command = f""mv {service_name} ~/.config/systemd/user/{service_name}""
  subprocess.run(command, shell=True, check=True)",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(command, shell=True, check=True)","    subprocess.run(mkdir_command, shell=True, check=True)
    print(""Created directory ~/.config/systemd/user/"")
  command = f""mv {service_name} ~/.config/systemd/user/{service_name}""
  subprocess.run(command, shell=True, check=True)
  print(f""Service file moved to ~/.config/systemd/user/{service_name}"")

",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"subprocess.run(command, shell=True, check=True)","      f""systemctl --user start {service_name}"",
  ]
  for command in commands:
    subprocess.run(command, shell=True, check=True)
    print(f""Executed: {command}"")

",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
_pywrap_tfcompile.Compile(,"
  output_prefix = _shlex_quote(output_prefix)

  _pywrap_tfcompile.Compile(
      graph=frozen_graph_def_location,
      config=config_pbtxt_location,
      cpp_class=cpp_class,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
regex_pattern = re.compile(count_exclude_pattern),"
  # Filter out tensors that we don't want to count
  if count_exclude_pattern:
    regex_pattern = re.compile(count_exclude_pattern)
    new_var_to_shape_map = {}
    exclude_num_tensors = 0
    exclude_num_params = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
input_dict[input_key] = eval(expr)  # pylint: disable=eval-used,"            f'Expression ""{expr}"" is not a valid python literal.') from exc
    else:
      # ast.literal_eval does not work with numpy expressions
      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used
  return input_dict

",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
input_dict[input_key] = eval(expr)  # pylint: disable=eval-used,"            f'Expression ""{expr}"" is not a valid python literal.') from exc
    else:
      # ast.literal_eval does not work with numpy expressions
      input_dict[input_key] = eval(expr)  # pylint: disable=eval-used
  return input_dict

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(variable_value, v.eval())","  def _init_and_validate_variable(self, sess, variable_name, variable_value):
    v = variables.Variable(variable_value, name=variable_name)
    sess.run(variables.global_variables_initializer())
    self.assertEqual(variable_value, v.eval())

  @test_util.deprecated_graph_mode_only
  def testReadSavedModelValid(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(b""k1"", v1.keys().eval())","      # Instantiate a wrapper object from the checkpointed reference.
      v1 = saver_test_utils.CheckpointedOp(
          name=""v1"", table_ref=ops.get_collection(""table_ref"")[0])
      self.assertEqual(b""k1"", v1.keys().eval())
      self.assertEqual(3.0, v1.values().eval())

  def testCustomSaver(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(3.0, v1.values().eval())","      v1 = saver_test_utils.CheckpointedOp(
          name=""v1"", table_ref=ops.get_collection(""table_ref"")[0])
      self.assertEqual(b""k1"", v1.keys().eval())
      self.assertEqual(3.0, v1.values().eval())

  def testCustomSaver(self):
    export_dir = self._get_export_dir(""test_custom_saver"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
asset_file_path = asset_list[0].eval(),"            sess, [""tag_name""], assets_list=asset_list)

        # Save the asset file path for later comparison.
        asset_file_path = asset_list[0].eval()

      # Save the SavedModel to disk.
      builder.save()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(asset_file_path, asset_list[0].eval())","        # contents should be unchanged.
        asset_list = ops.get_collection(ops.GraphKeys.ASSET_FILEPATHS)
        self.assertEqual(1, len(asset_list))
        self.assertEqual(asset_file_path, asset_list[0].eval())
        self.assertEqual(""scope_name/asset_file_tensor:0"", asset_list[0].name)
        # The static asset data inside graph_proto.collection_def should not be
        # scoped.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"""scope_name/constant_tensor_name:0"").eval())","        self.assertEqual(
            compat.as_bytes(""constant value""),
            ops.get_default_graph().get_tensor_by_name(
                ""scope_name/constant_tensor_name:0"").eval())

  def testClearDevices(self):
    export_dir = self._get_export_dir(""test_clear_devices"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(1, collection_vars[0].eval())","
        # Check value and metadata of the saved variables.
        self.assertEqual(len(collection_vars), 2)
        self.assertEqual(1, collection_vars[0].eval())
        self.assertEqual(2, collection_vars[1].eval())
        self._check_variable_info(collection_vars[0], var_x)
        self._check_variable_info(collection_vars[1], var_y)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(2, collection_vars[1].eval())","        # Check value and metadata of the saved variables.
        self.assertEqual(len(collection_vars), 2)
        self.assertEqual(1, collection_vars[0].eval())
        self.assertEqual(2, collection_vars[1].eval())
        self._check_variable_info(collection_vars[0], var_x)
        self._check_variable_info(collection_vars[1], var_y)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"os.path.abspath(os.path.join(__file__, '..', '..')),","
_ENABLE_TRACEBACK_FILTERING = threading.local()
_EXCLUDED_PATHS = (
    os.path.abspath(os.path.join(__file__, '..', '..')),
)

",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
"os.path.abspath(os.path.join(__file__, '..', '..')),","
_ENABLE_TRACEBACK_FILTERING = threading.local()
_EXCLUDED_PATHS = (
    os.path.abspath(os.path.join(__file__, '..', '..')),
)

",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(self.__name__),"  def _load(self):
    """"""Load the module and insert it into the parent's globals.""""""
    # Import the target module and insert it into the parent's namespace
    module = importlib.import_module(self.__name__)
    self._tfll_parent_module_globals[self._tfll_local_name] = module

    # Emit a warning if one was specified",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
restored = pickle.loads(pickle.dumps(module)),"  def testPickleSubmodule(self):
    name = PickleTest.__module__  # The current module is a submodule.
    module = module_wrapper.TFModuleWrapper(MockModule(name), name)
    restored = pickle.loads(pickle.dumps(module))
    self.assertEqual(restored.__name__, name)
    self.assertIsNotNone(restored.PickleTest)
",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"_reference_pattern = _re.compile(r'^@@(\w+)$', flags=_re.MULTILINE)","from tensorflow.python.util import tf_inspect as _tf_inspect


_reference_pattern = _re.compile(r'^@@(\w+)$', flags=_re.MULTILINE)


def make_all(module_name, doc_string_modules=None):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(symbol_loc_info[0]),"
    symbol_loc_info = self._tfmw_public_apis[name]
    if symbol_loc_info[0]:
      module = importlib.import_module(symbol_loc_info[0])
      attr = getattr(module, symbol_loc_info[1])
    else:
      attr = importlib.import_module(symbol_loc_info[1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
attr = importlib.import_module(symbol_loc_info[1]),"      module = importlib.import_module(symbol_loc_info[0])
      attr = getattr(module, symbol_loc_info[1])
    else:
      attr = importlib.import_module(symbol_loc_info[1])
    setattr(self._tfmw_wrapped_module, name, attr)
    self.__dict__[name] = attr
    # Cache the pair",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
restored = pickle.loads(pickle.dumps(lazy_loader_module)),"    name = PickleTest.__module__  # Try to pickle current module.
    lazy_loader_module = lazy_loader.LazyLoader(
        ""lazy_loader_module"", globals(), name)
    restored = pickle.loads(pickle.dumps(lazy_loader_module))
    self.assertEqual(restored.__name__, name)
    self.assertIsNotNone(restored.PickleTest)
",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
deserialized = pickle.loads(serialized),"    original = data_structures.ListWrapper([1, 2])
    serialized = pickle.dumps(original)
    del original
    deserialized = pickle.loads(serialized)
    self.assertEqual([1, 2], deserialized)

  def testSameStructure(self):",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
deserialized = pickle.loads(serialized),"    original = data_structures._DictWrapper(dict(a=1, b=2))
    serialized = pickle.dumps(original)
    del original
    deserialized = pickle.loads(serialized)
    self.assertEqual(dict(a=1, b=2), deserialized)

  def testListAddOrder(self):",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
deserialized = pickle.loads(serialized),"    original = data_structures._TupleWrapper((1, 2))
    serialized = pickle.dumps(original)
    del original
    deserialized = pickle.loads(serialized)
    self.assertEqual((1, 2), deserialized)

  def testNamedTuple(self):",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
unpickled = pickle.loads(pickle.dumps(exc)),"      # pylint: disable=protected-access
      exc = errors_impl._make_specific_exception(None, None, None, error_code)
      # pylint: enable=protected-access
      unpickled = pickle.loads(pickle.dumps(exc))
      self.assertEqual(exc.node_def, unpickled.node_def)
      self.assertEqual(exc.op, unpickled.op)
      self.assertEqual(exc.message, unpickled.message)",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(1.0, MyFn(1.0).eval())","        return array_ops.identity(x)

    with self.cached_session():
      self.assertEqual(1.0, MyFn(1.0).eval())
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""assertion""):
        _ = MyFn(100.0).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_ = MyFn(100.0).eval(),"      self.assertEqual(1.0, MyFn(1.0).eval())
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""assertion""):
        _ = MyFn(100.0).eval()

  @test_util.run_deprecated_v1
  def testWhileLoopCallsFunc(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"cell_func_call_pattern = re.compile(r""Cell[^/]*\("")","                do_common_subexpression_elimination=True,
                do_function_inlining=True,
                do_constant_folding=True)))
    cell_func_call_pattern = re.compile(r""Cell[^/]*\("")
    @function.Defun(dtype, noinline=noinline)
    def Cell(v):
      # If v is a vector [n, 1], x is a big square matrix.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_regex = re.compile(rf""node 'One'.*{defined_at}"", re.DOTALL)","          one_tag_with_a_fake_function_tag, ops.get_default_graph()
      )
      # Fragments the expression to avoid matching the pattern itself.
      expected_regex = re.compile(rf""node 'One'.*{defined_at}"", re.DOTALL)
      self.assertRegex(interpolated_string, expected_regex)
      self.assertNotIn(""function_node"", interpolated_string)
      self.assertNotIn(""node 'Two'"", interpolated_string)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
expected_regex = re.compile(,"          two_tags_no_seps, ops.get_default_graph()
      )
      # Fragments the expression to avoid matching the pattern itself.
      expected_regex = re.compile(
          rf""node 'One'.*{defined_at}.*node 'Three'.*{defined_at}"", re.DOTALL
      )
      self.assertRegex(interpolated_string, expected_regex)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
expected_regex = re.compile(,"          two_tags_with_seps, ops.get_default_graph()
      )
      # Fragments the expression to avoid matching the pattern itself.
      expected_regex = re.compile(
          rf""node 'Two'.*{defined_at}.*node 'Three'.*{defined_at}"", re.DOTALL
      )
      self.assertRegex(interpolated_string, expected_regex)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_regex = re.compile(rf""node 'One'.*{defined_at}"", re.DOTALL)","      interpolated_string = error_interpolation.interpolate_graph(
          newline, ops.get_default_graph()
      )
      expected_regex = re.compile(rf""node 'One'.*{defined_at}"", re.DOTALL)
      self.assertRegex(interpolated_string, expected_regex)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(,"
    with self.assertRaisesRegex(
        errors_impl.InvalidArgumentError,
        re.compile(
            r""defined at.*"" r""in testSimpleCall.*"" r""in func"", re.DOTALL
        ),
    ):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(,"
    with self.assertRaisesRegex(
        errors_impl.InvalidArgumentError,
        re.compile(
            r""defined at.*"" r""in testNestedCall.*"" r""in func.*"" r""in inner"",
            re.DOTALL,
        ),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile(r""defined at.*"" r""in testAssert.*"" r""in func"", re.DOTALL),","
    with self.assertRaisesRegex(
        errors_impl.InvalidArgumentError,
        re.compile(r""defined at.*"" r""in testAssert.*"" r""in func"", re.DOTALL),
    ):
      func()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(,"
    with self.assertRaisesRegex(
        errors_impl.InvalidArgumentError,
        re.compile(
            r""defined at.*"" r""in testControlFlow.*"" r""in func"", re.DOTALL
        ),
    ):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(wrappers, module.__dict__)","  module_spec = importlib.machinery.ModuleSpec(module_name, None)
  module = importlib.util.module_from_spec(module_spec)
  # pylint: disable=exec-used
  exec(wrappers, module.__dict__)
  # Allow this to be recognized by AutoGraph.
  setattr(module, '_IS_TENSORFLOW_PLUGIN', True)
  sys.modules[module_name] = module",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(wrappers, module.__dict__)","  module_spec = importlib.machinery.ModuleSpec(module_name, None)
  module = importlib.util.module_from_spec(module_spec)
  # pylint: disable=exec-used
  exec(wrappers, module.__dict__)
  # Allow this to be recognized by AutoGraph.
  setattr(module, '_IS_TENSORFLOW_PLUGIN', True)
  sys.modules[module_name] = module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_REGISTERED_NAME_RE = re.compile(r""^(\w+\.)+\w+$"")","_NAME_TO_TYPE_SPEC = {}

# Regular expression for valid TypeSpec names.
_REGISTERED_NAME_RE = re.compile(r""^(\w+\.)+\w+$"")


# TODO(b/173744905) tf_export this as ""tf.register_type_spec"".  (And add a",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: False}), 5.0)","        cond.cond(p, true_fn, false_fn)
        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 5.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 6.0)

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: True}), 6.0)","        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 5.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 6.0)

  @test_util.run_v1_only(""b/120545219"")
  def testCondMustRunSeparateRead(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
one.eval(feed_dict={p: False}),"        cond.cond(p, true_fn, false_fn)
        one = constant_op.constant(1.0)
        one = c.mark_as_return(one)
      one.eval(feed_dict={p: False})
      self.assertAllEqual(v.read_value(), 5.0)
      one.eval(feed_dict={p: True})
      self.assertAllEqual(v.read_value(), 6.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
one.eval(feed_dict={p: True}),"        one = c.mark_as_return(one)
      one.eval(feed_dict={p: False})
      self.assertAllEqual(v.read_value(), 5.0)
      one.eval(feed_dict={p: True})
      self.assertAllEqual(v.read_value(), 6.0)

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: False, q: False}), 3.0)","        with ops.name_scope(""final""):
          val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: False}), 3.0)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: True}), 6.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: True}), 7.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: False}), 8.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: False, q: True}), 6.0)","          val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: False}), 3.0)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: True}), 6.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: True}), 7.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: False}), 8.0)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: True, q: True}), 7.0)","        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: False}), 3.0)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: True}), 6.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: True}), 7.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: False}), 8.0)

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: True, q: False}), 8.0)","      self.assertAllEqual(val.eval(feed_dict={p: False, q: False}), 3.0)
      self.assertAllEqual(val.eval(feed_dict={p: False, q: True}), 6.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: True}), 7.0)
      self.assertAllEqual(val.eval(feed_dict={p: True, q: False}), 8.0)

  @test_util.run_v1_only(""b/120545219"")
  def testCondOneBranch(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: False}), 5.0)","        cond.cond(p, true_fn, false_fn)
        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 5.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 5.0)

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: True}), 5.0)","        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 5.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 5.0)

  @test_util.run_v1_only(""b/120545219"")
  def testCondOneBranchUpdateBefore(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: False}), 6.0)","        cond.cond(p, true_fn, false_fn)
        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 6.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 12.0)

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: True}), 12.0)","        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 6.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 12.0)

  @test_util.run_v1_only(""b/120545219"")
  def testCondOneBranchUpdateAfter(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: False}), 10.0)","        v.assign(v * 2)
        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 10.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 20.0)

  def testFunctionWhileLoopWithCapturedLoopVars(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(val.eval(feed_dict={p: True}), 20.0)","        val = v.read_value()
        val = c.mark_as_return(val)
      self.assertAllEqual(val.eval(feed_dict={p: False}), 10.0)
      self.assertAllEqual(val.eval(feed_dict={p: True}), 20.0)

  def testFunctionWhileLoopWithCapturedLoopVars(self):
    n = 3",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dtype2 = eval(repr(dtype)),"      dtype = dtypes.DType(enum)
      self.assertEqual(repr(dtype), ""tf."" + name)
      import tensorflow as tf
      dtype2 = eval(repr(dtype))
      self.assertEqual(type(dtype2), dtypes.DType)
      self.assertEqual(dtype, dtype2)
",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dtype2 = eval(repr(dtype)),"      dtype = dtypes.DType(enum)
      self.assertEqual(repr(dtype), ""tf."" + name)
      import tensorflow as tf
      dtype2 = eval(repr(dtype))
      self.assertEqual(type(dtype2), dtypes.DType)
      self.assertEqual(dtype, dtype2)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(pickle.loads(pickle.dumps(desc)), desc)","
  def testPickleSerialization(self):
    desc = tensor.TensorSpec([1, 5], dtypes.float32, ""test"")
    self.assertEqual(pickle.loads(pickle.dumps(desc)), desc)

  @test_util.deprecated_graph_mode_only
  def testTypeSpecFromValue(self):",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(pickle.loads(pickle.dumps(desc)), desc)","
  def testPickleSerialization(self):
    desc = tensor.BoundedTensorSpec([1, 5], dtypes.float32, -1, 1, ""test"")
    self.assertEqual(pickle.loads(pickle.dumps(desc)), desc)

  def testSerialization(self):
    nameless = tensor.BoundedTensorSpec([1], np.float32, 0, 1)",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
result = MyOperator(input).eval(),"      def testMyOperator(self):
        input = [1.0, 2.0, 3.0, 4.0, 5.0]
        with self.captureWritesToStream(sys.stdout) as captured:
          result = MyOperator(input).eval()
        self.assertStartsWith(captured.contents(), ""This was printed."")
    ```
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = MyOperator(valid_input).eval(),"      def testMyOperator(self):
        with self.session():
          valid_input = [1.0, 2.0, 3.0, 4.0, 5.0]
          result = MyOperator(valid_input).eval()
          self.assertEqual(result, [1.0, 2.0, 3.0, 5.0, 8.0]
          invalid_input = [-1.0, 2.0, 7.0]
          with self.assertRaisesOpError(""negative input not supported""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
MyOperator(invalid_input).eval(),"          self.assertEqual(result, [1.0, 2.0, 3.0, 5.0, 8.0]
          invalid_input = [-1.0, 2.0, 7.0]
          with self.assertRaisesOpError(""negative input not supported""):
            MyOperator(invalid_input).eval()
    ```

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = MyOperator(valid_input).eval(),"      def testMyOperator(self):
        with self.cached_session() as sess:
          valid_input = [1.0, 2.0, 3.0, 4.0, 5.0]
          result = MyOperator(valid_input).eval()
          self.assertEqual(result, [1.0, 2.0, 3.0, 5.0, 8.0]
          invalid_input = [-1.0, 2.0, 7.0]
          with self.assertRaisesOpError(""negative input not supported""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
MyOperator(invalid_input).eval(),"          self.assertEqual(result, [1.0, 2.0, 3.0, 5.0, 8.0]
          invalid_input = [-1.0, 2.0, 7.0]
          with self.assertRaisesOpError(""negative input not supported""):
            MyOperator(invalid_input).eval()
    ```

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(y.eval(feed_dict={x: 1}), 1)","        x = array_ops.placeholder(dtype=dtypes.int32)
        y = smart_cond.smart_cond(x > 0, lambda: constant_op.constant(1),
                                  lambda: constant_op.constant(2))
        self.assertEqual(y.eval(feed_dict={x: 1}), 1)
        self.assertEqual(y.eval(feed_dict={x: -1}), 2)

  def testEval(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(y.eval(feed_dict={x: -1}), 2)","        y = smart_cond.smart_cond(x > 0, lambda: constant_op.constant(1),
                                  lambda: constant_op.constant(2))
        self.assertEqual(y.eval(feed_dict={x: 1}), 1)
        self.assertEqual(y.eval(feed_dict={x: -1}), 2)

  def testEval(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(z.eval(feed_dict={x: 1}), 1)","        # branch shouldn't be evaluated at all.
        z = smart_cond.smart_cond(x * y > 0, lambda: constant_op.constant(1),
                                  raise_exception)
        self.assertEqual(z.eval(feed_dict={x: 1}), 1)

  def testPlaceholderWithDefault(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(y.eval(feed_dict={x: -1}), 2)","        y = smart_cond.smart_cond(x > 0, lambda: constant_op.constant(1),
                                  lambda: constant_op.constant(2))
        self.assertEqual(self.evaluate(y), 1)
        self.assertEqual(y.eval(feed_dict={x: -1}), 2)

  def testMissingArg1(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"raise ValueError(""Cannot evaluate tensor using `eval()`: No default ""","  if session is None:
    session = stack.get_default_session()
    if session is None:
      raise ValueError(""Cannot evaluate tensor using `eval()`: No default ""
                       ""session is registered. Use `with ""
                       ""sess.as_default()` or pass an explicit session to ""
                       ""`eval(session=sess)`"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"""`eval(session=sess)`"")","      raise ValueError(""Cannot evaluate tensor using `eval()`: No default ""
                       ""session is registered. Use `with ""
                       ""sess.as_default()` or pass an explicit session to ""
                       ""`eval(session=sess)`"")
    if session.graph is not graph:
      raise ValueError(""Cannot use the default session to evaluate tensor: ""
                       ""the tensor's graph is different from the session's """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"""`eval(session=sess)`."")","      raise ValueError(""Cannot use the default session to evaluate tensor: ""
                       ""the tensor's graph is different from the session's ""
                       ""graph. Pass an explicit session to ""
                       ""`eval(session=sess)`."")
  else:
    if session.graph is not graph:
      raise ValueError(""Cannot use the given session to evaluate tensor: """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, feed_dict=None, session=None):","    """"""
    self._disallow_bool_casting()

  def eval(self, feed_dict=None, session=None):
    """"""Evaluates this tensor in a `Session`.

    Note: If you are not using `compat.v1` libraries, you should not need this,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"*N.B.* Before invoking `Tensor.eval()`, its graph must have been","    produce the inputs needed for the operation that produces this
    tensor.

    *N.B.* Before invoking `Tensor.eval()`, its graph must have been
    launched in a session, and either a default session must be
    available, or `session` must be specified explicitly.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, feed_dict=None, session=None) -> NoReturn:","    raise NotImplementedError(
        ""_as_tf_output not supported when eager execution is enabled."")

  def eval(self, feed_dict=None, session=None) -> NoReturn:
    raise NotImplementedError(
        ""eval is not supported when eager execution is enabled, ""
        ""is .numpy() what you're looking for?"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_VALID_OP_NAME_REGEX: Pattern[str] = re.compile(,"
# Copied from core/framework/node_def_util.cc
# TODO(mrry,josh11b): Consolidate this validation in C++ code.
_VALID_OP_NAME_REGEX: Pattern[str] = re.compile(
    r""^[A-Za-z0-9.][A-Za-z0-9_.\\/>-]*$"")
_VALID_SCOPE_NAME_REGEX: Pattern[str] = re.compile(
    r""^[A-Za-z0-9_.\\/>-]*$"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_VALID_SCOPE_NAME_REGEX: Pattern[str] = re.compile(,"# TODO(mrry,josh11b): Consolidate this validation in C++ code.
_VALID_OP_NAME_REGEX: Pattern[str] = re.compile(
    r""^[A-Za-z0-9.][A-Za-z0-9_.\\/>-]*$"")
_VALID_SCOPE_NAME_REGEX: Pattern[str] = re.compile(
    r""^[A-Za-z0-9_.\\/>-]*$"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
regex = re.compile(scope),"        return list(collection)
      else:
        c = []
        regex = re.compile(scope)
        for item in collection:
          try:
            if regex.match(item.name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Tensor.eval() and Operation.run() calls. It is primarily intended for use,"  """"""Python ""with"" handler for defining a default session.

  This function provides a means of registering a session for handling
  Tensor.eval() and Operation.run() calls. It is primarily intended for use
  by session.Session, but can be used with any object that implements
  the Session.run() interface.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"Use with the ""with"" keyword to specify that Tensor.eval() and Operation.run()","  by session.Session, but can be used with any object that implements
  the Session.run() interface.

  Use with the ""with"" keyword to specify that Tensor.eval() and Operation.run()
  invocations within the scope of a block should be executed by a particular
  session.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(),"    sess = ...
    with ops.default_session(sess):
      c = tf.constant(5.0)
      result = c.eval()

    # 3. Overriding default_session():
    sess = ...",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
c.eval(session=sess),"    with ops.default_session(sess):
      c = tf.constant(5.0)
      with ops.default_session(...):
        c.eval(session=sess)

  Args:
    session: The session to be installed as the default session.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, feed_dict=None, session=None):","    return ""SparseTensor(indices=%s, values=%s, dense_shape=%s)"" % (
        self._indices, self._values, self._dense_shape)

  def eval(self, feed_dict=None, session=None):
    """"""Evaluates this sparse tensor in a `Session`.

    Calling this method will execute all preceding operations that",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"*N.B.* Before invoking `SparseTensor.eval()`, its graph must have been","    produce the inputs needed for the operation that produces this
    tensor.

    *N.B.* Before invoking `SparseTensor.eval()`, its graph must have been
    launched in a session, and either a default session must be
    available, or `session` must be specified explicitly.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"resources.shared_resources()).eval()), 1)","      self.assertEqual(
          len(
              resources.report_uninitialized_resources(
                  resources.shared_resources()).eval()), 1)
      resources.initialize_resources(resources.shared_resources()).run()
      self.assertEqual(
          len(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"resources.shared_resources()).eval()), 0)","      self.assertEqual(
          len(
              resources.report_uninitialized_resources(
                  resources.shared_resources()).eval()), 0)


class TensorAndShapeTest(test_util.TensorFlowTestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
test_ops.kernel_label().eval()),"  def testNoLabel(self):
    with self.cached_session():
      self.assertAllEqual(b""My label is: default"",
                          test_ops.kernel_label().eval())

  @test_util.run_deprecated_v1
  def testLabelMap(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
v = test_ops.graph_def_version().eval(),"    with ops.Graph().as_default() as g:
      version = g.graph_def_versions.producer
      with self.session(graph=g):
        v = test_ops.graph_def_version().eval()
        self.assertEqual(version, v)

  def testAddShapes(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_INTERPOLATION_PATTERN = re.compile(_INTERPOLATION_REGEX, re.DOTALL)","_NAME_REGEX = r""[A-Za-z0-9_.][A-Za-z0-9_.\-/]*?""
_TAG_REGEX = fr""{{{{(?P<type>{_NAME_REGEX}) (?P<name>{_NAME_REGEX})}}}}""
_INTERPOLATION_REGEX = fr""(?P<sep>.*?)(?P<tag>{_TAG_REGEX})""
_INTERPOLATION_PATTERN = re.compile(_INTERPOLATION_REGEX, re.DOTALL)

_ParseTag = collections.namedtuple(""_ParseTag"", [""type"", ""name""])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile(r""<embedded""),","# Patterns of filename patterns that should be considered internal to
# the TensorFlow framework.
_FRAMEWORK_FILENAME_PATTERNS = [
    re.compile(r""<embedded""),
]

# This is for OSS keras, since the package is load from local python env,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_FRAMEWORK_FILENAME_PATTERNS.append(re.compile(r""keras""))","except AttributeError:
  # if site.getsitepackages is not available somehow, we just use the ""keras"" as
  # the keyword to do the match.
  _FRAMEWORK_FILENAME_PATTERNS.append(re.compile(r""keras""))

# Patterns of filename patterns that should be considered external to
# TensorFlow regardless of framework prefix match.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile(r""_test\.py$""),","# TensorFlow regardless of framework prefix match.
_EXTERNAL_FILENAME_PATTERNS = [
    # Explicitly treat test frames as not part of the framework.
    re.compile(r""_test\.py$""),
]

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_PHYSICAL_DEVICE_DESCRIPTION_REGEX = re.compile(,"

# Matches the DeviceAttributes.physical_device_desc field.
_PHYSICAL_DEVICE_DESCRIPTION_REGEX = re.compile(
    r'name: ([^,]*), (?:.*compute capability: (\d+)\.(\d+))?')

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(pickle.loads(pickle.dumps(mt_spec)), mt_spec)","    mt_spec = MaskedTensorV1.Spec(values_spec, mask_spec)
    self.assertEqual(copy.copy(mt_spec), mt_spec)
    self.assertEqual(copy.deepcopy(mt_spec), mt_spec)
    self.assertEqual(pickle.loads(pickle.dumps(mt_spec)), mt_spec)

  def testCustomizeSpecTest(self):
    class WeightedTensor(extension_type.ExtensionType):",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"out.eval(feed_dict={small: [1, 2], big: [3, 4]})","          [check_ops.assert_equal(big, small, message=""fail"")]):
        out = array_ops.identity(small)
      with self.assertRaisesOpError(""fail.*big.*small""):
        out.eval(feed_dict={small: [1, 2], big: [3, 4]})

  def test_error_message_eager(self):
    expected_error_msg_full = r""""""big does not equal small",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"out.eval(feed_dict={small: [3, 1], big: [4, 2]})","      with ops.control_dependencies([check_ops.assert_equal(small, big)]):
        out = array_ops.identity(small)
      with self.assertRaisesOpError(""small.*big""):
        out.eval(feed_dict={small: [3, 1], big: [4, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_doesnt_raise_when_equal_and_broadcastable_shapes(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={tensor: 0}),"          [check_ops.assert_rank(
              tensor, desired_rank, message=""fail"")]):
        with self.assertRaisesOpError(""fail.*my_tensor.*rank""):
          array_ops.identity(tensor).eval(feed_dict={tensor: 0})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_zero_tensor_doesnt_raise_if_rank_just_right_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={tensor: 0}),"      desired_rank = 0
      with ops.control_dependencies(
          [check_ops.assert_rank(tensor, desired_rank)]):
        array_ops.identity(tensor).eval(feed_dict={tensor: 0})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_tensor_raises_if_rank_too_large_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})","      with ops.control_dependencies(
          [check_ops.assert_rank(tensor, desired_rank)]):
        with self.assertRaisesOpError(""my_tensor.*rank""):
          array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_tensor_doesnt_raise_if_rank_just_right_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})","      desired_rank = 1
      with ops.control_dependencies(
          [check_ops.assert_rank(tensor, desired_rank)]):
        array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_tensor_raises_if_rank_too_small_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})","      with ops.control_dependencies(
          [check_ops.assert_rank(tensor, desired_rank)]):
        with self.assertRaisesOpError(""my_tensor.*rank""):
          array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_raises_if_rank_is_not_scalar_static(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={rank_tensor: [1, 2]})","      with self.assertRaisesOpError(""Rank must be a scalar""):
        with ops.control_dependencies(
            [check_ops.assert_rank(tensor, rank_tensor)]):
          array_ops.identity(tensor).eval(feed_dict={rank_tensor: [1, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_raises_if_rank_is_not_integer_static(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={rank_tensor: .5}),"      with self.assertRaisesRegex(TypeError, ""must be of type tf.int32""):
        with ops.control_dependencies(
            [check_ops.assert_rank(tensor, rank_tensor)]):
          array_ops.identity(tensor).eval(feed_dict={rank_tensor: .5})


class AssertRankInTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor_rank0).eval(feed_dict={tensor_rank0: 42.0}),"      with ops.control_dependencies([
          check_ops.assert_rank_in(tensor_rank0, (1, 2), message=""fail"")]):
        with self.assertRaisesOpError(""fail.*my_tensor.*rank""):
          array_ops.identity(tensor_rank0).eval(feed_dict={tensor_rank0: 42.0})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_zero_tensor_doesnt_raise_if_rank_matches_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor_rank0).eval(feed_dict={tensor_rank0: 42.0}),"      for desired_ranks in ((0, 1, 2), (1, 0, 2), (1, 2, 0)):
        with ops.control_dependencies([
            check_ops.assert_rank_in(tensor_rank0, desired_ranks)]):
          array_ops.identity(tensor_rank0).eval(feed_dict={tensor_rank0: 42.0})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_tensor_doesnt_raise_if_rank_matches_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor_rank1).eval(feed_dict={,"      for desired_ranks in ((0, 1, 2), (1, 0, 2), (1, 2, 0)):
        with ops.control_dependencies([
            check_ops.assert_rank_in(tensor_rank1, desired_ranks)]):
          array_ops.identity(tensor_rank1).eval(feed_dict={
              tensor_rank1: (42.0, 43.0)
          })
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor_rank1).eval(feed_dict={,"      with ops.control_dependencies([
          check_ops.assert_rank_in(tensor_rank1, (0, 2))]):
        with self.assertRaisesOpError(""my_tensor.*rank""):
          array_ops.identity(tensor_rank1).eval(feed_dict={
              tensor_rank1: (42.0, 43.0)
          })
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={,"      with self.assertRaisesOpError(""Rank must be a scalar""):
        with ops.control_dependencies(
            (check_ops.assert_rank_in(tensor, desired_ranks),)):
          array_ops.identity(tensor).eval(feed_dict={
              desired_ranks[0]: 1,
              desired_ranks[1]: [2, 1],
          })",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={rank_tensor: .5}),"      with self.assertRaisesRegex(TypeError, ""must be of type tf.int32""):
        with ops.control_dependencies(
            [check_ops.assert_rank_in(tensor, (1, rank_tensor))]):
          array_ops.identity(tensor).eval(feed_dict={rank_tensor: .5})


class AssertRankAtLeastTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={tensor: 0}),"      with ops.control_dependencies(
          [check_ops.assert_rank_at_least(tensor, desired_rank)]):
        with self.assertRaisesOpError(""my_tensor.*rank""):
          array_ops.identity(tensor).eval(feed_dict={tensor: 0})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_zero_tensor_doesnt_raise_if_rank_just_right_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(tensor).eval(feed_dict={tensor: 0}),"      desired_rank = 0
      with ops.control_dependencies(
          [check_ops.assert_rank_at_least(tensor, desired_rank)]):
        array_ops.identity(tensor).eval(feed_dict={tensor: 0})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_ten_doesnt_raise_raise_if_rank_too_large_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})","      desired_rank = 0
      with ops.control_dependencies(
          [check_ops.assert_rank_at_least(tensor, desired_rank)]):
        array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_tensor_doesnt_raise_if_rank_just_right_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})","      desired_rank = 1
      with ops.control_dependencies(
          [check_ops.assert_rank_at_least(tensor, desired_rank)]):
        array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})

  @test_util.run_in_graph_and_eager_modes
  def test_rank_one_tensor_raises_if_rank_too_small_static_rank(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})","      with ops.control_dependencies(
          [check_ops.assert_rank_at_least(tensor, desired_rank)]):
        with self.assertRaisesOpError(""my_tensor.*rank""):
          array_ops.identity(tensor).eval(feed_dict={tensor: [1, 2]})


class AssertNonNegativeTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_ans = math_ops.trace(x).eval(),"  def compare(self, x):
    np_ans = np.trace(x, axis1=-2, axis2=-1)
    with self.cached_session():
      tf_ans = math_ops.trace(x).eval()
    self.assertAllClose(tf_ans, np_ans)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
update_op.eval(feed_dict={values_placeholder: values}),"        with self.cached_session():
          _, update_op = metrics.mean(values_placeholder, invalid_weight)
          variables.local_variables_initializer().run()
          update_op.eval(feed_dict={values_placeholder: values})


class MeanTensorTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertGreater(update_op.eval(feed_dict=feed_dict), .95)","      # if streaming_accuracy does not flatten the weight, accuracy would be
      # 0.33333334 due to an intended broadcast of weight. Due to flattening,
      # it will be higher than .95
      self.assertGreater(update_op.eval(feed_dict=feed_dict), .95)
      self.assertGreater(accuracy.eval(feed_dict=feed_dict), .95)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertGreater(accuracy.eval(feed_dict=feed_dict), .95)","      # 0.33333334 due to an intended broadcast of weight. Due to flattening,
      # it will be higher than .95
      self.assertGreater(update_op.eval(feed_dict=feed_dict), .95)
      self.assertGreater(accuracy.eval(feed_dict=feed_dict), .95)

  @test_util.run_deprecated_v1
  def testMultipleUpdatesWithWeightedValues(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_precision, update_op.eval(feed_dict=feed_dict))","      weighted_positives = (2.0 + 2.0) + (2.0 + 2.0)
      expected_precision = weighted_tp / weighted_positives
      self.assertAlmostEqual(
          expected_precision, update_op.eval(feed_dict=feed_dict))
      self.assertAlmostEqual(
          expected_precision, precision.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_precision, precision.eval(feed_dict=feed_dict))","      self.assertAlmostEqual(
          expected_precision, update_op.eval(feed_dict=feed_dict))
      self.assertAlmostEqual(
          expected_precision, precision.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testWeighted1d_placeholders(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_precision, update_op.eval(feed_dict=feed_dict))","      weighted_positives = (2.0 + 2.0) + (5.0 + 5.0)
      expected_precision = weighted_tp / weighted_positives
      self.assertAlmostEqual(
          expected_precision, update_op.eval(feed_dict=feed_dict))
      self.assertAlmostEqual(
          expected_precision, precision.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_precision, precision.eval(feed_dict=feed_dict))","      self.assertAlmostEqual(
          expected_precision, update_op.eval(feed_dict=feed_dict))
      self.assertAlmostEqual(
          expected_precision, precision.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testWeighted2d(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_precision, update_op.eval(feed_dict=feed_dict))","      weighted_positives = (1.0 + 3.0) + (4.0 + 2.0)
      expected_precision = weighted_tp / weighted_positives
      self.assertAlmostEqual(
          expected_precision, update_op.eval(feed_dict=feed_dict))
      self.assertAlmostEqual(
          expected_precision, precision.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_precision, precision.eval(feed_dict=feed_dict))","      self.assertAlmostEqual(
          expected_precision, update_op.eval(feed_dict=feed_dict))
      self.assertAlmostEqual(
          expected_precision, precision.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testAllIncorrect(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, update.eval())","
    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, metric.eval())","    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, update.eval())","      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, metric.eval())","      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())


def _test_precision_at_top_k(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
test_case.assertTrue(math.isnan(update.eval())),"
    # Run per-step op and assert expected values.
    if math.isnan(expected):
      test_case.assertTrue(math.isnan(update.eval()))
      test_case.assertTrue(math.isnan(metric.eval()))
    else:
      test_case.assertEqual(expected, update.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
test_case.assertTrue(math.isnan(metric.eval())),"    # Run per-step op and assert expected values.
    if math.isnan(expected):
      test_case.assertTrue(math.isnan(update.eval()))
      test_case.assertTrue(math.isnan(metric.eval()))
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, update.eval())","      test_case.assertTrue(math.isnan(update.eval()))
      test_case.assertTrue(math.isnan(metric.eval()))
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, metric.eval())","      test_case.assertTrue(math.isnan(metric.eval()))
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())


def _test_average_precision_at_k(predictions,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, update.eval())","
    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertAlmostEqual(expected, update.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, metric.eval())","    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertAlmostEqual(expected, update.eval())
      test_case.assertAlmostEqual(expected, metric.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertAlmostEqual(expected, update.eval())","      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertAlmostEqual(expected, update.eval())
      test_case.assertAlmostEqual(expected, metric.eval())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertAlmostEqual(expected, metric.eval())","      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertAlmostEqual(expected, update.eval())
      test_case.assertAlmostEqual(expected, metric.eval())


class SingleLabelPrecisionAtKTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, update.eval())","
    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, metric.eval())","    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, update.eval())","      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, metric.eval())","      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())


def _test_recall_at_top_k(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, update.eval())","
    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_assert_nan(test_case, metric.eval())","    # Run per-step op and assert expected values.
    if math.isnan(expected):
      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, update.eval())","      _assert_nan(test_case, update.eval())
      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"test_case.assertEqual(expected, metric.eval())","      _assert_nan(test_case, metric.eval())
    else:
      test_case.assertEqual(expected, update.eval())
      test_case.assertEqual(expected, metric.eval())


class SingleLabelRecallAtKTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(expected_bin_counts, hist.eval({placeholder: 5}))","      self.assertEqual(hist.shape.ndims, 1)
      self.assertIs(hist.shape.dims[0].value, None)
      self.assertEqual(dtypes.int32, hist.dtype)
      self.assertAllClose(expected_bin_counts, hist.eval({placeholder: 5}))

  def test_single_bin(self):
    hist = histogram_ops.histogram_fixed_width(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(0.0, var0.eval(session=worker_sessions[1]))","    worker_sessions[1].run([var2.initializer, var3.initializer])

    # Read values back in the opposite session
    self.assertAllEqual(0.0, var0.eval(session=worker_sessions[1]))
    self.assertAllEqual(1.0, var1.eval(session=worker_sessions[1]))
    self.assertAllEqual(2.0, var2.eval(session=worker_sessions[0]))
    self.assertAllEqual(3.0, var3.eval(session=worker_sessions[0]))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(1.0, var1.eval(session=worker_sessions[1]))","
    # Read values back in the opposite session
    self.assertAllEqual(0.0, var0.eval(session=worker_sessions[1]))
    self.assertAllEqual(1.0, var1.eval(session=worker_sessions[1]))
    self.assertAllEqual(2.0, var2.eval(session=worker_sessions[0]))
    self.assertAllEqual(3.0, var3.eval(session=worker_sessions[0]))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(2.0, var2.eval(session=worker_sessions[0]))","    # Read values back in the opposite session
    self.assertAllEqual(0.0, var0.eval(session=worker_sessions[1]))
    self.assertAllEqual(1.0, var1.eval(session=worker_sessions[1]))
    self.assertAllEqual(2.0, var2.eval(session=worker_sessions[0]))
    self.assertAllEqual(3.0, var3.eval(session=worker_sessions[0]))

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(3.0, var3.eval(session=worker_sessions[0]))","    self.assertAllEqual(0.0, var0.eval(session=worker_sessions[1]))
    self.assertAllEqual(1.0, var1.eval(session=worker_sessions[1]))
    self.assertAllEqual(2.0, var2.eval(session=worker_sessions[0]))
    self.assertAllEqual(3.0, var3.eval(session=worker_sessions[0]))


class CreateLocalClusterBenchmark(test.Benchmark):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"len(variables.report_uninitialized_variables().eval()), 2)","      # Assert that the variables are not initialized.
      if not context.executing_eagerly():
        self.assertEqual(
            len(variables.report_uninitialized_variables().eval()), 2)
        self.assertEqual(0, len(self.evaluate(v2.keys())))
        self.assertEqual(0, len(self.evaluate(v2.values())))
      # Restore the saved values in the parameter nodes.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"len(variables.report_uninitialized_variables().eval()), 2)","
        # Assert that the variables are not initialized.
        self.assertEqual(
            len(variables.report_uninitialized_variables().eval()), 2)
        self.assertEqual(0, len(self.evaluate(v2.keys())))
        self.assertEqual(0, len(self.evaluate(v2.values())))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# Calls .eval() to return the ndarray that makes up the full variable.,"    def _save(partitioner=None):
      # train.Saver is V1 only API.
      with ops_lib.Graph().as_default(), self.session() as sess:
        # Calls .eval() to return the ndarray that makes up the full variable.
        rnd = random_ops.random_uniform(var_full_shape).eval()

        if partitioner:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
rnd = random_ops.random_uniform(var_full_shape).eval(),"      # train.Saver is V1 only API.
      with ops_lib.Graph().as_default(), self.session() as sess:
        # Calls .eval() to return the ndarray that makes up the full variable.
        rnd = random_ops.random_uniform(var_full_shape).eval()

        if partitioner:
          vs = [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return new_vs[0].as_tensor().eval(),"        saver.restore(sess, saved_path)

        if partitioner:
          return new_vs[0].as_tensor().eval()
        else:
          return new_vs[0].eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return new_vs[0].eval(),"        if partitioner:
          return new_vs[0].as_tensor().eval()
        else:
          return new_vs[0].eval()

    for call_saver_with_dict in {False, True}:
      # Save PartitionedVariable and restore into full variable.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my1.eval(session), v1)","        ])

        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my3b.eval(session), v3)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my2.eval(session), v2)","
        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my3b.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my3.eval(session), v3)","        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my3b.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my3b.eval(session), v3)","        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my3b.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)

        # Check that tensors are not explicitly in the graph.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my4.eval(session), v4)","        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my3b.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)

        # Check that tensors are not explicitly in the graph.
        self.assertLess(len(str(session.graph.as_graph_def())), 32000)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my4.eval(session), v4)","                                            {""useful_scope/"": ""useful_scope/""})
      with self.session(graph=g) as session:
        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my4.eval(session), v4)
        self.assertAllEqual(my5.eval(session), my5_init)

  def testRestoreRunsOnSameDevice(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my5.eval(session), my5_init)","      with self.session(graph=g) as session:
        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my4.eval(session), v4)
        self.assertAllEqual(my5.eval(session), my5_init)

  def testRestoreRunsOnSameDevice(self):
    checkpoint_dir = self.get_temp_dir()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my1.eval(session), v1)","                                              {""/"": ""some_scope/"",})

        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my2.eval(session), v2)","
        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my3.eval(session), v3)","        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)

  def testInitToRootCheckpoint(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my4.eval(session), v4)","        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)

  def testInitToRootCheckpoint(self):
    checkpoint_dir = self.get_temp_dir()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my1.eval(session), v1)","                                              {""/"": ""/"",})

        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my2.eval(session), v2)","
        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my3.eval(session), v3)","        session.run(variables.global_variables_initializer())
        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)

  def testInitFromPartitionVar(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(my4.eval(session), v4)","        self.assertAllEqual(my1.eval(session), v1)
        self.assertAllEqual(my2.eval(session), v2)
        self.assertAllEqual(my3.eval(session), v3)
        self.assertAllEqual(my4.eval(session), v4)

  def testInitFromPartitionVar(self):
    checkpoint_dir = self.get_temp_dir()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","      gfile.MakeDirs(checkpoint_dir)
      v = variable_v1.VariableV1([6.0, 7.0, 8.0], name=""v"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
      session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())
      saver = saver_lib.Saver({""v"": v})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))

  def _test_recovered_variable(self,
                               checkpoint_dir=None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","    with ops.Graph().as_default():
      v = variable_v1.VariableV1(2, name=""v"")
      with session_lib.Session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())
      saver = saver_lib.Saver({""v"": v})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(v))

  def testRecoverSession(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=variables.report_uninitialized_variables(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(v))
      self.assertEqual(1, sess.run(w))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(
          False,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          False,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(v))

  def testRecoverSessionNoChkptStillRunsLocalInitOp(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(w))

  def testRecoverSessionFailsStillRunsLocalInitOp(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          False,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(w))

  def testWaitForSessionLocalInit(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(v))
      self.assertEqual(1, sess.run(w))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""x"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
      sm2 = session_manager.SessionManager(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""x"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())","      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=variables.report_uninitialized_variables(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""x:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""x:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(v))
      self.assertEqual(1, sess.run(w))
      self.assertEqual(3, sess.run(x))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          name=""x_res"")

      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
        self.assertEqual(False,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(v_res).eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())","      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(v_res).eval())
        self.assertEqual(False,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
variable_v1.is_variable_initialized(v_res).eval()),"        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(x).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(v_res).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(w_res).eval())
        self.assertEqual(False,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
variable_v1.is_variable_initialized(w_res).eval()),"        self.assertEqual(False,
                         variable_v1.is_variable_initialized(v_res).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(w_res).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(x_res).eval())
      sm2 = session_manager.SessionManager(local_init_op=[",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
variable_v1.is_variable_initialized(x_res).eval()),"        self.assertEqual(False,
                         variable_v1.is_variable_initialized(w_res).eval())
        self.assertEqual(False,
                         variable_v1.is_variable_initialized(x_res).eval())
      sm2 = session_manager.SessionManager(local_init_op=[
          w.initializer, x.initializer, w_res.initializer, x_res.initializer
      ])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          False,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""x:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""x:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(w))
      self.assertEqual(3, sess.run(x))
      self.assertEqual(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v_res:0"")).eval(session=sess))","      self.assertEqual(
          False,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v_res:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""w_res:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""w_res:0"")).eval(session=sess))
      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""x_res:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""x_res:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(w_res))
      self.assertEqual(3, sess.run(x_res))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","      i = while_loop.while_loop(lambda i: i < 1, lambda i: i + 1, [0])
      v = variable_v1.VariableV1(array_ops.identity(i), name=""v"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
      sm = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())
      sess = sm.prepare_session("""", init_op=v.initializer)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))

  def testPrepareSessionDidNotInitLocalVariable(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())
      with self.assertRaisesRegex(RuntimeError,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables())
      with self.assertRaisesRegex(RuntimeError,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=variables.report_uninitialized_variables(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","          collections=[ops.GraphKeys.LOCAL_VARIABLES],
          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())","          name=""w"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
        self.assertEqual(False, variable_v1.is_variable_initialized(w).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.report_uninitialized_variables(),
          ready_for_local_init_op=None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","      gfile.MakeDirs(checkpoint_dir)
      v = variable_v1.VariableV1([6.0, 7.0, 8.0], name=""v"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
      session_manager.SessionManager(
          ready_op=variables.assert_variables_initialized())
      saver = saver_lib.Saver({""v"": v})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))

  def testRecoverSession(self):
    # Create a checkpoint.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())","    with ops.Graph().as_default():
      v = variable_v1.VariableV1(2, name=""v"")
      with self.cached_session():
        self.assertEqual(False, variable_v1.is_variable_initialized(v).eval())
      sm2 = session_manager.SessionManager(
          ready_op=variables.assert_variables_initialized())
      saver = saver_lib.Saver({""v"": v})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))","      self.assertEqual(
          True,
          variable_v1.is_variable_initialized(
              sess.graph.get_tensor_by_name(""v:0"")).eval(session=sess))
      self.assertEqual(1, sess.run(v))

  def testWaitForSessionReturnsNoneAfterTimeout(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(c.eval(), 42.0)","      b = constant_op.constant(7.0, shape=[1, 1])
      c = math_ops.matmul(a, b, name='matmul')

      self.assertEqual(c.eval(), 42.0)
      self.assertEqual(len(sess.graph_def.node), 3)

      result = quantize_training.do_quantize_training_on_graphdef(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([2], array_ops.shape(slot).eval())","      self.evaluate(variables.global_variables_initializer())

      self.assertEqual(""var/slot"", slot.op.name)
      self.assertEqual([2], array_ops.shape(slot).eval())
      self.assertEqual(dtypes.float64, slot.dtype.base_dtype)
      self.assertAllEqual([0.0, 0.0], self.evaluate(slot))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([2], array_ops.shape(slot).eval())","      self.evaluate(variables.global_variables_initializer())

      self.assertEqual(""const/slot"", slot.op.name)
      self.assertEqual([2], array_ops.shape(slot).eval())
      self.assertEqual(dtypes.float64, slot.dtype.base_dtype)
      self.assertAllEqual([0.0, 0.0], self.evaluate(slot))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(expected_values[i], var.eval(sess))","  def _assert_cols_to_vars(self, cols_to_vars, cols_to_expected_values, sess):
    for col, expected_values in cols_to_expected_values.items():
      for i, var in enumerate(cols_to_vars[col]):
        self.assertAllClose(expected_values[i], var.eval(sess))

  def testWarmStartVar(self):
    _, prev_val = self._create_prev_run_var(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(prev_val, fruit_weights.eval(sess))","        checkpoint_utils.init_from_checkpoint(self.get_temp_dir(),
                                              {prev_tensor_name: var})
        self.evaluate(variables.global_variables_initializer())
        self.assertAllClose(prev_val, fruit_weights.eval(sess))

  def testWarmStartVarPrevVarPartitioned(self):
    _, weights = self._create_prev_run_var(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(prev_val, fruit_weights.eval(sess))","        checkpoint_utils.init_from_checkpoint(self.get_temp_dir(),
                                              {prev_tensor_name: var})
        self.evaluate(variables.global_variables_initializer())
        self.assertAllClose(prev_val, fruit_weights.eval(sess))

  def testWarmStartVarCurrentVarPartitioned(self):
    _, prev_val = self._create_prev_run_var(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)","        self.evaluate(variables.global_variables_initializer())
        fruit_weights = fruit_weights._get_variable_list()
        new_val = np.concatenate(
            [fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)
        self.assertAllClose(prev_val, new_val)

  def testWarmStartVarBothVarsPartitioned(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)","        self.evaluate(variables.global_variables_initializer())
        fruit_weights = fruit_weights._get_variable_list()
        new_val = np.concatenate(
            [fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)
        self.assertAllClose(prev_val, new_val)

  def testWarmStartVarBothVarsPartitioned(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)","        self.evaluate(variables.global_variables_initializer())
        fruit_weights = fruit_weights._get_variable_list()
        new_val = np.concatenate(
            [fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)
        self.assertAllClose(prev_val, new_val)

  def testWarmStartVarWithVocab(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)","        self.evaluate(variables.global_variables_initializer())
        fruit_weights = fruit_weights._get_variable_list()
        new_val = np.concatenate(
            [fruit_weights[0].eval(sess), fruit_weights[1].eval(sess)], axis=0)
        self.assertAllClose(prev_val, new_val)

  def testWarmStartVarWithVocab(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights.eval(sess)),"                                           self.get_temp_dir(), prev_vocab_path)
        self.evaluate(variables.global_variables_initializer())
        self.assertAllClose([[2.], [1.5], [1.], [0.5], [0.]],
                            fruit_weights.eval(sess))

  def testWarmStartVarWithColumnVocab(self):
    prev_vocab_path = self._write_vocab([""apple"", ""orange""], ""old_vocab"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[2.3, 2., 0.]], fruit_output_layer.eval(sess))","                                           axis=1)
        self.evaluate(variables.global_variables_initializer())
        self.assertAllClose([[0.3, 0.5, 0.], [0.8, 1.0, 0.], [1.2, 1.5, 0.],
                             [2.3, 2., 0.]], fruit_output_layer.eval(sess))

  def testWarmStartVarWithVocabConstrainedOldVocabSize(self):
    prev_vocab_path = self._write_vocab([""apple"", ""banana"", ""guava"", ""orange""],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights.eval(sess)),"        self.evaluate(variables.global_variables_initializer())
        # Old vocabulary limited to ['apple', 'banana'].
        self.assertAllClose([[0.], [0.], [1.], [0.5], [0.]],
                            fruit_weights.eval(sess))

  def testWarmStartVarWithVocabPrevVarPartitioned(self):
    prev_vocab_path = self._write_vocab([""apple"", ""banana"", ""guava"", ""orange""],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights.eval(sess)),"                                           self.get_temp_dir(), prev_vocab_path)
        self.evaluate(variables.global_variables_initializer())
        self.assertAllClose([[2.], [1.5], [1.], [0.5], [0.]],
                            fruit_weights.eval(sess))

  def testWarmStartVarWithColumnVocabPrevVarPartitioned(self):
    prev_vocab_path = self._write_vocab([""apple"", ""orange""], ""old_vocab"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[2.3, 2., 0.]], fruit_output_layer.eval(sess))","                                           axis=1)
        self.evaluate(variables.global_variables_initializer())
        self.assertAllClose([[0.3, 0.5, 0.], [0.8, 1.0, 0.], [1.2, 1.5, 0.],
                             [2.3, 2., 0.]], fruit_output_layer.eval(sess))

  def testWarmStartVarWithVocabCurrentVarPartitioned(self):
    prev_vocab_path = self._write_vocab([""apple"", ""banana"", ""guava"", ""orange""],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights_vars[0].eval(sess)),"            isinstance(fruit_weights, variables.PartitionedVariable))
        fruit_weights_vars = fruit_weights._get_variable_list()
        self.assertAllClose([[2.], [1.5], [1.]],
                            fruit_weights_vars[0].eval(sess))
        self.assertAllClose([[0.5], [0.], [0.]],
                            fruit_weights_vars[1].eval(sess))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights_vars[1].eval(sess)),"        self.assertAllClose([[2.], [1.5], [1.]],
                            fruit_weights_vars[0].eval(sess))
        self.assertAllClose([[0.5], [0.], [0.]],
                            fruit_weights_vars[1].eval(sess))

  def testWarmStartVarWithColumnVocabCurrentVarPartitioned(self):
    prev_vocab_path = self._write_vocab([""apple"", ""orange""], ""old_vocab"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_output_layer_vars[0].eval(sess)),"            isinstance(fruit_output_layer, variables.PartitionedVariable))
        fruit_output_layer_vars = fruit_output_layer._get_variable_list()
        self.assertAllClose([[0.3, 0.5, 0.], [0.8, 1.0, 0.]],
                            fruit_output_layer_vars[0].eval(sess))
        self.assertAllClose([[1.2, 1.5, 0.], [2.3, 2., 0.]],
                            fruit_output_layer_vars[1].eval(sess))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_output_layer_vars[1].eval(sess)),"        self.assertAllClose([[0.3, 0.5, 0.], [0.8, 1.0, 0.]],
                            fruit_output_layer_vars[0].eval(sess))
        self.assertAllClose([[1.2, 1.5, 0.], [2.3, 2., 0.]],
                            fruit_output_layer_vars[1].eval(sess))

  def testWarmStartVarWithVocabBothVarsPartitioned(self):
    prev_vocab_path = self._write_vocab([""apple"", ""banana"", ""guava"", ""orange""],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights_vars[0].eval(sess)),"            isinstance(fruit_weights, variables.PartitionedVariable))
        fruit_weights_vars = fruit_weights._get_variable_list()
        self.assertAllClose([[2.], [1.5], [1.]],
                            fruit_weights_vars[0].eval(sess))
        self.assertAllClose([[0.5], [0.], [0.]],
                            fruit_weights_vars[1].eval(sess))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_weights_vars[1].eval(sess)),"        self.assertAllClose([[2.], [1.5], [1.]],
                            fruit_weights_vars[0].eval(sess))
        self.assertAllClose([[0.5], [0.], [0.]],
                            fruit_weights_vars[1].eval(sess))

  def testWarmStartVarWithColumnVocabBothVarsPartitioned(self):
    prev_vocab_path = self._write_vocab([""apple"", ""orange""], ""old_vocab"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_output_layer_vars[0].eval(sess)),"            isinstance(fruit_output_layer, variables.PartitionedVariable))
        fruit_output_layer_vars = fruit_output_layer._get_variable_list()
        self.assertAllClose([[0.3, 0.5, 0.], [0.8, 1.0, 0.]],
                            fruit_output_layer_vars[0].eval(sess))
        self.assertAllClose([[1.2, 1.5, 0.], [2.3, 2., 0.]],
                            fruit_output_layer_vars[1].eval(sess))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fruit_output_layer_vars[1].eval(sess)),"        self.assertAllClose([[0.3, 0.5, 0.], [0.8, 1.0, 0.]],
                            fruit_output_layer_vars[0].eval(sess))
        self.assertAllClose([[1.2, 1.5, 0.], [2.3, 2., 0.]],
                            fruit_output_layer_vars[1].eval(sess))

  def testWarmStart_ListOfVariables(self):
    # Save checkpoint from which to warm-start.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_CAMEL_TO_SNAKE_R = re.compile(r""((?<=[a-z0-9])[A-Z]|(?!^)[A-Z](?=[a-z]))"")","def _is_module(obj):
  return isinstance(obj, Module)

_CAMEL_TO_SNAKE_R = re.compile(r""((?<=[a-z0-9])[A-Z]|(?!^)[A-Z](?=[a-z]))"")
_VALID_IDENTIFIER = re.compile(r""^[a-zA-Z_]([a-zA-Z0-9_])*$"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_VALID_IDENTIFIER = re.compile(r""^[a-zA-Z_]([a-zA-Z0-9_])*$"")","  return isinstance(obj, Module)

_CAMEL_TO_SNAKE_R = re.compile(r""((?<=[a-z0-9])[A-Z]|(?!^)[A-Z](?=[a-z]))"")
_VALID_IDENTIFIER = re.compile(r""^[a-zA-Z_]([a-zA-Z0-9_])*$"")


def valid_identifier(name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
regex = re.compile(regex_str),"    compile_flags = sysconfig_lib.get_compile_flags()

    def list_contains(items, regex_str):
      regex = re.compile(regex_str)
      return any(regex.match(item) for item in items)
    self.assertTrue(list_contains(compile_flags, "".*/include""))
    self.assertTrue(list_contains(compile_flags, "".*_GLIBCXX_USE_CXX11_ABI.*""))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = id_weight_pair.id_tensor.eval(,"
      self.assertIsNone(id_weight_pair.weight_tensor)
      with _initialized_session() as sess:
        result = id_weight_pair.id_tensor.eval(
            session=sess, feed_dict={input_placeholder: inputs})
        _assert_sparse_tensor_value(
            self, expected, result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_sequence_length_a, sequence_length_a.eval(session=sess))","
      with _initialized_session() as sess:
        self.assertAllEqual(
            expected_sequence_length_a, sequence_length_a.eval(session=sess))
        self.assertAllEqual(
            expected_sequence_length_b, sequence_length_b.eval(session=sess))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_sequence_length_b, sequence_length_b.eval(session=sess))","        self.assertAllEqual(
            expected_sequence_length_a, sequence_length_a.eval(session=sess))
        self.assertAllEqual(
            expected_sequence_length_b, sequence_length_b.eval(session=sess))


@test_util.run_all_in_graph_and_eager_modes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
invalid_char = re.compile('[^A-Za-z0-9_.\\-]'),"
def _sanitize_column_name_for_variable_scope(name):
  """"""Sanitizes user-provided feature names for use as variable scopes.""""""
  invalid_char = re.compile('[^A-Za-z0-9_.\\-]')
  return invalid_char.sub('_', name)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
predictions.eval(feed_dict={wire_tensor: wire_value})),"        self.assertAllClose(np.zeros((4, 1)), self.evaluate(wire_cast_var))
        self.assertAllClose(
            np.zeros((2, 1)),
            predictions.eval(feed_dict={wire_tensor: wire_value}))
        sess.run(wire_cast_var.assign([[10.], [100.], [1000.], [10000.]]))
        self.assertAllClose(
            [[1010.], [11000.]],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
predictions.eval(feed_dict={wire_tensor: wire_value})),"        sess.run(wire_cast_var.assign([[10.], [100.], [1000.], [10000.]]))
        self.assertAllClose(
            [[1010.], [11000.]],
            predictions.eval(feed_dict={wire_tensor: wire_value}))

  def test_sparse_combiner(self):
    wire_cast = fc.categorical_column_with_hash_bucket('wire_cast', 4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval(,"                indices=np.array(((0, 0), (1, 0), (1, 1)), dtype=np.int64),
                values=np.array((1, 3, 3), dtype=np.int64),
                dense_shape=np.array((2, 2), dtype=np.int64)),
            id_weight_pair.id_tensor.eval(
                feed_dict={
                    input_indices: ((0, 0), (1, 0), (1, 1)),
                    input_values: (1, -1, 99),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
embedding_lookup.eval(,"        self.assertAllEqual(embedding_values, self.evaluate(global_vars[0]))
        self.assertAllEqual(
            expected_lookups,
            embedding_lookup.eval(
                feed_dict={
                    input_indices: sparse_input.indices,
                    input_values: sparse_input.values,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_tensor_eval = id_weight_pair.id_tensor.eval(),"      })
      id_weight_pair = crossed2._get_sparse_tensors(builder)
      with _initialized_session():
        id_tensor_eval = id_weight_pair.id_tensor.eval()
        self.assertAllEqual(
            ((0, 0), (0, 1), (1, 0), (1, 1), (1, 2), (1, 3), (1, 4), (1, 5),
             (1, 6), (1, 7), (1, 8), (1, 9), (1, 10), (1, 11), (1, 12), (1, 13),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_tensor_eval = id_weight_pair.id_tensor.eval(),"      })
      id_weight_pair = crossed._get_sparse_tensors(builder)
      with _initialized_session():
        id_tensor_eval = id_weight_pair.id_tensor.eval()
        self.assertAllEqual(
            ((0, 0), (0, 1), (1, 0), (1, 1), (1, 2), (1, 3)),
            id_tensor_eval.indices)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
predictions.eval(feed_dict={wire_tensor: wire_value})),"        self.assertAllClose(np.zeros((4, 1)), self.evaluate(wire_cast_var))
        self.assertAllClose(
            np.zeros((2, 1)),
            predictions.eval(feed_dict={wire_tensor: wire_value}))
        sess.run(wire_cast_var.assign([[10.], [100.], [1000.], [10000.]]))
        self.assertAllClose(
            [[1010.], [11000.]],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
predictions.eval(feed_dict={wire_tensor: wire_value})),"        sess.run(wire_cast_var.assign([[10.], [100.], [1000.], [10000.]]))
        self.assertAllClose(
            [[1010.], [11000.]],
            predictions.eval(feed_dict={wire_tensor: wire_value}))

  def test_sparse_combiner(self):
    wire_cast = fc._categorical_column_with_hash_bucket('wire_cast', 4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([0.], cols_to_vars['bias'][0].eval())","          partitioner=partitioned_variables.fixed_size_partitioner(2, axis=0)):
        fc.linear_model(features, [price1, price2], cols_to_vars=cols_to_vars)
      with _initialized_session():
        self.assertEqual([0.], cols_to_vars['bias'][0].eval())
        # Partitioning shards the [2, 1] price1 var into 2 [1, 1] Variables.
        self.assertAllEqual([[0.]], cols_to_vars[price1][0])
        self.assertAllEqual([[0.]], cols_to_vars[price1][1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
predictions.eval(feed_dict={wire_tensor: wire_value})),"        self.assertAllClose(np.zeros((4, 1)), self.evaluate(wire_cast_var))
        self.assertAllClose(
            np.zeros((2, 1)),
            predictions.eval(feed_dict={wire_tensor: wire_value}))
        sess.run(wire_cast_var.assign([[10.], [100.], [1000.], [10000.]]))
        self.assertAllClose(
            [[1010.], [11000.]],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
predictions.eval(feed_dict={wire_tensor: wire_value})),"        sess.run(wire_cast_var.assign([[10.], [100.], [1000.], [10000.]]))
        self.assertAllClose(
            [[1010.], [11000.]],
            predictions.eval(feed_dict={wire_tensor: wire_value}))

  def test_sparse_combiner(self):
    wire_cast = fc._categorical_column_with_hash_bucket('wire_cast', 4)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([0.], cols_to_vars['bias'][0].eval())","        get_keras_linear_model_predictions(
            features, [price1, price2], cols_to_vars=cols_to_vars)
      with _initialized_session():
        self.assertEqual([0.], cols_to_vars['bias'][0].eval())
        # Partitioning shards the [2, 1] price1 var into 2 [1, 1] Variables.
        self.assertAllEqual([[0.]], cols_to_vars[price1][0])
        self.assertAllEqual([[0.]], cols_to_vars[price1][1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, -1, 0), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_none_vocabulary_size(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                                        values=np.array(
                                            (2, -1, 0), dtype=np.int64),
                                        dense_shape=inputs.dense_shape),
                                    id_weight_pair.id_tensor.eval())

  def test_transform_feature(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=((0, 0), (1, 0), (1, 1)),
                values=np.array((2, -1, 0), dtype=np.int64),
                dense_shape=(2, 2)),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_default_value_in_vocabulary(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, 2, 0), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_with_oov_buckets(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, 33, 0, 62), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_small_vocabulary_size(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((-1, -1, 0), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_int32(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, -1, 0, 4), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_int32_dense_input(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=((0, 0), (1, 0), (1, 1), (2, 2)),
                values=np.array((2, default_value, 0, 4), dtype=np.int64),
                dense_shape=(3, 3)),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_int32_with_oov_buckets(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, 60, 0, 4), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_linear_model(self):
    wire_column = fc._categorical_column_with_vocabulary_file(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()","        self.assertAllClose(((0.,), (0.,), (0.,), (0.,)),
                            self.evaluate(wire_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()
        # 'marlo' -> 2: wire_var[2] = 3
        # 'skywalker' -> 3, 'omar' -> 0: wire_var[3] + wire_var[0] = 4+1 = 5
        self.assertAllClose(((3.,), (5.,)), self.evaluate(predictions))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()","        self.assertAllClose(((0.,), (0.,), (0.,), (0.,)),
                            self.evaluate(wire_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()
        # 'marlo' -> 2: wire_var[2] = 3
        # 'skywalker' -> 3, 'omar' -> 0: wire_var[3] + wire_var[0] = 4+1 = 5
        self.assertAllClose(((3.,), (5.,)), self.evaluate(predictions))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, -1, 0), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_transform_feature(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dense_shape=(2, 2)), id_weight_pair.id_tensor.eval())","            sparse_tensor.SparseTensorValue(
                indices=((0, 0), (1, 0), (1, 1)),
                values=np.array((2, -1, 0), dtype=np.int64),
                dense_shape=(2, 2)), id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_default_value_in_vocabulary(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, 2, 0), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_with_oov_buckets(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, 33, 0, 62), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_int32(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, -1, 0, 4), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_int32_dense_input(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dense_shape=(3, 3)), id_weight_pair.id_tensor.eval())","            sparse_tensor.SparseTensorValue(
                indices=((0, 0), (1, 0), (1, 1), (2, 2)),
                values=np.array((2, default_value, 0, 4), dtype=np.int64),
                dense_shape=(3, 3)), id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_int32_with_oov_buckets(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((2, 60, 0, 4), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_linear_model(self):
    wire_column = fc._categorical_column_with_vocabulary_list(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()","        self.assertAllClose(((0.,), (0.,), (0.,), (0.,)),
                            self.evaluate(wire_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()
        # 'marlo' -> 2: wire_var[2] = 3
        # 'skywalker' -> 3, 'omar' -> 0: wire_var[3] + wire_var[0] = 4+1 = 5
        self.assertAllClose(((3.,), (5.,)), self.evaluate(predictions))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()","        self.assertAllClose(((0.,), (0.,), (0.,), (0.,)),
                            self.evaluate(wire_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        wire_var.assign(((1.,), (2.,), (3.,), (4.,))).eval()
        # 'marlo' -> 2: wire_var[2] = 3
        # 'skywalker' -> 3, 'omar' -> 0: wire_var[3] + wire_var[0] = 4+1 = 5
        self.assertAllClose(((3.,), (5.,)), self.evaluate(predictions))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((0, 1, 0), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_transform_feature(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=((0, 0), (1, 0), (1, 1)),
                values=np.array((0, 1, 0), dtype=np.int64),
                dense_shape=(2, 2)),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_with_inputs_too_big(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval()),"                indices=inputs.indices,
                values=np.array((1, 3, 3), dtype=np.int64),
                dense_shape=inputs.dense_shape),
            id_weight_pair.id_tensor.eval())

  def test_get_sparse_tensors_with_default_value_and_placeholder_inputs(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
id_weight_pair.id_tensor.eval(feed_dict={,"                indices=np.array(((0, 0), (1, 0), (1, 1)), dtype=np.int64),
                values=np.array((1, 3, 3), dtype=np.int64),
                dense_shape=np.array((2, 2), dtype=np.int64)),
            id_weight_pair.id_tensor.eval(feed_dict={
                input_indices: ((0, 0), (1, 0), (1, 1)),
                input_values: (1, -1, 99),
                input_shape: (2, 2),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign(((1.,), (2.,), (3.,))).eval()","        self.assertAllClose((0.,), self.evaluate(bias))
        self.assertAllClose(((0.,), (0.,), (0.,)), self.evaluate(weight_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        weight_var.assign(((1.,), (2.,), (3.,))).eval()
        # weight_var[0] = 1
        # weight_var[2] + weight_var[1] = 3+2 = 5
        self.assertAllClose(((1.,), (5.,)), self.evaluate(predictions))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign(((1.,), (2.,), (3.,))).eval()","        self.assertAllClose((0.,), self.evaluate(bias))
        self.assertAllClose(((0.,), (0.,), (0.,)), self.evaluate(weight_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        weight_var.assign(((1.,), (2.,), (3.,))).eval()
        # weight_var[0] = 1
        # weight_var[2] + weight_var[1] = 3+2 = 5
        self.assertAllClose(((1.,), (5.,)), self.evaluate(predictions))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign([[1.], [2.], [3.], [4.]]).eval()","        # All should be zero-initialized.
        self.assertAllClose([[0.], [0.], [0.], [0.]], self.evaluate(weight_var))
        self.assertAllClose([[0.]], self.evaluate(predictions))
        weight_var.assign([[1.], [2.], [3.], [4.]]).eval()
        self.assertAllClose([[2. + 3.]], self.evaluate(predictions))

  def test_keras_linear_model(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign([[1.], [2.], [3.], [4.]]).eval()","        # All should be zero-initialized.
        self.assertAllClose([[0.], [0.], [0.], [0.]], self.evaluate(weight_var))
        self.assertAllClose([[0.]], self.evaluate(predictions))
        weight_var.assign([[1.], [2.], [3.], [4.]]).eval()
        self.assertAllClose([[2. + 3.]], self.evaluate(predictions))

  def test_input_layer(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(expected_lookups, embedding_lookup.eval(","                          tuple([v.name for v in global_vars]))
    with _initialized_session():
      self.assertAllEqual(embedding_values, global_vars[0])
      self.assertAllEqual(expected_lookups, embedding_lookup.eval(
          feed_dict={
              input_indices: sparse_input.indices,
              input_values: sparse_input.values,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
)).eval(),"            (1., 2.),  # id 0
            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"linear_weights.assign(((4.,), (6.,))).eval()","            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]
        # example 2, ids [], embedding[2] = [0, 0]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
)).eval(),"            (1., 2.),  # id 0
            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"linear_weights.assign(((4.,), (6.,))).eval()","            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]
        # example 2, ids [], embedding[2] = [0, 0]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dense_shape=[1, 2]), features['aaa'].eval())","            sparse_tensor.SparseTensorValue(
                indices=[[0, 0], [0, 1]],
                values=np.array([b'omar', b'stringer'], dtype=np.object_),
                dense_shape=[1, 2]), features['aaa'].eval())
        _assert_sparse_tensor_value(
            self,
            sparse_tensor.SparseTensorValue(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dense_shape=[1, 2]), features['bbb'].eval())","            sparse_tensor.SparseTensorValue(
                indices=[[0, 0], [0, 1]],
                values=np.array([b'stringer', b'marlo'], dtype=np.object_),
                dense_shape=[1, 2]), features['bbb'].eval())

  def test_transform_feature(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
)).eval(),"            (1., 2.),  # id 0
            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights_a.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"linear_weights_a.assign(((4.,), (6.,))).eval()","            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights_a.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]
        # sum(embeddings * linear_weights)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"linear_weights_b.assign(((3.,), (5.,))).eval()","        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]
        # sum(embeddings * linear_weights)
        # = [4*7 + 6*11, 4*2 + 6*3.5] = [94, 29]
        linear_weights_b.assign(((3.,), (5.,))).eval()
        # example 0, ids [0], embedding[0] = [1, 2]
        # example 1, ids [], embedding[1] = 0, 0]
        # sum(embeddings * linear_weights)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
)).eval(),"            (1., 2.),  # id 0
            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights_a.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"linear_weights_a.assign(((4.,), (6.,))).eval()","            (3., 5.),  # id 1
            (7., 11.)  # id 2
        )).eval()
        linear_weights_a.assign(((4.,), (6.,))).eval()
        # example 0, ids [2], embedding[0] = [7, 11]
        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]
        # sum(embeddings * linear_weights)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"linear_weights_b.assign(((3.,), (5.,))).eval()","        # example 1, ids [0, 1], embedding[1] = mean([1, 2] + [3, 5]) = [2, 3.5]
        # sum(embeddings * linear_weights)
        # = [4*7 + 6*11, 4*2 + 6*3.5] = [94, 29]
        linear_weights_b.assign(((3.,), (5.,))).eval()
        # example 0, ids [0], embedding[0] = [1, 2]
        # example 1, ids [], embedding[1] = 0, 0]
        # sum(embeddings * linear_weights)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign(((1.,), (2.,), (3.,))).eval()","        self.assertAllClose((0.,), self.evaluate(bias))
        self.assertAllClose(((0.,), (0.,), (0.,)), self.evaluate(weight_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        weight_var.assign(((1.,), (2.,), (3.,))).eval()
        # weight_var[0] * weights[0, 0] = 1 * .5 = .5
        # weight_var[2] * weights[1, 0] + weight_var[1] * weights[1, 1]
        # = 3*1 + 2*.1 = 3+.2 = 3.2",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign(((1.,), (2.,), (3.,))).eval()","        self.assertAllClose((0.,), self.evaluate(bias))
        self.assertAllClose(((0.,), (0.,), (0.,)), self.evaluate(weight_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        weight_var.assign(((1.,), (2.,), (3.,))).eval()
        # weight_var[0] * weights[0, 0] = 1 * .5 = .5
        # weight_var[2] * weights[1, 0] + weight_var[1] * weights[1, 1]
        # = 3*1 + 2*.1 = 3+.2 = 3.2",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign(((1.,), (2.,), (3.,))).eval()","        self.assertAllClose((0.,), self.evaluate(bias))
        self.assertAllClose(((0.,), (0.,), (0.,)), self.evaluate(weight_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        weight_var.assign(((1.,), (2.,), (3.,))).eval()
        # weight_var[0] * weights[0, 0] = 1 * .5 = .5
        # weight_var[2] * weights[1, 0] + weight_var[1] * weights[1, 1]
        # = 3*1 + 2*.1 = 3+.2 = 3.2",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"weight_var.assign(((1.,), (2.,), (3.,))).eval()","        self.assertAllClose((0.,), self.evaluate(bias))
        self.assertAllClose(((0.,), (0.,), (0.,)), self.evaluate(weight_var))
        self.assertAllClose(((0.,), (0.,)), self.evaluate(predictions))
        weight_var.assign(((1.,), (2.,), (3.,))).eval()
        # weight_var[0] * weights[0, 0] = 1 * .5 = .5
        # weight_var[2] * weights[1, 0] + weight_var[1] * weights[1, 1]
        # = 3*1 + 2*.1 = 3+.2 = 3.2",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
matcher = re.compile(,"      op_types.append(b""Conv2D"")

    for op_type in op_types:
      matcher = re.compile(
          br""\s+"" + op_type + br"",\s*(\d+),\s*(\d+),\s*([\d\.eE+-]+)%,\s*"" +
          br""([\d\.eE+-]+)%,\s*(-?\d+),\s*(\d+),"", re.MULTILINE)
      m = matcher.search(report)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
all_vars_values = [var.eval(session=sess) for var in all_vars],"
      if restore:
        all_vars = ops.get_collection(ops.GraphKeys.GLOBAL_VARIABLES)
        all_vars_values = [var.eval(session=sess) for var in all_vars]
        return all_vars_values
      else:
        saver.save(sess, checkpoint_path)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
exec(cmd)  # pylint: disable=exec-used,"  # 1. Spawn the process for semaphore tracker.
  # 2. Spawn the initial process for forkserver.
  # 3. Spawn any process as requested by the ""spawn"" method.
  exec(cmd)  # pylint: disable=exec-used
  sys.exit(0)  # Semaphore tracker doesn't explicitly sys.exit.

",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
exec(cmd)  # pylint: disable=exec-used,"  # 1. Spawn the process for semaphore tracker.
  # 2. Spawn the initial process for forkserver.
  # 3. Spawn any process as requested by the ""spawn"" method.
  exec(cmd)  # pylint: disable=exec-used
  sys.exit(0)  # Semaphore tracker doesn't explicitly sys.exit.

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# /.../tensorflow/python/distribute/input_lib_test_multiworker_gpu,"        # argv[0] is in the form of
        # /.../tensorflow/python/distribute/input_lib_test.py
        # and the binary is
        # /.../tensorflow/python/distribute/input_lib_test_multiworker_gpu
        package_root_base = sys.argv[0][:sys.argv[0].rfind(package_root)]
        binary = os.environ['TEST_TARGET'][2:].replace(':', '/', 1)
        possible_path = os.path.join(package_root_base, package_root,",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(v.eval(), 1.)","    v = self.create_variable(1.)
    with self.cached_session():
      self.evaluate(variables_lib.global_variables_initializer())
      self.assertEqual(v.eval(), 1.)

  def testInitialValueEager(self):
    v = self.create_variable(1.)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","        return self._packed_var.handle
      return self._values[replica_id].handle

  def eval(self, session=None):
    return self._get_on_device_or_primary().eval(session)

  @property",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return self._get_on_device_or_primary().eval(session),"      return self._values[replica_id].handle

  def eval(self, session=None):
    return self._get_on_device_or_primary().eval(session)

  @property
  def _save_slice_info(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_VARIABLE_UNIQUIFYING_REGEX = re.compile(r""_\d/"")","
import re

_VARIABLE_UNIQUIFYING_REGEX = re.compile(r""_\d/"")
_VARIABLE_UNIQUIFYING_REGEX_AT_END = re.compile(r""_\d$"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_VARIABLE_UNIQUIFYING_REGEX_AT_END = re.compile(r""_\d$"")","import re

_VARIABLE_UNIQUIFYING_REGEX = re.compile(r""_\d/"")
_VARIABLE_UNIQUIFYING_REGEX_AT_END = re.compile(r""_\d$"")


def _canonicalize_variable_name(name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer=""rmsprop"", loss=""mse"", steps_per_execution=..., ...)","
  with strategy.scope():
    model = ...  # Make sure the `Model` is created within scope.
  model.compile(optimizer=""rmsprop"", loss=""mse"", steps_per_execution=..., ...)

  # Optional callbacks to checkpoint the model, back up the progress, etc.
  callbacks = [tf.keras.callbacks.ModelCheckpoint(...), ...]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# eval() is skipped since it shouldn't called in a tf.function.,"    _test(lambda: check_ops.assert_equal_v2(v.assign_sub(1.), 1.), v)
    # TODO(b/148689177): Implement batch_scatter_update.
    # count_up_to() is skipped since it's deprecated.
    # eval() is skipped since it shouldn't called in a tf.function.
    # experimental_ref() is skipped since it's deprecated.
    # from_proto() is skipped since it shouldn't called in a tf.function.
    # TODO(b/148689177): Implement gather_nd.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","  def sparse_read(self, indices, name=None):
    return self._v.sparse_read(indices, name=name)

  def eval(self, session=None):
    return self._v.eval(session)

  @property",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return self._v.eval(session),"    return self._v.sparse_read(indices, name=name)

  def eval(self, session=None):
    return self._v.eval(session)

  @property
  def graph(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","      return self.cached_read_value()
    return self._v.value()

  def eval(self, session=None):
    return self._v.eval(session)

  @property",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return self._v.eval(session),"    return self._v.value()

  def eval(self, session=None):
    return self._v.eval(session)

  @property
  def graph(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(),"    return dataset.repeat().batch(1, drop_remainder=True)
  dist_dataset = strategy.distribute_datasets_from_function(dataset_fn)

  model.compile()
  model.fit(dist_dataset)
  ```
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"`Tensor.eval()`, or `Operation.run()`.","
  **Important**: This tensor will produce an error if evaluated. Its value must
  be fed using the `feed_dict` optional argument to `Session.run()`,
  `Tensor.eval()`, or `Operation.run()`.

  For example:
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"`Session.run()`, `Tensor.eval()`, or `Operation.run()`.","
  **Important**: This sparse tensor will produce an error if evaluated.
  Its value must be fed using the `feed_dict` optional argument to
  `Session.run()`, `Tensor.eval()`, or `Operation.run()`.

  For example:
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
sp_value = sp.eval(session=sess),"
    sp = tf.sparse.SparseTensor(indices=indices, values=values,
                                dense_shape=shape)
    sp_value = sp.eval(session=sess)
    print(sess.run(y, feed_dict={x: sp_value}))  # Will succeed.
  ```
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_INVALID_SCOPE_CHARACTERS = re.compile(r""[^-_/.A-Za-z0-9]"")","  return ops.get_collection(_SUMMARY_WRITER_INIT_COLLECTION_NAME)


_INVALID_SCOPE_CHARACTERS = re.compile(r""[^-_/.A-Za-z0-9]"")


@tf_export(""summary.experimental.summary_scope"", v1=[])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","  def aggregation(self):
    raise NotImplementedError

  def eval(self, session=None):
    """"""In a session, computes and returns the value of this variable.

    This is not a graph construction method, it does not add ops to the graph.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(v.eval(sess)),"    with tf.compat.v1.Session() as sess:
        sess.run(init)
        # Usage passing the session explicitly.
        print(v.eval(sess))
        # Usage with the default session.  The 'with' block
        # above makes 'sess' the default session.
        print(v.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(v.eval()),"        print(v.eval(sess))
        # Usage with the default session.  The 'with' block
        # above makes 'sess' the default session.
        print(v.eval())
    ```

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(v.eval(sess)) # prints [2 3],"        sess.run(init)
        # Usage passing the session explicitly.
        v.load([2, 3], sess)
        print(v.eval(sess)) # prints [2 3]
        # Usage with the default session.  The 'with' block
        # above makes 'sess' the default session.
        v.load([3, 4], sess)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(v.eval()) # prints [3 4],"        # Usage with the default session.  The 'with' block
        # above makes 'sess' the default session.
        v.load([3, 4], sess)
        print(v.eval()) # prints [3 4]
    ```

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
z = nn_ops.relu(_get_weak_tensor(x)).eval(),"      x = np.zeros(i) + np.nan
      # TODO(b/178335491): This is broken on GPU today.
      with self.cached_session(use_gpu=False):
        z = nn_ops.relu(_get_weak_tensor(x)).eval()
        self.assertTrue(np.isnan(z).all())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_mean, mean.eval(feed_dict={x: x_numpy}))","
      # Check that the moments are correct.
      self.assertAllCloseAccordingToType(
          expected_mean, mean.eval(feed_dict={x: x_numpy}))
      self.assertAllCloseAccordingToType(
          expected_variance, var.eval(feed_dict={x: x_numpy}))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_variance, var.eval(feed_dict={x: x_numpy}))","      self.assertAllCloseAccordingToType(
          expected_mean, mean.eval(feed_dict={x: x_numpy}))
      self.assertAllCloseAccordingToType(
          expected_variance, var.eval(feed_dict={x: x_numpy}))

  def RunMomentTest(self, shape, axes, keep_dims, dtype):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"...   result = table.lookup(tf.constant(['a', 'c'])).eval()","  ...   init = tf.compat.v1.lookup.KeyValueTensorInitializer(['a', 'b'], [1, 2])
  ...   table = tf.compat.v1.lookup.StaticHashTable(init, default_value=-1)
  ...   tf.compat.v1.tables_initializer().run()
  ...   result = table.lookup(tf.constant(['a', 'c'])).eval()
  >>> result
  array([ 1, -1], dtype=int32)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(out.eval()),"      num_oov_buckets)
  out = table.lookup(input_tensor).
  table.init.run()
  print(out.eval())
  ```

  The hash function used for generating out-of-vocabulary buckets ID is handled",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"ids.eval()  ==> [0, 1, 3, 2]  # where 3 is the out-of-vocabulary bucket","  ...
  tf.compat.v1.tables_initializer().run()

  ids.eval()  ==> [0, 1, 3, 2]  # where 3 is the out-of-vocabulary bucket
  ```

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"ids.eval()  ==> [0, 1, 4, 2]","  ...
  tf.compat.v1.tables_initializer().run()

  ids.eval()  ==> [0, 1, 4, 2]
  ```

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"values.eval() ==> [""lake"", ""UNKNOWN""]","  ...
  tf.compat.v1.tables_initializer().run()

  values.eval() ==> [""lake"", ""UNKNOWN""]
  ```

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"values.eval() ==> [""lake"", ""UNKNOWN""]","  ...
  tf.compat.v1.tables_initializer().run()

  values.eval() ==> [""lake"", ""UNKNOWN""]
  ```

  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_INVALID_TAG_CHARACTERS = re.compile(r'[^-/\w\.]'),"    ops.add_to_collection(key, val)


_INVALID_TAG_CHARACTERS = re.compile(r'[^-/\w\.]')


def clean_tag(name):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(1, beta_ph.eval(feed_dict={ph: x_one}))","    with self.session():
      ph = array_ops.placeholder(dtypes.float32)
      beta_ph = math_ops.exp(special_math_ops.lbeta(ph))
      self.assertAllClose(1, beta_ph.eval(feed_dict={ph: x_one}))
      self.assertAllClose(0.5,
                          beta_ph.eval(feed_dict={ph: x_one_half}))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
beta_ph.eval(feed_dict={ph: x_one_half})),"      beta_ph = math_ops.exp(special_math_ops.lbeta(ph))
      self.assertAllClose(1, beta_ph.eval(feed_dict={ph: x_one}))
      self.assertAllClose(0.5,
                          beta_ph.eval(feed_dict={ph: x_one_half}))

  @test_util.run_deprecated_v1
  def test_four_dimensional_arg_with_partial_shape_dynamic(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
beta_ph.eval(feed_dict={x_ph: x_})),"      x_ph = array_ops.placeholder(dtypes.float32, [3, 2, 3, None])
      beta_ph = math_ops.exp(special_math_ops.lbeta(x_ph))
      self.assertAllClose(expected_beta_x,
                          beta_ph.eval(feed_dict={x_ph: x_}))

  @test_util.run_in_graph_and_eager_modes
  def test_two_dimensional_arg(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
beta_ph.eval(feed_dict={ph: x_one_half})),"      ph = array_ops.placeholder(dtypes.float32)
      beta_ph = math_ops.exp(special_math_ops.lbeta(ph))
      self.assertAllClose([0.5, 0.5],
                          beta_ph.eval(feed_dict={ph: x_one_half}))

  @test_util.run_in_graph_and_eager_modes
  def test_two_dimensional_proper_shape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"y_pos = y.eval(feed_dict=_extra_feeds(extra_feed_dict, {x: x_pos}))","    x_pos = x_data.copy()
    x_neg = x_data.copy()
    x_pos.ravel().view(x_dtype)[row] += delta
    y_pos = y.eval(feed_dict=_extra_feeds(extra_feed_dict, {x: x_pos}))
    x_neg.ravel().view(x_dtype)[row] -= delta
    y_neg = y.eval(feed_dict=_extra_feeds(extra_feed_dict, {x: x_neg}))
    diff = (y_pos - y_neg) / scale",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"y_neg = y.eval(feed_dict=_extra_feeds(extra_feed_dict, {x: x_neg}))","    x_pos.ravel().view(x_dtype)[row] += delta
    y_pos = y.eval(feed_dict=_extra_feeds(extra_feed_dict, {x: x_pos}))
    x_neg.ravel().view(x_dtype)[row] -= delta
    y_neg = y.eval(feed_dict=_extra_feeds(extra_feed_dict, {x: x_neg}))
    diff = (y_pos - y_neg) / scale
    jacobian[row, :] = diff.ravel().view(y_dtype)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
def eval(self):,"    """"""The string representation of this handle.""""""
    return self._handle

  def eval(self):
    """"""Return the value of the tensor represented by this handle.""""""
    if not self._auto_gc_enabled:
      raise TypeError(""Persistent tensor %s may have already been deleted.""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
z = nn_ops.relu(constant_op.constant(x)).eval(),"      x = np.zeros(i) + np.nan
      # TODO(b/178335491): This is broken on GPU today.
      with self.cached_session(use_gpu=False):
        z = nn_ops.relu(constant_op.constant(x)).eval()
        self.assertTrue(np.isnan(z).all())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_a = resize_bilinear_gradients.eval(feed_dict=feed_dict),"            colocate_gradients_with_ops=True)[0]
        for i in range(repeat_count):
          feed_dict = {upstream_gradients: self._randomNDArray(output_shape)}
          result_a = resize_bilinear_gradients.eval(feed_dict=feed_dict)
          result_b = resize_bilinear_gradients.eval(feed_dict=feed_dict)
          self.assertAllEqual(result_a, result_b)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_b = resize_bilinear_gradients.eval(feed_dict=feed_dict),"        for i in range(repeat_count):
          feed_dict = {upstream_gradients: self._randomNDArray(output_shape)}
          result_a = resize_bilinear_gradients.eval(feed_dict=feed_dict)
          result_b = resize_bilinear_gradients.eval(feed_dict=feed_dict)
          self.assertAllEqual(result_a, result_b)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","  def aggregation(self):
    return self._aggregation

  def eval(self, session=None):
    """"""In a session, computes and returns the value of this variable.

    This is not a graph construction method, it does not add ops to the graph.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(v.eval(sess)),"    with tf.compat.v1.Session() as sess:
        sess.run(init)
        # Usage passing the session explicitly.
        print(v.eval(sess))
        # Usage with the default session.  The 'with' block
        # above makes 'sess' the default session.
        print(v.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(v.eval()),"        print(v.eval(sess))
        # Usage with the default session.  The 'with' block
        # above makes 'sess' the default session.
        print(v.eval())
    ```

    Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return self._variable.eval(session=session),"    Returns:
      A numpy `ndarray` with a copy of the value of this variable.
    """"""
    return self._variable.eval(session=session)

  @property
  def initial_value(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
...   print(x.eval()),"  >>> with tf.compat.v1.Session():
  ...   x = tf.compat.v1.get_variable('x', shape=[2, 4], initializer=init)
  ...   x.initializer.run()
  ...   print(x.eval())
  [[0. 1. 2. 3.]
   [4. 5. 6. 7.]]
  >>> # Larger shape",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
...   print(y.eval()),"  >>> with tf.compat.v1.Session():
  ...   y = tf.compat.v1.get_variable('y', shape=[3, 4], initializer=init)
  ...   y.initializer.run()
  ...   print(y.eval())
  [[0.  1.  2.  3.]
   [4.  5.  6.  7.]
   [7.  7.  7.  7.]]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(6.0, grads[0].eval())","      z = y * 3.0
      grads = gradients.gradients(z, [x, y])
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(6.0, grads[0].eval())

  @test_util.run_v1_only(""b/120545219"")
  def testAggregationMethodAccumulateN(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(20.0, grads[0].eval())","          aggregation_method=gradients.AggregationMethod.
          EXPERIMENTAL_ACCUMULATE_N)
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(20.0, grads[0].eval())
      self.assertEqual(10.0, grads[1].eval())

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(10.0, grads[1].eval())","          EXPERIMENTAL_ACCUMULATE_N)
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(20.0, grads[0].eval())
      self.assertEqual(10.0, grads[1].eval())

  @test_util.run_v1_only(""b/120545219"")
  def testAggregationMethodAddN(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(20.0, grads[0].eval())","      grads = gradients.gradients(
          z, [x, y], aggregation_method=gradients.AggregationMethod.ADD_N)
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(20.0, grads[0].eval())
      self.assertEqual(10.0, grads[1].eval())

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(10.0, grads[1].eval())","          z, [x, y], aggregation_method=gradients.AggregationMethod.ADD_N)
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(20.0, grads[0].eval())
      self.assertEqual(10.0, grads[1].eval())

  @test_util.run_v1_only(""b/120545219"")
  def testAggregationMethodTree(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(20.0, grads[0].eval())","          z, [x, y],
          aggregation_method=gradients.AggregationMethod.EXPERIMENTAL_TREE)
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(20.0, grads[0].eval())
      self.assertEqual(10.0, grads[1].eval())

  def testNoGradientForStringOutputs(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(10.0, grads[1].eval())","          aggregation_method=gradients.AggregationMethod.EXPERIMENTAL_TREE)
      self.assertTrue(all(x is not None for x in grads))
      self.assertEqual(20.0, grads[0].eval())
      self.assertEqual(10.0, grads[1].eval())

  def testNoGradientForStringOutputs(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([3.0, 1.0], [g.eval() for g in totalg])","      y = 2 * x
      z = x + y
      totalg = gradients.gradients(z, [x, y])
      self.assertEqual([3.0, 1.0], [g.eval() for g in totalg])
      partialg = gradients.gradients(z, [x, y], stop_gradients=[x, y])
      self.assertEqual([1.0, 1.0], [g.eval() for g in partialg])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([1.0, 1.0], [g.eval() for g in partialg])","      totalg = gradients.gradients(z, [x, y])
      self.assertEqual([3.0, 1.0], [g.eval() for g in totalg])
      partialg = gradients.gradients(z, [x, y], stop_gradients=[x, y])
      self.assertEqual([1.0, 1.0], [g.eval() for g in partialg])

  @test_util.run_v1_only(""b/120545219"")
  def testStopGradients(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
hessians_actual = [hess.eval() for hess in hessians],"          for x, mat in zip(xs, mats)
      ]
      hessians = gradients.hessians(xs_mats_xs, xs)
      hessians_actual = [hess.eval() for hess in hessians]
    for hess_value, hess_actual in zip(hess_values, hessians_actual):
      self.assertAllClose(hess_value, hess_actual)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(g.eval(feed_dict={conditional: False}), [3.0])","      g, = gradients_impl.gradients(output, alpha)
      self.evaluate(variables.global_variables_initializer())
      self.assertAllEqual(g, [2.0])
      self.assertAllEqual(g.eval(feed_dict={conditional: False}), [3.0])

  def testRecursiveCustomGradient(self):
    @custom_gradient.custom_gradient",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","  def aggregation(self):
    return self._aggregation

  def eval(self, session=None):
    """"""Evaluates and returns the value of this variable.""""""
    if context.executing_eagerly():
      raise RuntimeError(""This operation is not supported """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return self._graph_element.eval(session=session),"    if context.executing_eagerly():
      raise RuntimeError(""This operation is not supported ""
                         ""when eager execution is enabled."")
    return self._graph_element.eval(session=session)

  def numpy(self):
    if context.executing_eagerly():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.compat.v1.Print(b, [b]).eval()","  with tf.control_dependencies([other_assign]):
    # Will print 2.0 because the value was read before other_assign ran. If
    # `a` was a tf.Variable instead, 2.0 or 3.0 could be printed.
    tf.compat.v1.Print(b, [b]).eval()
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"grads = gradients_impl.gradients(loss, logits)[0].eval()","      targets = constant_op.constant([0.0, 1.0], dtype=dtypes.float64)
      loss = nn_impl.sigmoid_cross_entropy_with_logits(
          labels=targets, logits=logits)
      grads = gradients_impl.gradients(loss, logits)[0].eval()
    self.assertAllClose(grads, [0.5, -0.5])

  def testShapeError(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
c = tf.keras.backend.eval(b),"  import tensorflow as tf
  a = [1, 10, 26.9, 2.8, 166.32, 62.3]
  b = tf.math.argmin(input = a)
  c = tf.keras.backend.eval(b)
  # c = 0
  # here a[0] = 1 which is the smallest element of a across axis 0
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd',","
  ```python
  # Used within Keras model
  model.compile(optimizer='sgd',
                loss='mse',
                metrics=[tf.keras.metrics.Accuracy()])
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
decode(io_ops.read_file(path)).eval(),"    with self.cached_session():
      for decode in image_ops.decode_jpeg, image_ops.decode_png:
        with self.assertRaisesOpError(r""Got 12 frames""):
          decode(io_ops.read_file(path)).eval()


class CombinedNonMaxSuppressionTest(test_util.TensorFlowTestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(num_valid_padded.eval(), 3)","      self.assertEqual(selected_indices.shape.is_fully_defined(), False)
      with self.cached_session():
        self.assertAllClose(selected_indices_padded, [3, 0, 5, 0, 0])
        self.assertEqual(num_valid_padded.eval(), 3)
        self.assertAllClose(selected_indices, [3, 0, 5])
        self.assertEqual(num_valid.eval(), 3)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(num_valid.eval(), 3)","        self.assertAllClose(selected_indices_padded, [3, 0, 5, 0, 0])
        self.assertEqual(num_valid_padded.eval(), 3)
        self.assertAllClose(selected_indices, [3, 0, 5])
        self.assertEqual(num_valid.eval(), 3)

  @parameterized.named_parameters([(""_RunEagerly"", True), (""_RunGraph"", False)])
  @test_util.disable_xla(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(num_valid.eval(), 3)","      self.assertEqual(selected_indices.shape.is_fully_defined(), False)
      with self.cached_session():
        self.assertAllClose(selected_indices, [0, 2, 4])
        self.assertEqual(num_valid.eval(), 3)

  @parameterized.named_parameters([(""_RunEagerly"", True), (""_RunGraph"", False)])
  @test_util.xla_allow_fallback(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_FLAG_SINGLE_QUOTE_PAT = re.compile(r""\s*--([^=]+)='([^']*)'"")","_SUBMODE_BRIEF = 'brief'
_SUBMODE_DETAILED = 'detailed'

_FLAG_SINGLE_QUOTE_PAT = re.compile(r""\s*--([^=]+)='([^']*)'"")
_FLAG_DOUBLE_QUOTE_PAT = re.compile(r'\s*--([^=]+)=""([^""]*)""')
_FLAG_NO_QUOTE_PAT = re.compile(r'\s*--([^=]+)=(\S*)')
_FLAG_NO_EQUAL_PAT = re.compile(r'\s*--([^=]+)\s*')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_FLAG_DOUBLE_QUOTE_PAT = re.compile(r'\s*--([^=]+)=""([^""]*)""')","_SUBMODE_DETAILED = 'detailed'

_FLAG_SINGLE_QUOTE_PAT = re.compile(r""\s*--([^=]+)='([^']*)'"")
_FLAG_DOUBLE_QUOTE_PAT = re.compile(r'\s*--([^=]+)=""([^""]*)""')
_FLAG_NO_QUOTE_PAT = re.compile(r'\s*--([^=]+)=(\S*)')
_FLAG_NO_EQUAL_PAT = re.compile(r'\s*--([^=]+)\s*')
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_FLAG_NO_QUOTE_PAT = re.compile(r'\s*--([^=]+)=(\S*)'),"
_FLAG_SINGLE_QUOTE_PAT = re.compile(r""\s*--([^=]+)='([^']*)'"")
_FLAG_DOUBLE_QUOTE_PAT = re.compile(r'\s*--([^=]+)=""([^""]*)""')
_FLAG_NO_QUOTE_PAT = re.compile(r'\s*--([^=]+)=(\S*)')
_FLAG_NO_EQUAL_PAT = re.compile(r'\s*--([^=]+)\s*')

FLAGS_ENV_VAR = 'TENSOR_TRACER_FLAGS'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_FLAG_NO_EQUAL_PAT = re.compile(r'\s*--([^=]+)\s*'),"_FLAG_SINGLE_QUOTE_PAT = re.compile(r""\s*--([^=]+)='([^']*)'"")
_FLAG_DOUBLE_QUOTE_PAT = re.compile(r'\s*--([^=]+)=""([^""]*)""')
_FLAG_NO_QUOTE_PAT = re.compile(r'\s*--([^=]+)=(\S*)')
_FLAG_NO_EQUAL_PAT = re.compile(r'\s*--([^=]+)\s*')

FLAGS_ENV_VAR = 'TENSOR_TRACER_FLAGS'
FLAG_NAME_ENABLE = 'enable'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_OP_RANGE_PAT = re.compile(r'(\d+):(\d+)'),"    FLAG_NAME_INSPECT_TRACE, FLAG_FLUSH_SUMMARY,
]

_OP_RANGE_PAT = re.compile(r'(\d+):(\d+)')
_TEST_UNDECLARED_OUTPUTS_DIR_ENV_VAR = 'TEST_UNDECLARED_OUTPUTS_DIR'

_TT_DEFAULT_TRACE_LEVEL = 3",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
r = re.compile(v),"      return re_list
    list_of_values = flag_value.split(',')
    for v in list_of_values:
      r = re.compile(v)
      re_list.append(r)
    return re_list
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
exec(  # pylint:disable=exec-used,"
  # expr is defined because we would have raised an error otherwise.
  new_ast = ast.Module(body=expr.body, type_ignores=[])  # pylint:disable=undefined-loop-variable
  exec(  # pylint:disable=exec-used
      compile(new_ast, '<ast>', 'exec'),
      globals(),
      wrapped_test_module.__dict__,",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
exec(  # pylint:disable=exec-used,"
  # expr is defined because we would have raised an error otherwise.
  new_ast = ast.Module(body=expr.body, type_ignores=[])  # pylint:disable=undefined-loop-variable
  exec(  # pylint:disable=exec-used
      compile(new_ast, '<ast>', 'exec'),
      globals(),
      wrapped_test_module.__dict__,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"compile(new_ast, '<ast>', 'exec'),","  # expr is defined because we would have raised an error otherwise.
  new_ast = ast.Module(body=expr.body, type_ignores=[])  # pylint:disable=undefined-loop-variable
  exec(  # pylint:disable=exec-used
      compile(new_ast, '<ast>', 'exec'),
      globals(),
      wrapped_test_module.__dict__,
  )",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"return importlib.import_module(FLAGS.wrapped_tpu_test_module_relative,","  Returns:
    The user test module.
  """"""
  return importlib.import_module(FLAGS.wrapped_tpu_test_module_relative,
                                 calculate_parent_python_path(sys.argv[0]))

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# A fetch had a corresponding direct TensorHandle feed. Call eval(),"        # If the fetch was in the feeds, use the fed value, otherwise
        # use the returned value.
        if self._fetches[i].ref() in self._feed_handles:
          # A fetch had a corresponding direct TensorHandle feed. Call eval()
          # to obtain the Tensor value from the TensorHandle.
          value = self._feed_handles[self._fetches[i].ref()].eval()
        else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
value = self._feed_handles[self._fetches[i].ref()].eval(),"        if self._fetches[i].ref() in self._feed_handles:
          # A fetch had a corresponding direct TensorHandle feed. Call eval()
          # to obtain the Tensor value from the TensorHandle.
          value = self._feed_handles[self._fetches[i].ref()].eval()
        else:
          value = self._feeds.get(self._fetches[i].ref())
        if value is None:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(c.eval()),"
    with sess.as_default():
      assert tf.compat.v1.get_default_session() is sess
      print(c.eval())
    ```

    To get the current default session, use `tf.compat.v1.get_default_session`.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(c.eval()),"    c = tf.constant(...)
    sess = tf.compat.v1.Session()
    with sess.as_default():
      print(c.eval())
    # ...
    with sess.as_default():
      print(c.eval())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(c.eval()),"      print(c.eval())
    # ...
    with sess.as_default():
      print(c.eval())

    sess.close()
    ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# We can just use 'c.eval()' without passing 'sess',"  a = tf.constant(5.0)
  b = tf.constant(6.0)
  c = a * b
  # We can just use 'c.eval()' without passing 'sess'
  print(c.eval())
  sess.close()
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(c.eval()),"  b = tf.constant(6.0)
  c = a * b
  # We can just use 'c.eval()' without passing 'sess'
  print(c.eval())
  sess.close()
  ```
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# We can also use 'c.eval()' here.,"  b = tf.constant(6.0)
  c = a * b
  with tf.compat.v1.Session():
    # We can also use 'c.eval()' here.
    print(c.eval())
  ```
  """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
print(c.eval()),"  c = a * b
  with tf.compat.v1.Session():
    # We can also use 'c.eval()' here.
    print(c.eval())
  ```
  """"""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_NODEDEF_NAME_RE = re.compile(,"  # both the old and the new formats:
  # Old format: [[Node: <node_name> = ...]]
  # New format: [[{{node <node_name>}} = ...]]
  _NODEDEF_NAME_RE = re.compile(
      r'\[\[(Node: )?(\{\{node )?([^\} ]*)(\}\})?\s*=*')

  def _do_run(self, handle, target_list, fetch_list, feed_dict, options,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
copy_val = copy.eval({'W1:0': arr}),"      # Test with feed.
      # TODO(mrry): Investigate why order='F' didn't work.
      arr = np.asarray([[0, 1, 2], [3, 4, 5]], dtype=np.float32, order='C')
      copy_val = copy.eval({'W1:0': arr})
      self.assertAllEqual(arr, copy_val)
      # Test without feed.
      copy_val = self.evaluate(copy)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(sp.dense_shape.eval(session=s), shape)","      shape = np.array([7, 9, 2]).astype(np.int64)
      sp = array_ops.sparse_placeholder(
          dtype=np.float32, shape=shape, name='placeholder1')
      self.assertAllEqual(sp.dense_shape.eval(session=s), shape)
      self.assertAllEqual(tensor_util.constant_value(sp.shape), shape)
      sp_indices = array_ops.identity(sp.indices)
      sp_values = array_ops.identity(sp.values)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"fed_c_val = c.eval(feed_dict={a.name: [[4.0, 4.0]]})","      c_val = self.evaluate(c)
      self.assertAllEqual([[4.0, 4.0, 4.0]], c_val)

      fed_c_val = c.eval(feed_dict={a.name: [[4.0, 4.0]]})
      self.assertAllEqual([[16.0, 16.0, 16.0]], fed_c_val)

  @test_util.run_v1_only('b/120545219')",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"assign_b_to_v.eval(feed_dict={'b:0': [[3.0, 3.0]]})","      v_val = self.evaluate(v)
      self.assertAllEqual([[2.0, 2.0]], v_val)

      assign_b_to_v.eval(feed_dict={'b:0': [[3.0, 3.0]]})
      v_val = self.evaluate(v)
      self.assertAllEqual([[3.0, 3.0]], v_val)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
val = c.eval(session=sess),"
      def run_step():
        ev.wait()
        val = c.eval(session=sess)
        self.assertEqual(val, 5.0)

      threads = [self.checkedThread(target=run_step) for _ in range(100)]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
c_2.eval(),"      self.assertEqual(sess.run(c_2), 5.0)
      with self.assertRaisesWithPredicateMatch(
          ValueError, lambda e: 'No default session is registered.' in str(e)):
        c_2.eval()

  @test_util.run_v1_only('b/120545219')
  def testInteractive(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out_t.eval(feed_dict={feed_t: feed_val}),"      with self.assertRaisesRegex(TypeError, 'cannot be a tf.Tensor object'):
        sess.run(out_t, feed_dict={feed_t: feed_val})
      with self.assertRaisesRegex(TypeError, 'cannot be a tf.Tensor object'):
        out_t.eval(feed_dict={feed_t: feed_val})
      with self.assertRaisesRegex(TypeError, 'cannot be a tf.Tensor object'):
        out_t.op.run(feed_dict={feed_t: feed_val})
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out = c.eval(feed_dict={feed_t: c_list}),"      c_list = [b'\n\x01\x00', b'\n\x00\x01']
      feed_t = array_ops.placeholder(dtype=dtypes.string, shape=[2])
      c = array_ops.identity(feed_t)
      out = c.eval(feed_dict={feed_t: c_list})
      self.assertEqual(c_list[0], out[0])
      self.assertEqual(c_list[1], out[1])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out = c.eval(feed_dict={feed_t: c_list}),"      feed_t = array_ops.placeholder(dtype=dtypes.string, shape=[len(c_list)])
      c = array_ops.identity(feed_t)

      out = c.eval(feed_dict={feed_t: c_list})
      for i in range(len(c_list)):
        self.assertEqual(c_list[i], out[i].decode('utf-8'))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"out = c.eval(feed_dict={feed_t: np.array(c_list, dtype=np.object_)})","      for i in range(len(c_list)):
        self.assertEqual(c_list[i], out[i].decode('utf-8'))

      out = c.eval(feed_dict={feed_t: np.array(c_list, dtype=np.object_)})
      for i in range(len(c_list)):
        self.assertEqual(c_list[i], out[i].decode('utf-8'))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t = pickle.load(f),"      pickle.dump(t, f)

    with open(fname, 'rb') as f:
      t = pickle.load(f)
      self.assertAllEqual(t.numpy(), 10.0)

  @test_util.run_gpu_only",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
self._thread_name_filter_pattern = (re.compile(thread_name_filter),"
    # The session being wrapped.
    self._sess = sess
    self._thread_name_filter_pattern = (re.compile(thread_name_filter)
                                        if thread_name_filter else None)
    # TODO(cais/kstevens): Unittest this pass through feature.
    self._pass_through_operrors = pass_through_operrors",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
pattern_re = re.compile(pattern),"

def _at_least_one_line_matches(pattern, lines):
  pattern_re = re.compile(pattern)
  for i, line in enumerate(lines):
    if pattern_re.search(line):
      return True, i",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_ARRAY_VALUE_SEPARATOR_REGEX = re.compile(r""(array|\(|\[|\]|\)|\||,)"")","
# Regular expression for separators between values in a string representation
# of an ndarray, exclusing whitespace.
_ARRAY_VALUE_SEPARATOR_REGEX = re.compile(r""(array|\(|\[|\]|\)|\||,)"")


def assert_array_lines_close(test, expected_array, array_lines):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_NUMBER_REGEX = re.compile(r""[-+]?([0-9][-+0-9eE\.]+|nan|inf)(\s|,|\])"")","_NUMPY_OMISSION = ""...,""
_NUMPY_DEFAULT_EDGE_ITEMS = 3

_NUMBER_REGEX = re.compile(r""[-+]?([0-9][-+0-9eE\.]+|nan|inf)(\s|,|\])"")

BEGIN_INDICES_KEY = ""i0""
OMITTED_INDICES_KEY = ""omitted""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
op_type_regex = re.compile(parsed.op_type_filter),"
    filter_strs = []
    if parsed.op_type_filter:
      op_type_regex = re.compile(parsed.op_type_filter)
      filter_strs.append(""Op type regex filter: \""%s\"""" % parsed.op_type_filter)
    else:
      op_type_regex = None",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
node_name_regex = re.compile(parsed.node_name_filter),"      op_type_regex = None

    if parsed.node_name_filter:
      node_name_regex = re.compile(parsed.node_name_filter)
      filter_strs.append(""Node name regex filter: \""%s\"""" %
                         parsed.node_name_filter)
    else:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
node_name_regex = (re.compile(parsed.node_name_filter),"    exec_time_interval = (
        command_parser.parse_time_interval(parsed.execution_time)
        if parsed.execution_time else None)
    node_name_regex = (re.compile(parsed.node_name_filter)
                       if parsed.node_name_filter else None)
    file_path_regex = (re.compile(parsed.file_path_filter)
                       if parsed.file_path_filter else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
file_path_regex = (re.compile(parsed.file_path_filter),"        if parsed.execution_time else None)
    node_name_regex = (re.compile(parsed.node_name_filter)
                       if parsed.node_name_filter else None)
    file_path_regex = (re.compile(parsed.file_path_filter)
                       if parsed.file_path_filter else None)
    op_type_regex = (re.compile(parsed.op_type_filter)
                     if parsed.op_type_filter else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
op_type_regex = (re.compile(parsed.op_type_filter),"                       if parsed.node_name_filter else None)
    file_path_regex = (re.compile(parsed.file_path_filter)
                       if parsed.file_path_filter else None)
    op_type_regex = (re.compile(parsed.op_type_filter)
                     if parsed.op_type_filter else None)

    output = debugger_cli_common.RichTextLines([""""])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
device_name_regex = (re.compile(parsed.device_name_filter),"                     if parsed.op_type_filter else None)

    output = debugger_cli_common.RichTextLines([""""])
    device_name_regex = (re.compile(parsed.device_name_filter)
                         if parsed.device_name_filter else None)
    data_generator = self._get_profile_data_generator()
    device_count = len(self._run_metadata.step_stats.dev_stats)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
device_name_regex = (re.compile(parsed.device_name_filter),"
    parsed = self._arg_parsers[""print_source""].parse_args(args)

    device_name_regex = (re.compile(parsed.device_name_filter)
                         if parsed.device_name_filter else None)

    profile_data = []",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_ELEMENT_REGEX = re.compile(,"
  # Regular expression for text representation of float numbers, possibly in
  # engineering notation.
  _ELEMENT_REGEX = re.compile(
      r""([+-]?(\d+(\.\d*)?|\.\d+)([eE][+-]?\d+)?|nan|inf|-inf)"")

  def _checkBeginIndicesAnnotations(self, out, a):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return eval(rewritten_expression)  # pylint: disable=eval-used,"          ""self._cached_tensor_values['"" + tensor_name + ""']"" +
          rewritten_expression[match.end(0):])

    return eval(rewritten_expression)  # pylint: disable=eval-used
",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return eval(rewritten_expression)  # pylint: disable=eval-used,"          ""self._cached_tensor_values['"" + tensor_name + ""']"" +
          rewritten_expression[match.end(0):])

    return eval(rewritten_expression)  # pylint: disable=eval-used
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_DUMP_TENSOR_PATTERN = re.compile(r""`.*?`"")","
from tensorflow.python.debug.lib import debug_data

_DUMP_TENSOR_PATTERN = re.compile(r""`.*?`"")
_DEVICE_NAME_PREFIX_PATTERN = re.compile(
    r""/job:(\w)+/replica:(\d)+/task:(\d)+/(\w)+:(\d)+:"")
_EXEC_INDEX_SUFFIX_PATTERN = re.compile(r""\[(\d)*\]$"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_DEVICE_NAME_PREFIX_PATTERN = re.compile(,"from tensorflow.python.debug.lib import debug_data

_DUMP_TENSOR_PATTERN = re.compile(r""`.*?`"")
_DEVICE_NAME_PREFIX_PATTERN = re.compile(
    r""/job:(\w)+/replica:(\d)+/task:(\d)+/(\w)+:(\d)+:"")
_EXEC_INDEX_SUFFIX_PATTERN = re.compile(r""\[(\d)*\]$"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_EXEC_INDEX_SUFFIX_PATTERN = re.compile(r""\[(\d)*\]$"")","_DUMP_TENSOR_PATTERN = re.compile(r""`.*?`"")
_DEVICE_NAME_PREFIX_PATTERN = re.compile(
    r""/job:(\w)+/replica:(\d)+/task:(\d)+/(\w)+:(\d)+:"")
_EXEC_INDEX_SUFFIX_PATTERN = re.compile(r""\[(\d)*\]$"")

_DEFAULT_DEBUG_OP = ""DebugIdentity""
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re_prog = re.compile(regex),"      annotations=orig_screen_output.annotations)

  try:
    re_prog = re.compile(regex)
  except re.error:
    raise ValueError(""Invalid regular expression: \""%s\"""" % regex)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_BRACKETS_PATTERN = re.compile(r""\[[^\]]*\]"")","import sys


_BRACKETS_PATTERN = re.compile(r""\[[^\]]*\]"")
_QUOTES_PATTERN = re.compile(r""(\""[^\""]*\""|\'[^\']*\')"")
_WHITESPACE_PATTERN = re.compile(r""\s+"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_QUOTES_PATTERN = re.compile(r""(\""[^\""]*\""|\'[^\']*\')"")","

_BRACKETS_PATTERN = re.compile(r""\[[^\]]*\]"")
_QUOTES_PATTERN = re.compile(r""(\""[^\""]*\""|\'[^\']*\')"")
_WHITESPACE_PATTERN = re.compile(r""\s+"")

_NUMBER_PATTERN = re.compile(r""[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_WHITESPACE_PATTERN = re.compile(r""\s+"")","
_BRACKETS_PATTERN = re.compile(r""\[[^\]]*\]"")
_QUOTES_PATTERN = re.compile(r""(\""[^\""]*\""|\'[^\']*\')"")
_WHITESPACE_PATTERN = re.compile(r""\s+"")

_NUMBER_PATTERN = re.compile(r""[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_NUMBER_PATTERN = re.compile(r""[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?"")","_QUOTES_PATTERN = re.compile(r""(\""[^\""]*\""|\'[^\']*\')"")
_WHITESPACE_PATTERN = re.compile(r""\s+"")

_NUMBER_PATTERN = re.compile(r""[-+]?(\d+(\.\d*)?|\.\d+)([eE][-+]?\d+)?"")


class Interval(object):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tensor_name_pattern = re.compile(tensor_name_regex),"    Returns:
      The GradientsDebugger instance itself.
    """"""
    tensor_name_pattern = re.compile(tensor_name_regex)
    with graph.as_default():
      for op in graph.get_operations():
        for output in op.outputs:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(path_regex_allowlist) if path_regex_allowlist else None),"  tensor_name_to_num_dumps = {}

  path_regex = (
      re.compile(path_regex_allowlist) if path_regex_allowlist else None)
  node_name_regex = (
      re.compile(node_name_regex_allowlist)
      if node_name_regex_allowlist else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(node_name_regex_allowlist),"  path_regex = (
      re.compile(path_regex_allowlist) if path_regex_allowlist else None)
  node_name_regex = (
      re.compile(node_name_regex_allowlist)
      if node_name_regex_allowlist else None)

  to_skip_file_paths = set()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
node_name_regex = re.compile(node_name_filter) if node_name_filter else None,"
  source_file_path = _norm_abs_path(source_file_path)

  node_name_regex = re.compile(node_name_filter) if node_name_filter else None
  op_type_regex = re.compile(op_type_filter) if op_type_filter else None

  line_to_profile_summary = {}",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
op_type_regex = re.compile(op_type_filter) if op_type_filter else None,"  source_file_path = _norm_abs_path(source_file_path)

  node_name_regex = re.compile(node_name_filter) if node_name_filter else None
  op_type_regex = re.compile(op_type_filter) if op_type_filter else None

  line_to_profile_summary = {}
  for profile_datum in profile_data:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
exclude_node_names = re.compile(exclude_node_names),"       timestamp.
    """"""
    if exclude_node_names:
      exclude_node_names = re.compile(exclude_node_names)

    matched_data = []
    for device in (self._dump_tensor_data if device_name is None",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(node_name_regex_allowlist),"    debug_ops = [debug_ops]

  node_name_pattern = (
      re.compile(node_name_regex_allowlist)
      if node_name_regex_allowlist else None)
  op_type_pattern = (
      re.compile(op_type_regex_allowlist) if op_type_regex_allowlist else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(op_type_regex_allowlist) if op_type_regex_allowlist else None),"      re.compile(node_name_regex_allowlist)
      if node_name_regex_allowlist else None)
  op_type_pattern = (
      re.compile(op_type_regex_allowlist) if op_type_regex_allowlist else None)
  tensor_dtype_pattern = (
      re.compile(tensor_dtype_regex_allowlist)
      if tensor_dtype_regex_allowlist else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(tensor_dtype_regex_allowlist),"  op_type_pattern = (
      re.compile(op_type_regex_allowlist) if op_type_regex_allowlist else None)
  tensor_dtype_pattern = (
      re.compile(tensor_dtype_regex_allowlist)
      if tensor_dtype_regex_allowlist else None)

  ops = graph.get_operations()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(node_name_regex_denylist),"    debug_ops = [debug_ops]

  node_name_pattern = (
      re.compile(node_name_regex_denylist)
      if node_name_regex_denylist else None)
  op_type_pattern = (
      re.compile(op_type_regex_denylist) if op_type_regex_denylist else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(op_type_regex_denylist) if op_type_regex_denylist else None),"      re.compile(node_name_regex_denylist)
      if node_name_regex_denylist else None)
  op_type_pattern = (
      re.compile(op_type_regex_denylist) if op_type_regex_denylist else None)
  tensor_dtype_pattern = (
      re.compile(tensor_dtype_regex_denylist)
      if tensor_dtype_regex_denylist else None)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile(tensor_dtype_regex_denylist),"  op_type_pattern = (
      re.compile(op_type_regex_denylist) if op_type_regex_denylist else None)
  tensor_dtype_pattern = (
      re.compile(tensor_dtype_regex_denylist)
      if tensor_dtype_regex_denylist else None)

  ops = graph.get_operations()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def compile(computation, inputs=None):  # pylint: disable=redefined-builtin","    None, 'xla.experimental.compile is deprecated. Consider using '
    '`@tf.function(jit_compile=True)`.',
    warn_once=True)
def compile(computation, inputs=None):  # pylint: disable=redefined-builtin
  """"""Builds an operator that compiles and runs `computation` with XLA.

  NOTE: In eager mode, `computation` will have `@tf.function` semantics.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"xla.compile() computation with attribute ""_xla_compile_id=XYZ"", where XYZ is","  THIS IS ONLY FOR TENSORFLOW INTERNAL IMPLEMENTATION, DO NO USE DIRECTLY.

  The primary role of `XLACompileContext` is to mark operators inside a
  xla.compile() computation with attribute ""_xla_compile_id=XYZ"", where XYZ is
  a unique name.

  `ControlFlowContext` is used to perform the annotation since it integrates",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"`ResourceVariable` is constructed inside a xla.compile() block, the","
  `ControlFlowContext` is used to perform the annotation since it integrates
  with Tensorflow constructs like ResourceVariables. For example, if a
  `ResourceVariable` is constructed inside a xla.compile() block, the
  `ResourceVariable` implementation can use
  `with ops.control_dependencies(None)` to build the variable's definition
  outside the compiled computation.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Summaries are not yet supported in xla.compile(). So we provide this context,"def _disable_summary_context():
  """"""Enters a context where all summary ops are skipped.

  Summaries are not yet supported in xla.compile(). So we provide this context
  manager that can skip creating summary ops. This is a temporary workaround due
  to XLA not supporting summary ops.
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
name=_remove_graph_sequence_number(tensor_info.name)).eval(),"    return _generate_random_tensor_ops(
        shape=shape,
        dtype=dtype,
        name=_remove_graph_sequence_number(tensor_info.name)).eval()


def _generate_random_tensor_v2(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
cloned = pickle.loads(pickle.dumps(func)),"        reduce_retracing=relax_shapes,
    )

    cloned = pickle.loads(pickle.dumps(func))

    self.assertEqual(func._name, cloned._name)
    self.assertEqual(input_signature, cloned.input_signature)",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"error_pattern = re.compile(r'Graph execution error.*test_fn', re.DOTALL)","        return script_ops.eager_py_func(
            func=lambda: array_ops.constant([2.]), inp=(), Tout=dtypes.int32)

    error_pattern = re.compile(r'Graph execution error.*test_fn', re.DOTALL)
    with self.assertRaisesRegex(errors.InvalidArgumentError, error_pattern):
      test_fn()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile('polymorphic_function_test.*out of scope', re.DOTALL)):","
    with self.assertRaisesRegex(
        TypeError,
        re.compile('polymorphic_function_test.*out of scope', re.DOTALL)):
      failing_function()

  def testSymbolicTensorIllegalCaptureCallTimeError(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile('polymorphic_function_test.*out of scope', re.DOTALL)):","    f1(constant_op.constant(1))
    with self.assertRaisesRegex(
        TypeError,
        re.compile('polymorphic_function_test.*out of scope', re.DOTALL)):
      f2(constant_op.constant(2))

  def testSymbolicTensorIllegalCaptureTraceTimeError(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"(True, False),                          # compile","      (None, converter.Feature.ALL),          # autograph_options
      (None, 'foo.bar'),                      # implements
      (None, True, False),                    # relax_shapes
      (True, False),                          # compile
      (True, False),                          # override_function
  ))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(sq.eval().reshape(-1), [7, 10, 15, 22])","
        sq = defun_matmul(t, t)
        double = add(t, t)
        self.assertAllEqual(sq.eval().reshape(-1), [7, 10, 15, 22])
        self.assertAllEqual(double.eval().reshape(-1), [2, 4, 6, 8])
        # Make sure the pre registered function is used, and no other function
        # is added.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(double.eval().reshape(-1), [2, 4, 6, 8])","        sq = defun_matmul(t, t)
        double = add(t, t)
        self.assertAllEqual(sq.eval().reshape(-1), [7, 10, 15, 22])
        self.assertAllEqual(double.eval().reshape(-1), [2, 4, 6, 8])
        # Make sure the pre registered function is used, and no other function
        # is added.
        self.assertLen(graph._functions, 6)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(dx.eval(feed_dict={x: 2.0}), 24.0)","      dx = gradients_impl.gradients(l, [x])[0]

      with self.cached_session():
        self.assertEqual(dx.eval(feed_dict={x: 2.0}), 24.0)

  def testDefunDifferentiable(self):
    v = resource_variable_ops.ResourceVariable(1.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(a.eval().indices, self.evaluate(b).indices)","      return
    self.assertTrue(isinstance(b, sparse_tensor.SparseTensor))
    with self.cached_session():
      self.assertAllEqual(a.eval().indices, self.evaluate(b).indices)
      self.assertAllEqual(a.eval().values, self.evaluate(b).values)
      self.assertAllEqual(a.eval().dense_shape, self.evaluate(b).dense_shape)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(a.eval().values, self.evaluate(b).values)","    self.assertTrue(isinstance(b, sparse_tensor.SparseTensor))
    with self.cached_session():
      self.assertAllEqual(a.eval().indices, self.evaluate(b).indices)
      self.assertAllEqual(a.eval().values, self.evaluate(b).values)
      self.assertAllEqual(a.eval().dense_shape, self.evaluate(b).dense_shape)

  @combinations.generate(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(a.eval().dense_shape, self.evaluate(b).dense_shape)","    with self.cached_session():
      self.assertAllEqual(a.eval().indices, self.evaluate(b).indices)
      self.assertAllEqual(a.eval().values, self.evaluate(b).values)
      self.assertAllEqual(a.eval().dense_shape, self.evaluate(b).dense_shape)

  @combinations.generate(
      combinations.times(test_base.graph_only_combinations(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(,"  # TODO(yuefengz): optimizer with slot variables doesn't work because of
  # optimizer's bug.
  # TODO(yuefengz): we should not allow non-v2 optimizer.
  model.compile(
      loss=tf.keras.losses.sparse_categorical_crossentropy,
      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),
      metrics=[""accuracy""])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0.0, util.get_regularization_loss().eval())","  def testGetRegularizationLoss(self):
    # Empty regularization collection should evaluate to 0.0.
    with self.cached_session():
      self.assertEqual(0.0, util.get_regularization_loss().eval())

    # Loss should sum.
    ops.add_to_collection(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(5.0, util.get_regularization_loss().eval())","    ops.add_to_collection(
        ops.GraphKeys.REGULARIZATION_LOSSES, constant_op.constant(3.0))
    with self.cached_session():
      self.assertEqual(5.0, util.get_regularization_loss().eval())

    # Check scope capture mechanism.
    with ops.name_scope('scope1'):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(-1.0, util.get_regularization_loss('scope1').eval())","      ops.add_to_collection(
          ops.GraphKeys.REGULARIZATION_LOSSES, constant_op.constant(-1.0))
    with self.cached_session():
      self.assertEqual(-1.0, util.get_regularization_loss('scope1').eval())


if __name__ == '__main__':",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"""""""Tests that `ragged_const(pylist).eval().tolist() == pylist`.","                      inner_shape=None,
                      expected_shape=None,
                      expected_dtype=None):
    """"""Tests that `ragged_const(pylist).eval().tolist() == pylist`.

    Args:
      pylist: The `pylist` argument for `ragged_const()`.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"`Session.run()`, `Tensor.eval()`, or `Operation.run()`.","
  **Important**: This ragged tensor will produce an error if evaluated.
  Its value must be fed using the `feed_dict` optional argument to
  `Session.run()`, `Tensor.eval()`, or `Operation.run()`.


  Args:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_FIELD_NAME_RE = re.compile('^[a-zA-Z][a-zA-Z0-9_]*$'),"# Regular expression used to determine whether a string is a valid field name.
# Note: we plan to relax (or possibly eliminate) this in the future; you
# should not rely on the fact that some field names are currently disallowed.
_FIELD_NAME_RE = re.compile('^[a-zA-Z][a-zA-Z0-9_]*$')

# =============================================================================
# Helper functions",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = xla.compile(,"    def vectorized_compute(x):
      return pfor_control_flow_ops.vectorized_map(compute, x)

    result = xla.compile(
        vectorized_compute, inputs=[array_ops.ones((10, 5, 3))])
    self.run_and_assert_equal(result, array_ops.ones((10, 1, 3)))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = xla.compile(while_compute, inputs=[array_ops.ones((10, 5, 3))])","          lambda i, y: (i + 1, y + vectorized_compute(x, i)),
          (0, array_ops.zeros([5, 1])))[1]

    result = xla.compile(while_compute, inputs=[array_ops.ones((10, 5, 3))])
    expected = array_ops.ones([5, 1]) * 10
    self.run_and_assert_equal(expected, result)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"out_xla_compile_f = xla.compile(f, inputs=[])","    if force_xla:
      out_exp_compile_f = def_function.function(jit_compile=True)(f)()
      self.run_and_assert_equal(out, out_exp_compile_f)
      out_xla_compile_f = xla.compile(f, inputs=[])
      self.run_and_assert_equal(out, out_xla_compile_f)

  def test_stateless_while(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
mode = dist.mode().eval(),"
  # Will raise exception if ANY batch member has a < 1 or b < 1.
  dist = distributions.beta(a, b, allow_nan_stats=False)
  mode = dist.mode().eval()

  # Will return NaN for batch members with either a < 1 or b < 1.
  dist = distributions.beta(a, b, allow_nan_stats=True)  # Default behavior",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
mode = dist.mode().eval(),"
  # Will return NaN for batch members with either a < 1 or b < 1.
  dist = distributions.beta(a, b, allow_nan_stats=True)  # Default behavior
  mode = dist.mode().eval()
  ```

  In all cases, an exception is raised if *invalid* parameters are passed, e.g.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dist.mean().eval(),"  # Will raise an exception if any Op is run.
  negative_a = -1.0 * a  # beta distribution by definition has a > 0.
  dist = distributions.beta(negative_a, b, allow_nan_stats=True)
  dist.mean().eval()
  ```

  """"""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer=optimizer, loss=loss_obj)","      optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)
      loss_obj = tf.keras.losses.CategoricalCrossentropy(
          label_smoothing=0.0, reduction=tf.keras.losses.Reduction.NONE)
      model.compile(optimizer=optimizer, loss=loss_obj)

    @tf.function
    def train_step(iterator):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_TPU_DEVICE_REGEX = re.compile(,"    return False


_TPU_DEVICE_REGEX = re.compile(
    r'.*task:(?P<host_id>\d+)/.*device:TPU:(?P<core_id>\d+)$')
_TPU_CONN_RETRIES = 120
DeviceDetails = collections.namedtuple(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(loss=""mse"", optimizer=opt)","
  opt = tf.keras.optimizers.SGD()
  opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)
  model.compile(loss=""mse"", optimizer=opt)

  x_train = np.random.random((1024, 64))
  y_train = np.random.random((1024, 64))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(50, h.eval())","      h = self.evaluate(h)

      # Get the tensor from its handle.
      self.assertEqual(50, h.eval())

  @test_util.run_deprecated_v1
  def testHandleAndValue(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(50, h.eval())","      v = math_ops.multiply(a, c)
      h, v = self.evaluate([h, v])

      self.assertEqual(50, h.eval())
      self.assertEqual(500, v)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(100, h.eval())","        # This exercises garbage collection.
        h = sess.run(h_x, feed_dict={f: h.handle})

      self.assertEqual(100, h.eval())

  @test_util.run_deprecated_v1
  def testHandleWhileLoop(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(101, h.eval())","        if not rp:
          break

      self.assertEqual(101, h.eval())

  @test_util.run_deprecated_v1
  def testHandleMover(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(3.0, c_handle.eval())","          session_ops.get_session_handle(c),
          feed_dict={a_p: a_handle.handle,
                     b_p: b_handle.handle})
      self.assertEqual(3.0, c_handle.eval())

  @test_util.run_deprecated_v1
  def testHandleGC(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(3.0, c_handle.eval())","          session_ops.get_session_handle(c),
          feed_dict={a_p: a_handle.handle,
                     b_p: b_handle.handle})
      self.assertEqual(3.0, c_handle.eval())

  @test_util.run_deprecated_v1
  def testFeedOneHandleDirectly(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"op(ref, indices, updates).eval()","        indices = np.array([[-1], [0], [5]])
        with self.assertRaisesOpError(
            r""indices\[0\] = \[-1\] does not index into shape \[6\]""):
          op(ref, indices, updates).eval()

        indices = np.array([[2], [0], [6]])
        with self.assertRaisesOpError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"op(ref, indices, updates).eval()","        indices = np.array([[2], [0], [6]])
        with self.assertRaisesOpError(
            r""indices\[2\] = \[6\] does not index into shape \[6\]""):
          op(ref, indices, updates).eval()


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_0.eval(feed_dict={ph_0: 1})),"      ph_0 = array_ops.placeholder(dtypes.int32, shape=[])
      result_0 = ops.convert_to_tensor([[0, 0, 0], [0, ph_0, 0], [0, 0, 0]])
      self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],
                          result_0.eval(feed_dict={ph_0: 1}))
      self.assertAllEqual([[0, 0, 0], [0, 2, 0], [0, 0, 0]],
                          result_0.eval(feed_dict={ph_0: 2}))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_0.eval(feed_dict={ph_0: 2})),"      self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],
                          result_0.eval(feed_dict={ph_0: 1}))
      self.assertAllEqual([[0, 0, 0], [0, 2, 0], [0, 0, 0]],
                          result_0.eval(feed_dict={ph_0: 2}))

      # Test using placeholder with an undefined shape.
      ph_1 = array_ops.placeholder(dtypes.int32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_1.eval(feed_dict={ph_1: 1})),"      ph_1 = array_ops.placeholder(dtypes.int32)
      result_1 = ops.convert_to_tensor([[0, 0, 0], [0, ph_1, 0], [0, 0, 0]])
      self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],
                          result_1.eval(feed_dict={ph_1: 1}))
      self.assertAllEqual([[0, 0, 0], [0, 2, 0], [0, 0, 0]],
                          result_1.eval(feed_dict={ph_1: 2}))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_1.eval(feed_dict={ph_1: 2})),"      self.assertAllEqual([[0, 0, 0], [0, 1, 0], [0, 0, 0]],
                          result_1.eval(feed_dict={ph_1: 1}))
      self.assertAllEqual([[0, 0, 0], [0, 2, 0], [0, 0, 0]],
                          result_1.eval(feed_dict={ph_1: 2}))

  @test_util.run_deprecated_v1
  # Placeholders and shape inference are only applicable in Graph mode.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_1.eval(feed_dict={ph_1: [1]}),"    result_1 = ops.convert_to_tensor([[0, 0, 0], [0, ph_1, 0], [0, 0, 0]])
    with self.session():
      with self.assertRaises(errors_impl.InvalidArgumentError):
        result_1.eval(feed_dict={ph_1: [1]})


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(math_ops.add(1, 41).eval(), 42)","    load_library.load_op_library(library_filename)

    with self.cached_session():
      self.assertEqual(math_ops.add(1, 41).eval(), 42)


if __name__ == '__main__':",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(ackermann.ackermann().eval(), b'A(m, 0) == A(m-1, 1)')","    ackermann = load_library.load_op_library(library_filename)

    with self.cached_session():
      self.assertEqual(ackermann.ackermann().eval(), b'A(m, 0) == A(m-1, 1)')


if __name__ == '__main__':",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
y.eval(feed_dict={labels: 0}),"      y = nn_ops.sparse_softmax_cross_entropy_with_logits_v2(
          labels=labels, logits=[[7.]])
      with self.assertRaisesOpError(expected_error_message):
        y.eval(feed_dict={labels: 0})

  def testLabelsPlaceholderScalar(self):
    """"""This method is structured to be easily overridden by a child class.""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
maximum_tf).eval(),"        with self.cached_session(use_gpu=False):
          maximum_tf = sparse_ops.sparse_maximum(sp_a, sp_b)
          maximum_tf_densified = sparse_ops.sparse_tensor_to_dense(
              maximum_tf).eval()
          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)
          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(
              minimum_tf).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
minimum_tf).eval(),"              maximum_tf).eval()
          minimum_tf = sparse_ops.sparse_minimum(sp_a, sp_b)
          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(
              minimum_tf).eval()

          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()
          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval(),"          minimum_tf_densified = sparse_ops.sparse_tensor_to_dense(
              minimum_tf).eval()

          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()
          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()

        self.assertAllEqual(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval(),"              minimum_tf).eval()

          a_densified = sparse_ops.sparse_tensor_to_dense(sp_a).eval()
          b_densified = sparse_ops.sparse_tensor_to_dense(sp_b).eval()

        self.assertAllEqual(
            np.maximum(a_densified, b_densified), maximum_tf_densified)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"np_x = math_ops.cast(tf_x, dtypes.float32).eval()","          a_is_sparse=sp_a,
          b_is_sparse=sp_b)
      out = self.evaluate(tf_ans)
      np_x = math_ops.cast(tf_x, dtypes.float32).eval()
      np_y = math_ops.cast(tf_y, dtypes.float32).eval()

    if tr_a:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"np_y = math_ops.cast(tf_y, dtypes.float32).eval()","          b_is_sparse=sp_b)
      out = self.evaluate(tf_ans)
      np_x = math_ops.cast(tf_x, dtypes.float32).eval()
      np_y = math_ops.cast(tf_y, dtypes.float32).eval()

    if tr_a:
      np_x = np.transpose(np_x)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 0)","    with self.cached_session():
      q = data_flow_ops.SparseConditionalAccumulator(
          dtypes_lib.float32, name=""Q"")
      self.assertEqual(q.num_accumulated().eval(), 0)

  @test_util.run_deprecated_v1
  def testAccumulatorSetGlobalStep(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 1)","              indices=[0, 2],
              values=np.array([[0, 0, 1], [3, 0, 4]]).astype(np.float32)))
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)

  @test_util.run_deprecated_v1
  def testDtypes(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 2)","              dense_shape=constant_op.constant(
                  [3, 3], dtype=dtypes_lib.int32)))
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 2)

      val = self.evaluate(q.take_indexed_slices_grad(1))
      self.assertAllEqual(val.indices, [0, 2])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
nn_ops.softplus(constant_op.constant(42)).eval(),"      with self.assertRaisesRegex(
          TypeError,
          ""'features' has DataType int32 not in list of allowed values""):
        nn_ops.softplus(constant_op.constant(42)).eval()


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = lrn_t.eval(feed_dict=params),"          alpha=alpha,
          beta=beta)
      params = {p: np.random.rand(*shape).astype(""f"")}
      result = lrn_t.eval(feed_dict=params)
    expected = self._LRN(
        params[p],
        lrn_depth_radius=lrn_depth_radius,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
r = grad.eval(feed_dict=params),"      lrn_op = nn.local_response_normalization(p, 2, 1.0, 0.0, 1.0, name=""lrn"")
      grad = gradients_impl.gradients([lrn_op], [p])[0]
      params = {p: inp_array}
      r = grad.eval(feed_dict=params)
    expected = np.ones(shape).astype(""f"")
    self.assertAllClose(r, expected)
    self.assertShapeEqual(expected, grad)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"nn_ops.softmax([1., 2., 3., 4.], axis=dim).eval()","      # inference error.
      dim = array_ops.placeholder_with_default(100, shape=[])
      with self.assertRaises(errors_impl.InvalidArgumentError):
        nn_ops.softmax([1., 2., 3., 4.], axis=dim).eval()

  def testInvalidAxis(self):
    # Test case for GitHub issue 22793.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"nn_ops.softmax(ones, axis=2).eval()","    with self.cached_session():
      ones = array_ops.ones(shape=[2, 3])
      with self.assertRaises(errors_impl.InvalidArgumentError):
        nn_ops.softmax(ones, axis=2).eval()

  def testLargeDims(self):
    # Make sure that we properly handle large inputs. See",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_comb = math_ops.betainc(a_comb, b_comb, x_comb).eval()","      combinations = list(itertools.product([-1, 0, 0.5, 1.0, 1.5], repeat=3))
      a_comb, b_comb, x_comb = np.asarray(list(zip(*combinations)), dtype=np_dt)
      with self.cached_session():
        tf_comb = math_ops.betainc(a_comb, b_comb, x_comb).eval()
      scipy_comb = special.betainc(a_comb, b_comb, x_comb, dtype=np_dt)
      self.assertAllCloseAccordingToType(
          scipy_comb, tf_comb, rtol=rtol, atol=atol)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.betainc(0.1, b_s, x_s).eval(),","      with self.cached_session():
        self.assertAllCloseAccordingToType(
            special.betainc(0.1, b_s, x_s, dtype=np_dt),
            math_ops.betainc(0.1, b_s, x_s).eval(),
            rtol=rtol,
            atol=atol)
        self.assertAllCloseAccordingToType(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.betainc(a_s, 0.1, x_s).eval(),","            atol=atol)
        self.assertAllCloseAccordingToType(
            special.betainc(a_s, 0.1, x_s, dtype=np_dt),
            math_ops.betainc(a_s, 0.1, x_s).eval(),
            rtol=rtol,
            atol=atol)
        self.assertAllCloseAccordingToType(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.betainc(a_s, b_s, 0.1).eval(),","            atol=atol)
        self.assertAllCloseAccordingToType(
            special.betainc(a_s, b_s, 0.1, dtype=np_dt),
            math_ops.betainc(a_s, b_s, 0.1).eval(),
            rtol=rtol,
            atol=atol)
        self.assertAllCloseAccordingToType(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.betainc(0.1, b_s, 0.1).eval(),","            atol=atol)
        self.assertAllCloseAccordingToType(
            special.betainc(0.1, b_s, 0.1, dtype=np_dt),
            math_ops.betainc(0.1, b_s, 0.1).eval(),
            rtol=rtol,
            atol=atol)
        self.assertAllCloseAccordingToType(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.betainc(0.1, 0.1, 0.1).eval(),","            atol=atol)
        self.assertAllCloseAccordingToType(
            special.betainc(0.1, 0.1, 0.1, dtype=np_dt),
            math_ops.betainc(0.1, 0.1, 0.1).eval(),
            rtol=rtol,
            atol=atol)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.betainc(a_p, b_p, x_p).eval(","          a_p = array_ops.placeholder(dtype)
          b_p = array_ops.placeholder(dtype)
          x_p = array_ops.placeholder(dtype)
          math_ops.betainc(a_p, b_p, x_p).eval(
              feed_dict={a_p: 0.5,
                         b_p: [0.5],
                         x_p: [[0.5]]})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"      print(""Construct ids"", ids.get_shape())
      embedding = embedding_ops.embedding_lookup(p, ids)

      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(params, id_vals, num_shards, vocab_size)
    self.assertAllEqual(np_result, tf_result)
    self.assertShapeEqual(np_result, embedding)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"      # Test that the PartitionedVariable components equal the list in p
      p_var_val = self.evaluate(list(p_variable))
      # Actual test
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(params, id_vals, num_shards, vocab_size)
    self.assertAllEqual(params_values, p_var_val)
    self.assertAllEqual(np_result, tf_result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"      ids = constant_op.constant(list(id_vals), dtype=dtypes.int32)

      embedding = embedding_ops.embedding_lookup(p, ids)
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(params, id_vals, num_shards, vocab_size)
    self.assertAllEqual(np_result, tf_result)
    self.assertShapeEqual(np_result, embedding)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"      ids = constant_op.constant(list(id_vals), dtype=dtypes.int64)

      embedding = embedding_ops.embedding_lookup(p, ids)
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(params, id_vals, num_shards, vocab_size)
    self.assertAllEqual(np_result, tf_result)
    self.assertShapeEqual(np_result, embedding)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"
      embedding = embedding_ops.embedding_lookup(
          p, ids, partition_strategy=""div"")
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(
        params, id_vals, num_shards, vocab_size, partition_strategy=""div"")
    self.assertAllEqual(np_result, tf_result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"      self.evaluate(variables.global_variables_initializer())
      embedding = embedding_ops.embedding_lookup(
          p_variable, ids, partition_strategy=""div"")
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(
        params, id_vals, num_shards, vocab_size, partition_strategy=""div"")
    self.assertAllEqual(np_result, tf_result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"
      embedding = embedding_ops.embedding_lookup(
          p, ids, partition_strategy=""div"")
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(
        params, id_vals, num_shards, vocab_size, partition_strategy=""div"")
    self.assertAllEqual(np_result, tf_result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_result = embedding.eval(feed_dict=feed_dict),"
      embedding = embedding_ops.embedding_lookup(
          p, ids, partition_strategy=""div"")
      tf_result = embedding.eval(feed_dict=feed_dict)
    np_result, _, _ = _EmbeddingResult(
        params, id_vals, num_shards, vocab_size, partition_strategy=""div"")
    self.assertAllEqual(np_result, tf_result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_embedding_sum = embedding_sum.eval(feed_dict=feed_dict),"      )
      self.assertEqual(embedding_sum.dtype, dtype)

      tf_embedding_sum = embedding_sum.eval(feed_dict=feed_dict)

      np_embedding_sum, np_weight_sum, np_weight_sq_sum = _EmbeddingResult(
          params,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
nn_ops.softsign(constant_op.constant(7)).eval(),"      with self.assertRaisesRegex(
          TypeError,
          ""'features' has DataType int32 not in list of allowed values""):
        nn_ops.softsign(constant_op.constant(7)).eval()


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"losses.softmax_cross_entropy(labels, logits, weights=weights).eval()","      weights = constant_op.constant([[3, 4, 5], [2, 6, 0], [8, 0, 1]])

      with self.assertRaises(ValueError):
        losses.softmax_cross_entropy(labels, logits, weights=weights).eval()

  @test_util.run_deprecated_v1
  def testSoftmaxLabelSmoothing(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels, logits, weights=weights).eval()","
      with self.assertRaises(ValueError):
        losses.sparse_softmax_cross_entropy(
            labels, logits, weights=weights).eval()

  def testInconsistentWeightSizeRaisesException(self):
    """"""The weight tensor has incorrect number of elements.""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels, logits, weights=weights).eval()","
      with self.assertRaises(ValueError):
        losses.sparse_softmax_cross_entropy(
            labels, logits, weights=weights).eval()

  def testInconsistentLabelSizeRaisesException(self):
    """"""The label tensor has incorrect number of elements.""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels, logits, weights=weights).eval()","
      with self.assertRaises(ValueError):
        losses.sparse_softmax_cross_entropy(
            labels, logits, weights=weights).eval()

  @test_util.run_deprecated_v1
  def testInconsistentWeightShapeRaisesException(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels, logits, weights=weights).eval()","
      with self.assertRaises(ValueError):
        losses.sparse_softmax_cross_entropy(
            labels, logits, weights=weights).eval()

  @test_util.run_deprecated_v1
  def testInconsistentLabelShapeRaisesException(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels, logits, weights=weights).eval()","          ValueError,
          '`labels.shape.rank` must equal `logits.shape.rank - 1`'):
        losses.sparse_softmax_cross_entropy(
            labels, logits, weights=weights).eval()


class SigmoidCrossEntropyLossTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"0.0, loss.eval(feed_dict={tf_predictions: self._np_labels}), 3)","    loss = losses.log_loss(self._labels, tf_predictions)
    with self.cached_session():
      self.assertAlmostEqual(
          0.0, loss.eval(feed_dict={tf_predictions: self._np_labels}), 3)

  def testNonZeroLoss(self):
    loss = losses.log_loss(self._labels, self._predictions)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_ = losses.hinge_loss(labels, logits).eval()","      logits = constant_op.constant([[-1.0], [2.1]])
      labels = constant_op.constant([0.0, 1.0])
      with self.assertRaises(ValueError):
        _ = losses.hinge_loss(labels, logits).eval()

  @test_util.run_deprecated_v1
  def testAllOutsideMargin(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_ = losses.huber_loss(labels, predictions).eval()","      predictions = constant_op.constant([[-1.0], [2.1]])
      labels = constant_op.constant([0.0, 1.0])
      with self.assertRaises(ValueError):
        _ = losses.huber_loss(labels, predictions).eval()

  @test_util.run_deprecated_v1
  def testAllQuadratic(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
labels=constant_op.constant(0)).eval()),"      self.assertEqual(
          0.0,
          losses.mean_squared_error(predictions=constant_op.constant(0),
                                    labels=constant_op.constant(0)).eval())

  @test_util.run_deprecated_v1
  def testAllCorrectNoLossWeight(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_loss, dynamic_inputs_op.eval(feed_dict=feed_dict), places=3)","          weights_placeholder: weights,
      }
      self.assertAlmostEqual(
          expected_loss, dynamic_inputs_op.eval(feed_dict=feed_dict), places=3)

  def testAllCorrectNoLossWeight(self):
    self._test_valid_weights(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dynamic_inputs_op.eval(feed_dict={,"        weights=weights_placeholder)
    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.OpError, expected_error_msg):
        dynamic_inputs_op.eval(feed_dict={
            predictions_placeholder: predictions,
            labels_placeholder: labels,
            weights_placeholder: weights,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self._raw_losses, unweighted_loss.eval(feed_dict))","          for unweighted_loss in unweighted_losses:
            if reduction == losses.Reduction.NONE:
              self.assertAllClose(
                  self._raw_losses, unweighted_loss.eval(feed_dict))
            elif reduction == losses.Reduction.SUM:
              self.assertAllClose(
                  np.sum(self._raw_losses), unweighted_loss.eval(feed_dict))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"np.sum(self._raw_losses), unweighted_loss.eval(feed_dict))","                  self._raw_losses, unweighted_loss.eval(feed_dict))
            elif reduction == losses.Reduction.SUM:
              self.assertAllClose(
                  np.sum(self._raw_losses), unweighted_loss.eval(feed_dict))
            else:
              # reduction one of MEAN, SUM_OVER_NONZERO_WEIGHTS,
              # SUM_BY_NONZERO_WEIGHTS or SUM_OVER_BATCH_SIZE.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"np.mean(self._raw_losses), unweighted_loss.eval(feed_dict))","              # reduction one of MEAN, SUM_OVER_NONZERO_WEIGHTS,
              # SUM_BY_NONZERO_WEIGHTS or SUM_OVER_BATCH_SIZE.
              self.assertAllClose(
                  np.mean(self._raw_losses), unweighted_loss.eval(feed_dict))

  def testScalarWeight(self):
    with ops.Graph().as_default():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
weighted_loss.eval(feed_dict={weights_placeholder: weights}),"      self.assertEqual(1, len(util.get_losses()))
      with self.cached_session():
        with self.assertRaisesRegex(errors_impl.OpError, expected_error_msg):
          weighted_loss.eval(feed_dict={weights_placeholder: weights})

  def testInvalidWeightTooManyDims(self):
    self._test_invalid_weights(np.zeros(shape=(2, 2, 2, 2)))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
weighted_loss.eval(feed_dict={weights_placeholder: weights}),"      self.assertEqual(1, len(util.get_losses()))
      with self.cached_session():
        with self.assertRaisesRegex(errors_impl.OpError, expected_error_msg):
          weighted_loss.eval(feed_dict={weights_placeholder: weights})

  def testInvalid3Weight(self):
    self._test_invalid_weights((17.0, 5.0, 2.0))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"nn_ops.nth_element(v, 0).eval(feed_dict={v: 5})","      with self.session(use_gpu=False):
        v = array_ops.placeholder(dtype=dtypes.int32)
        with self.assertRaisesOpError(""at least rank 1 but is rank 0""):
          nn_ops.nth_element(v, 0).eval(feed_dict={v: 5})

  def testInvalidN(self):
    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
values.eval(feed_dict={n: -1}),"        n = array_ops.placeholder(dtypes.int32)
        values = nn_ops.nth_element([5], n)
        with self.assertRaisesOpError(""non-negative but is -1""):
          values.eval(feed_dict={n: -1})

  def testNTooLarge(self):
    inputs = [[0.1, 0.2], [0.3, 0.4]]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
values.eval(feed_dict={n: 2}),"        n = array_ops.placeholder(dtypes.int32)
        values = nn_ops.nth_element(inputs, n)
        with self.assertRaisesOpError(""must have last dimension > n = 2""):
          values.eval(feed_dict={n: 2})

  def testGradients(self):
    x = [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_a = bias_gradients.eval(feed_dict=feed_dict),"            colocate_gradients_with_ops=True)[0]
        for i in range(repeat_count):
          feed_dict = {upstream_gradients: self._randomNDArray(output_shape)}
          result_a = bias_gradients.eval(feed_dict=feed_dict)
          result_b = bias_gradients.eval(feed_dict=feed_dict)
          self.assertAllEqual(result_a, result_b)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_b = bias_gradients.eval(feed_dict=feed_dict),"        for i in range(repeat_count):
          feed_dict = {upstream_gradients: self._randomNDArray(output_shape)}
          result_a = bias_gradients.eval(feed_dict=feed_dict)
          result_b = bias_gradients.eval(feed_dict=feed_dict)
          self.assertAllEqual(result_a, result_b)

  # TODO(duncanriach): Re-enable the following three tests for the error checks",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
actual = t.eval(feed_dict={,"      elif data_format == ""NCHW"":
        t = test_util.NCHWToNHWC(t)
      if v2:
        actual = t.eval(feed_dict={
            ksize_placeholder: ksize,
            strides_placeholder: strides
        })",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"t, ksize=ksize, strides=strides, padding=""SAME"").eval()","      t = constant_op.constant(1.0, shape=in_size)
      with self.assertRaisesRegex(errors_impl.UnimplementedError, error_msg):
        t = nn_ops.max_pool(
            t, ksize=ksize, strides=strides, padding=""SAME"").eval()

  @test_util.disable_xla(""b/123338077"")  # Passes with XLA
  def testDepthwiseMaxPoolInvalidConfigs(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"padding=""SAME"").eval()","        with self.assertRaisesOpError(""for CPU devices""):
          nn_ops.max_pool(
              t, ksize=[1, 1, 1, 2], strides=[1, 1, 1, 2],
              padding=""SAME"").eval()

  # The following are tests that verify that the CPU and GPU implementations
  # produce the same results.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"v.eval(feed_dict={x: np.ones([1, 1, 1, 1], np.float32)}))","          x, f, [1, 1, 1, 1], ""VALID"", rate=[2, 1], data_format=""NCHW"")
      self.assertAllEqual(
          np.ones([1, 1, 1, 1], np.float32),
          v.eval(feed_dict={x: np.ones([1, 1, 1, 1], np.float32)}))

  @test_util.run_v1_only(""b/120545219"")
  def testDepthwiseConv2DFormat(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2.dequeue().eval(), [10.0])","      self.assertEqual(self.evaluate(q1_size_t), [1])
      self.assertEqual(self.evaluate(q2_size_t), [1])

      self.assertEqual(q2.dequeue().eval(), [10.0])

      self.assertEqual(self.evaluate(q1_size_t), [0])
      self.assertEqual(self.evaluate(q2_size_t), [0])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1.dequeue().eval(), [20.0])","      self.assertEqual(self.evaluate(q1_size_t), [1])
      self.assertEqual(self.evaluate(q2_size_t), [1])

      self.assertEqual(q1.dequeue().eval(), [20.0])

      self.assertEqual(self.evaluate(q1_size_t), [0])
      self.assertEqual(self.evaluate(q2_size_t), [0])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.dequeue().eval(), 10.0)","        index = np.random.randint(num_queues)
        q = data_flow_ops.FIFOQueue.from_list(index, qlist)
        q.enqueue((10.,)).run()
        self.assertEqual(q.dequeue().eval(), 10.0)

  def testSelectQueueOutOfRange(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
enq_q.dequeue().eval(),"      q2 = data_flow_ops.FIFOQueue(15, dtypes_lib.float32)
      enq_q = data_flow_ops.FIFOQueue.from_list(3, [q1, q2])
      with self.assertRaisesOpError(""is not in""):
        enq_q.dequeue().eval()

  def testDtypes(self):
    with self.cached_session() as sess:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","        val_a, val_b = self.evaluate([cleanup_dequeue_a_t, cleanup_dequeue_b_t])
        self.assertEqual(elem_a, val_a)
        self.assertEqual(elem_b, val_b)
      self.assertEqual(0, q.size().eval())

  def testBlockingDequeueManyFromClosedEmptyQueue(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","
      for elem in [20.0, 30.0, 40.0, 50.0]:
        self.assertEqual(elem, self.evaluate(dequeued_t))
      self.assertEqual(0, q.size().eval())

  def testBlockingEnqueueManyBeforeClose(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
cleanup_elems = q.dequeue_many(49).eval(),"        dequeue_thread.join()

      # Dequeue the initial count of elements to clean up.
      cleanup_elems = q.dequeue_many(49).eval()
      for elem in cleanup_elems:
        self.assertTrue(elem in (10.0, 20.0))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","
      close_op.run()
      dequeue_thread.join()
      self.assertEqual(0, q.size().eval())

  def testMixtureOfDequeueAndDequeueMany(self):
    with self.cached_session() as sess:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dequeuemany_t.eval({,"          expected_range = np.arange(
              elements_dequeued, elements_dequeued + count, dtype=np.int32)
          self.assertAllEqual(expected_range,
                              dequeuemany_t.eval({
                                  count_placeholder: count
                              }))
          elements_dequeued += count",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","
      q.close().run()
      enqueue_thread.join()
      self.assertEqual(0, q.size().eval())

  def testBlockingDequeueMany(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(1, q.size().eval())","      enqueue_correct_op.run()
      with self.assertRaises(ValueError):
        q.enqueue(([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]],))
      self.assertEqual(1, q.size().eval())

  def testEnqueueManyWithShape(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(4, q.size().eval())","      q = data_flow_ops.PaddingFIFOQueue(
          10, [dtypes_lib.int32, dtypes_lib.int32], shapes=[(), (2,)])
      q.enqueue_many([[1, 2, 3, 4], [[1, 1], [2, 2], [3, 3], [4, 4]]]).run()
      self.assertEqual(4, q.size().eval())

  def testParallelEnqueue(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([0], q.size().eval())","  def testQueueSizeEmpty(self):
    with self.cached_session():
      q = data_flow_ops.PaddingFIFOQueue(10, dtypes_lib.float32, ((),))
      self.assertEqual([0], q.size().eval())

  def testQueueSizeAfterEnqueueAndDequeue(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
cleanup_elems = q.dequeue_many(49).eval(),"        dequeue_thread.join()

      # Dequeue the initial count of elements to clean up.
      cleanup_elems = q.dequeue_many(49).eval()
      for elem in cleanup_elems:
        self.assertTrue(elem in (10.0, 20.0))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","
      close_op.run()
      dequeue_thread.join()
      self.assertEqual(0, q.size().eval())

  def testMixtureOfDequeueAndDequeueMany(self):
    with self.cached_session() as sess:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dequeuemany_t.eval({,"          expected_range = np.arange(
              elements_dequeued, elements_dequeued + count, dtype=np.int32)
          self.assertAllEqual(expected_range,
                              dequeuemany_t.eval({
                                  count_placeholder: count
                              }))
          elements_dequeued += count",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","
      q.close().run()
      enqueue_thread.join()
      self.assertEqual(0, q.size().eval())

  def testBlockingDequeueMany(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","        val_a, val_b = self.evaluate([cleanup_dequeue_a_t, cleanup_dequeue_b_t])
        self.assertEqual(elem_a, val_a)
        self.assertEqual(elem_b, val_b)
      self.assertEqual(0, q.size().eval())

  def testBlockingDequeueManyFromClosedEmptyQueue(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","
      for elem in [20.0, 30.0, 40.0, 50.0]:
        self.assertEqual(elem, self.evaluate(dequeued_t))
      self.assertEqual(0, q.size().eval())

  def testBlockingEnqueueManyBeforeClose(self):
    # We need each thread to keep its own device stack or the device scopes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2.dequeue().eval(), [10.0])","      self.assertEqual(self.evaluate(q1_size_t), [1])
      self.assertEqual(self.evaluate(q2_size_t), [1])

      self.assertEqual(q2.dequeue().eval(), [10.0])

      self.assertEqual(self.evaluate(q1_size_t), [0])
      self.assertEqual(self.evaluate(q2_size_t), [0])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1.dequeue().eval(), [20.0])","      self.assertEqual(self.evaluate(q1_size_t), [1])
      self.assertEqual(self.evaluate(q2_size_t), [1])

      self.assertEqual(q1.dequeue().eval(), [20.0])

      self.assertEqual(self.evaluate(q1_size_t), [0])
      self.assertEqual(self.evaluate(q2_size_t), [0])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.dequeue().eval(), 10.0)","        index = np.random.randint(num_queues)
        q = data_flow_ops.PaddingFIFOQueue.from_list(index, qlist)
        q.enqueue((10.,)).run()
        self.assertEqual(q.dequeue().eval(), 10.0)

  def testSelectQueueOutOfRange(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
enq_q.dequeue().eval(),"      q2 = data_flow_ops.PaddingFIFOQueue(15, dtypes_lib.float32, ((),))
      enq_q = data_flow_ops.PaddingFIFOQueue.from_list(3, [q1, q2])
      with self.assertRaisesOpError(""is not in""):
        enq_q.dequeue().eval()

  def _blockingDequeue(self, sess, dequeue_op):
    with self.assertRaisesOpError(""was cancelled""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
deq_values = np.hstack([q.dequeue_many(100)[0].eval() for _ in range(10)]),"      q = data_flow_ops.PriorityQueue(2000, (dtypes.int64), (()))
      elem = np.random.randint(-100, 100, size=1000).astype(np.int64)
      q.enqueue_many((elem, elem)).run()
      deq_values = np.hstack([q.dequeue_many(100)[0].eval() for _ in range(10)])
      self.assertAllEqual(deq_values, sorted(elem))

  def testRoundTripInsertOnceReadOnceLotsSorts(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
deq_values = np.hstack([dequeue_op[0].eval() for _ in range(1000)]),"      elem = np.random.randint(-100, 100, size=1000).astype(np.int64)
      q.enqueue_many((elem, elem)).run()
      dequeue_op = q.dequeue()
      deq_values = np.hstack([dequeue_op[0].eval() for _ in range(1000)])
      self.assertAllEqual(deq_values, sorted(elem))

  def testInsertingNonInt64Fails(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = output.eval({placeholder_keys: [11, 12, 15]})","      placeholder_keys = array_ops.placeholder(dtypes.int64)
      output = table.lookup(placeholder_keys)
      self.assertAllEqual(None, output.get_shape())
      result = output.eval({placeholder_keys: [11, 12, 15]})
      self.assertAllEqual([0, 1, -1], result)

  def testMapStringToFloat(self, is_anonymous):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      self.assertAllEqual(0, table.size())
      table.insert(keys, values).run()
      self.assertAllEqual(4, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      keys2 = constant_op.constant([12, 15], dtypes.int64)
      table.remove(keys2).run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      keys2 = constant_op.constant([12, 15], dtypes.int64)
      table.remove(keys2).run()
      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      val = save.save(sess, save_path)
      self.assertIsInstance(val, str)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(64, len(table.export()[0].eval()))","          constant_op.constant([11, 14], dtypes.int64),
          constant_op.constant([12, 24], dtypes.int64)).run()
      self.assertAllEqual(2, table.size())
      self.assertAllEqual(64, len(table.export()[0].eval()))

      save = saver.Saver()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      save.restore(sess, save_path)

      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)
      output = table.lookup(input_string)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      self.assertAllEqual(0, table.size())
      table.insert(keys, values).run()
      self.assertAllEqual(4, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      keys2 = constant_op.constant([12, 15], dtypes.int64)
      table.remove(keys2).run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      keys2 = constant_op.constant([12, 15], dtypes.int64)
      table.remove(keys2).run()
      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      val = save.save(sess, save_path)
      self.assertIsInstance(val, str)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(64, len(table.export()[0].eval()))","          constant_op.constant([11, 14], dtypes.int64),
          constant_op.constant([12, 24], dtypes.int64)).run()
      self.assertAllEqual(2, table.size())
      self.assertAllEqual(64, len(table.export()[0].eval()))

      save = saver.Saver([table])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      save.restore(sess, save_path)

      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      input_string = constant_op.constant([10, 11, 12, 13, 14], dtypes.int64)
      output = table.lookup(input_string)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      self.assertAllEqual(0, table.size())
      table.insert(keys, values).run()
      self.assertAllEqual(4, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      keys2 = constant_op.constant([[12, 13], [16, 17]], dtypes.int64)
      table.remove(keys2).run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      keys2 = constant_op.constant([[12, 13], [16, 17]], dtypes.int64)
      table.remove(keys2).run()
      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      val = save.save(sess, save_path)
      self.assertIsInstance(val, str)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(64, len(table.export()[0].eval()))","          constant_op.constant([[11, 12], [13, 15]], dtypes.int64),
          constant_op.constant([[21, 22], [23, 24]], dtypes.int64)).run()
      self.assertAllEqual(2, table.size())
      self.assertAllEqual(64, len(table.export()[0].eval()))

      save = saver.Saver()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      save.restore(sess, save_path)

      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      input_string = constant_op.constant(
          [[11, 12], [11, 14], [11, 15], [13, 14], [13, 15]], dtypes.int64)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      self.assertAllEqual(0, table.size())
      table.insert(keys, values).run()
      self.assertAllEqual(4, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      keys2 = constant_op.constant([[12, 13], [15, 16]], dtypes.int64)
      table.remove(keys2).run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      keys2 = constant_op.constant([[12, 13], [15, 16]], dtypes.int64)
      table.remove(keys2).run()
      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      val = save.save(sess, save_path)
      self.assertIsInstance(val, str)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(64, len(table.export()[0].eval()))","          constant_op.constant([[11, 12], [13, 15]], dtypes.int64),
          constant_op.constant([3, 4], dtypes.int64)).run()
      self.assertAllEqual(2, table.size())
      self.assertAllEqual(64, len(table.export()[0].eval()))

      save = saver.Saver()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(32, len(table.export()[0].eval()))","      save.restore(sess, save_path)

      self.assertAllEqual(3, table.size())
      self.assertAllEqual(32, len(table.export()[0].eval()))

      input_string = constant_op.constant(
          [[11, 12], [11, 14], [11, 15], [13, 14], [13, 15]], dtypes.int64)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 0)","  def testAccumulatorSizeEmpty(self):
    with self.cached_session():
      q = data_flow_ops.ConditionalAccumulator(dtypes_lib.float32, name=""Q"")
      self.assertEqual(q.num_accumulated().eval(), 0)

  @test_util.run_deprecated_v1
  def testAccumulatorSetGlobalStep(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = accums[i].take_grad(1).eval(),"        accums[i].apply_grad((i + 10.0,)).run()

      for i in range(len(accums)):
        result = accums[i].take_grad(1).eval()
        self.assertEqual(result, i + 10.0)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 0)","      q = data_flow_ops.ConditionalAccumulator(
          dtypes_lib.float32, name=""Q"", shape=tensor_shape.TensorShape([1]))
      accum_op = q.apply_grad((10.0,))
      self.assertEqual(q.num_accumulated().eval(), 0)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)
      accum_op.run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 1)","      accum_op = q.apply_grad((10.0,))
      self.assertEqual(q.num_accumulated().eval(), 0)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 2)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 2)","      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 2)

  @test_util.run_deprecated_v1
  def testAccumulatorSizeAfterApplyGradAndTakeGrad(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 0)","      extract_t = q.take_grad(2)

      # Applying gradient multiple times to increase size from 0 to 2.
      self.assertEqual(q.num_accumulated().eval(), 0)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)
      accum_op.run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 1)","      # Applying gradient multiple times to increase size from 0 to 2.
      self.assertEqual(q.num_accumulated().eval(), 0)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 2)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 2)","      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 1)
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 2)

      # Extract will reduce size to 0
      extract_t.op.run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 0)","
      # Extract will reduce size to 0
      extract_t.op.run()
      self.assertEqual(q.num_accumulated().eval(), 0)

      # Take gradients always sets the size back to 0 if successful.
      accum_op = q.apply_grad((10.0,), local_step=1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 4)","      accum_op.run()
      accum_op.run()
      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 4)
      extract_t.op.run()
      self.assertEqual(q.num_accumulated().eval(), 0)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.num_accumulated().eval(), 0)","      accum_op.run()
      self.assertEqual(q.num_accumulated().eval(), 4)
      extract_t.op.run()
      self.assertEqual(q.num_accumulated().eval(), 0)

  @test_util.run_deprecated_v1
  def testAccumulatorTakeGradMean(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"ta.split([1.0, 2.0, 3.0], lengths).flow.eval(feed_dict={lengths: 1})","          self.evaluate(ta.split([1.0, 2.0, 3.0], 1))
        else:
          lengths = array_ops.placeholder(dtypes.int64)
          ta.split([1.0, 2.0, 3.0], lengths).flow.eval(feed_dict={lengths: 1})

      error_msg = (""Unused values in tensor. Length of tensor: 3 Values used: 1""
                   if control_flow_util.ENABLE_CONTROL_FLOW_V2 and",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
ta.stack().eval(),"          ""packing zero-size TensorArrays."")
      with self.assertRaisesOpError(
          v2_msg if control_flow_util.ENABLE_CONTROL_FLOW_V2 else v1_msg):
        ta.stack().eval()

  @test_util.run_deprecated_v1
  def testSkipEagerTensorArrayEvalEmpty(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, ta.size().eval())","    with self.cached_session():
      ta = tensor_array_ops.TensorArray(
          dtype=dtypes.float32, size=0, dynamic_size=False, infer_shape=True)
      self.assertEqual(0, ta.size().eval())
      # Don't actually perform the pack.  This stores the static shape.
      if control_flow_util.ENABLE_CONTROL_FLOW_V2:
        ta = ta.unstack(array_ops.zeros([0, 3, 5]))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
l.eval({tensor: 1}),"      with self.assertRaisesRegex(
          errors.InvalidArgumentError,
          r""Tensor must be at least a vector, but saw shape: \[\]""):
        l.eval({tensor: 1})

  @test_util.run_deprecated_v1
  def testSkipEagerSplitWithInvalidLengthsShapeFails(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
l.eval({lengths: 1}),"      with self.assertRaisesRegex(
          errors.InvalidArgumentError,
          r""Expected lengths to be a vector, received shape: \[\]""):
        l.eval({lengths: 1})

  def testSplitWithInvalidLengthsFails(self):
    with self.assertRaisesRegex(errors.InvalidArgumentError,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
l.eval({element_shape: []}),"        l = list_ops.tensor_list_split([1., 2.],
                                       element_shape=element_shape,
                                       lengths=[1, 1])
        l.eval({element_shape: []})

  def testEagerOnlySplitWithScalarElementShapeFails(self):
    if context.executing_eagerly():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
l.eval({element_shape: [1]}),"        l = list_ops.tensor_list_split([[1.], [2.]],
                                       element_shape=element_shape,
                                       lengths=[1, 1])
        l.eval({element_shape: [1]})

  def testEagerOnlySplitWithIncompatibleTensorShapeAndElementShapeFails(self):
    if context.executing_eagerly():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
linalg_ops.matrix_inverse(tensor3).eval(),"        # All rows of the matrix below add to zero.
        tensor3 = constant_op.constant([[1., 0., -1.], [-1., 1., 0.],
                                        [0., -1., 1.]])
        linalg_ops.matrix_inverse(tensor3).eval()

  def testEmpty(self):
    self._verifyInverseReal(np.empty([0, 2, 2]))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"(1, 2, 3, 5), operator.shape_tensor().eval(feed_dict=feed_dict))","    operator = linalg.LinearOperatorComposition(operators)
    with self.cached_session():
      self.assertAllEqual(
          (1, 2, 3, 5), operator.shape_tensor().eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def test_is_square_set_for_aat_form(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
shape_tensor = operator.shape_tensor().eval(feed_dict=feed_dict),"    }

    with self.cached_session():
      shape_tensor = operator.shape_tensor().eval(feed_dict=feed_dict)
      self.assertAllEqual([2, 2, 2, 3, 3], shape_tensor)
      dense = operator.to_dense().eval(feed_dict=feed_dict)
      self.assertAllEqual([2, 2, 2, 3, 3], dense.shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dense = operator.to_dense().eval(feed_dict=feed_dict),"    with self.cached_session():
      shape_tensor = operator.shape_tensor().eval(feed_dict=feed_dict)
      self.assertAllEqual([2, 2, 2, 3, 3], shape_tensor)
      dense = operator.to_dense().eval(feed_dict=feed_dict)
      self.assertAllEqual([2, 2, 2, 3, 3], dense.shape)

  def test_u_and_v_incompatible_batch_shape_raises(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
field_names=['double_value']).eval(feed_dict={,"                sizes=sizes,
                values=[values],
                message_type='tensorflow.contrib.proto.TestValue',
                field_names=['double_value']).eval(feed_dict={
                    sizes: sizes_value,
                    values: [[0.0]]
                })",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"field_names=['double_value', 'int32_value']).eval(feed_dict={","              sizes=[[1, 1]],
              values=[values1, values2],
              message_type='tensorflow.contrib.proto.TestValue',
              field_names=['double_value', 'int32_value']).eval(feed_dict={
                  values1: [[0.0]],
                  values2: [[0], [0]]
              }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(expected, dynamic_op.eval(feed_dict={","        weights=weights_placeholder, values=values_placeholder)
    with self.cached_session():
      self.assertAllEqual(expected, self.evaluate(static_op))
      self.assertAllEqual(expected, dynamic_op.eval(feed_dict={
          weights_placeholder: weights,
          values_placeholder: values,
      }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dynamic_op.eval(feed_dict={,"        weights=weights_placeholder, values=values_placeholder)
    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.OpError, error_msg):
        dynamic_op.eval(feed_dict={
            weights_placeholder: weights,
            values_placeholder: values,
        })",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out_tf.eval(),"    block_size = 1
    with self.assertRaises(ValueError):
      out_tf = self.batch_to_space(x_np, crops, block_size)
      out_tf.eval()

  @test_util.run_deprecated_v1
  def testBlockSizeLarger(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_ = t.eval({,"                                    paddings_placeholder)

    with self.assertRaises(ValueError):
      _ = t.eval({
          input_placeholder: np.zeros(input_shape, np.float32),
          block_shape_placeholder: block_shape,
          paddings_placeholder: paddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.tile(a, [[2, 3], [3, 4]]).eval()","        array_ops.tile(a, [1, 4, 2])
      # Wrong rank for multiples.
      with self.assertRaises(ValueError):
        array_ops.tile(a, [[2, 3], [3, 4]]).eval()

  def _RunAndVerifyResult(self, rank, use_gpu):
    with self.cached_session(use_gpu=use_gpu):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_ans = ops.convert_to_tensor(x).eval(),"  def _testCpu(self, x):
    np_ans = np.array(x)
    with self.cached_session(use_gpu=False):
      tf_ans = ops.convert_to_tensor(x).eval()
    dtype = dtypes_lib.as_dtype(np_ans.dtype)
    if dtype.is_floating or dtype.is_complex:
      self.assertAllClose(np_ans, tf_ans)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tf_ans = ops.convert_to_tensor(x).eval(),"  def _testGpu(self, x):
    np_ans = np.array(x)
    with self.cached_session():
      tf_ans = ops.convert_to_tensor(x).eval()
    dtype = dtypes_lib.as_dtype(np_ans.dtype)
    if dtype.is_floating or dtype.is_complex:
      self.assertAllClose(np_ans, tf_ans)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"val = ops.convert_to_tensor(b""\0\0\0\0"").eval()","  @test_util.run_deprecated_v1
  def testStringWithNulls(self):
    with self.cached_session():
      val = ops.convert_to_tensor(b""\0\0\0\0"").eval()
    self.assertEqual(len(val), 4)
    self.assertEqual(val, b""\0\0\0\0"")
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"val = ops.convert_to_tensor(b""xx\0xx"").eval()","    self.assertEqual(val, b""\0\0\0\0"")

    with self.cached_session():
      val = ops.convert_to_tensor(b""xx\0xx"").eval()
    self.assertEqual(len(val), 5)
    self.assertAllEqual(val, b""xx\0xx"")
    nested = [[b""\0\0\0\0"", b""xx\0xx""], [b""\0_\0_\0_\0"", b""\0""]]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
val = ops.convert_to_tensor(nested).eval(),"    nested = [[b""\0\0\0\0"", b""xx\0xx""], [b""\0_\0_\0_\0"", b""\0""]]

    with self.cached_session():
      val = ops.convert_to_tensor(nested).eval()
    # NOTE(mrry): Do not use assertAllEqual, because it converts nested to a
    #   numpy array, which loses the null terminators.
    self.assertEqual(val.tolist(), nested)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
z_value = z_var.eval(feed_dict=feed_dict),"      feed_dict = {}
      if not fully_defined_shape:
        feed_dict[d] = np.ones((2, 3), dtype=numpy_dtype)
      z_value = z_var.eval(feed_dict=feed_dict)
      self.assertFalse(np.any(z_value))
      self.assertEqual((2, 3), z_value.shape)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"y = array_ops.zeros_like(x, dtype=out_type).eval()","      for in_type in dtypes:
        x = np.arange(15).astype(in_type).reshape(*shape)
        for out_type in dtypes:
          y = array_ops.zeros_like(x, dtype=out_type).eval()
          self.assertEqual(y.dtype, out_type)
          self.assertEqual(y.shape, shape)
          self.assertAllEqual(y, np.zeros(shape, dtype=out_type))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"out = z.eval(feed_dict={h: 4, w: 16})","      h = array_ops.placeholder(dtypes_lib.int32, shape=[])
      w = array_ops.placeholder(dtypes_lib.int32, shape=[])
      z = array_ops.ones([h, w])
      out = z.eval(feed_dict={h: 4, w: 16})
    self.assertAllEqual(out, np.array([[1] * 16] * 4))

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_ans = array_ops.fill([2, 3], np_ans[0][0], name=""fill"").eval()","  def testFillString(self):
    np_ans = np.array([[b""yolo""] * 3] * 2)
    with self.session(use_gpu=False):
      tf_ans = array_ops.fill([2, 3], np_ans[0][0], name=""fill"").eval()
    self.assertAllEqual(np_ans, tf_ans)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
fill_t.eval({dims: shape}),"      fill_t = array_ops.fill(dims, 3.0)
      for shape in (-1,), (2, -1), (-1, 2), (-2), (-3):
        with self.assertRaises(errors_impl.InvalidArgumentError):
          fill_t.eval({dims: shape})

  @test_util.run_deprecated_v1
  def testShapeFunctionEdgeCases(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array}), feed_array)","      p_identity = array_ops.identity(p)
      feed_array = np.random.rand(10, 10)
      self.assertAllClose(
          p_identity.eval(feed_dict={p: feed_array}), feed_array)

      with self.assertRaisesOpError(
          ""must feed a value for placeholder tensor 'p' with dtype float""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array}), feed_array)","      p_identity = array_ops.identity(p)
      feed_array = np.random.rand(10, 10)
      self.assertAllClose(
          p_identity.eval(feed_dict={p: feed_array}), feed_array)

      with self.assertRaisesOpError(
          ""must feed a value for placeholder tensor 'p' with dtype float and """,code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array[:5, :5]})","
      with self.assertRaisesWithPredicateMatch(
          ValueError, lambda e: ""Cannot feed value of shape"" in str(e)):
        p_identity.eval(feed_dict={p: feed_array[:5, :5]})

  @test_util.run_deprecated_v1
  def testUnknownShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array}), feed_array)","      # can feed anything
      feed_array = np.random.rand(10, 3)
      self.assertAllClose(
          p_identity.eval(feed_dict={p: feed_array}), feed_array)
      feed_array = np.random.rand(4, 2, 5)
      self.assertAllClose(
          p_identity.eval(feed_dict={p: feed_array}), feed_array)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array}), feed_array)","          p_identity.eval(feed_dict={p: feed_array}), feed_array)
      feed_array = np.random.rand(4, 2, 5)
      self.assertAllClose(
          p_identity.eval(feed_dict={p: feed_array}), feed_array)

  @test_util.run_deprecated_v1
  def testScalarShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(p_identity.eval(feed_dict={p: 5}), 5)","    with self.cached_session():
      p = array_ops.placeholder(dtypes_lib.float32, shape=[], name=""p"")
      p_identity = array_ops.identity(p)
      self.assertAllClose(p_identity.eval(feed_dict={p: 5}), 5)

  @test_util.run_deprecated_v1
  def testPartialShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array}), feed_array)","      p_identity = array_ops.identity(p)
      feed_array = np.random.rand(10, 3)
      self.assertAllClose(
          p_identity.eval(feed_dict={p: feed_array}), feed_array)

      with self.assertRaisesWithPredicateMatch(
          ValueError, lambda e: ""Cannot feed value of shape"" in str(e)):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"p_identity.eval(feed_dict={p: feed_array[:5, :2]})","
      with self.assertRaisesWithPredicateMatch(
          ValueError, lambda e: ""Cannot feed value of shape"" in str(e)):
        p_identity.eval(feed_dict={p: feed_array[:5, :2]})

  @test_util.run_deprecated_v1
  def testPartialShapeWhenNotFed(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(10, d.eval(feed_dict={p: val}))","        c = constant_op.constant(5, dtypes_lib.int32)
      d = math_ops.multiply(p, c)
      val = np.array(2).astype(np.int64)
      self.assertEqual(10, d.eval(feed_dict={p: val}))

  @test_util.run_deprecated_v1
  def testBadShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual([2.0, 3.0], ret.eval(feed_dict={p: [1.0, 2.0]}))","      # of 21, a shape of {} is interpreted as ""any shape"".  If
      # producer version were 22, then we'd get a shape mismatch
      # error.
      self.assertAllEqual([2.0, 3.0], ret.eval(feed_dict={p: [1.0, 2.0]}))


class PlaceholderWithDefaultTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[[3, 3], [3, 3]], a.eval(feed_dict={p: [[3, 3], [3, 3]]}))","      a = array_ops.identity(p)
      self.assertAllEqual([[2, 2], [2, 2]], self.evaluate(a))
      self.assertAllEqual(
          [[3, 3], [3, 3]], a.eval(feed_dict={p: [[3, 3], [3, 3]]}))

      with self.assertRaises(ValueError):
        a.eval(feed_dict={p: [[6, 6, 6], [6, 6, 6]]})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a.eval(feed_dict={p: [[6, 6, 6], [6, 6, 6]]})","          [[3, 3], [3, 3]], a.eval(feed_dict={p: [[3, 3], [3, 3]]}))

      with self.assertRaises(ValueError):
        a.eval(feed_dict={p: [[6, 6, 6], [6, 6, 6]]})

  @test_util.run_deprecated_v1
  def testPartialShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual([3, 37], a.eval(feed_dict={p: [3, 37]}))","      p = array_ops.placeholder_with_default([1, 2, 3], shape=[None])
      a = array_ops.identity(p)
      self.assertAllEqual([1, 2, 3], self.evaluate(a))
      self.assertAllEqual([3, 37], a.eval(feed_dict={p: [3, 37]}))

      with self.assertRaises(ValueError):
        a.eval(feed_dict={p: [[2, 2], [2, 2]]})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a.eval(feed_dict={p: [[2, 2], [2, 2]]})","      self.assertAllEqual([3, 37], a.eval(feed_dict={p: [3, 37]}))

      with self.assertRaises(ValueError):
        a.eval(feed_dict={p: [[2, 2], [2, 2]]})

  @test_util.run_deprecated_v1
  def testNoShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual([3, 37], a.eval(feed_dict={p: [3, 37]}))","      p = array_ops.placeholder_with_default([17], shape=None)
      a = array_ops.identity(p)
      self.assertAllEqual([17], self.evaluate(a))
      self.assertAllEqual([3, 37], a.eval(feed_dict={p: [3, 37]}))
      self.assertAllEqual(
          [[3, 3], [3, 3]], a.eval(feed_dict={p: [[3, 3], [3, 3]]}))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"[[3, 3], [3, 3]], a.eval(feed_dict={p: [[3, 3], [3, 3]]}))","      self.assertAllEqual([17], self.evaluate(a))
      self.assertAllEqual([3, 37], a.eval(feed_dict={p: [3, 37]}))
      self.assertAllEqual(
          [[3, 3], [3, 3]], a.eval(feed_dict={p: [[3, 3], [3, 3]]}))

  @test_util.run_deprecated_v1
  def testGradient(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out_tf.eval(),"    block_size = 0
    with self.assertRaises(ValueError):
      out_tf = self.space_to_batch(x_np, paddings, block_size)
      out_tf.eval()

  @test_util.run_deprecated_v1
  def testBlockSizeOne(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out_tf.eval(),"    block_size = 1
    with self.assertRaises(ValueError):
      out_tf = self.space_to_batch(x_np, paddings, block_size)
      out_tf.eval()

  @test_util.run_deprecated_v1
  def testBlockSizeLarger(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_ = t.eval({,"                                    paddings_placeholder)

    with self.assertRaises(ValueError):
      _ = t.eval({
          input_placeholder: np.zeros(input_shape, np.float32),
          block_shape_placeholder: block_shape,
          paddings_placeholder: paddings",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
paddings_result = t_paddings.eval(assignments),"        input_shape_placeholder, block_shape_placeholder,
        base_paddings_placeholder)
    with self.cached_session():
      paddings_result = t_paddings.eval(assignments)
      crops_result = t_crops.eval(assignments)
    self.assertAllEqual(paddings_result, paddings_const)
    self.assertAllEqual(crops_result, crops_const)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
crops_result = t_crops.eval(assignments),"        base_paddings_placeholder)
    with self.cached_session():
      paddings_result = t_paddings.eval(assignments)
      crops_result = t_crops.eval(assignments)
    self.assertAllEqual(paddings_result, paddings_const)
    self.assertAllEqual(crops_result, crops_const)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"state_ops.scatter_nd_update(res, [[0]], [0.22]).eval()","    with self.cached_session():
      self.evaluate(res.initializer)
      with self.assertRaisesOpError(""Output must be at least 1-D""):
        state_ops.scatter_nd_update(res, [[0]], [0.22]).eval()

  def testExtraIndicesDimensions(self):
    indices = array_ops.zeros([1, 1, 2], dtypes.int32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"op(ref, indices, updates).eval()","
        # Indices all in range, no problem.
        indices = np.array([2, 0, 5])
        op(ref, indices, updates).eval()

        # Indices out of range should not fail.
        indices = np.array([-1, 0, 5])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"op(ref, indices, updates).eval()","
        # Indices out of range should not fail.
        indices = np.array([-1, 0, 5])
        op(ref, indices, updates).eval()
        indices = np.array([2, 0, 6])
        op(ref, indices, updates).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"op(ref, indices, updates).eval()","        indices = np.array([-1, 0, 5])
        op(ref, indices, updates).eval()
        indices = np.array([2, 0, 6])
        op(ref, indices, updates).eval()


class StatefulScatterNdDeterminismTest(StatefulScatterNdTest):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.scatter_nd(indices, updates, shape).eval(","      with self.cached_session():
        with self.assertRaisesOpError(
            ""Indices and updates specified for empty (input|output)""):
          self.scatter_nd(indices, updates, shape).eval(
              feed_dict={
                  indices: np.zeros([2, 2, 2], dtype=np.int32),
                  updates: np.zeros([2, 2, 2], dtype=np.int32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init1(shape).eval(),"  if shape is None:
    shape = [100]
  with tc.test_session(graph=ops.Graph()):
    t1 = init1(shape).eval()
  with tc.test_session(graph=ops.Graph()):
    t2 = init2(shape).eval()
  return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init2(shape).eval(),"  with tc.test_session(graph=ops.Graph()):
    t1 = init1(shape).eval()
  with tc.test_session(graph=ops.Graph()):
    t2 = init2(shape).eval()
  return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init(shape).eval(),"    shape = [100]
  with tc.test_session(graph=ops.Graph()):
    random_seed.set_random_seed(graph_seed)
    t1 = init(shape).eval()
    t2 = init(shape).eval()
    return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init(shape).eval(),"  with tc.test_session(graph=ops.Graph()):
    random_seed.set_random_seed(graph_seed)
    t1 = init(shape).eval()
    t2 = init(shape).eval()
    return np.allclose(t1, t2, rtol=1e-15, atol=1e-15)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return init([num]).eval(),"
  def func():
    with tc.test_session():
      return init([num]).eval()

  return func
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"actual = array_ops.reshape(x, [-1]).eval()","      x = variable_scope.get_variable(name, shape=shape, initializer=init)
      self.evaluate(x.initializer)

      actual = array_ops.reshape(x, [-1]).eval()
      self.assertEqual(len(actual), len(expected))
      for a, e in zip(actual, expected):
        self.assertEqual(a, e)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"actual = array_ops.reshape(x, [-1]).eval()","      x = variable_scope.get_variable(name, shape=shape, initializer=init)
      self.evaluate(x.initializer)

      actual = array_ops.reshape(x, [-1]).eval()
      self.assertGreater(len(actual), len(expected))
      for i in range(len(actual)):
        a = actual[i]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
x = init(shape).eval(),"      test.mock.patch.object(
          random_ops, ""truncated_normal"", wraps=random_ops.truncated_normal) \
          as mock_truncated_normal:
      x = init(shape).eval()
      self.assertTrue(mock_truncated_normal.called)

    self.assertNear(np.mean(x), expect_mean, err=1e-2)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
x = init(shape).eval(),"      test.mock.patch.object(
          random_ops, ""truncated_normal"", wraps=random_ops.truncated_normal) \
          as mock_truncated_normal:
      x = init(shape).eval()
      self.assertTrue(mock_truncated_normal.called)

    self.assertNear(np.mean(x), expect_mean, err=1e-2)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
x = init(shape).eval(),"      test.mock.patch.object(
          random_ops, ""random_normal"", wraps=random_ops.random_normal) \
          as mock_random_normal:
      x = init(shape).eval()
      self.assertTrue(mock_random_normal.called)

    self.assertNear(np.mean(x), expect_mean, err=1e-2)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
x = init(shape).eval(),"    init = init_ops.variance_scaling_initializer(distribution=""uniform"")

    with self.session():
      x = init(shape).eval()

    self.assertNear(np.mean(x), expect_mean, err=1e-2)
    self.assertNear(np.var(x), expect_var, err=1e-2)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init1(shape).eval(),"      init1 = init_ops.orthogonal_initializer(seed=1, dtype=dtype)
      init2 = init_ops.orthogonal_initializer(gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init2(shape).eval(),"      init2 = init_ops.orthogonal_initializer(gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t = init(shape).eval(),"        tol = 1e-5 if dtype == dtypes.float32 else 1e-12
        with self.session(graph=ops.Graph(), use_gpu=True):
          # Check the shape
          t = init(shape).eval()
          self.assertAllEqual(shape, t.shape)
          # Check orthogonality by computing the inner product
          t = t.reshape((np.prod(t.shape[:-1]), t.shape[-1]))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init1(shape).eval(),"      init2 = init_ops.convolutional_delta_orthogonal(
          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init2(shape).eval(),"          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init1(shape).eval(),"      init2 = init_ops.convolutional_orthogonal_1d(
          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init2(shape).eval(),"          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init1(shape).eval(),"      init2 = init_ops.convolutional_orthogonal_2d(
          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init2(shape).eval(),"          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t1 = init1(shape).eval(),"      init2 = init_ops.convolutional_orthogonal_3d(
          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
t2 = init2(shape).eval(),"          gain=3.14, seed=1, dtype=dtype)
      with self.session(graph=ops.Graph(), use_gpu=True):
        t1 = init1(shape).eval()
        t2 = init2(shape).eval()
      self.assertAllClose(t1, t2 / 3.14)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.gather(params, [[7]], axis=0).eval()","    with self.session():
      params = [[0, 1, 2], [3, 4, 5]]
      with self.assertRaisesOpError(r""indices\[0,0\] = 7 is not in \[0, 2\)""):
        array_ops.gather(params, [[7]], axis=0).eval()
      with self.assertRaisesOpError(r""indices\[0,0\] = 7 is not in \[0, 3\)""):
        array_ops.gather(params, [[7]], axis=1).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.gather(params, [[7]], axis=1).eval()","      with self.assertRaisesOpError(r""indices\[0,0\] = 7 is not in \[0, 2\)""):
        array_ops.gather(params, [[7]], axis=0).eval()
      with self.assertRaisesOpError(r""indices\[0,0\] = 7 is not in \[0, 3\)""):
        array_ops.gather(params, [[7]], axis=1).eval()

  def testBadAxis(self):
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(feed_dict=params),"          p1: np.random.rand(4, 4).astype(""f""),
          p2: np.random.rand(4, 4).astype(""f"")
      }
      result = c.eval(feed_dict=params)

    self.assertEqual(result.shape, c.get_shape())
    self.assertAllEqual(result[:4, :], params[p1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(feed_dict=params),"          p1: np.random.rand(4, 4).astype(""f""),
          p2: np.random.rand(4, 4).astype(""f"")
      }
      result = c.eval(feed_dict=params)

    self.assertEqual(result.shape, c.get_shape())
    self.assertAllEqual(result[:, :4], params[p1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(feed_dict=params),"          p1: np.random.rand(2, 3, 1, 1).astype(""f""),
          p2: np.random.rand(2, 3, 4, 1).astype(""f"")
      }
      result = c.eval(feed_dict=params)

    self.assertEqual(result.shape, c.get_shape())
    self.assertAllEqual(result[:, :, :1, :], params[p1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(feed_dict=params),"      c = array_ops.concat(concat_inputs, concat_dim)
      if dtype != dtype_feed:
        c = math_ops.cast(c, dtype_feed)
      result = c.eval(feed_dict=params)

    self.assertEqual(result.shape, c.get_shape())
    cur_offset = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = concated_grad.eval(feed_dict=params),"          x: np.random.rand(10, 2, 3).astype(""f""),
          y: np.random.rand(10, 2, 6).astype(""f"")
      }
      result = concated_grad.eval(feed_dict=params)

      self.assertAllEqual(result, grad_inp)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(feed_dict=params),"
        concat_inputs = p
        c = array_ops.concat(concat_inputs, concat_dim)
        result = c.eval(feed_dict=params)

        self.assertEqual(result.shape, c.get_shape())
        cur_offset = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"gen_array_ops.concat_v2([t1, t2], 1).eval()","      with test_util.use_gpu():
        t1 = [1]
        t2 = [2]
        gen_array_ops.concat_v2([t1, t2], 1).eval()

  def testConcatInvalidAxisInTfFunction(self):
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = concated_grad.eval(feed_dict=feed_dict),"          grad_inp.flatten(), shape=output_shape)
      grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])
      concated_grad = array_ops.concat(grad, axis)
      result = concated_grad.eval(feed_dict=feed_dict)
      self.assertAllEqual(result, grad_inp)

  def _testIndexedSlicesGradientsForAxis(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = concated_grad.eval(feed_dict=feed_dict),"      grad = gradients_impl.gradients([c], inp_tensors, [grad_tensor])
      concated_grad = array_ops.gather(
          array_ops.concat(grad, axis), gather_indexes)
      result = concated_grad.eval(feed_dict=feed_dict)
      self.assertAllEqual(result, grad_inp)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"3, -10).eval()","      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""is out of range""):
        manip_ops.roll(np.random.randint(-100, 100, (4, 4)).astype(np.int32),
                       3, -10).eval()

  @test_util.run_deprecated_v1
  def testEmptyInput(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"manip_ops.roll(tensor, shift, axis).eval(feed_dict={tensor: 7})","    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""input must be 1-D or higher""):
        manip_ops.roll(tensor, shift, axis).eval(feed_dict={tensor: 7})

  @test_util.run_deprecated_v1
  def testInvalidAxisShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"manip_ops.roll(tensor, shift, axis).eval(feed_dict={axis: [[0, 1]]})","    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""axis must be a scalar or a 1-D vector""):
        manip_ops.roll(tensor, shift, axis).eval(feed_dict={axis: [[0, 1]]})

  @test_util.run_deprecated_v1
  def testInvalidShiftShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"manip_ops.roll(tensor, shift, axis).eval(feed_dict={shift: [[0, 1]]})","    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""shift must be a scalar or a 1-D vector""):
        manip_ops.roll(tensor, shift, axis).eval(feed_dict={shift: [[0, 1]]})

  @test_util.run_deprecated_v1
  def testInvalidShiftAndAxisNotEqualShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"manip_ops.roll(tensor, shift, axis).eval(feed_dict={shift: [1]})","    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""shift and axis must have the same size""):
        manip_ops.roll(tensor, shift, axis).eval(feed_dict={shift: [1]})

  def testRollAxisOutOfRangeRaises(self):
    tensor = [1, 2]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"manip_ops.roll(tensor, shift, axis).eval()","    with self.cached_session():
      with self.assertRaisesRegex(errors_impl.InvalidArgumentError,
                                  ""is out of range""):
        manip_ops.roll(tensor, shift, axis).eval()


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_val = array_ops.where(c_vec, x * x, -x).eval()","    c_vec = np.array([False, True] * 8192)  # [16384]
    np_val = np.where(c_mat, x * x, -x)
    with self.session():
      tf_val = array_ops.where(c_vec, x * x, -x).eval()
    self.assertAllEqual(tf_val, np_val)

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"mode=""REFLECT"").eval()","            [1], shape=[2]),
                      constant_op.constant(
                          [2, 0], shape=[1, 2]),
                      mode=""REFLECT"").eval()
      with self.assertRaises(Exception):
        array_ops.pad(constant_op.constant(
            [1], shape=[2]),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"mode=""SYMMETRIC"").eval()","            [1], shape=[2]),
                      constant_op.constant(
                          [0, 3], shape=[1, 2]),
                      mode=""SYMMETRIC"").eval()

  def testInvalid(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
x = ops.convert_to_tensor(args).eval(),"      if isinstance(args, tuple):
        return [placeholders(x, feed) for x in args]
      else:
        x = ops.convert_to_tensor(args).eval()
        fake = array_ops.placeholder(np.asarray(x).dtype)
        feed[fake] = x
        return fake",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
# (no eval()),"            self.assertAllEqual(x_tf_5, np.asarray(x_np)[::-1, ::-1])

  # This test covers the axis validation in the shape function
  # (no eval())
  def testInvalidAxis(self):
    x_np = np.array([[1, 2, 3], [4, 5, 6]], dtype=np.float32)
    with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_ = checker[..., :, ...].eval()","      # multiple ellipses not allowed
      with self.assertRaisesRegex((ValueError, errors.InvalidArgumentError),
                                  ""Multiple ellipses""):
        _ = checker[..., :, ...].eval()

  def testShrink(self):
    with test_util.device(use_gpu=True):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.matrix_diag(v).eval(feed_dict={v: 0.0}),"    with self.session():
      v = array_ops.placeholder(dtype=dtypes_lib.float32)
      with self.assertRaisesOpError(""diagonal must be at least 1-dim""):
        array_ops.matrix_diag(v).eval(feed_dict={v: 0.0})

  @test_util.run_deprecated_v1
  def testGrad(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.matrix_set_diag(v, [v]).eval(feed_dict={v: 0.0})","    with self.session():
      v = array_ops.placeholder(dtype=dtypes_lib.float32)
      with self.assertRaisesOpError(""input must be at least 2-dim""):
        array_ops.matrix_set_diag(v, [v]).eval(feed_dict={v: 0.0})
      with self.assertRaisesOpError(""diagonal must be at least 1-dim""):
        array_ops.matrix_set_diag([[v]], v).eval(feed_dict={v: 0.0})
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.matrix_set_diag([[v]], v).eval(feed_dict={v: 0.0})","      with self.assertRaisesOpError(""input must be at least 2-dim""):
        array_ops.matrix_set_diag(v, [v]).eval(feed_dict={v: 0.0})
      with self.assertRaisesOpError(""diagonal must be at least 1-dim""):
        array_ops.matrix_set_diag([[v]], v).eval(feed_dict={v: 0.0})

      d = array_ops.placeholder(dtype=dtypes_lib.float32)
      with self.assertRaisesOpError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"array_ops.matrix_set_diag(v, d).eval(feed_dict={","      d = array_ops.placeholder(dtype=dtypes_lib.float32)
      with self.assertRaisesOpError(
          ""first dimensions of diagonal don't match""):
        array_ops.matrix_set_diag(v, d).eval(feed_dict={
            v: np.zeros((2, 3, 3)),
            d: np.ones((2, 4))
        })",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result_eval = result.eval(feed_dict={matrix: input_matrix}),"    result = array_ops.matrix_diag_part(matrix, k=-1)
    input_matrix = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]
    with self.session():
      result_eval = result.eval(feed_dict={matrix: input_matrix})
    self.assertAllEqual([4, 8], result_eval)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.matrix_diag_part(v).eval(feed_dict={v: 0.0}),"    with self.session():
      v = array_ops.placeholder(dtype=dtypes_lib.float32)
      with self.assertRaisesOpError(""input must be at least 2-dim""):
        array_ops.matrix_diag_part(v).eval(feed_dict={v: 0.0})

  @test_util.run_deprecated_v1
  def testGrad(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = tensor.eval(,"      serialized = array_ops.placeholder(dtypes.string)
      tensor = parsing_ops.parse_tensor(serialized, dtypes.float32)

      result = tensor.eval(
          feed_dict={serialized: tensor_proto.SerializeToString()})

      self.assertAllEqual(expected, result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = tensor.eval(,"      serialized = array_ops.placeholder(dtypes.string)
      tensor = parsing_ops.parse_tensor(serialized, dtypes.uint8)

      result = tensor.eval(
          feed_dict={serialized: tensor_proto.SerializeToString()})

      self.assertAllEqual(expected, result)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
tensor.eval(feed_dict={serialized: tensor_proto.SerializeToString()}),"      with self.assertRaisesOpError(
          r""Type mismatch between parsed tensor \(uint8\) and dtype ""
          r""\(uint16\)""):
        tensor.eval(feed_dict={serialized: tensor_proto.SerializeToString()})

  @test_util.run_deprecated_v1
  def testInvalidInput(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tensor.eval(feed_dict={serialized: ""bogus""})","
      with self.assertRaisesOpError(
          ""Could not parse `serialized` as TensorProto, base64: Ym9ndXM=""):
        tensor.eval(feed_dict={serialized: ""bogus""})

      with self.assertRaisesOpError(
          r""Expected `serialized` to be a scalar, got shape: \[1\]""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tensor.eval(feed_dict={serialized: [""bogus""]})","
      with self.assertRaisesOpError(
          r""Expected `serialized` to be a scalar, got shape: \[1\]""):
        tensor.eval(feed_dict={serialized: [""bogus""]})


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"io_ops.matching_files(f.name).eval(), compat.as_bytes(f.name))","      # Test exact match without wildcards.
      for f in files:
        self.assertEqual(
            io_ops.matching_files(f.name).eval(), compat.as_bytes(f.name))

      # We will look for files matching ""ABxDEF.GH*"" where ""x"" is some wildcard.
      directory_path = files[0].name[:files[0].name.find(cases[0])]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"set(io_ops.matching_files(pattern % 'z').eval()),","      pattern = directory_path + 'AB%sDEF.GH*'

      self.assertEqual(
          set(io_ops.matching_files(pattern % 'z').eval()),
          self._subset(files, [1]))
      self.assertEqual(
          set(io_ops.matching_files(pattern % '?').eval()),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"set(io_ops.matching_files(pattern % '?').eval()),","          set(io_ops.matching_files(pattern % 'z').eval()),
          self._subset(files, [1]))
      self.assertEqual(
          set(io_ops.matching_files(pattern % '?').eval()),
          self._subset(files, [0, 1, 3, 4]))
      self.assertEqual(
          set(io_ops.matching_files(pattern % '*').eval()),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"set(io_ops.matching_files(pattern % '*').eval()),","          set(io_ops.matching_files(pattern % '?').eval()),
          self._subset(files, [0, 1, 3, 4]))
      self.assertEqual(
          set(io_ops.matching_files(pattern % '*').eval()),
          self._subset(files, [0, 1, 2, 3, 4, 5]))
      # NOTE(mrry): Windows uses PathMatchSpec to match file patterns, which
      # does not support the following expressions.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"set(io_ops.matching_files(pattern % '[cxz]').eval()),","      # does not support the following expressions.
      if os.name != 'nt':
        self.assertEqual(
            set(io_ops.matching_files(pattern % '[cxz]').eval()),
            self._subset(files, [0, 1]))
        self.assertEqual(
            set(io_ops.matching_files(pattern % '[0-9]').eval()),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"set(io_ops.matching_files(pattern % '[0-9]').eval()),","            set(io_ops.matching_files(pattern % '[cxz]').eval()),
            self._subset(files, [0, 1]))
        self.assertEqual(
            set(io_ops.matching_files(pattern % '[0-9]').eval()),
            self._subset(files, [3, 4]))

      # Test an empty list input.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertItemsEqual(io_ops.matching_files([]).eval(), [])","            self._subset(files, [3, 4]))

      # Test an empty list input.
      self.assertItemsEqual(io_ops.matching_files([]).eval(), [])

      # Test multiple exact filenames.
      self.assertItemsEqual(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"files[0].name, files[1].name, files[2].name]).eval(),","      # Test multiple exact filenames.
      self.assertItemsEqual(
          io_ops.matching_files([
              files[0].name, files[1].name, files[2].name]).eval(),
          self._subset(files, [0, 1, 2]))

      # Test multiple globs.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"pattern % '?', directory_path + 'X?Z*']).eval(),","      # Test multiple globs.
      self.assertItemsEqual(
          io_ops.matching_files([
              pattern % '?', directory_path + 'X?Z*']).eval(),
          self._subset(files, [0, 1, 3, 4, 6]))

    for f in files:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"gen_io_ops.sharded_filename(""foo"", 4, 100).eval(),","    with session.Session(
        target="""", config=config_pb2.ConfigProto(device_count={""CPU"": 2})):
      self.assertEqual(
          gen_io_ops.sharded_filename(""foo"", 4, 100).eval(),
          b""foo-00004-of-00100"")
      self.assertEqual(
          gen_io_ops.sharded_filespec(""foo"", 100).eval(), b""foo-?????-of-00100"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"gen_io_ops.sharded_filespec(""foo"", 100).eval(), b""foo-?????-of-00100"")","          gen_io_ops.sharded_filename(""foo"", 4, 100).eval(),
          b""foo-00004-of-00100"")
      self.assertEqual(
          gen_io_ops.sharded_filespec(""foo"", 100).eval(), b""foo-?????-of-00100"")


class ShapeInferenceTest(test.TestCase, parameterized.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(False, variable_v1.is_variable_initialized(v0).eval())","    for use_gpu in [True, False]:
      with self.test_session(use_gpu=use_gpu):
        v0 = state_ops.variable_op([1, 2], dtypes.float32)
        self.assertEqual(False, variable_v1.is_variable_initialized(v0).eval())
        state_ops.assign(v0, [[2.0, 3.0]]).eval()
        self.assertEqual(True, variable_v1.is_variable_initialized(v0).eval())
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"state_ops.assign(v0, [[2.0, 3.0]]).eval()","      with self.test_session(use_gpu=use_gpu):
        v0 = state_ops.variable_op([1, 2], dtypes.float32)
        self.assertEqual(False, variable_v1.is_variable_initialized(v0).eval())
        state_ops.assign(v0, [[2.0, 3.0]]).eval()
        self.assertEqual(True, variable_v1.is_variable_initialized(v0).eval())

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(True, variable_v1.is_variable_initialized(v0).eval())","        v0 = state_ops.variable_op([1, 2], dtypes.float32)
        self.assertEqual(False, variable_v1.is_variable_initialized(v0).eval())
        state_ops.assign(v0, [[2.0, 3.0]]).eval()
        self.assertEqual(True, variable_v1.is_variable_initialized(v0).eval())


  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(result.eval(), b""data"")","    result = state_ops.assign(buffer_var, data, validate_shape=False)
    with self.cached_session() as sess:
      sess.run(variables.local_variables_initializer())
      self.assertEqual(result.eval(), b""data"")


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
new_v = pickle.load(f),"        pickle.dump(v, f)

      with open(fname, ""rb"") as f:
        new_v = pickle.load(f)
        self.assertEqual(new_v.name, v.name)
        self.assertEqual(new_v.shape, v.shape)
        self.assertEqual(new_v.dtype, v.dtype)",pickle.load,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(1.0, v.read_value().eval())","      # read_value should not rerun the initializer_op if the variable
      # has already been initialized elsewhere.
      self.evaluate(v.assign(1.0))
      self.assertEqual(1.0, v.read_value().eval())

    v_def.ClearField(""initial_value_name"")
    with ops.Graph().as_default(), self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(2, math_ops.add(w, 1).eval())","      self.evaluate(variables.global_variables_initializer())

      w = resource_variable_ops.ResourceVariable.from_proto(v.to_proto())
      self.assertEqual(2, math_ops.add(w, 1).eval())

      self.assertEqual(v._handle, w._handle)
      self.assertEqual(v._graph_element, w._graph_element)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"resource_variable_ops.read_variable_op(x, v.dtype.base_dtype).eval()","          container=ops.get_default_graph()._container)
      with self.assertRaisesOpError(
          ""(Resource .*/var5/.* does not exist|uninitialized)""):
        resource_variable_ops.read_variable_op(x, v.dtype.base_dtype).eval()

  @test_util.run_deprecated_v1
  def testSharedNameWithNamescope(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(3.0, v.value().eval())","      # in the constructor.
      v = resource_variable_ops.ResourceVariable(2.0)
      v.initializer.run(feed_dict={v.initial_value: 3.0})
      self.assertEqual(3.0, v.value().eval())

  @test_util.run_v1_only(""b/120545219"")
  def testControlFlowInitialization(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"pattern = re.compile(""shapes must be equal"", re.IGNORECASE)","  def testAssignIncompatibleShape(self):
    v = resource_variable_ops.ResourceVariable([0, 1, 2, 3])
    self.evaluate(v.initializer)
    pattern = re.compile(""shapes must be equal"", re.IGNORECASE)
    with self.assertRaisesRegex(Exception, pattern):
      self.evaluate(v.assign_add(1))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result, = output.eval(feed_dict={input_string: [instr]})","
      # Check all the good input/output pairs.
      for instr, outnum in good_pairs:
        result, = output.eval(feed_dict={input_string: [instr]})
        self.assertAllClose([outnum], [result])

      # Check that the bad inputs produce the right errors.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
output.eval(feed_dict={input_string: [instr]}),"      # Check that the bad inputs produce the right errors.
      for instr, outstr in bad_pairs:
        with self.assertRaisesOpError(outstr):
          output.eval(feed_dict={input_string: [instr]})

  @test_util.run_deprecated_v1
  def testToFloat(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
outputs = string_ops.unicode_script(input_vector).eval(),"    ]
    with self.cached_session():
      input_vector = constant_op.constant(inputs, dtypes.int32)
      outputs = string_ops.unicode_script(input_vector).eval()
      self.assertAllEqual(
          outputs,
          [",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
outputs = string_ops.unicode_script(input_vector).eval(),"    inputs = [-100, 0xffffff]
    with self.cached_session():
      input_vector = constant_op.constant(inputs, dtypes.int32)
      outputs = string_ops.unicode_script(input_vector).eval()
      self.assertAllEqual(outputs, [-1, -1])

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
self._decoded_f.eval(feed_dict={self._encoded_f: enc}),"
  def testInvalidInput(self):
    def try_decode(enc):
      self._decoded_f.eval(feed_dict={self._encoded_f: enc})

    with self.cached_session():
      # Invalid length.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = output.eval(feed_dict={input_string: ['a', 'b', 'c']})","    with self.cached_session():
      input_string = array_ops.placeholder(dtypes.string)
      output = string_ops.string_to_hash_bucket_fast(input_string, 1)
      result = output.eval(feed_dict={input_string: ['a', 'b', 'c']})

      self.assertAllEqual([0, 0, 0], result)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = output.eval(feed_dict={input_string: ['a', 'b', 'c', 'd']})","    with self.cached_session():
      input_string = array_ops.placeholder(dtypes.string)
      output = string_ops.string_to_hash_bucket_fast(input_string, 10)
      result = output.eval(feed_dict={input_string: ['a', 'b', 'c', 'd']})

      # Fingerprint64('a') -> 12917804110809363939 -> mod 10 -> 9
      # Fingerprint64('b') -> 11795596070477164822 -> mod 10 -> 2",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = output.eval(feed_dict={input_string: ['a', 'b', 'c']})","    with self.cached_session():
      input_string = array_ops.placeholder(dtypes.string)
      output = string_ops.string_to_hash_bucket(input_string, 1)
      result = output.eval(feed_dict={input_string: ['a', 'b', 'c']})

      self.assertAllEqual([0, 0, 0], result)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = output.eval(feed_dict={input_string: ['a', 'b', 'c']})","    with self.cached_session():
      input_string = array_ops.placeholder(dtypes.string)
      output = string_ops.string_to_hash_bucket(input_string, 10)
      result = output.eval(feed_dict={input_string: ['a', 'b', 'c']})

      # Hash64('a') -> 2996632905371535868 -> mod 10 -> 8
      # Hash64('b') -> 5795986006276551370 -> mod 10 -> 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"input_string, 10, key=[98765]).eval()","      input_string = constant_op.constant(['a', 'b', 'c'])
      with self.assertRaisesOpError('Key must have 2 elements'):
        string_ops.string_to_hash_bucket_strong(
            input_string, 10, key=[98765]).eval()


if __name__ == '__main__':",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
output_array = reduced.eval(feed_dict={placeholder.name: input_array}),"    with self.cached_session():
      placeholder = array_ops.placeholder(dtypes.string, name=""placeholder"")
      reduced = string_ops.reduce_join(placeholder, axis=0)
      output_array = reduced.eval(feed_dict={placeholder.name: input_array})
      self.assertAllEqualUnicode(truth, output_array)
      self.assertAllEqual(truth_shape, reduced.get_shape())
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
output_array_dim_zero = reduced.eval(feed_dict={placeholder.name: [0]}),"    with self.cached_session():
      placeholder = array_ops.placeholder(dtypes.int32, name=""placeholder"")
      reduced = string_ops.reduce_join(input_array, axis=placeholder)
      output_array_dim_zero = reduced.eval(feed_dict={placeholder.name: [0]})
      output_array_dim_one = reduced.eval(feed_dict={placeholder.name: [1]})
      self.assertAllEqualUnicode(truth_dim_zero, output_array_dim_zero)
      self.assertAllEqualUnicode(truth_dim_one, output_array_dim_one)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
output_array_dim_one = reduced.eval(feed_dict={placeholder.name: [1]}),"      placeholder = array_ops.placeholder(dtypes.int32, name=""placeholder"")
      reduced = string_ops.reduce_join(input_array, axis=placeholder)
      output_array_dim_zero = reduced.eval(feed_dict={placeholder.name: [0]})
      output_array_dim_one = reduced.eval(feed_dict={placeholder.name: [1]})
      self.assertAllEqualUnicode(truth_dim_zero, output_array_dim_zero)
      self.assertAllEqualUnicode(truth_dim_one, output_array_dim_one)
      self.assertAllEqual(truth_shape, reduced.get_shape())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"index_too_high.eval(feed_dict={placeholder.name: [""""]})","      index_too_high = string_ops.reduce_join(placeholder, axis=1)
      duplicate_index = string_ops.reduce_join(placeholder, axis=[-1, 1])
      with self.assertRaisesOpError(""Invalid reduction dimension 1""):
        index_too_high.eval(feed_dict={placeholder.name: [""""]})
      with self.assertRaisesOpError(""Duplicate reduction dimension 1""):
        duplicate_index.eval(feed_dict={placeholder.name: [[""""]]})
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"duplicate_index.eval(feed_dict={placeholder.name: [[""""]]})","      with self.assertRaisesOpError(""Invalid reduction dimension 1""):
        index_too_high.eval(feed_dict={placeholder.name: [""""]})
      with self.assertRaisesOpError(""Duplicate reduction dimension 1""):
        duplicate_index.eval(feed_dict={placeholder.name: [[""""]]})

  @test_util.run_deprecated_v1
  def testInvalidArgsUnknownIndices(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
reduced.eval(feed_dict={placeholder.name: -2}),"      reduced = string_ops.reduce_join([""test"", ""test2""], axis=placeholder)

      with self.assertRaisesOpError(""reduction dimension -2""):
        reduced.eval(feed_dict={placeholder.name: -2})
      with self.assertRaisesOpError(""reduction dimension 2""):
        reduced.eval(feed_dict={placeholder.name: 2})
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
reduced.eval(feed_dict={placeholder.name: 2}),"      with self.assertRaisesOpError(""reduction dimension -2""):
        reduced.eval(feed_dict={placeholder.name: -2})
      with self.assertRaisesOpError(""reduction dimension 2""):
        reduced.eval(feed_dict={placeholder.name: 2})

  def testDeprecatedArgs(self):
    foobar = constant_op.constant([""foobar""])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"matched = op(input_tensor, ""a.*a"").eval()","    values = [""abaaba"", ""abcdabcde""]
    with self.cached_session():
      input_tensor = constant_op.constant(values, dtypes.string)
      matched = op(input_tensor, ""a.*a"").eval()
      self.assertAllEqual([True, False], matched)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"matched = op(input_tensor, ""a.*a"").eval()","    values = [[""abaaba"", ""abcdabcde""], [""acdcba"", ""ebcda""]]
    with self.cached_session():
      input_tensor = constant_op.constant(values, dtypes.string)
      matched = op(input_tensor, ""a.*a"").eval()
      self.assertAllEqual([[True, False], [True, False]], matched)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"matched = op(input_tensor, """").eval()","    values = [""abc"", ""1""]
    with self.cached_session():
      input_tensor = constant_op.constant(values, dtypes.string)
      matched = op(input_tensor, """").eval()
      self.assertAllEqual([False, False], matched)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = math_ops.segment_sum(data=tf_x, segment_ids=indices).eval()","      with self.cached_session(use_gpu=use_gpu):
        tf_x, _ = self._input(shape, dtype=dtypes_lib.float32)
        indices = [0, 0, 0, 1]
        result = math_ops.segment_sum(data=tf_x, segment_ids=indices).eval()
        self.assertAllEqual([[15, 18, 21, 24], [13, 14, 15, 16]], result)

  def testSegmentIdsGreaterThanZero(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(unsorted.eval(), np.zeros((2, 1), dtype=np.float32))","      data = np.ones((2, 1), dtype=np.float32)
      segment_ids = np.array([-1, -1], dtype=np.int32)
      unsorted = math_ops.unsorted_segment_sum(data, segment_ids, 2)
      self.assertAllClose(unsorted.eval(), np.zeros((2, 1), dtype=np.float32))

  @test_util.run_deprecated_v1
  def testBadNumSegments(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"y = math_ops.reduce_mean(x, [0]).eval()","                    dtypes.float64):
        # A large number is needed to get Eigen to die
        x = array_ops.zeros((0, 9938), dtype=dtype)
        y = math_ops.reduce_mean(x, [0]).eval()
        self.assertEqual(y.shape, (9938,))
        self.assertTrue(np.all(np.isnan(y)))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"y = math_ops.reduce_euclidean_norm(x, [0]).eval()","                    dtypes.float64):
        # A large number is needed to get Eigen to die
        x = array_ops.zeros((0, 9938), dtype=dtype)
        y = math_ops.reduce_euclidean_norm(x, [0]).eval()
        self.assertEqual(y.shape, (9938,))
        self.assertAllEqual(y, np.zeros(9938))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"op([], 0).eval()","      for op in math_ops.argmin, math_ops.argmax:
        with self.assertRaisesOpError(
            r""Reduction axis 0 is empty in shape \[0\]""):
          op([], 0).eval()

  @test_util.run_deprecated_v1
  def testDefaultAxis(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
ans = op([1]).eval(),"  def testDefaultAxis(self):
    with self.cached_session():
      for op in math_ops.argmin, math_ops.argmax:
        ans = op([1]).eval()
        self.assertAllEqual(ans, 0)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"ret = op(array_ops.zeros(shape=[1, 0, 2]), axis=-1).eval()","  def testOutputEmpty(self):
    with self.cached_session():
      for op in math_ops.argmin, math_ops.argmax:
        ret = op(array_ops.zeros(shape=[1, 0, 2]), axis=-1).eval()
        self.assertEqual(ret.shape, (1, 0))

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"z = fn(c, xt, yt).eval()","    with self.cached_session():
      xt = x.astype(np.float32)
      yt = y.astype(np.float32)
      z = fn(c, xt, yt).eval()
      self.assertAllEqual(z_expected, z)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"x = fn(c, a, b).eval()","        for a in 7.0, np.nan:
          for b in 5.0, np.nan:
            with self.subTest(c=c, a=a, b=b):
              x = fn(c, a, b).eval()
              y = a if c else b
              self.assertEqual(np.isnan(x), np.isnan(y))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
num_classes=num_classes).eval(),"      dtype = predictions.dtype
      ans = confusion_matrix.confusion_matrix(
          labels, predictions, dtype=dtype, weights=weights,
          num_classes=num_classes).eval()
      self.assertAllClose(truth, ans, atol=1e-10)
      self.assertEqual(ans.dtype, dtype)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))","      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testSameShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))","      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testSameShapeExpectedRankDiff0(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))","      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testSqueezableLabels(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          expected_label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))","      self.assertAllEqual(
          expected_label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testSqueezableLabelsExpectedRankDiffPlus1(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          expected_label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))","      self.assertAllEqual(
          expected_label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testSqueezablePredictions(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          expected_prediction_values,
          dynamic_predictions.eval(feed_dict=feed_dict))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dynamic_predictions.eval(feed_dict=feed_dict)),"          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          expected_prediction_values,
          dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testSqueezablePredictionsExpectedRankDiffMinus1(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          expected_prediction_values,
          dynamic_predictions.eval(feed_dict=feed_dict))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dynamic_predictions.eval(feed_dict=feed_dict)),"          label_values, dynamic_labels.eval(feed_dict=feed_dict))
      self.assertAllEqual(
          expected_prediction_values,
          dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testUnsqueezableLabels(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          prediction_values, dynamic_predictions.eval(feed_dict=feed_dict))

  @test_util.run_deprecated_v1
  def testUnsqueezablePredictions(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"label_values, dynamic_labels.eval(feed_dict=feed_dict))","          predictions_placeholder: prediction_values
      }
      self.assertAllEqual(
          label_values, dynamic_labels.eval(feed_dict=feed_dict))


if __name__ == ""__main__"":",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0.0, z.eval(feed_dict={p: [0.0]}))","      z += 0.0  # Makes sure we release the tensor aliasing the numpy array x[0]
      # above instead of using its memory as the return value of
      # session.run
      self.assertEqual(0.0, z.eval(feed_dict={p: [0.0]}))

  def testStateful(self):
    # Not using self.cached_session(), which disables optimization.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_out = math_ops.cumsum(x, axis, exclusive, reverse).eval()","  def _compare(self, x, axis, exclusive, reverse):
    np_out = handle_options(np.cumsum, x, axis, exclusive, reverse)
    with self.cached_session():
      tf_out = math_ops.cumsum(x, axis, exclusive, reverse).eval()

    self.assertAllClose(np_out, tf_out)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_out = math_ops.cumsum(x, axis).eval()","      for axis_dtype in [dtypes.int64, dtypes.int32]:
        with self.cached_session():
          axis = constant_op.constant(0, axis_dtype)
          tf_out = math_ops.cumsum(x, axis).eval()

  @test_util.run_deprecated_v1
  def testNaN(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.cumsum(input_tensor, -3).eval()","      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Expected scan axis in the range [-2, 2)"" in str(e)):
        math_ops.cumsum(input_tensor, -3).eval()
      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Expected scan axis in the range [-2, 2)"" in str(e)):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.cumsum(input_tensor, 2).eval()","      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Expected scan axis in the range [-2, 2)"" in str(e)):
        math_ops.cumsum(input_tensor, 2).eval()
      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""axis must be a scalar"" in str(e)):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.cumsum(input_tensor, [0]).eval()","      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""axis must be a scalar"" in str(e)):
        math_ops.cumsum(input_tensor, [0]).eval()

  def _compareGradient(self, shape, axis, exclusive, reverse):
    x = np.arange(0, 50).reshape(shape).astype(np.float64)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_out = math_ops.cumprod(x, axis, exclusive, reverse).eval()","  def _compare(self, x, axis, exclusive, reverse):
    np_out = handle_options(np.cumprod, x, axis, exclusive, reverse)
    with self.cached_session():
      tf_out = math_ops.cumprod(x, axis, exclusive, reverse).eval()

    atol = rtol = 1e-6
    if x.dtype == dtypes.bfloat16.as_numpy_dtype:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf_out = math_ops.cumprod(x, axis).eval()","      for axis_dtype in [dtypes.int64, dtypes.int32]:
        with self.cached_session():
          axis = constant_op.constant(0, axis_dtype)
          tf_out = math_ops.cumprod(x, axis).eval()

  @test_util.run_deprecated_v1
  def testNaN(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.cumprod(input_tensor, -3).eval()","      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Expected scan axis in the range [-2, 2)"" in str(e)):
        math_ops.cumprod(input_tensor, -3).eval()
      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Expected scan axis in the range [-2, 2)"" in str(e)):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.cumprod(input_tensor, 2).eval()","      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""Expected scan axis in the range [-2, 2)"" in str(e)):
        math_ops.cumprod(input_tensor, 2).eval()
      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""axis must be a scalar"" in str(e)):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.cumprod(input_tensor, [0]).eval()","      with self.assertRaisesWithPredicateMatch(
          errors_impl.InvalidArgumentError,
          lambda e: ""axis must be a scalar"" in str(e)):
        math_ops.cumprod(input_tensor, [0]).eval()

  def _compareGradient(self, shape, axis, exclusive, reverse):
    x = np.arange(1, 9).reshape(shape).astype(np.float64)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Body)[0].eval(),"              errors.InvalidArgumentError,
              ""Expected a single scalar.*got 2 tensors.""):
            functional_ops.While([5., 0.], CondReturnsTooManyArgs,
                                 Body)[0].eval()
          with self.assertRaisesRegex(
              errors.InvalidArgumentError,
              ""While loop body returned 3 arguments. Expected: 2""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
BodyReturnsTooManyArgs)[0].eval(),"              errors.InvalidArgumentError,
              ""While loop body returned 3 arguments. Expected: 2""):
            functional_ops.While([5., 0.], Cond,
                                 BodyReturnsTooManyArgs)[0].eval()

  def testWhileInMultipleSubgraphs(self):
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"functional_ops.For([0], 10, 1, [0.0], Foo)[0].eval()","    with self.test_session():
      with self.assertRaisesRegex(errors.InvalidArgumentError,
                                  ""must be a scalar""):
        functional_ops.For([0], 10, 1, [0.0], Foo)[0].eval()
      with self.assertRaisesRegex(errors.InvalidArgumentError,
                                  ""Invalid start/limit/delta""):
        functional_ops.For(0, 10, -1, [0.0], Foo)[0].eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"functional_ops.For(0, 10, -1, [0.0], Foo)[0].eval()","        functional_ops.For([0], 10, 1, [0.0], Foo)[0].eval()
      with self.assertRaisesRegex(errors.InvalidArgumentError,
                                  ""Invalid start/limit/delta""):
        functional_ops.For(0, 10, -1, [0.0], Foo)[0].eval()
      with self.assertRaisesRegex(
          errors.InvalidArgumentError,
          ""For loop body returned 2 arguments. Expected: 1""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"functional_ops.For(0, 10, 1, [0.0], ReturnsTooManyArgs)[0].eval()","      with self.assertRaisesRegex(
          errors.InvalidArgumentError,
          ""For loop body returned 2 arguments. Expected: 1""):
        functional_ops.For(0, 10, 1, [0.0], ReturnsTooManyArgs)[0].eval()

  @test_util.run_deprecated_v1
  def testGradient(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
lambda: 2.0).eval(),"
      cond_v2.cond_v2(array_ops.placeholder_with_default(False, None),
                      true_branch,
                      lambda: 2.0).eval()
      self.assertAllEqual(self.evaluate(v), 2.0)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"cond_v2.cond_v2(constant_op.constant(True), fn, fn).eval(), 3)","
        with ops.colocate_with(a.op):
          self.assertEqual(
              cond_v2.cond_v2(constant_op.constant(True), fn, fn).eval(), 3)

        def fn2():
          c = constant_op.constant(3.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"cond_v2.cond_v2(constant_op.constant(True), fn2, fn2).eval(), 3)","        with ops.colocate_with(a.op):
          with ops.colocate_with(b.op):
            self.assertEqual(
                cond_v2.cond_v2(constant_op.constant(True), fn2, fn2).eval(), 3)

  def testColocateWithInAndOutOfCond(self):
    with ops.Graph().as_default() as g:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"cond_v2.cond_v2(constant_op.constant(True), fn2, fn2).eval(), 3)","
        with ops.colocate_with(a.op):
          self.assertEqual(
              cond_v2.cond_v2(constant_op.constant(True), fn2, fn2).eval(), 3)

          d = constant_op.constant([2.0], name=""d"")
          self.assertEqual([b""loc:@a""], d.op.colocation_groups())",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"cond_v2.cond_v2(constant_op.constant(True), fn2, fn2).eval(), 3)","
        with ops.device(""/device:CPU:0""):
          self.assertEqual(
              cond_v2.cond_v2(constant_op.constant(True), fn2, fn2).eval(), 3)

          d = constant_op.constant(4.0)
          self.assertEqual(""/device:CPU:0"", d.op.device)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
res.eval(feed_dict={data: 1.0}),"      enter_2 = gen_control_flow_ops.enter(data, ""foo_2"", False)
      res = math_ops.add(enter_1, enter_2)
      with self.assertRaisesOpError(""has inputs from different frames""):
        res.eval(feed_dict={data: 1.0})

  @test_util.run_deprecated_v1
  def testCondBool(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(1000, r.eval(feed_dict={i0: 0}))","      i0 = constant_op.constant(0)
      r = while_loop_tf.while_loop(
          lambda i: i < 1000, lambda i: math_ops.square(c) + i, [i0])
      self.assertEqual(1000, r.eval(feed_dict={i0: 0}))
      feedable_tensors = all_feedables()
      for t in feedable_tensors:
        sess.run(r, feed_dict={t: 3})",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(42.0, grad.eval(feed_dict={c: 1}))","      r = tf_cond.cond(pred, fn1, fn2)

      grad = gradients_impl.gradients(r, [x])[0]
      self.assertAllEqual(42.0, grad.eval(feed_dict={c: 1}))
      self.assertAllEqual(3.0, grad.eval(feed_dict={c: 3}))

  @test_util.disable_control_flow_v2(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(3.0, grad.eval(feed_dict={c: 3}))","
      grad = gradients_impl.gradients(r, [x])[0]
      self.assertAllEqual(42.0, grad.eval(feed_dict={c: 1}))
      self.assertAllEqual(3.0, grad.eval(feed_dict={c: 3}))

  @test_util.disable_control_flow_v2(
      ""b/110550782 (gradient w.r.t external variable)"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(980.0, r.eval(feed_dict={c: 1}))","      y = math_ops.multiply(7.0, ox)
      r = tf_cond.cond(pred, lambda: fn1(y), fn2)

      self.assertAllEqual(980.0, r.eval(feed_dict={c: 1}))
      self.assertAllEqual(30.0, r.eval(feed_dict={c: 3}))

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(30.0, r.eval(feed_dict={c: 3}))","      r = tf_cond.cond(pred, lambda: fn1(y), fn2)

      self.assertAllEqual(980.0, r.eval(feed_dict={c: 1}))
      self.assertAllEqual(30.0, r.eval(feed_dict={c: 3}))

  @test_util.run_deprecated_v1
  def testCondGradMultiDevice(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose([810.0, 2560.0], r.eval(feed_dict={x: [3.0, 4.0]}))","
      r = gradients_impl.gradients(r[1], x)[0]
      self.assertEqual([None], r.get_shape().as_list())
      self.assertAllClose([810.0, 2560.0], r.eval(feed_dict={x: [3.0, 4.0]}))

  @test_util.run_deprecated_v1
  def testWhileGrad_BaseShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))","        return gradients_impl.gradients(r, x)[0]

      r = tf_cond.cond(math_ops.less(1, 2), fn1, lambda: x)
      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))

  @test_util.disable_control_flow_v2(""b/116340060"")
  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))","
      r = while_loop_tf.while_loop(
          lambda n: n < 6.0, b1, [n], [tensor_shape.unknown_shape()])
      self.assertAllClose(9.0, r.eval(feed_dict={x: 1.0}))

  @test_util.run_v1_only(""b/120545219"")
  def testCondGradInNestedWhiles(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([1], i.eval(feed_dict={c: 2}))","      self.assertTrue(isinstance(i, tensor_lib.Tensor))

      # True case: c = 2 is >= 1
      self.assertEqual([1], i.eval(feed_dict={c: 2}))

      # False case: c = 0 is not >= 1
      self.assertEqual([2], i.eval(feed_dict={c: 0}))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual([2], i.eval(feed_dict={c: 0}))","      self.assertEqual([1], i.eval(feed_dict={c: 2}))

      # False case: c = 0 is not >= 1
      self.assertEqual([2], i.eval(feed_dict={c: 0}))

  @test_util.run_deprecated_v1
  def testExampleCond(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(4.0, i.eval(feed_dict={d: 1}))","        return math_ops.reduce_sum(math_ops.abs(x))

      i = tf_cond.cond(math_ops.equal(d, 2), l2, l1)
      self.assertAllClose(4.0, i.eval(feed_dict={d: 1}))
      self.assertAllClose(2.0 * math.sqrt(2), i.eval(feed_dict={d: 2}))

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(2.0 * math.sqrt(2), i.eval(feed_dict={d: 2}))","
      i = tf_cond.cond(math_ops.equal(d, 2), l2, l1)
      self.assertAllClose(4.0, i.eval(feed_dict={d: 1}))
      self.assertAllClose(2.0 * math.sqrt(2), i.eval(feed_dict={d: 2}))

  @test_util.run_v1_only(""b/120545219"")
  def testCase(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(1, i.eval(feed_dict={c.name: 2}))","      self.assertEqual(0, self.evaluate(v))

      # True case: c = 2 is >= 1, v is set to 1.
      self.assertEqual(1, i.eval(feed_dict={c.name: 2}))
      self.assertEqual(1, self.evaluate(v))

      # False case: c = 0 is not >= 1, v is set to 2.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(2, i.eval(feed_dict={c.name: 0}))","      self.assertEqual(1, self.evaluate(v))

      # False case: c = 0 is not >= 1, v is set to 2.
      self.assertEqual(2, i.eval(feed_dict={c.name: 0}))
      self.assertEqual(2, self.evaluate(v))

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
out_val_tmp = out_val.eval(feed_dict=feed_dict),"                                                      test_case['strides'],
                                                      test_case['rates'],
                                                      padding)
            out_val_tmp = out_val.eval(feed_dict=feed_dict)
            out_shape = out_val_tmp.shape

            err = gradient_checker.compute_gradient_error(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
os.path.join(,"    # So if the above path is not available, try another way to access the file:
    if not os.path.exists(image_file_path):
      image_file_path = resource_loader.get_path_to_datafile(
          os.path.join(
              '..', '..', 'core', 'lib', 'jpeg', 'testdata', image_name))

    if tile is None:",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
self.assertTrue(np.linalg.norm(diff.eval()) > 0.1),"        rnd1 = random_ops.random_normal(shape, 0.0, 1.0, dtypes.float32)
        rnd2 = random_ops.random_normal(shape, 0.0, 1.0, dtypes.float32)
        diff = rnd2 - rnd1
        self.assertTrue(np.linalg.norm(diff.eval()) > 0.1)

  @test_util.run_deprecated_v1
  def testSingleSessionNotConstant(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
self.assertTrue(np.linalg.norm(diff.eval()) > 0.1),"      rnd1 = random_ops.truncated_normal(shape, 0.0, 1.0, dtypes.float32)
      rnd2 = random_ops.truncated_normal(shape, 0.0, 1.0, dtypes.float32)
      diff = rnd2 - rnd1
      self.assertTrue(np.linalg.norm(diff.eval()) > 0.1)

  def testEagerSeed(self):
    with context.eager_mode():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
diff = (rnd2 - rnd1).eval(),"      with self.session():
        rnd1 = random_ops.random_uniform(shape, 0, 17, dtype=dtype)
        rnd2 = random_ops.random_uniform(shape, 0, 17, dtype=dtype)
        diff = (rnd2 - rnd1).eval()
        self.assertTrue(np.linalg.norm(diff) > 0.1)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"      # Dequeue every element using a single thread.
      results = []
      for _ in range(len(elems)):
        results.append(dequeued_t.eval())
      self.assertItemsEqual(elems, results)

  def testParallelDequeue(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
vals = [dequeued_t.eval() for _ in range(len(elems))],"      for enqueue_op in enqueue_ops:
        enqueue_op.run()

      vals = [dequeued_t.eval() for _ in range(len(elems))]
      self.assertItemsEqual(elems, vals)

  def testEnqueueAndBlockingDequeue(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(0, q.size().eval())","  def testQueueSizeEmpty(self):
    with self.cached_session():
      q = data_flow_ops.RandomShuffleQueue(10, 5, dtypes_lib.float32)
      self.assertEqual(0, q.size().eval())

  def testQueueSizeAfterEnqueueAndDequeue(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"
      results = []
      for _ in range(8):
        results.append(dequeued_t.eval())
      self.assertItemsEqual(elems + elems, results)

  def testEmptyEnqueueMany(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.extend(dequeued_t.eval()),"      enqueue_op.run()

      results = self.evaluate(dequeued_t).tolist()
      results.extend(dequeued_t.eval())
      self.assertItemsEqual(elems, results)

  def testDequeueUpToNoBlocking(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.extend(dequeued_t.eval()),"      enqueue_op.run()

      results = self.evaluate(dequeued_t).tolist()
      results.extend(dequeued_t.eval())
      self.assertItemsEqual(elems, results)

  def testMultiDequeueMany(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertItemsEqual(dequeued_t.eval().tolist(), elems.tolist())","      dequeued_t = q.dequeue_many(10)

      enqueue_op.run()
      self.assertItemsEqual(dequeued_t.eval().tolist(), elems.tolist())

  def testParallelEnqueueMany(self):
    with self.cached_session() as sess:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertItemsEqual(dequeued_t.eval(), elems * 10)","      for thread in threads:
        thread.join()

      self.assertItemsEqual(dequeued_t.eval(), elems * 10)

  def testParallelDequeueMany(self):
    with self.cached_session() as sess:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dequeued_elems.extend(dequeued_t.eval()),"
      dequeued_elems = []
      for _ in dequeue_counts:
        dequeued_elems.extend(dequeued_t.eval())
      self.assertItemsEqual(elems, dequeued_elems)

  def testDequeueUpToWithTensorParameter(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dequeued_elems.extend(dequeued_t.eval()),"
      dequeued_elems = []
      for _ in dequeue_counts:
        dequeued_elems.extend(dequeued_t.eval())
      self.assertItemsEqual(elems, dequeued_elems)

  def testDequeueFromClosedQueue(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results = [dequeued_t.eval() for _ in elems],"
      enqueue_op.run()
      close_op.run()
      results = [dequeued_t.eval() for _ in elems]
      expected = [[elem] for elem in elems]
      self.assertItemsEqual(expected, results)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"      time.sleep(0.1)
      results = []
      for _ in elems:
        results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      self.assertItemsEqual(elems + [50.0], results)
      # There wasn't room for 50.0 in the queue when the first element was",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"      results = []
      for _ in elems:
        results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      self.assertItemsEqual(elems + [50.0], results)
      # There wasn't room for 50.0 in the queue when the first element was
      # dequeued.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"      results = []
      for _ in elems:
        time.sleep(0.01)
        results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      self.assertItemsEqual(elems + [50.0, 60.0], results)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"      for _ in elems:
        time.sleep(0.01)
        results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      self.assertItemsEqual(elems + [50.0, 60.0], results)
      # There wasn't room for 50.0 or 60.0 in the queue when the first",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"        time.sleep(0.01)
        results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      results.append(dequeued_t.eval())
      self.assertItemsEqual(elems + [50.0, 60.0], results)
      # There wasn't room for 50.0 or 60.0 in the queue when the first
      # element was dequeued.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(dequeued_t.eval()),"      results = []
      # Dequeue to unblock the first blocking_enqueue_op, after which the
      # close will complete.
      results.append(dequeued_t.eval())
      self.assertTrue(results[0] in elems)
      thread2.join()
      thread1.join()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(size_t.eval(), 3)","      size_t = q.size()

      enqueue_op.run()
      self.assertEqual(size_t.eval(), 3)

      def blocking_enqueue():
        # This will block until the dequeue after the close.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
q.dequeue().eval(),"      thread2.start()

      # Unblock the first blocking_enqueue_op in blocking_enqueue.
      q.dequeue().eval()

      thread2.join()
      thread1.join()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1_size_t.eval(), 1)","      q1_size_t = q1.size()
      q2_size_t = q2.size()

      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

      self.assertEqual(q2.dequeue().eval(), 10.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2_size_t.eval(), 1)","      q2_size_t = q2.size()

      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

      self.assertEqual(q2.dequeue().eval(), 10.0)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2.dequeue().eval(), 10.0)","      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

      self.assertEqual(q2.dequeue().eval(), 10.0)

      self.assertEqual(q1_size_t.eval(), 0)
      self.assertEqual(q2_size_t.eval(), 0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1_size_t.eval(), 0)","
      self.assertEqual(q2.dequeue().eval(), 10.0)

      self.assertEqual(q1_size_t.eval(), 0)
      self.assertEqual(q2_size_t.eval(), 0)

      q2.enqueue((20.0,)).run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2_size_t.eval(), 0)","      self.assertEqual(q2.dequeue().eval(), 10.0)

      self.assertEqual(q1_size_t.eval(), 0)
      self.assertEqual(q2_size_t.eval(), 0)

      q2.enqueue((20.0,)).run()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1_size_t.eval(), 1)","
      q2.enqueue((20.0,)).run()

      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

      self.assertEqual(q1.dequeue().eval(), 20.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2_size_t.eval(), 1)","      q2.enqueue((20.0,)).run()

      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

      self.assertEqual(q1.dequeue().eval(), 20.0)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1.dequeue().eval(), 20.0)","      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

      self.assertEqual(q1.dequeue().eval(), 20.0)

      self.assertEqual(q1_size_t.eval(), 0)
      self.assertEqual(q2_size_t.eval(), 0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1_size_t.eval(), 0)","
      self.assertEqual(q1.dequeue().eval(), 20.0)

      self.assertEqual(q1_size_t.eval(), 0)
      self.assertEqual(q2_size_t.eval(), 0)

  def testSharedQueueSameSessionGraphSeedNone(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2_size_t.eval(), 0)","      self.assertEqual(q1.dequeue().eval(), 20.0)

      self.assertEqual(q1_size_t.eval(), 0)
      self.assertEqual(q2_size_t.eval(), 0)

  def testSharedQueueSameSessionGraphSeedNone(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q1_size_t.eval(), 1)","      q1_size_t = q1.size()
      q2_size_t = q2.size()

      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

  def testIncompatibleSharedQueueErrors(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q2_size_t.eval(), 1)","      q2_size_t = q2.size()

      self.assertEqual(q1_size_t.eval(), 1)
      self.assertEqual(q2_size_t.eval(), 1)

  def testIncompatibleSharedQueueErrors(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(q.dequeue().eval(), 10.0)","        index = np.random.randint(num_queues)
        q = data_flow_ops.RandomShuffleQueue.from_list(index, qlist)
        q.enqueue((10.,)).run()
        self.assertEqual(q.dequeue().eval(), 10.0)

  def testSelectQueueOutOfRange(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
enq_q.dequeue().eval(),"      q2 = data_flow_ops.RandomShuffleQueue(15, 0, dtypes_lib.float32)
      enq_q = data_flow_ops.RandomShuffleQueue.from_list(3, [q1, q2])
      with self.assertRaisesOpError(""is not in""):
        enq_q.dequeue().eval()

  def _blockingDequeue(self, sess, dequeue_op):
    with self.assertRaisesOpError(""was cancelled""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[0].extend(deq1.eval()),"
      results = [[], [], [], []]

      results[0].extend(deq1.eval())
      results[1].extend(deq2.eval())

      q1.close().run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[1].extend(deq2.eval()),"      results = [[], [], [], []]

      results[0].extend(deq1.eval())
      results[1].extend(deq2.eval())

      q1.close().run()
      q2.close().run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[2].extend(deq1.eval()),"      q1.close().run()
      q2.close().run()

      results[2].extend(deq1.eval())
      results[3].extend(deq2.eval())

      # No two should match",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[3].extend(deq2.eval()),"      q2.close().run()

      results[2].extend(deq1.eval())
      results[3].extend(deq2.eval())

      # No two should match
      for i in range(1, 4):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[0].extend(deq1.eval()),"
      results = [[], [], [], []]

      results[0].extend(deq1.eval())
      results[1].extend(deq2.eval())

      q1.close().run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[1].extend(deq2.eval()),"      results = [[], [], [], []]

      results[0].extend(deq1.eval())
      results[1].extend(deq2.eval())

      q1.close().run()
      q2.close().run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[2].extend(deq1.eval()),"      q1.close().run()
      q2.close().run()

      results[2].extend(deq1.eval())
      results[3].extend(deq2.eval())

      # No two should match",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[3].extend(deq2.eval()),"      q2.close().run()

      results[2].extend(deq1.eval())
      results[3].extend(deq2.eval())

      # No two should match
      for i in range(1, 4):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[0].append(deq1.eval()),"      results = [[], [], [], []]

      for _ in range(5):
        results[0].append(deq1.eval())
        results[1].append(deq2.eval())

      q1.close().run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[1].append(deq2.eval()),"
      for _ in range(5):
        results[0].append(deq1.eval())
        results[1].append(deq2.eval())

      q1.close().run()
      q2.close().run()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[2].append(deq1.eval()),"      q2.close().run()

      for _ in range(5):
        results[2].append(deq1.eval())
        results[3].append(deq2.eval())

      # No two should match",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results[3].append(deq2.eval()),"
      for _ in range(5):
        results[2].append(deq1.eval())
        results[3].append(deq2.eval())

      # No two should match
      for i in range(1, 4):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(deq.eval())  # Will only complete after the enqueue starts.,"
      # The enqueue should start and then block.
      results = []
      results.append(deq.eval())  # Will only complete after the enqueue starts.
      self.assertEqual(len(enq_done), 1)
      self.assertEqual(self.evaluate(size_op), 5)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(deq.eval()),"      self.assertEqual(self.evaluate(size_op), 5)

      for _ in range(3):
        results.append(deq.eval())

      time.sleep(0.1)
      self.assertEqual(len(enq_done), 1)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(deq.eval()),"      self.assertEqual(self.evaluate(size_op), 5)

      # This dequeue will unblock the thread.
      results.append(deq.eval())
      time.sleep(0.1)
      self.assertEqual(len(enq_done), 2)
      thread.join()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(size_op.eval(), 5 - i)","      thread.join()

      for i in range(5):
        self.assertEqual(size_op.eval(), 5 - i)
        results.append(deq.eval())
        self.assertEqual(size_op.eval(), 5 - i - 1)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
results.append(deq.eval()),"
      for i in range(5):
        self.assertEqual(size_op.eval(), 5 - i)
        results.append(deq.eval())
        self.assertEqual(size_op.eval(), 5 - i - 1)

      self.assertItemsEqual(elem, results)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(size_op.eval(), 5 - i - 1)","      for i in range(5):
        self.assertEqual(size_op.eval(), 5 - i)
        results.append(deq.eval())
        self.assertEqual(size_op.eval(), 5 - i - 1)

      self.assertItemsEqual(elem, results)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"(seed, pure.eval(feed_dict={seed_t: seed})) for seed in seeds","        seed_t = array_ops.placeholder(seed_type, shape=[2])
        pure = stateless_op(seed=seed_t)
        values = [
            (seed, pure.eval(feed_dict={seed_t: seed})) for seed in seeds
        ]
      for s0, v0 in values:
        for s1, v1 in values:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"crop = random_crop_ops.random_crop(value, shape).eval()","    for shape in (2, 1, 1), (2, 1, 3), (4, 5, 3):
      value = np.arange(0, np.prod(shape), dtype=np.int32).reshape(shape)
      with self.cached_session():
        crop = random_crop_ops.random_crop(value, shape).eval()
        self.assertAllEqual(crop, value)

  def testContains(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"shape, new_seed, mean, stddev, minval, maxval).eval()","                                               maxval=(2**31 - 1),
                                               dtype=np.int32)
          samples = stateless.stateless_parameterized_truncated_normal(
              shape, new_seed, mean, stddev, minval, maxval).eval()
        else:
          samples = random_ops.parameterized_truncated_normal(
              shape, mean, stddev, minval, maxval).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"shape, mean, stddev, minval, maxval).eval()","              shape, new_seed, mean, stddev, minval, maxval).eval()
        else:
          samples = random_ops.parameterized_truncated_normal(
              shape, mean, stddev, minval, maxval).eval()
        assert (~np.isnan(samples)).all()
      moments = calculate_moments(samples, self.max_moment)
      expected_moments = TruncatedNormalMoments(mean, stddev, minval, maxval)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"shape, new_seed, mean, stddev, minval, maxval).eval()","                                               maxval=(2**31 - 1),
                                               dtype=np.int32)
          samples = stateless.stateless_parameterized_truncated_normal(
              shape, new_seed, mean, stddev, minval, maxval).eval()
        else:
          samples = random_ops.parameterized_truncated_normal(
              shape, mean, stddev, minval, maxval).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"shape, mean, stddev, minval, maxval).eval()","              shape, new_seed, mean, stddev, minval, maxval).eval()
        else:
          samples = random_ops.parameterized_truncated_normal(
              shape, mean, stddev, minval, maxval).eval()

      assert (~np.isnan(samples)).all()
      minval = max(mean - stddev * 10, minval)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertGreaterEqual(np.linalg.norm(diff.eval()), 1)","        diff = rnd2 - rnd1
        # Since these are all positive integers, the norm will
        # be at least 1 if they are different.
        self.assertGreaterEqual(np.linalg.norm(diff.eval()), 1)

  def testZeroShape(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dtype=out_dt).eval(),"        for out_dt in _SUPPORTED_DTYPES:
          random_ops.random_poisson(
              constant_op.constant([1], dtype=lam_dt), [10],
              dtype=out_dt).eval()

  @test_util.run_deprecated_v1
  def testInfRate(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertGreater(np.linalg.norm(diff.eval()), 0.1)","        rnd1 = random_ops.random_gamma([24], 2.0, dtype=dtype)
        rnd2 = random_ops.random_gamma([24], 2.0, dtype=dtype)
        diff = rnd2 - rnd1
        self.assertGreater(np.linalg.norm(diff.eval()), 0.1)

  @test_util.run_deprecated_v1
  def testShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"math_ops.less_equal(x, 0.), dtype=dtypes.int64)).eval())","      with self.cached_session():
        x = random_ops.random_gamma(shape=[n], alpha=0.001, dtype=dt, seed=0)
        self.assertEqual(0, math_ops.reduce_sum(math_ops.cast(
            math_ops.less_equal(x, 0.), dtype=dtypes.int64)).eval())

  def testSizeTooLarge(self):
    # Grappler asserts on size overflow, so this error is only caught when",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
.image.encoded_image_string).eval(),"
          # Decode the first image and check consistency
          image = image_ops.decode_png(image_summ.value[0]
                                       .image.encoded_image_string).eval()
          self.assertAllEqual(image[1, 2], bad_color)
          image[1, 2] = adjusted[0, 1, 2]
          self.assertAllClose(image, adjusted[0], rtol=2e-5, atol=2e-5)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
.image.encoded_image_string).eval(),"        # Decode the first image and check consistency.
        # Since we're uint8, everything should be exact.
        image = image_ops.decode_png(image_summ.value[0]
                                     .image.encoded_image_string).eval()
        self.assertAllEqual(image, images[0])

        # Check the rest of the proto",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
a.kl_divergence(a).eval(),"        self.evaluate(kl)
      with self.assertRaisesOpError(
          ""KL calculation between .* and .* returned NaN values""):
        a.kl_divergence(a).eval()
      a = MyDistException(loc=0.0, scale=1.0, allow_nan_stats=True)
      kl_ok = kullback_leibler.kl_divergence(a, a)
      self.assertAllEqual([float(""nan"")], self.evaluate(kl_ok))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(3, dist.event_shape_tensor().eval())","    with self.cached_session():
      p = [.1, .3, .6]
      dist = multinomial.Multinomial(total_count=1., probs=p)
      self.assertEqual(3, dist.event_shape_tensor().eval())
      self.assertAllEqual([], dist.batch_shape_tensor())
      self.assertEqual(tensor_shape.TensorShape([3]), dist.event_shape)
      self.assertEqual(tensor_shape.TensorShape([]), dist.batch_shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(2, dist.event_shape_tensor().eval())","      p = 0.5 * np.ones([3, 2, 2], dtype=np.float32)
      n = [[3., 2], [4, 5], [6, 7]]
      dist = multinomial.Multinomial(total_count=n, probs=p)
      self.assertEqual(2, dist.event_shape_tensor().eval())
      self.assertAllEqual([3, 2], dist.batch_shape_tensor())
      self.assertEqual(tensor_shape.TensorShape([2]), dist.event_shape)
      self.assertEqual(tensor_shape.TensorShape([3, 2]), dist.batch_shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"lp = dist.log_prob([1., 0.]).eval()[0]","    logits = np.array([[-200, 0]], dtype=np.float32)
    with self.cached_session():
      dist = multinomial.Multinomial(total_count=1., logits=logits)
      lp = dist.log_prob([1., 0.]).eval()[0]
      self.assertAllClose(-200, lp, atol=0, rtol=1e-6)

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([2., 3, 0]).eval()","    n = [[5.]]
    with self.cached_session():
      dist = multinomial.Multinomial(total_count=n, probs=p, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([3., 0, 2]).eval()","    with self.cached_session():
      dist = multinomial.Multinomial(total_count=n, probs=p, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()
      with self.assertRaisesOpError(""counts must sum to `self.total_count`""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([-1., 4, 2]).eval()","      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()
      with self.assertRaisesOpError(""counts must sum to `self.total_count`""):
        dist.prob([3., 3, 0]).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([3., 3, 0]).eval()","      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()
      with self.assertRaisesOpError(""counts must sum to `self.total_count`""):
        dist.prob([3., 3, 0]).eval()

  @test_util.run_v1_only(""b/120545219"")
  def testPmfNonIntegerCounts(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multinom.prob([2., 1, 2]).eval()","      # No errors with integer n.
      multinom = multinomial.Multinomial(
          total_count=n, probs=p, validate_args=True)
      multinom.prob([2., 1, 2]).eval()
      multinom.prob([3., 0, 2]).eval()
      # Counts don't sum to n.
      with self.assertRaisesOpError(""counts must sum to `self.total_count`""):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multinom.prob([3., 0, 2]).eval()","      multinom = multinomial.Multinomial(
          total_count=n, probs=p, validate_args=True)
      multinom.prob([2., 1, 2]).eval()
      multinom.prob([3., 0, 2]).eval()
      # Counts don't sum to n.
      with self.assertRaisesOpError(""counts must sum to `self.total_count`""):
        multinom.prob([2., 3, 2]).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multinom.prob([2., 3, 2]).eval()","      multinom.prob([3., 0, 2]).eval()
      # Counts don't sum to n.
      with self.assertRaisesOpError(""counts must sum to `self.total_count`""):
        multinom.prob([2., 3, 2]).eval()
      # Counts are non-integers.
      x = array_ops.placeholder(dtypes.float32)
      with self.assertRaisesOpError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multinom.prob(x).eval(feed_dict={x: [1.0, 2.5, 1.5]})","      x = array_ops.placeholder(dtypes.float32)
      with self.assertRaisesOpError(
          ""cannot contain fractional components.""):
        multinom.prob(x).eval(feed_dict={x: [1.0, 2.5, 1.5]})

      multinom = multinomial.Multinomial(
          total_count=n, probs=p, validate_args=False)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multinom.prob([1., 2., 2.]).eval()","
      multinom = multinomial.Multinomial(
          total_count=n, probs=p, validate_args=False)
      multinom.prob([1., 2., 2.]).eval()
      # Non-integer arguments work.
      multinom.prob([1.0, 2.5, 1.5]).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multinom.prob([1.0, 2.5, 1.5]).eval()","          total_count=n, probs=p, validate_args=False)
      multinom.prob([1., 2., 2.]).eval()
      # Non-integer arguments work.
      multinom.prob([1.0, 2.5, 1.5]).eval()

  def testPmfBothZeroBatches(self):
    with self.cached_session():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"grad_eval = gradients_impl.gradients(fn(grid), grid)[0].eval()","      # Do the same checks but explicitly compute the gradient.
      # (We did this because we're not sure if we trust
      # tf.test.compute_gradient.)
      grad_eval = gradients_impl.gradients(fn(grid), grid)[0].eval()
      self.assert_all_false(np.isnan(grad_eval))
      if self._use_log:
        g = np.reshape(grad_eval, [-1])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
actual = sm.log_cdf_laplace(grid).eval(),"  def _test_grid_log(self, dtype, scipy_dtype, grid_spec, error_spec):
    with self.cached_session():
      grid = _make_grid(dtype, grid_spec)
      actual = sm.log_cdf_laplace(grid).eval()

      # Basic tests.
      # isfinite checks for NaN and Inf.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(x).eval(feed_dict=feed_dict),"                 z: [1.0001, 5, 10, 15, 20], w: [1e-8, 5, 10, 15, 20]}
    with self.cached_session():
      with ops.control_dependencies([du.assert_integer_form(x)]):
        array_ops.identity(x).eval(feed_dict=feed_dict)

      with self.assertRaisesOpError(""has non-integer components""):
        with ops.control_dependencies(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(y).eval(feed_dict=feed_dict),"      with self.assertRaisesOpError(""has non-integer components""):
        with ops.control_dependencies(
            [du.assert_integer_form(y)]):
          array_ops.identity(y).eval(feed_dict=feed_dict)

      with self.assertRaisesOpError(""has non-integer components""):
        with ops.control_dependencies(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(z).eval(feed_dict=feed_dict),"      with self.assertRaisesOpError(""has non-integer components""):
        with ops.control_dependencies(
            [du.assert_integer_form(z)]):
          array_ops.identity(z).eval(feed_dict=feed_dict)

      with self.assertRaisesOpError(""has non-integer components""):
        with ops.control_dependencies(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
array_ops.identity(w).eval(feed_dict=feed_dict),"      with self.assertRaisesOpError(""has non-integer components""):
        with ops.control_dependencies(
            [du.assert_integer_form(w)]):
          array_ops.identity(w).eval(feed_dict=feed_dict)


class MaybeGetStaticTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
prob.eval(feed_dict={p: np.ones([int(2**11+1)])}),"        p = array_ops.placeholder(dtype=dtypes.float16)
        _, prob = du.get_logits_and_probs(
            probs=p, multidimensional=True, validate_args=True)
        prob.eval(feed_dict={p: np.ones([int(2**11+1)])})

  @test_util.run_deprecated_v1
  def testLogitsMultidimShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
logit.eval(feed_dict={l: np.ones([int(2**11+1)])}),"        l = array_ops.placeholder(dtype=dtypes.float16)
        logit, _ = du.get_logits_and_probs(
            logits=l, multidimensional=True, validate_args=True)
        logit.eval(feed_dict={l: np.ones([int(2**11+1)])})


class EmbedCheckCategoricalEventShapeTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
checked_param.eval(feed_dict={param: np.ones([1])}),"        param = array_ops.placeholder(dtype=dtypes.float16)
        checked_param = du.embed_check_categorical_event_shape(
            param)
        checked_param.eval(feed_dict={param: np.ones([1])})

  @test_util.run_deprecated_v1
  def testTooLarge(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
checked_param.eval(feed_dict={param: np.ones([int(2**11+1)])}),"        param = array_ops.placeholder(dtype=dtypes.float16)
        checked_param = du.embed_check_categorical_event_shape(
            param)
        checked_param.eval(feed_dict={param: np.ones([int(2**11+1)])})

  @test_util.disable_tfrt(""b/169901260"")
  @test_util.run_in_graph_and_eager_modes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"x_checked.eval(feed_dict={x: np.array([1, -1], dtype=np.float16)})","        x = array_ops.placeholder(dtype=dtypes.float16)
        x_checked = du.embed_check_integer_casting_closed(
            x, target_dtype=dtypes.int16)
        x_checked.eval(feed_dict={x: np.array([1, -1], dtype=np.float16)})

  @test_util.run_deprecated_v1
  def testCorrectlyAssersIntegerForm(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"x_checked.eval(feed_dict={x: np.array([1, 1.5], dtype=np.float16)})","        x = array_ops.placeholder(dtype=dtypes.float16)
        x_checked = du.embed_check_integer_casting_closed(
            x, target_dtype=dtypes.int16)
        x_checked.eval(feed_dict={x: np.array([1, 1.5], dtype=np.float16)})

  @test_util.run_deprecated_v1
  def testCorrectlyAssertsLargestPossibleInteger(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"x_checked.eval(feed_dict={x: np.array([1, 2**15], dtype=np.int32)})","        x = array_ops.placeholder(dtype=dtypes.int32)
        x_checked = du.embed_check_integer_casting_closed(
            x, target_dtype=dtypes.int16)
        x_checked.eval(feed_dict={x: np.array([1, 2**15], dtype=np.int32)})

  @test_util.run_deprecated_v1
  def testCorrectlyAssertsSmallestPossibleInteger(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"x_checked.eval(feed_dict={x: np.array([1, -1], dtype=np.int32)})","        x = array_ops.placeholder(dtype=dtypes.int32)
        x_checked = du.embed_check_integer_casting_closed(
            x, target_dtype=dtypes.uint16, assert_nonnegative=False)
        x_checked.eval(feed_dict={x: np.array([1, -1], dtype=np.int32)})


@test_util.run_all_in_graph_and_eager_modes",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(scalar, scalar1).eval({","
      # Scalar
      self.assertTrue(
          du.same_dynamic_shape(scalar, scalar1).eval({
              scalar1: 2.0
          }))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(vector, vector1).eval({","      # Vector

      self.assertTrue(
          du.same_dynamic_shape(vector, vector1).eval({
              vector1: [2.0, 3.0, 4.0]
          }))
      self.assertTrue(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(vector1, vector2).eval({","              vector1: [2.0, 3.0, 4.0]
          }))
      self.assertTrue(
          du.same_dynamic_shape(vector1, vector2).eval({
              vector1: [2.0, 3.0, 4.0],
              vector2: [2.0, 3.5, 6.0]
          }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multidimensional, multidimensional1).eval({","      # Multidimensional
      self.assertTrue(
          du.same_dynamic_shape(
              multidimensional, multidimensional1).eval({
                  multidimensional1: [[2.0, 3.0], [3.0, 4.0]]
              }))
      self.assertTrue(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multidimensional1, multidimensional2).eval({","              }))
      self.assertTrue(
          du.same_dynamic_shape(
              multidimensional1, multidimensional2).eval({
                  multidimensional1: [[2.0, 3.0], [3.0, 4.0]],
                  multidimensional2: [[1.0, 3.5], [6.3, 2.3]]
              }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(scalar, vector1).eval({","
      # Scalar, X
      self.assertFalse(
          du.same_dynamic_shape(scalar, vector1).eval({
              vector1: [2.0, 3.0, 4.0]
          }))
      self.assertFalse(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(scalar1, vector1).eval({","              vector1: [2.0, 3.0, 4.0]
          }))
      self.assertFalse(
          du.same_dynamic_shape(scalar1, vector1).eval({
              scalar1: 2.0,
              vector1: [2.0, 3.0, 4.0]
          }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(scalar, multidimensional1).eval({","              vector1: [2.0, 3.0, 4.0]
          }))
      self.assertFalse(
          du.same_dynamic_shape(scalar, multidimensional1).eval({
              multidimensional1: [[2.0, 3.0], [3.0, 4.0]]
          }))
      self.assertFalse(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(scalar1, multidimensional1).eval(","              multidimensional1: [[2.0, 3.0], [3.0, 4.0]]
          }))
      self.assertFalse(
          du.same_dynamic_shape(scalar1, multidimensional1).eval(
              {
                  scalar1: 2.0,
                  multidimensional1: [[2.0, 3.0], [3.0, 4.0]]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(vector, vector1).eval({","
      # Vector, X
      self.assertFalse(
          du.same_dynamic_shape(vector, vector1).eval({
              vector1: [2.0, 3.0]
          }))
      self.assertFalse(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(vector1, vector2).eval({","              vector1: [2.0, 3.0]
          }))
      self.assertFalse(
          du.same_dynamic_shape(vector1, vector2).eval({
              vector1: [2.0, 3.0, 4.0],
              vector2: [6.0]
          }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(vector, multidimensional1).eval({","              vector2: [6.0]
          }))
      self.assertFalse(
          du.same_dynamic_shape(vector, multidimensional1).eval({
              multidimensional1: [[2.0, 3.0], [3.0, 4.0]]
          }))
      self.assertFalse(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"du.same_dynamic_shape(vector1, multidimensional1).eval(","              multidimensional1: [[2.0, 3.0], [3.0, 4.0]]
          }))
      self.assertFalse(
          du.same_dynamic_shape(vector1, multidimensional1).eval(
              {
                  vector1: [2.0, 3.0, 4.0],
                  multidimensional1: [[2.0, 3.0], [3.0, 4.0]]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multidimensional, multidimensional1).eval({","      # Multidimensional, X
      self.assertFalse(
          du.same_dynamic_shape(
              multidimensional, multidimensional1).eval({
                  multidimensional1: [[1.0, 3.5, 5.0], [6.3, 2.3, 7.1]]
              }))
      self.assertFalse(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"multidimensional1, multidimensional2).eval({","              }))
      self.assertFalse(
          du.same_dynamic_shape(
              multidimensional1, multidimensional2).eval({
                  multidimensional1: [[2.0, 3.0], [3.0, 4.0]],
                  multidimensional2: [[1.0, 3.5, 5.0], [6.3, 2.3, 7.1]]
              }))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(2, rank.eval(feed_dict={x: np.zeros((2, 3))}))","    x = array_ops.placeholder(np.float64, shape=None)
    rank = du.prefer_static_rank(x)
    with self.cached_session():
      self.assertAllEqual(2, rank.eval(feed_dict={x: np.zeros((2, 3))}))

  @test_util.run_deprecated_v1
  def testDynamicRankEndsUpBeingEmpty(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(1, rank.eval(feed_dict={x: []}))","    x = array_ops.placeholder(np.int32, shape=None)
    rank = du.prefer_static_rank(x)
    with self.cached_session():
      self.assertAllEqual(1, rank.eval(feed_dict={x: []}))

  @test_util.run_deprecated_v1
  def testDynamicRankEndsUpBeingScalar(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(0, rank.eval(feed_dict={x: 1}))","    x = array_ops.placeholder(np.int32, shape=None)
    rank = du.prefer_static_rank(x)
    with self.cached_session():
      self.assertAllEqual(0, rank.eval(feed_dict={x: 1}))


class PreferStaticShapeTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual((2, 3), shape.eval(feed_dict={x: np.zeros((2, 3))}))","    x = array_ops.placeholder(np.float64, shape=None)
    shape = du.prefer_static_shape(x)
    with self.cached_session():
      self.assertAllEqual((2, 3), shape.eval(feed_dict={x: np.zeros((2, 3))}))

  @test_util.run_deprecated_v1
  def testDynamicShapeEndsUpBeingEmpty(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(np.array([0]), shape.eval(feed_dict={x: []}))","    x = array_ops.placeholder(np.int32, shape=None)
    shape = du.prefer_static_shape(x)
    with self.cached_session():
      self.assertAllEqual(np.array([0]), shape.eval(feed_dict={x: []}))

  @test_util.run_deprecated_v1
  def testDynamicShapeEndsUpBeingScalar(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(np.array([]), shape.eval(feed_dict={x: 1}))","    x = array_ops.placeholder(np.int32, shape=None)
    shape = du.prefer_static_shape(x)
    with self.cached_session():
      self.assertAllEqual(np.array([]), shape.eval(feed_dict={x: 1}))


class PreferStaticValueTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"value.eval(feed_dict={x: np.zeros((2, 3))}))","    value = du.prefer_static_value(x)
    with self.cached_session():
      self.assertAllEqual(np.zeros((2, 3)),
                          value.eval(feed_dict={x: np.zeros((2, 3))}))

  @test_util.run_deprecated_v1
  def testDynamicValueEndsUpBeingEmpty(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(np.array([]), value.eval(feed_dict={x: []}))","    x = array_ops.placeholder(np.int32, shape=None)
    value = du.prefer_static_value(x)
    with self.cached_session():
      self.assertAllEqual(np.array([]), value.eval(feed_dict={x: []}))

  @test_util.run_deprecated_v1
  def testDynamicValueEndsUpBeingScalar(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(np.array(1), value.eval(feed_dict={x: 1}))","    x = array_ops.placeholder(np.int32, shape=None)
    value = du.prefer_static_value(x)
    with self.cached_session():
      self.assertAllEqual(np.array(1), value.eval(feed_dict={x: 1}))


class FillTriangularTest(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dist.prob(event1).eval({,"      event1 = [1, 0, 1]
      event2 = [[1, 0, 1]]
      self.assertAllClose(
          dist.prob(event1).eval({
              p: [0.2, 0.3, 0.4]
          }), [0.2, 0.7, 0.4])
      self.assertAllClose(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dist.prob(event2).eval({,"              p: [0.2, 0.3, 0.4]
          }), [0.2, 0.7, 0.4])
      self.assertAllClose(
          dist.prob(event2).eval({
              p: [0.2, 0.3, 0.4]
          }), [[0.2, 0.7, 0.4]])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(np.log(0.5), dist.log_prob(1).eval({p: 0.5}))","    with self.cached_session():
      p = array_ops.placeholder(dtypes.float32)
      dist = bernoulli.Bernoulli(probs=p)
      self.assertAllClose(np.log(0.5), dist.log_prob(1).eval({p: 0.5}))
      self.assertAllClose(
          np.log([0.5, 0.5, 0.5]), dist.log_prob([1, 1, 1]).eval({
              p: 0.5",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"np.log([0.5, 0.5, 0.5]), dist.log_prob([1, 1, 1]).eval({","      dist = bernoulli.Bernoulli(probs=p)
      self.assertAllClose(np.log(0.5), dist.log_prob(1).eval({p: 0.5}))
      self.assertAllClose(
          np.log([0.5, 0.5, 0.5]), dist.log_prob([1, 1, 1]).eval({
              p: 0.5
          }))
      self.assertAllClose(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"np.log([0.5, 0.5, 0.5]), dist.log_prob(1).eval({","              p: 0.5
          }))
      self.assertAllClose(
          np.log([0.5, 0.5, 0.5]), dist.log_prob(1).eval({
              p: [0.5, 0.5, 0.5]
          }))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(2, len(dist.log_prob(1).eval({p: [[0.5], [0.5]]}).shape))","    with self.cached_session():
      p = array_ops.placeholder(dtypes.float32, shape=[None, 1])
      dist = bernoulli.Bernoulli(probs=p)
      self.assertEqual(2, len(dist.log_prob(1).eval({p: [[0.5], [0.5]]}).shape))

      dist = bernoulli.Bernoulli(probs=0.5)
      self.assertEqual(2, len(self.evaluate(dist.log_prob([[1], [1]])).shape))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(10, dist.event_size.eval())","        self.assertAllEqual(batch_shape, dist.batch_shape_tensor())
        self.assertAllEqual([], dist.event_shape)
        self.assertAllEqual([], dist.event_shape_tensor())
        self.assertEqual(10, dist.event_size.eval())
        # event_size is available as a constant because the shape is
        # known at graph build time.
        self.assertEqual(10, tensor_util.constant_value(dist.event_size))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(10, dist.event_size.eval())","        self.assertAllEqual(batch_shape, dist.batch_shape_tensor())
        self.assertAllEqual([], dist.event_shape)
        self.assertAllEqual([], dist.event_shape_tensor())
        self.assertEqual(10, dist.event_size.eval())

  def testDtype(self):
    dist = make_categorical([], 5, dtype=dtypes.int32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sample_value = sample.eval(feed_dict={logits: [-1000.0, 1000.0]})","      dist = categorical.Categorical(logits)
      sample = dist.sample()
      # Will sample class 1.
      sample_value = sample.eval(feed_dict={logits: [-1000.0, 1000.0]})
      self.assertEqual(1, sample_value)

      # Batch entry 0 will sample class 1, batch entry 1 will sample class 0.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
sample_value_batch = sample.eval(,"      self.assertEqual(1, sample_value)

      # Batch entry 0 will sample class 1, batch entry 1 will sample class 0.
      sample_value_batch = sample.eval(
          feed_dict={logits: [[-1000.0, 1000.0], [1000.0, -1000.0]]})
      self.assertAllEqual([1, 0], sample_value_batch)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAlmostEqual(cdf_op.eval(), expected_cdf)","    cdf_op = dist.cdf(event)

    with self.cached_session():
      self.assertAlmostEqual(cdf_op.eval(), expected_cdf)

  @test_util.run_deprecated_v1
  def testCDFBroadcasting(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"bij.forward_log_det_jacobian(1., event_ndims=event_ndims).eval({","    event_ndims = array_ops.placeholder(dtype=np.int32, shape=None)
    with self.cached_session():
      with self.assertRaisesOpError(""Expected scalar""):
        bij.forward_log_det_jacobian(1., event_ndims=event_ndims).eval({
            event_ndims: (1, 2)})
      with self.assertRaisesOpError(""Expected scalar""):
        bij.inverse_log_det_jacobian(1., event_ndims=event_ndims).eval({",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"bij.inverse_log_det_jacobian(1., event_ndims=event_ndims).eval({","        bij.forward_log_det_jacobian(1., event_ndims=event_ndims).eval({
            event_ndims: (1, 2)})
      with self.assertRaisesOpError(""Expected scalar""):
        bij.inverse_log_det_jacobian(1., event_ndims=event_ndims).eval({
            event_ndims: (1, 2)})

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(3, dist.event_shape_tensor().eval())","    with self.cached_session():
      alpha = np.random.rand(3)
      dist = ds.DirichletMultinomial(1., alpha)
      self.assertEqual(3, dist.event_shape_tensor().eval())
      self.assertAllEqual([], dist.batch_shape_tensor())
      self.assertEqual(tensor_shape.TensorShape([3]), dist.event_shape)
      self.assertEqual(tensor_shape.TensorShape([]), dist.batch_shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertEqual(2, dist.event_shape_tensor().eval())","      alpha = np.random.rand(3, 2, 2)
      n = [[3., 2], [4, 5], [6, 7]]
      dist = ds.DirichletMultinomial(n, alpha)
      self.assertEqual(2, dist.event_shape_tensor().eval())
      self.assertAllEqual([3, 2], dist.batch_shape_tensor())
      self.assertEqual(tensor_shape.TensorShape([2]), dist.event_shape)
      self.assertEqual(tensor_shape.TensorShape([3, 2]), dist.batch_shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([2., 3, 0]).eval()","    n = [[5.]]
    with self.cached_session():
      dist = ds.DirichletMultinomial(n, alpha, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([3., 0, 2]).eval()","    with self.cached_session():
      dist = ds.DirichletMultinomial(n, alpha, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()
      with self.assertRaisesOpError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([-1., 4, 2]).eval()","      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      with self.assertRaisesOpError(""must be non-negative""):
        dist.prob([-1., 4, 2]).eval()
      with self.assertRaisesOpError(
          ""last-dimension must sum to `self.total_count`""):
        dist.prob([3., 3, 0]).eval()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([3., 3, 0]).eval()","        dist.prob([-1., 4, 2]).eval()
      with self.assertRaisesOpError(
          ""last-dimension must sum to `self.total_count`""):
        dist.prob([3., 3, 0]).eval()

  @test_util.run_deprecated_v1
  def testPmfNonIntegerCounts(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([2., 3, 0]).eval()","    n = [[5.]]
    with self.cached_session():
      dist = ds.DirichletMultinomial(n, alpha, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      dist.prob([3.0, 0, 2.0]).eval()
      # Both equality and integer checking fail.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([3., 0, 2]).eval()","    with self.cached_session():
      dist = ds.DirichletMultinomial(n, alpha, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      dist.prob([3.0, 0, 2.0]).eval()
      # Both equality and integer checking fail.
      placeholder = array_ops.placeholder(dtypes.float32)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([3.0, 0, 2.0]).eval()","      dist = ds.DirichletMultinomial(n, alpha, validate_args=True)
      dist.prob([2., 3, 0]).eval()
      dist.prob([3., 0, 2]).eval()
      dist.prob([3.0, 0, 2.0]).eval()
      # Both equality and integer checking fail.
      placeholder = array_ops.placeholder(dtypes.float32)
      with self.assertRaisesOpError(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob(placeholder).eval(feed_dict={placeholder: [1.0, 2.5, 1.5]})","      placeholder = array_ops.placeholder(dtypes.float32)
      with self.assertRaisesOpError(
          ""cannot contain fractional components""):
        dist.prob(placeholder).eval(feed_dict={placeholder: [1.0, 2.5, 1.5]})
      dist = ds.DirichletMultinomial(n, alpha, validate_args=False)
      dist.prob([1., 2., 3.]).eval()
      # Non-integer arguments work.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([1., 2., 3.]).eval()","          ""cannot contain fractional components""):
        dist.prob(placeholder).eval(feed_dict={placeholder: [1.0, 2.5, 1.5]})
      dist = ds.DirichletMultinomial(n, alpha, validate_args=False)
      dist.prob([1., 2., 3.]).eval()
      # Non-integer arguments work.
      dist.prob([1.0, 2.5, 1.5]).eval()
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"dist.prob([1.0, 2.5, 1.5]).eval()","      dist = ds.DirichletMultinomial(n, alpha, validate_args=False)
      dist.prob([1., 2., 3.]).eval()
      # Non-integer arguments work.
      dist.prob([1.0, 2.5, 1.5]).eval()

  def testPmfBothZeroBatches(self):
    # The probabilities of one vote falling into class k is the mean for class",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
mean = dist.mean().eval(),"        counts = np.zeros([3], dtype=np.float32)
        counts[class_num] = 1
        dist = ds.DirichletMultinomial(1., alpha)
        mean = dist.mean().eval()
        pmf = dist.prob(counts).eval()

        self.assertAllClose(mean[class_num], pmf)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
pmf = dist.prob(counts).eval(),"        counts[class_num] = 1
        dist = ds.DirichletMultinomial(1., alpha)
        mean = dist.mean().eval()
        pmf = dist.prob(counts).eval()

        self.assertAllClose(mean[class_num], pmf)
        self.assertAllEqual([3], mean.shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
mean1 = dist1.mean().eval(),"        dist1 = ds.DirichletMultinomial(1., alpha)
        dist2 = ds.DirichletMultinomial(2., alpha)

        mean1 = dist1.mean().eval()
        mean2 = dist2.mean().eval()

        self.assertAllClose(mean2[class_num], 2 * mean1[class_num])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
mean2 = dist2.mean().eval(),"        dist2 = ds.DirichletMultinomial(2., alpha)

        mean1 = dist1.mean().eval()
        mean2 = dist2.mean().eval()

        self.assertAllClose(mean2[class_num], 2 * mean1[class_num])
        self.assertAllEqual([3], mean1.shape)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
dist.prob(counts).eval()  # Should not raise.,"      counts = [[1., 0], [0., -1]]  # counts should be non-negative.
      n = [-5.3]  # n should be a non negative integer equal to counts.sum.
      dist = ds.DirichletMultinomial(n, alpha, validate_args=False)
      dist.prob(counts).eval()  # Should not raise.

  @test_util.run_deprecated_v1
  def testSampleUnbiasedNonScalarBatch(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(name),"def try_import(name):  # pylint: disable=invalid-name
  module = None
  try:
    module = importlib.import_module(name)
  except ImportError as e:
    tf_logging.warning(""Could not import %s: %s"" % (name, str(e)))
  return module",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"_VALID_REGISTERED_NAME = re.compile(r""^[a-zA-Z0-9._-]+$"")","

# Only allow valid file/directory characters
_VALID_REGISTERED_NAME = re.compile(r""^[a-zA-Z0-9._-]+$"")


class _PredicateRegistry(object):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
importlib.import_module(package),"  # Populate `sys.modules` with modules containing tf_export().
  packages = args.packages.split(',')
  for package in packages:
    importlib.import_module(package)
  packages_to_ignore = args.packages_to_ignore.split(',')

  # Determine if the modules shall be loaded lazily or statically.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
importlib.import_module(FLAGS.package),"      help='API name: tensorflow')
  FLAGS, unparsed = parser.parse_known_args()

  importlib.import_module(FLAGS.package)

  # Now update argv, so that unittest library does not get confused.
  sys.argv = [sys.argv[0]] + unparsed",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
importlib.import_module(package),"
def _traverse_packages(packages):
  for package in packages:
    importlib.import_module(package)


def _get_module_from_symbol(symbol):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_DOCSTRING_PATTERN: re.Pattern[str] = re.compile(,"    'Prefix for all exported symbols and docstrings.',
)

_DOCSTRING_PATTERN: re.Pattern[str] = re.compile(
    r'\s*API\s+docstring:\s*([\w.]+)\s*'
)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile('adam', 'mean_squared_error')","  >>> model = tf.keras.models.Sequential()
  >>> model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
  ...    input_shape=(4, 4, 1)))
  >>> model.compile('adam', 'mean_squared_error')
  >>> model.predict(input_image, steps=1)
  array([[[[2.],
           [4.]],",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile('rmsprop', 'mse')","  >>> # Now model.output_shape is (None, 10, 64), where `None` is the batch
  >>> # dimension.
  >>> input_array = np.random.randint(1000, size=(32, 10))
  >>> model.compile('rmsprop', 'mse')
  >>> output_array = model.predict(input_array)
  >>> print(output_array.shape)
  (32, 10, 64)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def eval(self, session=None):","  def aggregation(self):
    return self._variable.aggregation

  def eval(self, session=None):
    return self._variable.eval(session)

  def initialized_value(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
return self._variable.eval(session),"    return self._variable.aggregation

  def eval(self, session=None):
    return self._variable.eval(session)

  def initialized_value(self):
    return self._variable.initialized_value()",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(tf.keras.optimizers.SGD(), loss=""mse"")","
  ```python
  model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
  model.compile(tf.keras.optimizers.SGD(), loss=""mse"")

  def dataset_fn(input_context):
    global_batch_size = 64",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(tf.keras.optimizers.SGD(), loss=""mse"")","      cluster_resolver)
  with strategy.scope():
    model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
  model.compile(tf.keras.optimizers.SGD(), loss=""mse"")
  ...
  ```
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
module = importlib.import_module(self.__name__),"  def _load(self):
    """"""Load the module and insert it into the parent's globals.""""""
    # Import the target module and insert it into the parent's namespace
    module = importlib.import_module(self.__name__)
    self._parent_module_globals[self._local_name] = module
    # Update this object's dict so that if someone keeps a reference to the
    #   LazyLoader, lookups are efficient (__getattr__ is only called on lookups",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
epoch = backend.eval(self._ckpt_saved_epoch),"      at. Otherwise, return the `initial_epoch` the user passes in.
    """"""

    epoch = backend.eval(self._ckpt_saved_epoch)
    if mode == mode_keys.ModeKeys.TRAIN and epoch >= 0:
      # The most recently saved epoch is one epoch prior to the epoch it
      # failed at, so return the value of 'self._ckpt_saved_epoch' plus one.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
updated_model.compile(,"  if mode == ModeKeys.PREDICT and inputs is not None:  # TPU predict case
    _custom_compile_for_predict(updated_model)
  else:
    updated_model.compile(
        model.optimizer,
        model.loss,
        metrics=metrics_module.clone_metrics(model._compile_metrics),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
cloned_model.compile(,"  if mode == ModeKeys.PREDICT and inputs is not None:  # TPU predict case
    _custom_compile_for_predict(cloned_model)
  else:
    cloned_model.compile(
        optimizer,
        model.loss,
        metrics=metrics_module.clone_metrics(model._compile_metrics),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),","      decay_rate=0.96,
      staircase=True)

  model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(optimizer=tf.keras.optimizers.SGD(,"      end_learning_rate,
      power=0.5)

  model.compile(optimizer=tf.keras.optimizers.SGD(
                    learning_rate=learning_rate_fn),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(optimizer=tf.keras.optimizers.SGD(,"  learning_rate_fn = keras.optimizers.schedules.InverseTimeDecay(
    initial_learning_rate, decay_steps, decay_rate)

  model.compile(optimizer=tf.keras.optimizers.SGD(
                    learning_rate=learning_rate_fn),
                loss='sparse_categorical_crossentropy',
                metrics=['accuracy'])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> m.compile(opt, loss='mse')","
    >>> opt = tf.keras.optimizers.RMSprop()
    >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
    >>> m.compile(opt, loss='mse')
    >>> data = np.arange(100).reshape(5, 20)
    >>> labels = np.zeros(5)
    >>> results = m.fit(data, labels)  # Training.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> m.compile(opt, loss='mse')","
    >>> opt = tf.keras.optimizers.RMSprop()
    >>> m = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
    >>> m.compile(opt, loss='mse')
    >>> data = np.arange(100).reshape(5, 20)
    >>> labels = np.zeros(5)
    >>> results = m.fit(data, labels)  # Training.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(**saving_utils.compile_args_from_training_config(,"      training_config = json_utils.decode(training_config)

      # Compile model.
      model.compile(**saving_utils.compile_args_from_training_config(
          training_config, custom_objects), from_serialized=True)
      saving_utils.try_build_compiled_arguments(model)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"'sample_weight_mode=""temporal"" in compile(); founssd ""{}"" '","      raise ValueError(
          'Found a sample_weight array with shape {}. In order to '
          'use timestep-wise sample weights, you should specify '
          'sample_weight_mode=""temporal"" in compile(); founssd ""{}"" '
          'instead. If you just mean to use sample-wise weights, '
          'make sure your sample_weight array is 1D.'.format(
              sample_weight.shape, sample_weight_mode))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(tf.optimizers.RMSprop(0.001), loss='mse')","  model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(4,)),
    tf.keras.layers.Dense(8)])
  model.compile(tf.optimizers.RMSprop(0.001), loss='mse')
  model.fit(np.zeros((10, 4)),
            np.ones((10, 8)))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(tf.optimizers.RMSprop(0.001), loss='mse')","  # Keras will add a input for the model behind the scene.
  model = tf.keras.Sequential([
    tf.keras.layers.Dense(8, input_shape=(4,))])
  model.compile(tf.optimizers.RMSprop(0.001), loss='mse')
  model.fit(np.zeros((10, 4)),
            np.ones((10, 8)))
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"model.compile(optimizer='sgd', loss='mse')","  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(8))
  model.add(tf.keras.layers.Dense(1))
  model.compile(optimizer='sgd', loss='mse')
  # This builds the model for the first time:
  model.fit(x, y, batch_size=32, epochs=10)
  ```",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def compile(self, run_eagerly=None, steps_per_execution=None):","    self._adapt_function = adapt_fn
    return self._adapt_function

  def compile(self, run_eagerly=None, steps_per_execution=None):
    """"""Configures the layer for `adapt`.

    Arguments:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
>>> model.compile() # This is needed to re-compile model.predict!,"    >>> model.predict([0, 1, 2])
    array([-1.,  0.,  1.], dtype=float32)
    >>> layer.adapt([-1, 1])
    >>> model.compile() # This is needed to re-compile model.predict!
    >>> model.predict([0, 1, 2])
    array([0., 1., 2.], dtype=float32)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
self.compile()  # Compile with defaults.,"      raise ValueError('{} does not supporting calling `adapt` twice without '
                       'resetting the state.'.format(self.__class__.__name__))
    if not self._is_compiled:
      self.compile()  # Compile with defaults.
    if self.built and reset_state:
      self.reset_state()
    data_handler = data_adapter.DataHandler(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def compile(self, run_eagerly=None, steps_per_execution=None):","    if self._adapt_accumulator is not None:
      self._set_accumulator(self._adapt_accumulator)

  def compile(self, run_eagerly=None, steps_per_execution=None):
    # TODO(omalleyt): Remove this once sublayers are switched to new APIs.
    if run_eagerly is None:
      run_eagerly = True",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"super(CombinerPreprocessingLayer, self).compile(","    # TODO(omalleyt): Remove this once sublayers are switched to new APIs.
    if run_eagerly is None:
      run_eagerly = True
    super(CombinerPreprocessingLayer, self).compile(
        run_eagerly=run_eagerly, steps_per_execution=steps_per_execution)

  def adapt(self, data, batch_size=None, steps=None, reset_state=True):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def compile(self,","    return super(Model, self).load_weights(filepath, by_name, skip_mismatch)

  @trackable.no_automatic_dependency_tracking
  def compile(self,
              optimizer='rmsprop',
              loss=None,
              metrics=None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"'  model.compile(...)'% (v, strategy))","                'to the following.\n'
                'with strategy.scope():\n'
                '  model=_create_model()\n'
                '  model.compile(...)'% (v, strategy))

  @trackable.no_automatic_dependency_tracking
  def _init_distributed_function_cache_if_not_compiled(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"`sample_weight_mode=""temporal""` in `compile()`. This argument is not","            `(samples, sequence_length)`,
            to apply a different weight to every timestep of every sample.
            In this case you should make sure to specify
            `sample_weight_mode=""temporal""` in `compile()`. This argument is not
            supported when `x` is a dataset, generator, or
           `keras.utils.Sequence` instance, instead provide the sample_weights
            as the third element of `x`.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"`sample_weight_mode=""temporal""` in `compile()`. This argument is not","            `(samples, sequence_length)`,
            to apply a different weight to every timestep of every sample.
            In this case you should make sure to specify
            `sample_weight_mode=""temporal""` in `compile()`. This argument is not
            supported when `x` is a dataset, instead pass
            sample weights as the third element of `x`.
        steps: Integer or `None`.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sample_weight_mode=""temporal"" in compile(). This argument is not","          temporal data, you can pass a 2D array with shape (samples,
          sequence_length), to apply a different weight to every timestep of
          every sample. In this case you should make sure to specify
          sample_weight_mode=""temporal"" in compile(). This argument is not
          supported when `x` is a dataset.
        class_weight: Optional dictionary mapping class indices (integers) to a
          weight (float) to apply to the model's loss for the samples from this",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"sample_weight_mode=""temporal"" in compile(). This argument is not","            with shape (samples, sequence_length),
            to apply a different weight to every timestep of every sample.
            In this case you should make sure to specify
            sample_weight_mode=""temporal"" in compile(). This argument is not
            supported when `x` is a dataset.
        reset_metrics: If `True`, the metrics returned will be only for this
          batch. If `False`, the metrics will be statefully accumulated across",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
self.compile(,"      else:
        target_tensors = None

    self.compile(
        optimizer=self.optimizer,
        loss=self.loss,
        metrics=self._compile_metrics,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"'Use `model.compile(optimizer, loss)`.')","    if not self._compile_was_called:
      raise RuntimeError('You must compile your model before '
                         'training/testing. '
                         'Use `model.compile(optimizer, loss)`.')

  def _in_multi_worker_mode(self):
    """"""Method to infer if this `Model` is working in multi-worker settings.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
the information is provided via `compile()`,"
    Note that the output and output_name should be stable as long as the model
    structure doesn't change. The training_target suppose to be mutable since
    the information is provided via `compile()`

    Args:
      output: the output tensor of the model.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"with `model.compile()`, train the model with `model.fit()`, or use the model","  ```

  Once the model is created, you can config the model with losses and metrics
  with `model.compile()`, train the model with `model.fit()`, or use the model
  to do prediction with `model.predict()`.
  """"""
  _TF_MODULE_IGNORED_PROPERTIES = frozenset(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"def compile(self,","    raise NotImplementedError('When subclassing the `Model` class, you should '
                              'implement a `call` method.')

  def compile(self,
              optimizer='rmsprop',
              loss=None,
              metrics=None,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
Note: Metrics passed to `compile()` are available only after a `keras.Model`,"  def metrics(self):
    """"""Returns the model's metrics added using `compile`, `add_metric` APIs.

    Note: Metrics passed to `compile()` are available only after a `keras.Model`
    has been trained/evaluated on actual data.

    Examples:",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])","    >>> inputs = tf.keras.layers.Input(shape=(3,))
    >>> outputs = tf.keras.layers.Dense(2)(inputs)
    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    >>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])
    >>> [m.name for m in model.metrics]
    []
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae"", ""acc""])","    ...    inputs=inputs, outputs=[output_1, output_2])
    >>> model.add_metric(
    ...    tf.reduce_sum(output_2), name='mean', aggregation='mean')
    >>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae"", ""acc""])
    >>> model.fit(x, (y, y))
    >>> [m.name for m in model.metrics]
    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])","    >>> inputs = tf.keras.layers.Input(shape=(3,))
    >>> outputs = tf.keras.layers.Dense(2)(inputs)
    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    >>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])
    >>> model.metrics_names
    []
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae"", ""acc""])","    >>> output_2 = d(inputs)
    >>> model = tf.keras.models.Model(
    ...    inputs=inputs, outputs=[output_1, output_2])
    >>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae"", ""acc""])
    >>> model.fit(x, (y, y))
    >>> model.metrics_names
    ['loss', 'out_loss', 'out_1_loss', 'out_mae', 'out_acc', 'out_1_mae',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
">>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])","    >>> inputs = tf.keras.layers.Input(shape=(3,))
    >>> outputs = tf.keras.layers.Dense(2)(inputs)
    >>> model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    >>> model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])

    >>> x = np.random.random((2, 3))
    >>> y = np.random.randint(0, 2, (2, 2))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"'  model.compile(...)' % (v, strategy))","              'to the following.\n'
              'with strategy.scope():\n'
              '  model=_create_model()\n'
              '  model.compile(...)' % (v, strategy))

    # Model metrics must be created in the same distribution strategy scope
    # as the model.",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"'Use `model.compile(optimizer, loss)`.')","    if not self._is_compiled:
      raise RuntimeError('You must compile your model before '
                         'training/testing. '
                         'Use `model.compile(optimizer, loss)`.')

  def _set_inputs(self, inputs, outputs=None, training=None):
    """"""This method is for compat with Modelv1. Only inputs are needed here.""""""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
model.compile(**saving_utils.compile_args_from_training_config(,"    training_config = model._serialized_attributes['metadata'].get(
        'training_config', None)
    if training_config is not None:
      model.compile(**saving_utils.compile_args_from_training_config(
          training_config), from_serialized=True)
      saving_utils.try_build_compiled_arguments(model)
      if isinstance(model.optimizer, optimizer_v2.OptimizerV2):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
pattern = re.compile('layer-(\\d+)'),"    # ""layer-N"". Use this to generate the list of layers.
    num_layers = 0
    child_layers = {}
    pattern = re.compile('layer-(\\d+)')

    for child in self._proto.nodes[node_id].children:
      m = pattern.match(child.local_name)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(textwrap.dedent(dynamic_code), temp_mod.__dict__)  # pylint:disable=exec-used","      def foo(x):
        return x + 1
    """"""
    exec(textwrap.dedent(dynamic_code), temp_mod.__dict__)  # pylint:disable=exec-used
    opts = converter.ConversionOptions(optional_features=None)

    x = api.converted_call(temp_mod.foo, (1,), None, options=opts)",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(textwrap.dedent(dynamic_code), temp_mod.__dict__)  # pylint:disable=exec-used","      def foo(x):
        return x + 1
    """"""
    exec(textwrap.dedent(dynamic_code), temp_mod.__dict__)  # pylint:disable=exec-used
    opts = converter.ConversionOptions(optional_features=None)

    x = api.converted_call(temp_mod.foo, (1,), None, options=opts)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile('must be defined.*tried to define.*unsupported type',","
    with self.assertRaisesRegex(
        ValueError,
        re.compile('must be defined.*tried to define.*unsupported type',
                   re.DOTALL)):
      control_flow.while_stmt(
          test=lambda: constant_op.constant(True),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile('must be defined.*tried to define.*testing', re.DOTALL)):","
    with self.assertRaisesRegex(
        ValueError,
        re.compile('must be defined.*tried to define.*testing', re.DOTALL)):
      control_flow.while_stmt(
          test=lambda: math_ops.equal(s, 0),
          body=body,",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
r = re.compile('.*'.join([,"    origin_info.resolve(node, source, 'test_file', 100, 0)
    tg.visit(node)

    r = re.compile('.*'.join([
        r'x = 1',
        r'if \(?x > 0\)? {',
        r'x = 2',",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile(('""/path/one.py"", line 11, in test_fn_1.*'","        converter_filename=None)
    self.assertRegex(
        em.get_message(),
        re.compile(('""/path/one.py"", line 11, in test_fn_1.*'
                    '""/path/two.py"", line 171, in test_fn_2.*'
                    'Test message'), re.DOTALL))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
re.compile((r'converted_fn  \*.*',"    result = em.get_message()
    self.assertRegex(
        result,
        re.compile((r'converted_fn  \*.*'
                    r'""/path/three.py"", line 171, in test_fn_3.*'
                    r'Test message'), re.DOTALL))
    self.assertNotRegex(result, re.compile('test_fn_1'))",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertNotRegex(result, re.compile('test_fn_1'))","        re.compile((r'converted_fn  \*.*'
                    r'""/path/three.py"", line 171, in test_fn_3.*'
                    r'Test message'), re.DOTALL))
    self.assertNotRegex(result, re.compile('test_fn_1'))

  def test_get_message_call_overload(self):
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"re.compile((r'""/path/one.py"", line 11, in test_fn_1.*'","        converter_filename='/path/two.py')
    self.assertRegex(
        em.get_message(),
        re.compile((r'""/path/one.py"", line 11, in test_fn_1.*'
                    r'""/path/three.py"", line 171, in test_fn_3  \*\*.*'
                    r'Test message'), re.DOTALL))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(code, globs)  # pylint:disable=exec-used","
  def _eval_code(self, code, name):
    globs = {}
    exec(code, globs)  # pylint:disable=exec-used
    return globs[name]

  def test_dedent_block_basic(self):",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(code, globs)  # pylint:disable=exec-used","
  def _eval_code(self, code, name):
    globs = {}
    exec(code, globs)  # pylint:disable=exec-used
    return globs[name]

  def test_dedent_block_basic(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
expected_exception_text = re.compile(r'found multiple definitions',"
    l = lambda x: lambda x: 2 * x

    expected_exception_text = re.compile(r'found multiple definitions'
                                         r'.+'
                                         r'\(?lambda x: \(?lambda x'
                                         r'.+'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
_LEADING_WHITESPACE = re.compile(r'\s*'),"STANDARD_PREAMBLE_LEN = STANDARD_PREAMBLE.count('__future__')


_LEADING_WHITESPACE = re.compile(r'\s*')


def _unfold_continuations(code_string):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec('computed' + 5 + 'stuff', globals(), locals())","def exec_test_function():
  # The point is to test A-normal form conversion of exec
  # pylint: disable=exec-used
  exec('computed' + 5 + 'stuff', globals(), locals())


def exec_expected_result():",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(tmp_1002, tmp_1003, tmp_1004)","  tmp_1002 = tmp_1001 + 'stuff'
  tmp_1003 = globals()
  tmp_1004 = locals()
  exec(tmp_1002, tmp_1003, tmp_1004)


class AnfTestBase(test.TestCase):",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec('computed' + 5 + 'stuff', globals(), locals())","def exec_test_function():
  # The point is to test A-normal form conversion of exec
  # pylint: disable=exec-used
  exec('computed' + 5 + 'stuff', globals(), locals())


def exec_expected_result():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(tmp_1002, tmp_1003, tmp_1004)","  tmp_1002 = tmp_1001 + 'stuff'
  tmp_1003 = globals()
  tmp_1004 = locals()
  exec(tmp_1002, tmp_1003, tmp_1004)


class AnfTestBase(test.TestCase):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(),"      b = constant_op.constant(7.0, shape=[1, 1])
      c = math_ops.matmul(a, b, name='matmul')
    with session.Session(graph=g, config=self._config):
      result = c.eval()
      self.assertAllEqual(result, [[42.0]])

  def testUseDefaultGraph(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(),"      b = constant_op.constant(7.0, shape=[1, 1])
      c = math_ops.matmul(a, b, name='matmul')
      with session.Session(config=self._config):
        result = c.eval()
        self.assertAllEqual(result, [[42.0]])

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(code, globals())  # pylint: disable=exec-used","  def test_func_wo_source_code(self):
    code = ""def f_exec():\n  return 1""
    # Use `exec` to generate a function without source code
    exec(code, globals())  # pylint: disable=exec-used
    txt = free_vars_detect.generate_free_var_logging(f_exec)  # pylint: disable=undefined-variable
    self.assertIsNone(txt)
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(code, globals())  # pylint: disable=exec-used","  def test_func_wo_source_code(self):
    code = ""def f_exec():\n  return 1""
    # Use `exec` to generate a function without source code
    exec(code, globals())  # pylint: disable=exec-used
    txt = free_vars_detect.generate_free_var_logging(f_exec)  # pylint: disable=undefined-variable
    self.assertIsNone(txt)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
cloned = pickle.loads(pickle.dumps(original)),"        function_type.Parameter(""z"", function_type.Parameter.KEYWORD_ONLY,
                                False, None)
    ])
    cloned = pickle.loads(pickle.dumps(original))
    self.assertEqual(original, cloned)

",pickle.loads,Unsafe Deserialization,CRITICAL,CWE-502,Python,ML/AI,1,ML/AI:TensorFlow Core
"foo = eval(""lambda x, y, /, z: x + y + z"")  # pylint: disable=eval-used","      self.skipTest(""Positional only args are supported in Python 3.8+"")

    # Raises syntax error in 3.7 but is important coverage for 3.8+.
    foo = eval(""lambda x, y, /, z: x + y + z"")  # pylint: disable=eval-used

    polymorphic_type = function_type.FunctionType.from_callable(foo)
    mono_type, _ = function_type.canonicalize_to_monomorphic(",eval,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"foo = eval(""lambda x, y, /, z: x + y + z"")  # pylint: disable=eval-used","      self.skipTest(""Positional only args are supported in Python 3.8+"")

    # Raises syntax error in 3.7 but is important coverage for 3.8+.
    foo = eval(""lambda x, y, /, z: x + y + z"")  # pylint: disable=eval-used

    polymorphic_type = function_type.FunctionType.from_callable(foo)
    mono_type, _ = function_type.canonicalize_to_monomorphic(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"text = ""tf.xla.experimental.compile(0)""","    _, _, _, new_text = self._upgrade(text)
    self.assertEqual(new_text, expected_text)

    text = ""tf.xla.experimental.compile(0)""
    expected_text = ""tf.xla.experimental.compile(0)""
    _, _, _, new_text = self._upgrade(text)
    self.assertEqual(new_text, expected_text)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"expected_text = ""tf.xla.experimental.compile(0)""","    self.assertEqual(new_text, expected_text)

    text = ""tf.xla.experimental.compile(0)""
    expected_text = ""tf.xla.experimental.compile(0)""
    _, _, _, new_text = self._upgrade(text)
    self.assertEqual(new_text, expected_text)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"FIND_OPEN = re.compile(r""^\s*(\[).*$"")","

# Some regular expressions we will need for parsing
FIND_OPEN = re.compile(r""^\s*(\[).*$"")
FIND_STRING_CHARS = re.compile(r""['\""]"")

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"FIND_STRING_CHARS = re.compile(r""['\""]"")","
# Some regular expressions we will need for parsing
FIND_OPEN = re.compile(r""^\s*(\[).*$"")
FIND_STRING_CHARS = re.compile(r""['\""]"")


INFO = ""INFO""",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"output = subprocess.check_output(args, shell=True, stderr=subprocess.STDOUT)","    output as string.
  """"""
  try:
    output = subprocess.check_output(args, shell=True, stderr=subprocess.STDOUT)
  except subprocess.CalledProcessError as e:
    output = e.output
  return output.strip()",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))","import subprocess


os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))


def check_output_despite_error(args):",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
"os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))","import subprocess


os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), '../../..')))


def check_output_despite_error(args):",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
__import__(short_package_name),"      # skip non-importable paths
      if short_package_name not in self.packages_for_skip:
        try:
          __import__(short_package_name)
        except ImportError:
          logging.exception(""error importing %s"", short_package_name)
          failed_packages.append(package_name)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), ""../../.."")))","import os
import subprocess

os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), ""../../.."")))

PIP_PACKAGE_QUERY_EXPRESSION = (
    ""deps(//tensorflow/tools/pip_package:build_pip_package)"")",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
"os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), ""../../.."")))","import os
import subprocess

os.chdir(os.path.abspath(os.path.join(os.path.dirname(__file__), ""../../.."")))

PIP_PACKAGE_QUERY_EXPRESSION = (
    ""deps(//tensorflow/tools/pip_package:build_pip_package)"")",path_traversal,ML-path_traversal,MEDIUM,CWE-22,Python,ML/AI,1,ML/AI:TensorFlow Core
"runfiles_matcher = re.compile(r""(/.*\.runfiles/)"")","
# For test cases, only show the ones that failed that have text (a log)
seen = collections.Counter()
runfiles_matcher = re.compile(r""(/.*\.runfiles/)"")


for f in files.strip().splitlines():",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
proc = subprocess.Popen(,"  Returns:
    Tuple output (stdoutdata, stderrdata) from running the shell commands.
  """"""
  proc = subprocess.Popen(
      args,
      shell=True,
      stdout=subprocess.PIPE,",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"shell=True,","  """"""
  proc = subprocess.Popen(
      args,
      shell=True,
      stdout=subprocess.PIPE,
      stderr=subprocess.STDOUT
  )",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(f.read(), f_globals, f_locals)  # pylint: disable=exec-used","  with open(filepath, 'r') as f:
    f_globals = {'repository_rule': repository_rule}
    f_locals = {}
    exec(f.read(), f_globals, f_locals)  # pylint: disable=exec-used

  return set(f_locals['VALID_LIBS'])
",exec,Code Execution,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"exec(f.read(), f_globals, f_locals)  # pylint: disable=exec-used","  with open(filepath, 'r') as f:
    f_globals = {'repository_rule': repository_rule}
    f_locals = {}
    exec(f.read(), f_globals, f_locals)  # pylint: disable=exec-used

  return set(f_locals['VALID_LIBS'])
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"gcc_path = subprocess.check_output(gcc_path_cmd, shell=True,","    gcc_path = """"
    gcc_path_cmd = ""command -v gcc""
    try:
      gcc_path = subprocess.check_output(gcc_path_cmd, shell=True,
                                         stderr=subprocess.STDOUT).\
        strip()
      print(""gcc located here: {}"".format(gcc_path))",command_injection,ML-command_injection,HIGH,CWE-78,Python,ML/AI,1,ML/AI:TensorFlow Core
diff = (rnd2 - rnd1).eval(),"      with self.session(use_gpu=True):
        rnd1 = random_ops.random_uniform(shape, 0, 17, dtype=dtype)
        rnd2 = random_ops.random_uniform(shape, 0, 17, dtype=dtype)
        diff = (rnd2 - rnd1).eval()
        self.assertGreater(np.linalg.norm(diff), 0.1)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","        x = np.array([1, 2, 3], dtype=dtype)
        v_tf = array_ops.broadcast_to(constant_op.constant(x), [3, 3])
        v_np = np.broadcast_to(x, [3, 3])
        self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToString(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","      x = np.array([b""1"", b""2"", b""3""])
      v_tf = array_ops.broadcast_to(constant_op.constant(x), [3, 3])
      v_np = np.broadcast_to(x, [3, 3])
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToBool(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","      x = np.array([True, False, True], dtype=np.bool)
      v_tf = array_ops.broadcast_to(constant_op.constant(x), [3, 3])
      v_np = np.broadcast_to(x, [3, 3])
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToShape(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","          x = np.array(np.random.randint(5, size=input_shape), dtype=np.int32)
          v_tf = array_ops.broadcast_to(constant_op.constant(x), output_shape)
          v_np = np.broadcast_to(x, output_shape)
          self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToShapeInnerDim(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","      x = np.array(np.random.randint(5, size=input_shape), dtype=np.int32)
      v_tf = array_ops.broadcast_to(constant_op.constant(x), output_shape)
      v_np = np.broadcast_to(x, output_shape)
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToShapeLargerDim(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","      x = np.array(np.random.randint(5, size=input_shape), dtype=np.int32)
      v_tf = array_ops.broadcast_to(constant_op.constant(x), output_shape)
      v_np = np.broadcast_to(x, output_shape)
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToShapeLargerDim2(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","      x = np.array(np.random.randint(5, size=input_shape), dtype=np.int32)
      v_tf = array_ops.broadcast_to(constant_op.constant(x), output_shape)
      v_np = np.broadcast_to(x, output_shape)
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToScalar(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","      x = np.array(1, dtype=np.int32)
      v_tf = array_ops.broadcast_to(constant_op.constant(x), [3, 3])
      v_np = np.broadcast_to(x, [3, 3])
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastScalarToNonScalar(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","          constant_op.constant(1.0), [2, 3, 4, 1, 1, 1]
      )
      v_np = np.broadcast_to(x, [2, 3, 4, 1, 1, 1])
      self.assertAllEqual(v_tf.eval(), v_np)

  @test_util.run_deprecated_v1
  def testBroadcastToShapeTypeAndInference(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(v_tf.eval(), v_np)","        )
        shape = v_tf.get_shape().as_list()
        v_np = np.broadcast_to(x, [3, 3])
        self.assertAllEqual(v_tf.eval(), v_np)
        # check shape inference when shape input is constant
        self.assertAllEqual(shape, v_np.shape)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
result = c.eval(feed_dict=params),"      c = array_ops.concat(concat_inputs, concat_dim)
      if dtype != dtype_feed:
        c = math_ops.cast(c, dtype_feed)
      result = c.eval(feed_dict=params)

    self.assertEqual(result.shape, c.get_shape())
    cur_offset = 0",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"z = fn(c, xt, yt).eval()","    with self.cached_session():
      xt = x.astype(np.float32)
      yt = y.astype(np.float32)
      z = fn(c, xt, yt).eval()
      self.assertAllEqual(z_expected, z)

  @test_util.run_deprecated_v1",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"x = fn(c, a, b).eval()","      for c in False, True:
        for a in 7.0, np.nan:
          for b in 5.0, np.nan:
            x = fn(c, a, b).eval()
            y = a if c else b
            self.assertEqual(np.isnan(x), np.isnan(y))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"pattern = re.compile(""shapes must be equal"", re.IGNORECASE)","  def testAssignIncompatibleShape(self):
    v = resource_variable_ops.ResourceVariable([0, 1, 2, 3])
    self.evaluate(v.initializer)
    pattern = re.compile(""shapes must be equal"", re.IGNORECASE)
    with self.assertRaisesRegex(Exception, pattern):
      self.evaluate(v.assign_add(1))
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"b, reduction_indices=dim0).eval(), [True, True])","
      self.assertAllEqual(
          tf.reduce_any(
              b, reduction_indices=dim0).eval(), [True, True])
      self.assertAllEqual(
          tf.reduce_all(
              b, reduction_indices=[0]).eval(), [False, False, False])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"b, reduction_indices=[0]).eval(), [False, False, False])","              b, reduction_indices=dim0).eval(), [True, True])
      self.assertAllEqual(
          tf.reduce_all(
              b, reduction_indices=[0]).eval(), [False, False, False])
      self.assertAllEqual(
          tf.reduce_all(
              b, reduction_indices=dim1).eval(), [False, False])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"b, reduction_indices=dim1).eval(), [False, False])","              b, reduction_indices=[0]).eval(), [False, False, False])
      self.assertAllEqual(
          tf.reduce_all(
              b, reduction_indices=dim1).eval(), [False, False])
      self.assertAllEqual(
          tf.reduce_sum(
              a, reduction_indices=[1]).eval(), [6., 15.])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[1]).eval(), [6., 15.])","              b, reduction_indices=dim1).eval(), [False, False])
      self.assertAllEqual(
          tf.reduce_sum(
              a, reduction_indices=[1]).eval(), [6., 15.])
      self.assertAllEqual(
          tf.reduce_sum(
              a, reduction_indices=[0, 1]).eval(), 21.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[0, 1]).eval(), 21.0)","              a, reduction_indices=[1]).eval(), [6., 15.])
      self.assertAllEqual(
          tf.reduce_sum(
              a, reduction_indices=[0, 1]).eval(), 21.0)
      self.assertAllEqual(tf.reduce_sum(a, [0, 1]).eval(), 21.0)
      self.assertAllEqual(
          tf.reduce_prod(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.reduce_sum(a, [0, 1]).eval(), 21.0)","      self.assertAllEqual(
          tf.reduce_sum(
              a, reduction_indices=[0, 1]).eval(), 21.0)
      self.assertAllEqual(tf.reduce_sum(a, [0, 1]).eval(), 21.0)
      self.assertAllEqual(
          tf.reduce_prod(
              a, reduction_indices=[1]).eval(), [6., 120.])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[1]).eval(), [6., 120.])","      self.assertAllEqual(tf.reduce_sum(a, [0, 1]).eval(), 21.0)
      self.assertAllEqual(
          tf.reduce_prod(
              a, reduction_indices=[1]).eval(), [6., 120.])
      self.assertAllEqual(
          tf.reduce_prod(
              a, reduction_indices=[0, 1]).eval(), 720.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[0, 1]).eval(), 720.0)","              a, reduction_indices=[1]).eval(), [6., 120.])
      self.assertAllEqual(
          tf.reduce_prod(
              a, reduction_indices=[0, 1]).eval(), 720.0)
      self.assertAllEqual(tf.reduce_prod(a, [0, 1]).eval(), 720.0)
      self.assertAllEqual(
          tf.reduce_mean(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.reduce_prod(a, [0, 1]).eval(), 720.0)","      self.assertAllEqual(
          tf.reduce_prod(
              a, reduction_indices=[0, 1]).eval(), 720.0)
      self.assertAllEqual(tf.reduce_prod(a, [0, 1]).eval(), 720.0)
      self.assertAllEqual(
          tf.reduce_mean(
              a, reduction_indices=[1]).eval(), [2., 5.])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[1]).eval(), [2., 5.])","      self.assertAllEqual(tf.reduce_prod(a, [0, 1]).eval(), 720.0)
      self.assertAllEqual(
          tf.reduce_mean(
              a, reduction_indices=[1]).eval(), [2., 5.])
      self.assertAllEqual(
          tf.reduce_mean(
              a, reduction_indices=[0, 1]).eval(), 3.5)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[0, 1]).eval(), 3.5)","              a, reduction_indices=[1]).eval(), [2., 5.])
      self.assertAllEqual(
          tf.reduce_mean(
              a, reduction_indices=[0, 1]).eval(), 3.5)
      self.assertAllEqual(tf.reduce_mean(a, [0, 1]).eval(), 3.5)
      self.assertAllEqual(
          tf.reduce_min(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.reduce_mean(a, [0, 1]).eval(), 3.5)","      self.assertAllEqual(
          tf.reduce_mean(
              a, reduction_indices=[0, 1]).eval(), 3.5)
      self.assertAllEqual(tf.reduce_mean(a, [0, 1]).eval(), 3.5)
      self.assertAllEqual(
          tf.reduce_min(
              a, reduction_indices=[1]).eval(), [1., 4.])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[1]).eval(), [1., 4.])","      self.assertAllEqual(tf.reduce_mean(a, [0, 1]).eval(), 3.5)
      self.assertAllEqual(
          tf.reduce_min(
              a, reduction_indices=[1]).eval(), [1., 4.])
      self.assertAllEqual(
          tf.reduce_min(
              a, reduction_indices=[0, 1]).eval(), 1.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[0, 1]).eval(), 1.0)","              a, reduction_indices=[1]).eval(), [1., 4.])
      self.assertAllEqual(
          tf.reduce_min(
              a, reduction_indices=[0, 1]).eval(), 1.0)
      self.assertAllEqual(tf.reduce_min(a, [0, 1]).eval(), 1.0)
      self.assertAllEqual(
          tf.reduce_max(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.reduce_min(a, [0, 1]).eval(), 1.0)","      self.assertAllEqual(
          tf.reduce_min(
              a, reduction_indices=[0, 1]).eval(), 1.0)
      self.assertAllEqual(tf.reduce_min(a, [0, 1]).eval(), 1.0)
      self.assertAllEqual(
          tf.reduce_max(
              a, reduction_indices=[1]).eval(), [3., 6.])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[1]).eval(), [3., 6.])","      self.assertAllEqual(tf.reduce_min(a, [0, 1]).eval(), 1.0)
      self.assertAllEqual(
          tf.reduce_max(
              a, reduction_indices=[1]).eval(), [3., 6.])
      self.assertAllEqual(
          tf.reduce_max(
              a, reduction_indices=[0, 1]).eval(), 6.0)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"a, reduction_indices=[0, 1]).eval(), 6.0)","              a, reduction_indices=[1]).eval(), [3., 6.])
      self.assertAllEqual(
          tf.reduce_max(
              a, reduction_indices=[0, 1]).eval(), 6.0)
      self.assertAllEqual(tf.reduce_max(a, [0, 1]).eval(), 6.0)
      self.assertAllClose(tf.reduce_logsumexp(a, reduction_indices=[1]).eval(),
                          [3.40760589, 6.40760612])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.reduce_max(a, [0, 1]).eval(), 6.0)","      self.assertAllEqual(
          tf.reduce_max(
              a, reduction_indices=[0, 1]).eval(), 6.0)
      self.assertAllEqual(tf.reduce_max(a, [0, 1]).eval(), 6.0)
      self.assertAllClose(tf.reduce_logsumexp(a, reduction_indices=[1]).eval(),
                          [3.40760589, 6.40760612])
      self.assertAllClose(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllClose(tf.reduce_logsumexp(a, reduction_indices=[1]).eval(),","          tf.reduce_max(
              a, reduction_indices=[0, 1]).eval(), 6.0)
      self.assertAllEqual(tf.reduce_max(a, [0, 1]).eval(), 6.0)
      self.assertAllClose(tf.reduce_logsumexp(a, reduction_indices=[1]).eval(),
                          [3.40760589, 6.40760612])
      self.assertAllClose(
          tf.reduce_logsumexp(a, reduction_indices=[0, 1]).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.reduce_logsumexp(a, reduction_indices=[0, 1]).eval(),","      self.assertAllClose(tf.reduce_logsumexp(a, reduction_indices=[1]).eval(),
                          [3.40760589, 6.40760612])
      self.assertAllClose(
          tf.reduce_logsumexp(a, reduction_indices=[0, 1]).eval(),
          6.45619344711)
      self.assertAllClose(
          tf.reduce_logsumexp(a, [0, 1]).eval(), 6.45619344711)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.reduce_logsumexp(a, [0, 1]).eval(), 6.45619344711)","          tf.reduce_logsumexp(a, reduction_indices=[0, 1]).eval(),
          6.45619344711)
      self.assertAllClose(
          tf.reduce_logsumexp(a, [0, 1]).eval(), 6.45619344711)
      self.assertAllEqual(
          tf.expand_dims([[1, 2], [3, 4]], axis=1).eval(),
          [[[1, 2]], [[3, 4]]])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.expand_dims([[1, 2], [3, 4]], axis=1).eval(),","      self.assertAllClose(
          tf.reduce_logsumexp(a, [0, 1]).eval(), 6.45619344711)
      self.assertAllEqual(
          tf.expand_dims([[1, 2], [3, 4]], axis=1).eval(),
          [[[1, 2]], [[3, 4]]])

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),","  def testArgMinMax(self):
    with self.cached_session():
      self.assertAllEqual(
          tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),
          [0, 2])
      self.assertAllEqual(
          tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),","          tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),
          [0, 2])
      self.assertAllEqual(
          tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),
          [0, 1, 1])
      self.assertAllEqual(
          tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),","          tf.argmin([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),
          [0, 1, 1])
      self.assertAllEqual(
          tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),
          [2, 0])
      self.assertAllEqual(
          tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),","          tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=1).eval(),
          [2, 0])
      self.assertAllEqual(
          tf.argmax([[1, 2, 3], [4, 1, 0]], dimension=0).eval(),
          [1, 0, 0])

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.expand_dims(tf.squeeze(a, [0]), 0).eval(),","      # TODO(aselle): sparse_split, sparse_reduce_sum,
      #  sparse_reduce_sum_sparse, reduce_join
      a = [[1, 2, 3]]
      self.assertAllEqual(tf.expand_dims(tf.squeeze(a, [0]), 0).eval(),
                          a)
      self.assertAllEqual(tf.squeeze(tf.expand_dims(a, 1), [1]).eval(),
                          a)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"self.assertAllEqual(tf.squeeze(tf.expand_dims(a, 1), [1]).eval(),","      a = [[1, 2, 3]]
      self.assertAllEqual(tf.expand_dims(tf.squeeze(a, [0]), 0).eval(),
                          a)
      self.assertAllEqual(tf.squeeze(tf.expand_dims(a, 1), [1]).eval(),
                          a)
      self.assertAllEqual(
          tf.expand_dims(tf.squeeze([[1, 2, 3]], axis=[0]), dim=0).eval(), a)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.expand_dims(tf.squeeze([[1, 2, 3]], axis=[0]), dim=0).eval(), a)","      self.assertAllEqual(tf.squeeze(tf.expand_dims(a, 1), [1]).eval(),
                          a)
      self.assertAllEqual(
          tf.expand_dims(tf.squeeze([[1, 2, 3]], axis=[0]), dim=0).eval(), a)
      self.assertAllEqual(
          tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)","      self.assertAllEqual(
          tf.expand_dims(tf.squeeze([[1, 2, 3]], axis=[0]), dim=0).eval(), a)
      self.assertAllEqual(
          tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)

      self.assertAllEqual(
          tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)","          tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)

      self.assertAllEqual(
          tf.squeeze(tf.expand_dims([[1, 2, 3]], dim=1), axis=[1]).eval(), a)

  @test_util.run_v1_only(""b/120545219"")
  def testArithmeticRenames(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.neg(tf.mul(tf.add(1, 2), tf.sub(5, 3))).eval(),","      self.assertAllEqual(vals,
                          [[[1, 2], [4, 5]], [[3, 4], [6, 7]]])
      self.assertAllEqual(
          tf.neg(tf.mul(tf.add(1, 2), tf.sub(5, 3))).eval(),
          -6)
      self.assertAllEqual(
          s.run(tf.listdiff([1, 2, 3], [3, 3, 4]))[0], [1, 2])",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.list_diff([1, 2, 3], [3, 3, 4])[0].eval(), [1, 2])","      self.assertAllEqual(
          s.run(tf.listdiff([1, 2, 3], [3, 3, 4]))[0], [1, 2])
      self.assertAllEqual(
          tf.list_diff([1, 2, 3], [3, 3, 4])[0].eval(), [1, 2])
      a = [[1., 2., 3.], [4., 5., 6.]]
      foo = np.where(np.less(a, 2), np.negative(a), a)
      self.assertAllEqual(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.select(tf.less(a, 2), tf.neg(a), a).eval(),","      a = [[1., 2., 3.], [4., 5., 6.]]
      foo = np.where(np.less(a, 2), np.negative(a), a)
      self.assertAllEqual(
          tf.select(tf.less(a, 2), tf.neg(a), a).eval(),
          foo)
      self.assertAllEqual(
          tf.complex_abs(tf.constant(3 + 4.j)).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.complex_abs(tf.constant(3 + 4.j)).eval(),","          tf.select(tf.less(a, 2), tf.neg(a), a).eval(),
          foo)
      self.assertAllEqual(
          tf.complex_abs(tf.constant(3 + 4.j)).eval(),
          5)
      #     # TODO(aselle): (tf.batch_*)
      # ]",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result = tf.matmul(mat, mat).eval()","    with self.cached_session():
      mat = [[1., 2.], [2., 3.]]
      batched_mat = tf.expand_dims(mat, [0])
      result = tf.matmul(mat, mat).eval()
      result_batched = tf.batch_matmul(batched_mat, batched_mat).eval()
      self.assertAllEqual(result_batched, np.expand_dims(result, 0))
      self.assertAllEqual(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"result_batched = tf.batch_matmul(batched_mat, batched_mat).eval()","      mat = [[1., 2.], [2., 3.]]
      batched_mat = tf.expand_dims(mat, [0])
      result = tf.matmul(mat, mat).eval()
      result_batched = tf.batch_matmul(batched_mat, batched_mat).eval()
      self.assertAllEqual(result_batched, np.expand_dims(result, 0))
      self.assertAllEqual(
          tf.svd(mat, False, True).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.svd(mat, False, True).eval(),","      result_batched = tf.batch_matmul(batched_mat, batched_mat).eval()
      self.assertAllEqual(result_batched, np.expand_dims(result, 0))
      self.assertAllEqual(
          tf.svd(mat, False, True).eval(),
          tf.svd(mat, compute_uv=False, full_matrices=True).eval())

  @test_util.run_v1_only(""b/120545219"")",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"tf.svd(mat, compute_uv=False, full_matrices=True).eval())","      self.assertAllEqual(result_batched, np.expand_dims(result, 0))
      self.assertAllEqual(
          tf.svd(mat, False, True).eval(),
          tf.svd(mat, compute_uv=False, full_matrices=True).eval())

  @test_util.run_v1_only(""b/120545219"")
  def testCrossEntropy(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"logits, labels).eval(),","      logits = [.9, .1, .3, .1]
      self.assertAllEqual(
          tf.nn.softmax_cross_entropy_with_logits(
              logits, labels).eval(),
          tf.nn.softmax_cross_entropy_with_logits(
              labels=labels, logits=logits).eval())
      self.assertAllEqual(",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels=labels, logits=logits).eval())","          tf.nn.softmax_cross_entropy_with_logits(
              logits, labels).eval(),
          tf.nn.softmax_cross_entropy_with_logits(
              labels=labels, logits=logits).eval())
      self.assertAllEqual(
          tf.nn.sigmoid_cross_entropy_with_logits(
              logits, labels).eval(),",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"logits, labels).eval(),","              labels=labels, logits=logits).eval())
      self.assertAllEqual(
          tf.nn.sigmoid_cross_entropy_with_logits(
              logits, labels).eval(),
          tf.nn.sigmoid_cross_entropy_with_logits(
              labels=labels, logits=logits).eval())
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"labels=labels, logits=logits).eval())","          tf.nn.sigmoid_cross_entropy_with_logits(
              logits, labels).eval(),
          tf.nn.sigmoid_cross_entropy_with_logits(
              labels=labels, logits=logits).eval())

  @test_util.run_v1_only(""b/120545219"")
  def testVariables(self):",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
RESULT_STORE_LINK_RE = re.compile(,"
ResultDictType = Dict[str, Dict[str, Union[str, int]]]

RESULT_STORE_LINK_RE = re.compile(
    r'^INFO: Streaming build results to: (https://[\w./\-]+)')
FAILED_BUILD_LINE = 'FAILED: Build did NOT complete successfully'
BUILD_STATUS_LINE = 'INFO: Build'",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"TESTS_FAILED_RE = re.compile(r'^INFO: Build completed, \d+ tests? FAILED')","    r'^INFO: Streaming build results to: (https://[\w./\-]+)')
FAILED_BUILD_LINE = 'FAILED: Build did NOT complete successfully'
BUILD_STATUS_LINE = 'INFO: Build'
TESTS_FAILED_RE = re.compile(r'^INFO: Build completed, \d+ tests? FAILED')
BAZEL_COMMAND_RE = re.compile(
    r'(^| )(?P<command>bazel (.*? )?(?P<type>test|build) .+)')
",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
BAZEL_COMMAND_RE = re.compile(,"FAILED_BUILD_LINE = 'FAILED: Build did NOT complete successfully'
BUILD_STATUS_LINE = 'INFO: Build'
TESTS_FAILED_RE = re.compile(r'^INFO: Build completed, \d+ tests? FAILED')
BAZEL_COMMAND_RE = re.compile(
    r'(^| )(?P<command>bazel (.*? )?(?P<type>test|build) .+)')

",code_exec,ML-code_exec,CRITICAL,CWE-95,Python,ML/AI,1,ML/AI:TensorFlow Core
"command = ""gio trash \"""" + itemPath.string() + ""\""""",system(command.c_str()),system,Command Injection,HIGH,CWE-78,C++,Desktop Application,1,kDrive:command_injection
"unsafe { std::slice::from_raw_parts_mut(buf as *mut u8, sz as usize) }",// SAFETY: Per WolfSSL callback rules,from_raw_parts_mut,Safe FFI,LOW,CWE-119,Rust,VPN/Security,0,Lightway:ffi_safe
