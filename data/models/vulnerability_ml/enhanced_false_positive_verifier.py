#!/usr/bin/env python3
"""
ENHANCED FALSE POSITIVE VERIFIER
Integrated with Ollama validation learnings and advanced pattern detection
"""

import json
import re
import numpy as np
from typing import Dict, List, Tuple, Any
from datetime import datetime

class EnhancedFalsePositiveVerifier:
    """
    Enhanced false positive verifier with Ollama validation learnings integrated.
    """

    def __init__(self, training_data_dir="../training"):
        self.training_data_dir = training_data_dir
        self.load_all_training_data()
        self.initialize_enhanced_patterns()

    def load_all_training_data(self):
        """Load all available training data including Ollama validation."""
        self.training_data = {}

        # Load original false positive training
        try:
            with open(f"{self.training_data_dir}/false_positive_training_20251013_140908.json", 'r') as f:
                self.training_data['original_fp'] = json.load(f)
        except:
            self.training_data['original_fp'] = {}

        # Load comprehensive case study
        try:
            with open(f"{self.training_data_dir}/comprehensive_vulnhunter_case_study_report.json", 'r') as f:
                self.training_data['case_study'] = json.load(f)
        except:
            self.training_data['case_study'] = {}

        # Load Ollama validation learnings
        try:
            with open(f"{self.training_data_dir}/ollama_validation_training_20250114_180000.json", 'r') as f:
                self.training_data['ollama_validation'] = json.load(f)
                print("✅ Ollama validation learnings loaded")
        except:
            self.training_data['ollama_validation'] = {}
            print("⚠️ Ollama validation learnings not found")

    def initialize_enhanced_patterns(self):
        """Initialize enhanced detection patterns from all training data."""

        # Original fabrication patterns
        self.fabrication_patterns = [
            r'unsafe\s*\{\s*transmute\(',
            r'unsafe\s*\{\s*std::ptr::write\(',
            r'unsafe\s*\{\s*slice::from_raw_parts\(',
            r'(\d+)\s*vulnerabilities.*98\.\d+.*confidence',
            r'line\s*(\d{4,})',
        ]

        # Enhanced patterns from Ollama validation
        self.controlled_parameter_patterns = [
            r'exec\.Command\([^)]*os\.Executable\(\)',
            r'exec\.Command\([^)]*hardcoded[^)]*\)',
            r'cmd_path\s*:=\s*"[^"]*system32[^"]*"',
            r'exec\.Command\([^)]*filepath\.Dir\([^)]*\)',
        ]

        # Middleware detection patterns
        self.middleware_patterns = [
            r'allowedHostsMiddleware',
            r'cors\.New\(',
            r'gin\.Use\([^)]*middleware[^)]*\)',
            r'middleware\.[A-Z][a-zA-Z]*\(\)',
        ]

        # Validation control patterns
        self.validation_control_patterns = [
            r'errFilePath\s*=\s*errors\.New',
            r'file\s*path\s*must\s*be\s*relative',
            r'\.ShouldBindJSON\(',
            r'validation.*error',
            r'path.*validation',
        ]

        # Framework security patterns
        self.framework_security_patterns = [
            r'gin\.Context',
            r'c\.ShouldBindJSON',
            r'http\.DefaultTransport',
            r'gin\.Engine',
        ]

    def analyze_parameter_source(self, description: str, location: str = "", code_context: str = "") -> Dict[str, Any]:
        """Analyze if parameters in command execution are user-controlled or application-controlled."""

        analysis = {
            'parameter_source': 'unknown',
            'control_level': 'uncertain',
            'confidence': 0.5,
            'indicators': []
        }

        text = (description + " " + location + " " + code_context).lower()

        # Check for application-controlled indicators
        app_controlled_indicators = [
            'os.executable()',
            'hardcoded',
            'system32',
            'filepath.dir(',
            'static path',
            'application path'
        ]

        user_controlled_indicators = [
            'user input',
            'user-controlled',
            'user provided',
            'request parameter',
            'form data',
            'query parameter'
        ]

        app_score = sum(1 for indicator in app_controlled_indicators if indicator in text)
        user_score = sum(1 for indicator in user_controlled_indicators if indicator in text)

        if app_score > user_score:
            analysis['parameter_source'] = 'application_controlled'
            analysis['control_level'] = 'safe'
            analysis['confidence'] = min(0.9, 0.6 + (app_score * 0.1))
            analysis['indicators'].append(f'Application-controlled indicators: {app_score}')
        elif user_score > app_score:
            analysis['parameter_source'] = 'user_controlled'
            analysis['control_level'] = 'dangerous'
            analysis['confidence'] = min(0.9, 0.6 + (user_score * 0.1))
            analysis['indicators'].append(f'User-controlled indicators: {user_score}')

        # Check for controlled parameter patterns
        for pattern in self.controlled_parameter_patterns:
            if re.search(pattern, text):
                analysis['parameter_source'] = 'application_controlled'
                analysis['control_level'] = 'safe'
                analysis['confidence'] = max(analysis['confidence'], 0.8)
                analysis['indicators'].append(f'Controlled parameter pattern: {pattern}')

        return analysis

    def assess_middleware_protection(self, description: str, location: str = "", code_context: str = "") -> Dict[str, Any]:
        """Assess presence and effectiveness of security middleware."""

        assessment = {
            'middleware_present': False,
            'protection_level': 'none',
            'confidence': 0.0,
            'middleware_types': []
        }

        text = (description + " " + location + " " + code_context).lower()

        # Check for middleware patterns
        for pattern in self.middleware_patterns:
            if re.search(pattern, text):
                assessment['middleware_present'] = True
                assessment['middleware_types'].append(pattern)

        if assessment['middleware_present']:
            assessment['protection_level'] = 'configuration_dependent'
            assessment['confidence'] = 0.7

            # Specific middleware assessments
            if 'allowedhostsmiddleware' in text:
                assessment['protection_level'] = 'host_based'
                assessment['confidence'] = 0.8

            if 'cors' in text:
                assessment['protection_level'] = 'cors_configured'
                assessment['confidence'] = 0.6  # CORS can be misconfigured

        return assessment

    def detect_validation_controls(self, description: str, location: str = "", code_context: str = "") -> Dict[str, Any]:
        """Detect explicit validation controls and security measures."""

        detection = {
            'validation_present': False,
            'validation_types': [],
            'mitigation_level': 'none',
            'confidence': 0.0
        }

        text = (description + " " + location + " " + code_context).lower()

        # Check for validation control patterns
        for pattern in self.validation_control_patterns:
            if re.search(pattern, text):
                detection['validation_present'] = True
                detection['validation_types'].append(pattern)

        if detection['validation_present']:
            detection['mitigation_level'] = 'explicit_controls'
            detection['confidence'] = 0.8

            # Specific validation assessments
            if 'file path must be relative' in text or 'errfilepath' in text:
                detection['mitigation_level'] = 'path_traversal_protected'
                detection['confidence'] = 0.9

        return detection

    def assess_framework_security(self, description: str, location: str = "", code_context: str = "") -> Dict[str, Any]:
        """Assess framework-provided security features."""

        assessment = {
            'framework_detected': None,
            'security_features': [],
            'default_protection': 'unknown',
            'confidence': 0.0
        }

        text = (description + " " + location + " " + code_context).lower()

        # Framework detection
        if 'gin' in text or 'shouldbindjson' in text:
            assessment['framework_detected'] = 'gin'
            assessment['security_features'].append('gin_context_binding')
            assessment['default_protection'] = 'framework_defaults'
            assessment['confidence'] = 0.7

        if 'http.defaulttransport' in text:
            assessment['security_features'].append('go_http_defaults')
            assessment['default_protection'] = 'go_defaults'
            assessment['confidence'] = max(assessment['confidence'], 0.6)

        return assessment

    def comprehensive_false_positive_analysis(self, description: str, location: str = "",
                                            code_context: str = "", severity: str = "Medium") -> Dict[str, Any]:
        """Comprehensive false positive analysis with enhanced patterns."""

        analysis = {
            'false_positive_score': 0.0,
            'confidence': 0.0,
            'red_flags': [],
            'validation_insights': {},
            'severity_adjustment': 1.0,
            'bounty_adjustment': 1.0,
            'classification': 'unknown'
        }

        # Original fabrication detection
        text = (description + " " + location + " " + code_context).lower()

        for pattern in self.fabrication_patterns:
            if re.search(pattern, text):
                analysis['false_positive_score'] += 0.3
                analysis['red_flags'].append(f'Fabrication pattern: {pattern}')

        # Enhanced analysis components
        param_analysis = self.analyze_parameter_source(description, location, code_context)
        middleware_assessment = self.assess_middleware_protection(description, location, code_context)
        validation_detection = self.detect_validation_controls(description, location, code_context)
        framework_assessment = self.assess_framework_security(description, location, code_context)

        # Store detailed insights
        analysis['validation_insights'] = {
            'parameter_analysis': param_analysis,
            'middleware_assessment': middleware_assessment,
            'validation_detection': validation_detection,
            'framework_assessment': framework_assessment
        }

        # Adjust false positive score based on insights
        if param_analysis['control_level'] == 'safe':
            analysis['false_positive_score'] += 0.4
            analysis['red_flags'].append('Application-controlled parameters detected')

        if middleware_assessment['middleware_present']:
            analysis['false_positive_score'] -= 0.2  # Reduce FP score if middleware exists
            analysis['red_flags'].append('Security middleware detected')

        if validation_detection['validation_present']:
            analysis['false_positive_score'] -= 0.3  # Reduce FP score if explicit validation
            analysis['red_flags'].append('Explicit validation controls detected')

        # Severity and bounty adjustments
        if param_analysis['control_level'] == 'safe':
            analysis['severity_adjustment'] = 0.1  # Major reduction
            analysis['bounty_adjustment'] = 0.1

        if middleware_assessment['protection_level'] == 'configuration_dependent':
            analysis['severity_adjustment'] = 0.6
            analysis['bounty_adjustment'] = 0.6

        if validation_detection['mitigation_level'] != 'none':
            analysis['severity_adjustment'] = 0.3
            analysis['bounty_adjustment'] = 0.3

        if framework_assessment['default_protection'] != 'unknown':
            analysis['severity_adjustment'] *= 0.7
            analysis['bounty_adjustment'] *= 0.7

        # Final classification
        analysis['false_positive_score'] = min(analysis['false_positive_score'], 1.0)

        if analysis['false_positive_score'] > 0.7:
            analysis['classification'] = 'HIGH_FALSE_POSITIVE'
        elif analysis['false_positive_score'] > 0.4:
            analysis['classification'] = 'MEDIUM_FALSE_POSITIVE'
        elif analysis['false_positive_score'] > 0.2:
            analysis['classification'] = 'LOW_FALSE_POSITIVE'
        else:
            analysis['classification'] = 'LIKELY_VALID'

        # Overall confidence
        confidence_factors = [
            param_analysis['confidence'],
            middleware_assessment['confidence'],
            validation_detection['confidence'],
            framework_assessment['confidence']
        ]
        analysis['confidence'] = np.mean([c for c in confidence_factors if c > 0])

        return analysis

    def validate_with_training_patterns(self, description: str, location: str = "") -> Dict[str, Any]:
        """Validate against all training patterns including Ollama learnings."""

        validation = {
            'training_matches': [],
            'pattern_confidence': 0.0,
            'historical_accuracy': 0.0
        }

        # Check against Ollama validation patterns
        if 'ollama_validation' in self.training_data:
            learnings = self.training_data['ollama_validation'].get('validation_learnings', [])

            for learning in learnings:
                if learning['learning_type'] == 'false_positive_pattern':
                    pattern_claim = learning['pattern_detected']['claim'].lower()
                    if any(word in description.lower() for word in pattern_claim.split()[:3]):
                        validation['training_matches'].append({
                            'type': 'ollama_false_positive',
                            'pattern': pattern_claim,
                            'confidence': 0.9
                        })

        # Historical accuracy from training
        if validation['training_matches']:
            validation['pattern_confidence'] = np.mean([m['confidence'] for m in validation['training_matches']])
            validation['historical_accuracy'] = 0.67  # From Ollama validation

        return validation

def test_enhanced_verifier():
    """Test the enhanced false positive verifier."""
    print("🔍 Testing Enhanced False Positive Verifier...")

    verifier = EnhancedFalsePositiveVerifier("../training")

    test_cases = [
        {
            'description': 'Command execution through exec.Command with insufficient validation',
            'location': 'cmd/start_windows.go:50',
            'code_context': 'cmd_path := "c:\\\\Windows\\\\system32\\\\cmd.exe"; cmd := exec.Command(cmd_path, "/c", appExe, "--hide", "--fast-startup")',
            'expected': 'Should detect application-controlled parameters'
        },
        {
            'description': 'HTTP server without authentication on API endpoints',
            'location': 'server/routes.go:1456',
            'code_context': 'r.Use(allowedHostsMiddleware(s.addr)); r.POST("/api/pull", s.PullHandler)',
            'expected': 'Should detect middleware protection'
        },
        {
            'description': 'Path traversal vulnerability in file operations',
            'location': 'server/create.go:41',
            'code_context': 'errFilePath = errors.New("file path must be relative")',
            'expected': 'Should detect validation controls'
        },
        {
            'description': 'unsafe { transmute(raw_ptr) } found in oauth.rs:715',
            'location': 'oauth.rs:715',
            'code_context': '',
            'expected': 'Should detect fabrication pattern'
        }
    ]

    for i, test_case in enumerate(test_cases, 1):
        print(f"\\n📝 Test Case {i}: {test_case['expected']}")
        print(f"Description: {test_case['description'][:50]}...")

        result = verifier.comprehensive_false_positive_analysis(
            test_case['description'],
            test_case['location'],
            test_case['code_context']
        )

        print(f"FP Score: {result['false_positive_score']:.2f}")
        print(f"Classification: {result['classification']}")
        print(f"Severity Adjustment: {result['severity_adjustment']:.2f}")
        print(f"Red Flags: {len(result['red_flags'])}")

        if result['validation_insights']:
            insights = result['validation_insights']
            if insights['parameter_analysis']['parameter_source'] != 'unknown':
                print(f"Parameter Source: {insights['parameter_analysis']['parameter_source']}")
            if insights['middleware_assessment']['middleware_present']:
                print(f"Middleware: {insights['middleware_assessment']['protection_level']}")

if __name__ == "__main__":
    test_enhanced_verifier()