$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
type: command

# Job metadata
display_name: "VulnHunter V11 Massive Dataset Training"
description: "VulnHunter V11 training with massive multi-source dataset integration from next.txt analysis"
experiment_name: "vulnhunter-v11-massive-dataset"

# Code and environment
code: ../training
command: python vulnhunter_v11_massive_dataset_trainer.py

# Use enhanced environment for dataset processing
environment: azureml:AzureML-ACPT-pytorch-1.13-py38-cuda11.7-gpu@latest

# Enhanced compute configuration for massive datasets
compute: azureml:vulnhunter-16core

# Resource allocation for massive dataset processing
resources:
  instance_count: 1

# Environment variables for massive dataset training
environment_variables:
  OMP_NUM_THREADS: "16"
  MKL_NUM_THREADS: "16"
  VULNHUNTER_CPU_CORES: "16"
  VULNHUNTER_MEMORY_GB: "128"

  # Dataset configuration
  HUGGING_FACE_CACHE_DIR: "/tmp/hf_cache"
  TRANSFORMERS_CACHE: "/tmp/transformers_cache"
  HF_HOME: "/tmp/hf_home"

  # Training optimization
  VULNHUNTER_BATCH_SIZE: "64"
  VULNHUNTER_MAX_WORKERS: "8"
  VULNHUNTER_DATASET_STREAMING: "true"

  # Memory optimization
  TOKENIZERS_PARALLELISM: "false"
  CUDA_LAUNCH_BLOCKING: "1"

# Job tags for tracking
tags:
  version: "11.0.0-massive-dataset"
  infrastructure: "16-core-Standard_E16s_v3"
  datasets: "stack-v2,smartbugs,sanctuary,solidifi,defihacklabs,codenet"
  mathematical_foundations: "category-theory,tda,quantum-gnn,differential-homology,stochastic-verification"
  total_samples: "372500+"
  total_size_gb: "6.0+"
  cores: "16"
  memory: "128GB"
  training_type: "massive-multi-source"
  academic_ready: "true"