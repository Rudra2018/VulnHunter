$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
type: command

# Job metadata
display_name: "VulnHunter V11 Corrected Full Training"
description: "VulnHunter V11 full training with corrected code - fixed f-string issues"
experiment_name: "vulnhunter-v11-corrected"

# Code and environment
code: ../training
command: >
  pip install --no-cache-dir datasets==2.12.0 transformers==4.30.0 huggingface-hub==0.15.1 accelerate==0.20.0 &&
  python vulnhunter_v11_massive_dataset_trainer.py

# Use simple base environment
environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest

# Compute configuration
compute: azureml:vulnhunter-16core

# Resource allocation
resources:
  instance_count: 1

# Environment variables for massive dataset training
environment_variables:
  OMP_NUM_THREADS: "16"
  MKL_NUM_THREADS: "16"
  VULNHUNTER_CPU_CORES: "16"
  VULNHUNTER_MEMORY_GB: "128"

  # Dataset configuration
  HUGGING_FACE_CACHE_DIR: "/tmp/hf_cache"
  TRANSFORMERS_CACHE: "/tmp/transformers_cache"
  HF_HOME: "/tmp/hf_home"
  HF_DATASETS_CACHE: "/tmp/hf_datasets_cache"

  # Training optimization
  VULNHUNTER_BATCH_SIZE: "64"
  VULNHUNTER_MAX_WORKERS: "8"
  VULNHUNTER_DATASET_STREAMING: "true"

  # Memory optimization
  TOKENIZERS_PARALLELISM: "false"

  # Dataset sources configuration
  ENABLE_THE_STACK_V2: "true"
  ENABLE_SMARTBUGS: "true"
  ENABLE_SANCTUARY: "true"
  ENABLE_SOLIDIFI: "true"
  ENABLE_DEFIHACKLABS: "true"
  ENABLE_CODENET: "true"

# Job tags
tags:
  version: "11.0.0-corrected-full"
  infrastructure: "16-core-Standard_E16s_v3"
  datasets: "stack-v2,smartbugs,sanctuary,solidifi,defihacklabs,codenet"
  mathematical_foundations: "category-theory,tda,quantum-gnn,differential-homology,stochastic-verification"
  total_samples: "372500+"
  total_size_gb: "6.0+"
  cores: "16"
  memory: "128GB"
  training_type: "massive-multi-source-corrected"
  code_status: "f-string-issues-fixed"
  dependencies_method: "minimal-essential"